[
["index.html", "AnalizaR Datos Políticos Capítulo 1 Introducción 1.1 Inicio 1.2 ¿Cómo contribuir? 1.3 Prefacio 1.4 Qué esperar del libro 1.5 Estructura del libro 1.6 Prerrequisitos", " AnalizaR Datos Políticos Francisco Urdinez y Andrés Cruz Labrín (editores) 15 de marzo de 2019; versión 0.1 Capítulo 1 Introducción 1.1 Inicio AnalizaR Datos Políticos es un libro que podrás usar como manual de referencia cuando estés quebrandote la cabeza usando R. Por eso, decidimos que sea más aplicado que teórico, y hemos considerado tanto temas de ciencia política como de relaciones internacionales. De ahí el subtítulo del libro: “Manual aplicado para politólogos y relacionistas internacionales en R”. Una gran ventaja del libro es que utiliza para cada tarea la opción más actualizada y sencilla disponible en la red. ¡Además, ocupa cada vez que es posible el tidyverse, el grupo de paquetes que ha revolucionado el uso de R recientemente por su sencillez! Construido gracias a la tecnología de libro digital llamada bookdown e inspirado por la filosofía de software libre y código abierto, este libro es de uso libre y gratuito. Esto garantiza que sus contenidos sean reproducibles y accesibles públicamente para personas de todo el mundo. La versión del libro que estás leyendo ahora (0.1) se construyó el 15 de marzo de 2019. A medida que el libro reciba retroalimentación de los usuarios, iremos actualizando los contenidos. Este libro digital se encuentra bajo una licencia Creative Commons Atribución-NoComercial-SinDerivadas 4.0 Internacional. 1.2 ¿Cómo contribuir? bookdown hace que la edición de un libro sea tan fácil como editar un wiki, siempre que tengas una cuenta de GitHub. Puedes proponer cambios en el libro desde el foro “Issues” en el repositorio del libro en GitHub, que contiene todo el código utilizado para generarlo. Adicionalmente, si tienes más experiencia con GitHub y git, también puedes proponer pull requests. Haciendo click en el ícono de edición, como se aprecia en la siguiente figura, puedes rápidamente crear un fork del repositorio y comenzar a editar. ¡Creemos que las contribuciones –a través de issues o pull requests– nos ayudarán muchísimo a mejorar el libro! Figura 1.1: Botón de ‘Editar’ en cada capítulo Adicionalmente, si encuentras el libro útil, por favor apóyalo de alguna de estas formas: Leyéndolo. Recomendándolo o compartiéndolo en redes sociales. 1.2.1 Sobre los autores y autoras Todos los autores son politólogos pertenecientes al Instituto de Ciencia Política de la Pontificia Universidad Católica de Chile, e investigadores y colaboradores del Instituto Milenio de Fundamento de los Datos. 1.3 Prefacio Este libro nació haciendo análisis de datos políticos. Es decir, es hijo de la praxis. Por ello su naturaleza es aplicada, y tiene su foco puesto en ser una caja de herramientas para el lector. AnalizaR Datos Políticos está pensado para ser un manual de referencia que podrá ser consultado tanto por un estudiante universitario viviendo en Bogotá, como por un consultor político viviendo en México D.F. o un o funcionario público en Brasilia, todos con la necesidad de transformar sus bases de datos en conclusiones sustantivas y fácilmente interpretables. Trabajando juntos en la cátedra de Análisis Cuantitativo de Datos II del Instituto de Ciencia Política de la Universidad Católica de Chile encontramos que ni aquí, ni en otras universidades de la región, había material didáctico y aplicado hecho en casa para enseñar a nuestros alumnos de ciencia política cómo extraer conclusiones a partir de datos duros. Todo el material utilizado en nuestra cátedra era publicado en inglés, por politólogos anglosajones trabajando en universidades anglosajonas. Por ello AnalizaR Datos Políticos tiene como público imaginario al politólogo latinoamericano, ya sea alumno de pregrado o posgrado, o ya en el mercado. Hemos querido que nuestro libro esté disponible en español y portugués, y esto lo hace extensible a otras universidades de realidades similares fuera de América Latina, como en los países lusófonos de África y en la región ibérica. Los autores son todos politólogos que tuvieron que enfrentarse a problemas aplicados y tuvieron curvas de aprendizaje, más o menos empinadas, en el uso de R. Algunos han migrado de otros software de análisis de datos, otros han comenzado su experiencia directamente en R. Algunos son hábiles usuarios, otros, usuarios funcionales, pero todos tienen en común que tienen conocimiento aplicado que será de utilidad a quien quiera tener material de apoyo. Las universidades latinoamericanas han hecho grandes esfuerzos en que sus alumnos de politología se alfabeticen en herramientas estadísticas y de análisis de datos, algo que hasta hace diez años era algo poco frecuente. Hoy las cinco mejores universidades de la región, según el ranking de Times Higher Education, tienen cursos de análisis cuantitativo de datos en sus programas de ciencia política. Algunos departamentos, como el Departamento de Ciencia Política de la Universidad de São Paulo, que co-organiza la escuela de verano de IPSA en métodos, el Instituto de Ciencia Política de la Universidad Católica de Chile, que organiza su escuela de verano en Métodos Mixtos, o la División de Estudios Políticos del CIDE, han hecho esfuerzos por exponer a sus alumnos a profesores norteamericanos y europeos que cuentan con muchas décadas de tradición cuantitativa en sus programas. Lo bueno es que poco a poco comienzan a aparecer metodólogos nacidos y formados en América Latina. Entendemos que, hoy por hoy, ningún politólogo puede salir al mercado laboral sin saber utilizar con holgura software de análisis cuantitativo, y es a esa demanda a la que apuntamos aquí. Ahora mismo, R es probablemente la mejor opción que el mercado provee para análisis estadístico de datos. Esto puede ser sorpresivo para un lector recién salido de una máquina del tiempo: hace diez años, o tal vez menos, R era simplemente mirado como la alternativa gratis a los programas comerciales de verdad, que sí podían realizar análisis cuantitativo serio. Sin embargo, esto ha cambiado drásticamente en los últimos años. La Figura 1.2 muestra las tendencias de búsqueda en Google en América Latina para los programas más comúnmente utilizados en ciencia. R ha pasado a ocupar un lugar en el mercado que hace 15 años le correspondía a SPSS, y los programas de nicho -como Stata y Minitab- son cada vez menos buscados. La tendencia sugiere que R será cada vez más popular en la ciencia latinoamericana, siguiendo una tendencia global. Figura 1.2: Elaborada por los autores usando el paquete ggplot2 de R, y datos extraídos de Google Trends. Los datos corresponden a promedios anuales para países latinoamericanos en el sector ‘ciencia’ El modelo de software libre en el que se basa R —con licencias de derechos de autor permisivas, que ponen prácticamente todas las herramientas en forma gratuita a disposición del público, tanto para su uso como para su reformulación— finalmente rindió frutos. Una activa comunidad de desarrolladores se ha anclado en R, añadiéndole nuevas funcionalidades que lo han dotado de elegancia, simplicidad y flexibilidad. R ya no solo brilla en la generación de modelos estadísticos, sino que hoy es hogar de un vasto universo de herramientas que permite al usuario importar, ordenar, transformar, visualizar, modelar y comunicar los datos que le interesen, sin tener que cambiar de herramienta. Es esta la novedad tecnológica que queremos acercar al lector interesado en el análisis político, con la esperanza de que contribuya a optimizar el proceso entre la pregunta que le quita el sueño (y/o le promete el pan) y su solución. Esto sin desconocer, claro está, que el beneficio inicial de R, el que estaba incluso cuando nadie quisiera usarlo si no era a regañadientes, permanece. Sabemos que el lector —y tal vez su casa de estudios— agradecerá la amable coincidencia de que el mejor software disponible en términos de calidad es también el mejor para su bolsillo. Francisco Urdinez y Andrés Cruz. Santiago de Chile, 2019. 1.3.1 Agradecimientos Queremos agradecerle al Instituto Milenio Fundamentos de los Datos por apoyar este proyecto. 1.4 Qué esperar del libro El análisis cuantitativo de datos es una de las tantas herramientas que los investigadores tenemos para abordar las preguntas que nos interesan, ya sea en el mundo profesional o en la academia (o “por amor al arte” en muy encendidas noches de viernes, por qué no). Es por esto que AnalizaR Datos Políticos tiene un fuerte énfasis en ejemplos politológicos aplicados. Utilizar ejemplos de texto trillados e idealizados sobre autitos o islas imaginarias sería una falta de respeto para el lector, a quien sabemos ávido por ocupar las herramientas de este libro en las preguntas de investigación política que le parecen importantes. Por el contrario, queremos mostrar el potencial de dichas herramientas metiendo las manos en la masa, con datos de verdad, investigaciones que colegas ya han realizado y dificultades particulares de llevar el análisis de datos a preguntas políticas. Será bueno que nos acompañe a lo largo del libro con RStudio abierto en su computador, nada mejor que aprender juntos. El libro se alimenta del avance enorme que se ha hecho en los últimos años a partir de la creación del lenguaje de tidyverse y de la publicación de R for Data Science, el cual puede ser accedido de manera gratuita en español gracias al trabajo colaborativo de sus usuarios en el siguiente link. Todo el trabajo hecho por Garrett Grolemund y Hadley Wickham por facilitar la sintaxis de R es una excelente invitación para quienes nunca han utilizado software estadístico o para quienes quieren mudarse de otras alternativas como Stata o SPSS. 1.5 Estructura del libro # Capítulo Autor(es) 1 Introducción I. Introducción a R 2 R Básico Andrés Cruz 3 Manejo de datos Andrés Cruz 4 Visualización de datos Soledad Araya 5 Carga de bases Soledad Araya y Andrés Cruz II. Modelos 6 Modelos lineales Inés Fynn y Lihuen Nocetto 7 Modelos binarios Francisco Urdinez 8 Modelos de supervivencia Francisco Urdinez III. Aplicaciones 9 Manejo avanzado de datos políticos Andrés Cruz y Francisco Urdinez 10 Creación de índices con PCA Caterina Labrín y Francisco Urdinez 11 Selección de casos a partir de regresiones Inés Fynn y Lihuen Nocetto 12 Minería de datos Gonzalo Barría 13 Análisis de redes Andrés Cruz 14 Análisis cuantitativo de textos Sebastián Huneeus 15 Generación de mapas Andrea Escobar y Gabriel Ortiz El libro está organizado en tres secciones temáticas. Dentro de las secciones, cada capítulo se esfuerza por resolver problemas puntuales, balanceando teoría y práctica en R. Al iniciar el capítulo recomendamos literatura que te ayudará a profundizar en el tema si es que te interesó. También te invitamos a que reproduzcas los capítulos con tus propios datos a modo de ejercicio práctico. La sección I está dedicada al manejo de datos. Lo ideal es que el lector consiga algo más que mirar una base de datos con cara de “no entiendo nada”. Introduciremos R desde su instalación y aprenderemos a sacarle el jugo para obtener datos, conocerlos en profundidad, transformarlos de acuerdo a las preguntas que nos interesan y representarlos gráficamente en formas tanto funcionales como atractivas. En la sección II está el corazón del libro. Veremos cómo responder a preguntas políticas desde una perspectiva estadística —siempre podemos contestar desde nuestra intuición aunque esto suela ser menos serio y poco científico—. En general, la sección trata modelos estadísticos, que intentan explicar y predecir la variación de ciertas variables (dependientes) de acuerdo a cómo varía otras variables (independientes). Exploraremos distintos tipos de modelos de acuerdo a las distintas formas de variables dependientes que se encuentran comúnmente en la arena de lo político. Revisaremos cómo interpretar resultados y presentarlos en forma clara y atractiva, cómo elegir entre modelos competidores y cómo comprobar simplemente algunos de los supuestos estadísticos necesarios para que los modelos funcionen. Debemos notar que este no es un libro de econometría, claro está, por lo que para cada modelo haremos referencia a trabajos más avanzados en términos teóricos, con el fin de que el lector pueda profundizar por su cuenta si cree que debe utilizar algún modelo en específico para responder a sus preguntas de interés. Por último, en la sección III dejaremos el mundo ideal y nos adentraremos en la resolución de problemas. Ya sea porque un colega nos prestó su base de datos y se ve más bien como una obra de arte surrealista, o simplemente porque la dificultad de los problemas a los que nos enfrentamos deja corto lo que aprendimos al principio del libro, aquí presentaremos un popurrí de herramientas para que el lector integre en su flujo de trabajo cotidiano. Estas han sido seleccionadas desde nuestra experiencia y son cuáles creemos las más requeridas en la práctica del análisis de datos políticos. 1.6 Prerrequisitos Este libro está pensado para alumnos que más que brillantes son motivados: el análisis cuantitativo de datos exige sobre todo tenacidad y curiosidad. Es altamente deseable que el lector tenga nociones básicas de matemática, probabilidad y/o estadística universitaria antes de leer este libro, aun cuando nos esforzamos por mantenerlo lo más simple que pudimos en dichas materias. En términos de hardware, prácticamente cualquier computador moderno con acceso a internet será suficiente, pues las herramientas que utilizaremos son más bien livianas. Todo el software que utilizaremos es gratuito. En el Capítulo 2, de R Básico, se explica cómo instalar todo el software que se utilizará en el libro. Si ya sabes un poco sobre el funcionamiento de R, debes saber que en la subsección 2.2.4.3 instalaremos dos paquetes que luego asumiremos instalados en el resto del libro: tidyverse y paqueteadp (un paquete especialmente creado para apoyar al libro). "],
["rbas.html", "Capítulo 2 R básico 2.1 Instalación 2.2 Partes de RStudio", " Capítulo 2 R básico Por Andrés Cruz 2.1 Instalación 2.1.1 R R (R Core Team, 2017) es un lenguaje de programación especialmente desarrollado para realizar análisis estadístico. Una de sus principales características, como se ha dejado a entrever en el prefacio, es que es de código libre: aparte de ser gratis, esto significa que las licencias que protegen legalmente a R son muy permisivas. Al amparo de esas licencias, miles de desarrolladores alrededor del mundo han añadido su granito de arena a la usabilidad y atractivo de R. ¡En AnalizaR Datos Políticos le sacaremos el jugo a esa diversidad! Instalar R es fácil, independiente de si usas Windows, Mac o Linux. Basta con ingresar a https://cran.r-project.org/ y seguir las instrucciones de descarga e instalación. 2.1.2 RStudio Como dijimos, R es un lenguaje de programación. En términos informales, es una forma ordenada de pedirle al computador que realice ciertas operaciones. Esto significa que es posible usar R exclusivamente desde una consola o terminal -las pantallas negras de los hackers de las películas. Aunque esto tiene algunos atractivos -entre ellos, parecer hacker-, en general queremos interfaces más amigables. Ahí es cuando entra en escena RStudio, el programa más popular para utilizar R. Una vez esté instalado, todos nuestros análisis ocurrirán dentro de RStudio, que es también de código libre. Para instalar RStudio, es necesario ya haber instalado R. La descarga e instalación es accesible en Windows, Mac y Linux. El link es https://www.rstudio.com/products/rstudio/download/#download Instale ambos R y RStudio, que nosotros lo esperamos aquí. 2.2 Partes de RStudio Si lograste bajar e instalar R y RStudio, bastará con ingresar a `RStudio´ para comenzar a trabajar. Se pillará con una pantalla como la de la figura 2.1. Figura 2.1: Cuatro paneles de la interfaz básica de RStudio. La pantalla de RStudio se divide en cuatro paneles. A continuación, vamos a explicar sus funciones. La idea en esta sección es familiarizar al lector con lo básico de R en el camino. 2.2.1 Consola El panel inferior izquierdo de RStudio. Es nuestro espacio de comunicación directa con el computador, en el que le solicitamos, hablando “lenguaje R”, realizar tareas específicas. Llamaremos comandos a estas solicitudes. Probemos correr un comando que realiza una operación aritmética básica: 2 + 2 ## [1] 4 Un truco importante de la consola es que con tus flechas del teclado de arriba y abajo podrás navegar en el historial de comandos recientes. Recomendamos al lector probar de realizar otros comandos con operaciones aritméticas y volver atrás con los botones de arriba y abajo. 2.2.2 Script El panel superior izquierdo de RStudio puede describirse como una suerte de “bitácora de comandos”. Aunque la consola puede ser útil para unos pocos comandos, análisis complejos requerirán que llevemos un registro de nuestros comandos. Para abrir un script nuevo, basta con presionar Ctrl + Shift + N o ir a File &gt; New File &gt; R Script (utilizar atajos de teclado suele ser una buena idea, y no solo por el factor hacker. La pantalla en blanco de un nuevo script es similar a un bloc de notas sin usar, con la particularidad de que cada línea debe pensarse como un comando. El lector debe notar que escribir un comando en el script y presionar Enter no consigue nada más que un salto de párrafo. Para correr el comando de una línea basta con presionar Ctrl + Enter (en el caso de Mac, Cmd + Enter) mientras se tiene el teclado en ella. ¡Es posible seleccionar múltiples líneas/comandos a la vez y correrlas de una pasada con Ctrl + Enter! También es fundamental para trabajar bien dejar comentarios explicativos en nuestros scripts. Esto no es solo relevante en el trabajo en grupo (el código ajeno puede ser inentendible sin una guía clara), sino que también denota atención por nuestros yo del futuro. En varias ocasiones nos ha tocado revisar código que escribimos hace un par de meses, no entender nada, y maldecir a nuestros yo del pasado por su poca consideración. A la hora de interpretar comandos, R reconoce que todo lo que siga a un numeral (# o hashtag) es un comentario. Así, hay dos formas de dejar comentarios, como “comandos estériles” o como apéndices de comandos funcionales: # Este es un comando estéril. R sabe que es solo un comentario, por lo que no retorna nada. 2 + 2 # Este es un comando-apéndice. ¡R corre el comando hasta el # y luego sabe que es un comentario! ## [1] 4 Para guardar un script, basta con presionar Ctrl + S o clickear File &gt; Save. 2.2.3 Objetos Este es el panel superior derecho de RStudio. Aunque tiene tres pestañas (“Environment”, “History” y “Connections”), la gran estrella es “Environment” que sirve como registro para los objetos que vayamos creando a medida que trabajamos. Una de las características centrales de Res que permite almacenar objetos para luego correr comandos con ellos. La forma para crear un objeto es usando la flechita &lt;- de tal manera que nombre_del_objeto &lt;- contenido. Por ejemplo: objeto_1 &lt;- 2 + 2 El lector notará que en la pestaña “Environment” aparece un nuevo objeto, objeto_1. Este contiene el resultado de 2 + 2. Es posible preguntarle a R qué contiene un objeto simplemente corriendo su nombre como si fuera un comando: objeto_1 ## [1] 4 Los objetos pueden insertarse en otros comandos, haciendo referencia a sus contenidos. Por ejemplo: objeto_1 + 10 ## [1] 14 También es posible reasignar a los objetos. ¡Si nos aburrimos de objeto_1 como un 4, podemos asignarle cualquier valor que queramos! Valores de texto o no númericos se pueden asignar entre comillas: objeto_1 &lt;- &quot;democracia&quot; objeto_1 ## [1] &quot;democracia&quot; Borrar objetos es también muy simple. Aunque suene como perder nuestro duro trabajo, tener un “Environment” limpio y fácil de leer a menudo lo vale la pena. Para ello usamos la función rm rm(objeto_1) 2.2.3.1 Vectores Hasta ahora hemos conocido los objetos más simples de R, que contienen un solo valor. Objetos un poco más complejos son los vectores o “lineas” de valores. Crear un vector es simple, basta con insertar sus componentes dentro de c(), separados por comas: vector_1 &lt;- c(15, 10, 20) vector_1 ## [1] 15 10 20 2.2.3.2 Funciones Mira el siguiente comando de ejemplo: 2 + sqrt(25) - log(2) ## [1] 6.3 R interpreta que sqrt(25) es la raíz cuadrada 2, mientras que log(2) es el logaritmo natural de 2. Tanto sqrt() como log() son funciones de R. En términos muy básicos, una función es un procedimiento que se puede esquematizar de la siguiente forma: Figura 2.2: Las funciones nos permitirán hacer las transformaciones necesarias a nuestros datos. Adaptada de Wikimedia Commons. sqrt() toma un valor numérico como input y devuelve su raíz cuadrada como output. log() toma el mismo input, pero devuelve su logaritmo común (o en base a 10). c(), otra función que utilizamos antes, toma distintos valores únicos como input y devuelve un vector que los concatena. Es a propósito de los vectores que las funciones de R comienzan a brillar y a alejarse de las cualidades básicas de una calculadora (que, a grandes rasgos, es lo que hemos visto ahora de R, nada muy impresionante). Veamos algunas funciones que extraen información útil sobre algún vector. ¿Qué hace cada una? mean(vector_1) # media ## [1] 15 median(vector_1) # mediana ## [1] 15 sd(vector_1) # desviación estándar ## [1] 5 sum(vector_1) # suma ## [1] 45 min(vector_1) # valor mínimo ## [1] 10 max(vector_1) # valor máximo ## [1] 20 length(vector_1) # longitud (cantidad de valores) ## [1] 3 sort(vector_1) # ... ## [1] 10 15 20 El lector podría haber deducido que sort(), la última función del lote anterior, ordena al vector de menor a mayor. ¿Qué pasa si quisiéramos ordenarlo de mayor a menor? Esto nos permite introducir a los argumentos, partes de las funciones que nos permiten modificar su comportamiento. A continuación agregaremos el argumento decreasing = TRUE al comando anterior, consiguiendo nuestro objetivo: sort(vector_1, decreasing = TRUE) ## [1] 20 15 10 2.2.4 Archivos / gráficos / paquetes / ayuda Es el cuadrante inferior derecho de la pantalla de RStudio. ¡Estas cuatro pestañas son las que se roban la película! Vamos una por una: 2.2.4.1 Archivos y RStudio projects Esta pestaña es una ventana a los archivos que tenemos en nuestro directorio de trabajo. Funcionando como un pequeño gestor, nos permite moverlos, renombrarlos, copiarlos, etcétera. A propósito de archivos, una de las grandes novedades recientes de R son los RStudio Projects, o proyectos de RStudio. Los desarrolladores de RStudio se dieron cuenta de que sus usuarios tenían scripts y otros archivos de R desperdigados a lo largo y ancho de sus discos duros, sin orden alguno. Por eso implementaron la filosofía de “un proyecto, una carpeta”. “Un proyecto, una carpeta” es tan simple como suena: la idea es que cada proyecto en el que trabajemos sea autosuficiente, que incluya todo lo que necesitemos para trabajar. Se pueden manejar los proyectos desde la esquina superior derecha de R. ¿Viste las tres solapas “Environment”, “History”, “Connections”? Bueno, mira un poquito más arriba y verás el logo de RStudio Projects. Aquí debes ser cuidadoso y notar que crear o abrir un proyecto reiniciará tu sesión de R, borrando todo el trabajo que no guardes. Como no has creado proyectos aún, tu sesión dirá Project: (None). Al clickear en “New Project” se ofrecen tres alternativas: Una vez que hayas creado un proyecto y estés trabajando en él, todos los enlaces a archivos serán locales. Por ejemplo, si en tu carpeta de proyecto tienes una subcarpeta “datos” con un archivo “ejemplo.csv” adentro, la referencia al archivo será simplemente “datos/ejemplo.csv”. Recuerda el lema: “un proyecto, una carpeta”. ¡Te recomendamos crear un proyecto de RStudio por cada capítulo del libro que quieras seguir con código! 2.2.4.2 Gráficos Aquí aparecen los gráficos que realizamos con R. ¡En el capítulo 4 aprenderemos a crearlos! 2.2.4.3 Instalar paquetes Una de las cualidades de R a la que más hincapié hemos dado es su versatilidad. Su código libre hace que muchos desarrolladores se sientan atraídos a aportar a la comunidad de R con nuevas funcionalidades. En general, realizan esto a través de paquetes, que los usuarios pueden instalar como apéndices adicionales a R. Los paquetes contienen nuevas funciones, bases de datos, etcétera. La pestaña de RStudio aquí reseñada nos permite acceder a nuestros paquetes instalados. En la introducción mencionamos la enorme contribución que significó la creación de tidyverse. Instalar un paquete es bastante simple, a través de la función install.packages(). A continuación vamos a instalar el paquete tidyverse, central en nuestros próximos análisis. El tidyverse es una recopilación que incluye algunos de los mejores paquetes modernos para análisis de datos en R. install.packages(&quot;tidyverse&quot;) Cada vez que el usuario abre una nueva sesión de R, este se abre como “recién salido de fábrica”. Es decir, no solo se abre sin objetos sino que solo con los paquetes básicos que permiten a R funcionar. Tenemos que cargar los paquetes extra que queramos usar, entonces. Es más o menos como cuando compramos un smartphone y descargamos las aplicaciones que más usaremos, para que se ajuste a nuestras necesidades cotidianas. La forma más común de hacer esto es a través de la función library(), como se ve a continuación. Nota que tidyverse no está entre comillas1. library(tidyverse) Adicionalmente, para sacar todo el provecho de este libro debes instalar nuestro paquete complementario, paqueteadp. Este te dará acceso a las bases de datos a utilizar en los diferentes capítulos del libro, además de algunas funciones de apoyo. La instalación es ligeramente distinta, porque es un paquete en desarrollo. Para instalarlo, primero debes haber instalado el paquete “remotes”, que permite utilizar paquetes almacenados en GitHub, como el nuestro. install.packages(&quot;remotes&quot;) Teniendo el paquete “remotes” cargado, su función install_github() te permitirá instalar el paquete del libro: library(remotes) install_github(&quot;arcruz0/paqueteadp&quot;) Nota que “arcruz0” es el usuario de uno de nosotros en GitHub, donde está almacenado el paquete “paqueteadp”. ¡Ahora este está instalado en tu sistema! Cada vez que lo necesites en una sesión de R, debes cargarlo con library(): library(paqueteadp) 2.2.4.4 Ayuda Buscar ayuda es central a la hora de programar en R. Mira la figura 2.3:Esta pestaña de RStudio abre los archivos de ayuda que necesitemos, permitiéndonos buscar en ellos. Las funciones tienen archivos de ayuda para cada una de ellas. Por ejemplo, podemos acceder al archivo de ayuda de la función sqrt() a través del comando help(sqrt) (también sirve ?sqrt, que es lo mismo). Los paquetes en su conjunto también tienen archivos de ayuda, más comprensivos. Por ejemplo, para ver los archivos de ayuda del tidyverse solo debemos recurrir al argumento “package”: help(package = tidyverse). El lector debe notar que los archivos de ayuda de paquetes y funciones de paquetes solo están disponibles si el paquete ha sido cargado. Figura 2.3: En la solapa Help podemos buscar los documentos de ayuda de los paquetes que queremos utilizar. ¡Esta es la mejor forma de aprender R (después de este libro)! Dentro del archivo de ayuda para un paquete podemos buscar sub-funciones o dudas sobre comandos específicos en el cuadrante que hemos señalado en rojo. Ejercicios antes de continuar al próximo capítulo ¿Qué significa “correr” un comando desde un script? ¿Cómo se hace? ¿Cuál es la media de los dígitos del hit de Rafaella Carrà, 0 3 0 3 4 5 6? ¿Y la mediana? Por último, ordénalos de mayor a menor. Busca ayuda para el paquete ggparliament, recomendado para maravillarte con la variedad de los paquetes de R. Revisaremos brevemente dicho paquete más tarde, en el Capítulo 4, de visualización de datos. Esta es la convención más utilizada para library(). El comando funcionará con comillas, aunque no es muy común verlo así: library(&quot;tidyverse&quot;). ¡Seguir las convenciones es buena idea, por eso recomendamos omitir las comillas!↩ "],
["manejo-de-datos.html", "Capítulo 3 Manejo de datos 3.1 Introducción 3.2 Operaciones en bases 3.3 Reformatear bases", " Capítulo 3 Manejo de datos Por Andrés Cruz 3.1 Introducción Cuando hablamos de análisis de datos, casi siempre nos referimos a análisis de bases de datos. Aunque hay varios formatos de bases de datos disponibles, en ciencias sociales generalmente usamos y creamos bases de datos tabulares, que son las que este libro tratará. Muy probablemente el lector estará familiarizado con la estructura básica de este tipo de bases, gracias a las planillas de Microsoft Excel, Google Spreadsheets y/o LibreOffice Calc. La primera fila suele ser un header o encabezado, que indica qué datos registran las celdas de esa columna. En general, queremos que nuestras bases de datos tabulares tengan una estructura tidy (véase R4DS). La idea de una base tidy es simple: cada columna es una variable, cada fila una observación (de acuerdo a la unidad de análisis) y, por lo tanto, cada celda es una observación. 3.1.1 Nuestra base de datos Para este capítulo usaremos una sección de la base de datos de Quality of Government (QoG, 2017), un proyecto que registra diversos datos de países. Las variables de la base de datos están descritas a continuación: variable descripción cname Nombre del país wdi_gdppppcon2011 GDP PPP, en dólares del 2011, según los datos de WDI (p. 635 del codebook) wdi_pop Población, según los datos de WDI (p. 665) ti_cpi Índice de Percepción de la Corrupción de TI. Va de 0 a 100, con 0 lo más corrupto (p. 560) lp_muslim80 Porcentaje de población de religión musulmana, para 1980, según LP (p. 447) fh_ipolity2 Nivel de democracia según FH. Va de 0 a 10, con 0 como menos democrático (p. 291) region Región del país, según WDI (añadida a la base) Para comenzar a trabajar carguemos el paquete tidyverse, uno de los centrales del libro, que nos dará funciones útiles para trabajar con nuestra base datos. library(tidyverse) Ahora carguemos la base de datos a nuestra sesión de R R. Se llama “qog”, y podemos cargarla con facilidad desde el paquete del libro, por medio de la función data(): library(paqueteadp) data(qog) Puedes chequear que la base se cargó correctamente utilizando el comando ls() (o viendo la pestaña de Environment en RStudio): ls() ## [1] &quot;qog&quot; 3.1.2 Describir la base Para aproximarnos a nuestra base recién cargada tenemos varias opciones. Podemos, como antes, simplemente usar su nombre como un comando para un resumen rápido: qog ## # A tibble: 139 x 7 ## cname wdi_gdppppcon2011 wdi_pop ti_cpi lp_muslim80 fh_ipolity2 region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afgha… 57566228480 3.07e7 8 99.3 2.02 Souther… ## 2 Alban… 28715335680 2.90e6 31 20.5 8.08 Souther… ## 3 Alger… 507901640704 3.82e7 36 99.1 4.25 Norther… ## 4 Angola 152477499392 2.34e7 23 0 3.25 Middle … ## 5 Austr… 990474338304 2.31e7 81 0.200 10 Austral… ## 6 Austr… 373413642240 8.48e6 69 0.600 10 Western… ## 7 Baham… 8497731584 3.78e5 71 0 10 Caribbe… ## 8 Bahra… 56583507968 1.35e6 48 95 0.833 Western… ## 9 Bangl… 446835425280 1.57e8 27 85.9 6.42 Souther… ## 10 Barba… 4333428224 2.83e5 75 0.200 10 Caribbe… ## # … with 129 more rows También podemos utilizar la función glimpse() para tener un resumen desde otra perspectiva: glimpse(qog) ## Observations: 139 ## Variables: 7 ## $ cname &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;,… ## $ wdi_gdppppcon2011 &lt;dbl&gt; 5.8e+10, 2.9e+10, 5.1e+11, 1.5e+11, 9.9e+11, 3… ## $ wdi_pop &lt;dbl&gt; 3.1e+07, 2.9e+06, 3.8e+07, 2.3e+07, 2.3e+07, 8… ## $ ti_cpi &lt;dbl&gt; 8, 31, 36, 23, 81, 69, 71, 48, 27, 75, 75, 63,… ## $ lp_muslim80 &lt;dbl&gt; 99.3, 20.5, 99.1, 0.0, 0.2, 0.6, 0.0, 95.0, 85… ## $ fh_ipolity2 &lt;dbl&gt; 2.02, 8.08, 4.25, 3.25, 10.00, 10.00, 10.00, 0… ## $ region &lt;chr&gt; &quot;Southern Asia&quot;, &quot;Southern Europe&quot;, &quot;Northern … Una alternativa que nos permite ver la base completa es la función View(), análoga a clickear nuestro objeto en la pestaña “Environment” de Rstudio: View(qog) 3.2 Operaciones en bases A continuación veremos varias operaciones en nuestra base de datos, en línea con lo enseñado en el capítulo del R4DS. 3.2.1 Ordenar una base Una de las operaciones más comunes con bases de datos es ordenarlas de acuerdo a alguna de las variables. Esto nos puede dar insights (¿traducción?) inmediatos sobre nuestras observaciones. Por ejemplo, ordenemos la base de acuerdo a población: arrange(qog, wdi_pop) ## # A tibble: 139 x 7 ## cname wdi_gdppppcon20… wdi_pop ti_cpi lp_muslim80 fh_ipolity2 region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Dominica 722668736 72005 58 0 10 Caribb… ## 2 Seychel… 2229991168 89900 54 0.300 6.94 Easter… ## 3 Tonga 513904512 105139 31.4 0 8.59 Polyne… ## 4 Kiribati 183852816 108544 30.8 0 10 Micron… ## 5 St Lucia 1871906688 182305 71 0 10 Caribb… ## 6 Sao Tom… 540452288 182386 42 0 8.59 Middle… ## 7 Samoa 1047012608 190390 52 0 8.59 Polyne… ## 8 Barbados 4333428224 282503 75 0.200 10 Caribb… ## 9 Iceland 13266284544 323764 78 0 10 Northe… ## 10 Bahamas 8497731584 377841 71 0 10 Caribb… ## # … with 129 more rows El lector debe notar cómo el primer argumento, “qog”, toma la base de datos y los siguientes enuncian cómo ordenarla, en este caso, por “wdi_pop”, la variable de población. Debe notar también cómo el comando anterior no crea ningún objeto, solo muestra los resultados en la consola. Para crear uno tenemos que seguir la fórmula típica de asignación: qog_ordenada &lt;- arrange(qog, wdi_pop) Podemos realizar ambas operaciones, mostrar los resultados y crear el objeto, rodeando este último comando con paréntesis: ( qog_ordenada &lt;- arrange(qog, wdi_pop) ) ## # A tibble: 139 x 7 ## cname wdi_gdppppcon20… wdi_pop ti_cpi lp_muslim80 fh_ipolity2 region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Dominica 722668736 72005 58 0 10 Caribb… ## 2 Seychel… 2229991168 89900 54 0.300 6.94 Easter… ## 3 Tonga 513904512 105139 31.4 0 8.59 Polyne… ## 4 Kiribati 183852816 108544 30.8 0 10 Micron… ## 5 St Lucia 1871906688 182305 71 0 10 Caribb… ## 6 Sao Tom… 540452288 182386 42 0 8.59 Middle… ## 7 Samoa 1047012608 190390 52 0 8.59 Polyne… ## 8 Barbados 4333428224 282503 75 0.200 10 Caribb… ## 9 Iceland 13266284544 323764 78 0 10 Northe… ## 10 Bahamas 8497731584 377841 71 0 10 Caribb… ## # … with 129 more rows La operación para ordenar realizada antes iba de menor a mayor, en términos de población. Si queremos el orden inverso (decreciente), basta con añadir un signo menos (-) antes de la variable: arrange(qog, -wdi_pop) ## # A tibble: 139 x 7 ## cname wdi_gdppppcon2011 wdi_pop ti_cpi lp_muslim80 fh_ipolity2 region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 China 16023988207616 1.36e9 40 2.40 1.17 Easter… ## 2 India 6566166134784 1.28e9 36 11.6 8.5 Southe… ## 3 United… 16230494765056 3.16e8 73 0.800 10 Northe… ## 4 Indone… 2430921605120 2.51e8 32 43.4 7.83 South-… ## 5 Brazil 3109301780480 2.04e8 42 0.100 8.67 South … ## 6 Pakist… 810954457088 1.81e8 28 96.8 6.33 Southe… ## 7 Nigeria 941462781952 1.73e8 25 45 5.58 Wester… ## 8 Bangla… 446835425280 1.57e8 27 85.9 6.42 Southe… ## 9 Japan 4535077044224 1.27e8 74 0 10 Easter… ## 10 Mexico 1997247479808 1.24e8 34 0 7.83 Centra… ## # … with 129 more rows ¡Con eso tenemos los países con mayor población en el mundo! ¿Qué pasa si queremos los países con mayor población dentro de cada región? Tendríamos que realizar un ordenamiento en dos pasos: primero por región y luego por población. Con arrange() esto es simple: arrange(qog, region, -wdi_pop) ## # A tibble: 139 x 7 ## cname wdi_gdppppcon20… wdi_pop ti_cpi lp_muslim80 fh_ipolity2 region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Austra… 990474338304 2.31e7 81 0.200 10 Austral… ## 2 New Ze… 148189986816 4.44e6 91 0 10 Austral… ## 3 Cuba 226685157376 1.14e7 46 0 1.17 Caribbe… ## 4 Haiti 16999370752 1.04e7 19 0 4.58 Caribbe… ## 5 Domini… 122657308672 1.03e7 29 0 8.25 Caribbe… ## 6 Jamaica 22884503552 2.71e6 38 0.100 8.5 Caribbe… ## 7 Trinid… 40973463552 1.35e6 38 6.5 9.17 Caribbe… ## 8 Bahamas 8497731584 3.78e5 71 0 10 Caribbe… ## 9 Barbad… 4333428224 2.83e5 75 0.200 10 Caribbe… ## 10 St Luc… 1871906688 1.82e5 71 0 10 Caribbe… ## # … with 129 more rows A propósito del resultado anterior, el lector puede deducir que cuando arrange() ordena variables categóricas (en vez de numéricas) lo hace alfabéticamente. Añadir un signo menos (-) antes de la variable hará que el orden sea al revés en términos del alfabeto: arrange(qog, desc(region), -wdi_pop) # no sé por qué - no funciona, ARREGLAR ## # A tibble: 139 x 7 ## cname wdi_gdppppcon20… wdi_pop ti_cpi lp_muslim80 fh_ipolity2 region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 France … 2459435270144 6.59e7 71 3 9.75 Wester… ## 2 Netherl… 762382450688 1.68e7 83 1 10 Wester… ## 3 Belgium 452154359808 1.12e7 75 1.10 9.5 Wester… ## 4 Austria 373413642240 8.48e6 69 0.600 10 Wester… ## 5 Switzer… 444201566208 8.09e6 85 0.300 10 Wester… ## 6 Luxembo… 48842280960 5.43e5 80 0 10 Wester… ## 7 Turkey 1392197369856 7.50e7 50 99.2 7.67 Wester… ## 8 Iraq 510895521792 3.38e7 16 95.8 4.5 Wester… ## 9 Saudi A… 1478747750400 3.02e7 46 98.8 0 Wester… ## 10 United … 560986259456 9.04e6 69 94.9 1.33 Wester… ## # … with 129 more rows 3.2.2 Seleccionar columnas A veces queremos trabajar solo con algunas variables de una base de datos. Para esto existe la función select(). Pensemos que queremos solo el nombre de cada país (cname) y su porcentaje de población musulmana para 1980: select(qog, cname, lp_muslim80) ## # A tibble: 139 x 2 ## cname lp_muslim80 ## &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 99.3 ## 2 Albania 20.5 ## 3 Algeria 99.1 ## 4 Angola 0 ## 5 Australia 0.200 ## 6 Austria 0.600 ## 7 Bahamas 0 ## 8 Bahrain 95 ## 9 Bangladesh 85.9 ## 10 Barbados 0.200 ## # … with 129 more rows Al igual que para arrange(), aquí el primer argumento designa la base a modificar y los demás cómo se debería hacer eso -en este caso, qué variables deben ser seleccionadas. Añadir un signo menos (-) aquí indica qué variables no seleccionar. Por ejemplo, quitemos el porcentaje de población musulmana para 1980 de la base: select(qog, -lp_muslim80) ## # A tibble: 139 x 6 ## cname wdi_gdppppcon2011 wdi_pop ti_cpi fh_ipolity2 region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanis… 57566228480 3.07e7 8 2.02 Southern Asia ## 2 Albania 28715335680 2.90e6 31 8.08 Southern Europe ## 3 Algeria 507901640704 3.82e7 36 4.25 Northern Africa ## 4 Angola 152477499392 2.34e7 23 3.25 Middle Africa ## 5 Australia 990474338304 2.31e7 81 10 Australia and N… ## 6 Austria 373413642240 8.48e6 69 10 Western Europe ## 7 Bahamas 8497731584 3.78e5 71 10 Caribbean ## 8 Bahrain 56583507968 1.35e6 48 0.833 Western Asia ## 9 Banglade… 446835425280 1.57e8 27 6.42 Southern Asia ## 10 Barbados 4333428224 2.83e5 75 10 Caribbean ## # … with 129 more rows Aparte de seleccionar variables específicas, select() es capaz de entender referencias a intervalos de variables. Por ejemplo, podemos querer las cuatro primeras variables: select(qog, cname:ti_cpi) ## # A tibble: 139 x 4 ## cname wdi_gdppppcon2011 wdi_pop ti_cpi ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 57566228480 30682500 8 ## 2 Albania 28715335680 2897366 31 ## 3 Algeria 507901640704 38186136 36 ## 4 Angola 152477499392 23448202 23 ## 5 Australia 990474338304 23125868 81 ## 6 Austria 373413642240 8479375 69 ## 7 Bahamas 8497731584 377841 71 ## 8 Bahrain 56583507968 1349427 48 ## 9 Bangladesh 446835425280 157157392 27 ## 10 Barbados 4333428224 282503 75 ## # … with 129 more rows select(qog, 1:4) # lo mismo, aunque no recomendado ## # A tibble: 139 x 4 ## cname wdi_gdppppcon2011 wdi_pop ti_cpi ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 57566228480 30682500 8 ## 2 Albania 28715335680 2897366 31 ## 3 Algeria 507901640704 38186136 36 ## 4 Angola 152477499392 23448202 23 ## 5 Australia 990474338304 23125868 81 ## 6 Austria 373413642240 8479375 69 ## 7 Bahamas 8497731584 377841 71 ## 8 Bahrain 56583507968 1349427 48 ## 9 Bangladesh 446835425280 157157392 27 ## 10 Barbados 4333428224 282503 75 ## # … with 129 more rows Otra herramienta para complejizar nuestra selección se encuentra en las funciones de ayuda. Entre ellas, starts_with es de particular utilidad, permitiendo seleccionar variables que empiecen con cierto patrón. Por ejemplo, podríamos querer, a partir del nombre del país, todas las variables que provengan de los World Development Indicators (WDI) del Banco Mundial: select(qog, cname, starts_with(&quot;wdi_&quot;)) ## # A tibble: 139 x 3 ## cname wdi_gdppppcon2011 wdi_pop ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 57566228480 30682500 ## 2 Albania 28715335680 2897366 ## 3 Algeria 507901640704 38186136 ## 4 Angola 152477499392 23448202 ## 5 Australia 990474338304 23125868 ## 6 Austria 373413642240 8479375 ## 7 Bahamas 8497731584 377841 ## 8 Bahrain 56583507968 1349427 ## 9 Bangladesh 446835425280 157157392 ## 10 Barbados 4333428224 282503 ## # … with 129 more rows Otra función de ayuda útil es everything(), que se lee como “todas las demás variables”. Es especialmente útil para cambiar el orden de las variables en una bases de datos. Por ejemplo, pasemos región al segundo lugar entre las variables: select(qog, cname, region, everything()) ## # A tibble: 139 x 7 ## cname region wdi_gdppppcon20… wdi_pop ti_cpi lp_muslim80 fh_ipolity2 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afgha… Southern… 57566228480 3.07e7 8 99.3 2.02 ## 2 Alban… Southern… 28715335680 2.90e6 31 20.5 8.08 ## 3 Alger… Northern… 507901640704 3.82e7 36 99.1 4.25 ## 4 Angola Middle A… 152477499392 2.34e7 23 0 3.25 ## 5 Austr… Australi… 990474338304 2.31e7 81 0.200 10 ## 6 Austr… Western … 373413642240 8.48e6 69 0.600 10 ## 7 Baham… Caribbean 8497731584 3.78e5 71 0 10 ## 8 Bahra… Western … 56583507968 1.35e6 48 95 0.833 ## 9 Bangl… Southern… 446835425280 1.57e8 27 85.9 6.42 ## 10 Barba… Caribbean 4333428224 2.83e5 75 0.200 10 ## # … with 129 more rows 3.2.3 Renombrar columnas La notación para el GDP es un poco confusa. ¿Y si queremos cambiar el nombre de la variable? Aprovechemos también de cambiar el nombre de la variable de identificación por país. rename(qog, wdi_gdp = wdi_gdppppcon2011, country_name = cname) ## # A tibble: 139 x 7 ## country_name wdi_gdp wdi_pop ti_cpi lp_muslim80 fh_ipolity2 region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan 5.76e10 3.07e7 8 99.3 2.02 Southern … ## 2 Albania 2.87e10 2.90e6 31 20.5 8.08 Southern … ## 3 Algeria 5.08e11 3.82e7 36 99.1 4.25 Northern … ## 4 Angola 1.52e11 2.34e7 23 0 3.25 Middle Af… ## 5 Australia 9.90e11 2.31e7 81 0.200 10 Australia… ## 6 Austria 3.73e11 8.48e6 69 0.600 10 Western E… ## 7 Bahamas 8.50e 9 3.78e5 71 0 10 Caribbean ## 8 Bahrain 5.66e10 1.35e6 48 95 0.833 Western A… ## 9 Bangladesh 4.47e11 1.57e8 27 85.9 6.42 Southern … ## 10 Barbados 4.33e 9 2.83e5 75 0.200 10 Caribbean ## # … with 129 more rows 3.2.4 Filtrar observaciones Es muy común el querer filtrar nuestras observaciones de acuerdo a algún tipo de criterio lógico. Para esto R cuenta con operadores lógicos. Los más comunes son los siguientes: operador descripción == es igual a != es distinto a &gt; es mayor a &lt; es menor a &gt;= es mayor o igual a &lt;= es menor o igual a &amp; y (intersección) Por ejemplo, podríamos querer solo los países (observaciones) sudamericanos. Hacer esto con filter() es simple, con la ayuda de operadores lógicos: filter(qog, region == &quot;South America&quot;) ## # A tibble: 11 x 7 ## cname wdi_gdppppcon20… wdi_pop ti_cpi lp_muslim80 fh_ipolity2 region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Bolivia 63344840704 1.04e7 34 0 7.58 South A… ## 2 Brazil 3109301780480 2.04e8 42 0.100 8.67 South A… ## 3 Chile 383162515456 1.76e7 71 0 10 South A… ## 4 Colomb… 582488686592 4.73e7 36 0.200 7.17 South A… ## 5 Ecuador 166412222464 1.57e7 35 0 7.08 South A… ## 6 Guyana 5066298368 7.61e5 27 9 7.75 South A… ## 7 Paragu… 53195030528 6.47e6 24 0 8.08 South A… ## 8 Peru 346126876672 3.06e7 38 0 8.5 South A… ## 9 Surina… 8388805120 5.33e5 36 13 7.92 South A… ## 10 Uruguay 65827684352 3.41e6 73 0 10 South A… ## 11 Venezu… 535572054016 3.03e7 20 0 5.17 South A… ¿Qué pasa si queremos solo las filas de países sudamericanos con más de 10 millones de habitantes (nos quedamos con 8 de 12)? ¿Cuáles son los filtros que aplican los siguientes comandos? filter(qog, fh_ipolity2 &gt; 9) filter(qog, wdi_pop &gt; 10e7) filter(qog, cname != &quot;Albania&quot;) filter(qog, lp_muslim80 &gt;= 95) filter(qog, region == &quot;South America&quot; &amp; wdi_pop &gt; 10e6) filter(qog, region == &quot;South America&quot; | region == &quot;South-Eastern Asia&quot;) 3.2.5 Crear nuevas variables Muchas veces queremos crear nuevas variables, a partir de las que ya tenemos. Por ejemplo, podríamos querer el GDP per capita, en vez del absoluto. Tenemos los ingredientes para calcularlo: el GDP absoluto y la población. Creemos una nueva variable, entonces: mutate(qog, gdp_ppp_per_capita = wdi_gdppppcon2011/wdi_pop) ## # A tibble: 139 x 8 ## cname wdi_gdppppcon20… wdi_pop ti_cpi lp_muslim80 fh_ipolity2 region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afgh… 57566228480 3.07e7 8 99.3 2.02 South… ## 2 Alba… 28715335680 2.90e6 31 20.5 8.08 South… ## 3 Alge… 507901640704 3.82e7 36 99.1 4.25 North… ## 4 Ango… 152477499392 2.34e7 23 0 3.25 Middl… ## 5 Aust… 990474338304 2.31e7 81 0.200 10 Austr… ## 6 Aust… 373413642240 8.48e6 69 0.600 10 Weste… ## 7 Baha… 8497731584 3.78e5 71 0 10 Carib… ## 8 Bahr… 56583507968 1.35e6 48 95 0.833 Weste… ## 9 Bang… 446835425280 1.57e8 27 85.9 6.42 South… ## 10 Barb… 4333428224 2.83e5 75 0.200 10 Carib… ## # … with 129 more rows, and 1 more variable: gdp_ppp_per_capita &lt;dbl&gt; Otra nueva variable que podría interesarnos es el número de musulmanes por país. Con la proporción de musulmanes y la población total del país podemos hacer una buena estimación: mutate(qog, n_muslim = wdi_pop * lp_muslim80) ## # A tibble: 139 x 8 ## cname wdi_gdppppcon20… wdi_pop ti_cpi lp_muslim80 fh_ipolity2 region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afgh… 57566228480 3.07e7 8 99.3 2.02 South… ## 2 Alba… 28715335680 2.90e6 31 20.5 8.08 South… ## 3 Alge… 507901640704 3.82e7 36 99.1 4.25 North… ## 4 Ango… 152477499392 2.34e7 23 0 3.25 Middl… ## 5 Aust… 990474338304 2.31e7 81 0.200 10 Austr… ## 6 Aust… 373413642240 8.48e6 69 0.600 10 Weste… ## 7 Baha… 8497731584 3.78e5 71 0 10 Carib… ## 8 Bahr… 56583507968 1.35e6 48 95 0.833 Weste… ## 9 Bang… 446835425280 1.57e8 27 85.9 6.42 South… ## 10 Barb… 4333428224 2.83e5 75 0.200 10 Carib… ## # … with 129 more rows, and 1 more variable: n_muslim &lt;dbl&gt; ¡Es posible crear más de una variable con el mismo comando! Creemos las dos de antes, a la vez: mutate(qog, gdp_ppp_per_capita = wdi_gdppppcon2011/wdi_pop, n_muslim = wdi_pop * lp_muslim80) ## # A tibble: 139 x 9 ## cname wdi_gdppppcon20… wdi_pop ti_cpi lp_muslim80 fh_ipolity2 region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afgh… 57566228480 3.07e7 8 99.3 2.02 South… ## 2 Alba… 28715335680 2.90e6 31 20.5 8.08 South… ## 3 Alge… 507901640704 3.82e7 36 99.1 4.25 North… ## 4 Ango… 152477499392 2.34e7 23 0 3.25 Middl… ## 5 Aust… 990474338304 2.31e7 81 0.200 10 Austr… ## 6 Aust… 373413642240 8.48e6 69 0.600 10 Weste… ## 7 Baha… 8497731584 3.78e5 71 0 10 Carib… ## 8 Bahr… 56583507968 1.35e6 48 95 0.833 Weste… ## 9 Bang… 446835425280 1.57e8 27 85.9 6.42 South… ## 10 Barb… 4333428224 2.83e5 75 0.200 10 Carib… ## # … with 129 more rows, and 2 more variables: gdp_ppp_per_capita &lt;dbl&gt;, ## # n_muslim &lt;dbl&gt; 3.2.6 Concatenar comandos A menudo no queremos hacer una sola de las operaciones con bases de datos reseñadas antes, sino que una seguidilla de estas. Si quisiéramos crear una nueva base a través de, por ejemplo, (1) seleccionar las variables de país, población y GDP, (2) crear la variable de GDP per capita, y (3) ordenar los países de mayor a menor según GDP per capita, nuestro procedimiento en R sería algo como esto: qog_seguidilla_1 &lt;- select(qog, cname, wdi_pop, wdi_gdppppcon2011) qog_seguidilla_2 &lt;- mutate(qog_seguidilla_1, gdp_ppp_per_capita = wdi_gdppppcon2011/wdi_pop) qog_seguidilla_3 &lt;- arrange(qog_seguidilla_2, -gdp_ppp_per_capita) qog_seguidilla_3 ## # A tibble: 139 x 4 ## cname wdi_pop wdi_gdppppcon2011 gdp_ppp_per_capita ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Qatar 2101288 280302059520 133395. ## 2 Luxembourg 543360 48842280960 89889. ## 3 Singapore 5399200 419630612480 77721. ## 4 Kuwait 3593689 266584637440 74181. ## 5 Norway 5079623 321651408896 63322. ## 6 United Arab Emirates 9039978 560986259456 62056. ## 7 Switzerland 8089346 444201566208 54912. ## 8 United States 316497536 16230494765056 51282. ## 9 Saudi Arabia 30201052 1478747750400 48963. ## 10 Ireland 4598294 212357021696 46182. ## # … with 129 more rows El lector notará que esto es bastante complicado y nos deja con dos objetos intermedios que no nos interesan, “qog_seguidilla_1” y “qog_seguidilla_2”. La solución del paquete tidyverse que estamos utilizando son las pipes. El lector notará que en las tres funciones de nuestra seguidilla anterior (select, mutate y arrange) el primer argumento es la base de datos a tratar. En vez de crear objetos intermedios podemos “chutear” la base de datos a través de nuestros comandos con pipes, omitiendo los primeros argumentos: qog_seguidilla &lt;- qog %&gt;% select(cname, wdi_pop, wdi_gdppppcon2011) %&gt;% mutate(gdp_ppp_per_capita = wdi_gdppppcon2011/wdi_pop) %&gt;% arrange(-gdp_ppp_per_capita) qog_seguidilla ## # A tibble: 139 x 4 ## cname wdi_pop wdi_gdppppcon2011 gdp_ppp_per_capita ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Qatar 2101288 280302059520 133395. ## 2 Luxembourg 543360 48842280960 89889. ## 3 Singapore 5399200 419630612480 77721. ## 4 Kuwait 3593689 266584637440 74181. ## 5 Norway 5079623 321651408896 63322. ## 6 United Arab Emirates 9039978 560986259456 62056. ## 7 Switzerland 8089346 444201566208 54912. ## 8 United States 316497536 16230494765056 51282. ## 9 Saudi Arabia 30201052 1478747750400 48963. ## 10 Ireland 4598294 212357021696 46182. ## # … with 129 more rows Las pipes pueden leerse como “pero luego”. Nuestra seguidilla anterior, entonces, se leería de la siguiente forma: qog_seguilla es igual a qog; pero luego seleccionamos las variables cname, wdi_pop, wdi_gdppppcon2011; pero luego creamos la variable gdp_ppp_per_capita; pero luego ordenamos la base en forma decreciente según gdp_ppp_per_capita. 3.2.7 Operaciones agrupadas PENDIENTE 3.3 Reformatear bases PENDIENTE Ejercicios antes de continuar al próximo capítulo - Cree una base nueva, llamada qog_2, con una nueva variable llamada porc_muslim, que sea el porcentaje de población musulmana del país. Cree una base nueva, llamada qog_3, que incluya solo países latinoamericanos, China y Sudáfrica. Cree una base nueva, llamada qog_4, que incluya solo países con población mayor a la media de población entre todos los países. Debe contener solo las variables de nombre del país, región y población (en ese orden). Ordene la base según población, de mayor a menor. ¡Use pipes! "],
["dataviz.html", "Capítulo 4 Visualización de datos 4.1 ¿Por qué quiero visualizar mis datos? 4.2 Primeros pasos 4.3 Elecciones locales y visualización de datos 4.4 Para seguir aprendiendo", " Capítulo 4 Visualización de datos Por Soledad Araya Lecturas de referencia Kastellec, J. P., &amp; Leoni, E. L. (2007). Using graphs instead of tables in political science. Perspectives on Politics, 5(4), 755-771. Tufte, E. R. (2006). Beautiful evidence (Vol. 1). Cheshire, CT: Graphics Press. 4.1 ¿Por qué quiero visualizar mis datos? Ya aprendiste a manejar comandos del tidyverse y tienes ganas de, rápidamente, sumergirte en el mundo de los gráficos y aplicar el conocimiento adquirido en tu propia base de datos. Con el tidyverse y ggplot2 el manejo de datos en R se simplifica de una manera impresionante, pero hay pasos anteriores que debes realizar antes de empezar a escribir tu código. Por ejemplo, conocer tus variables. ¿Son variables continuas o categóricas?Y si es categórica ¿tiene dos o más niveles?, y esos niveles ¿están ordenados o no?. Y estas no son las únicas preguntas. Parece muy simple, pero si no tienes considerado esto previo a tu trabajo con ggplot2 las cosas se pueden poner feas muy rápido. Ejemplos de esto hay en accidental aRt. Pregunta rápida: ¿por qué representamos gráficamente nuestros datos? Primero, sé que a muchos nos interesa representar nuestros datos gráficamente porque es una forma atractiva de hacerlo. Pero tener buen o mal gusto no significará una real diferencia si nuestros datos no están expresados de la forma correcta. Por lo tanto, es necesario entender qué queremos expresar y eso, a veces, puede ser una tarea titánica si no comprendemos por qué estamos haciendo este tipo de representación. A veces, podemos usar tablas para establecer cantidades y/o patrones, pero el manejo de datos en la actualidad convierte esto es una tarea compleja e ineficiente. Por eso, volvamos a la pregunta central: ¿por qué visualizar? ¿por qué no simplemente hacer tablas que expresen lo que queremos decir? A través de la visualización podemos entender otro tipo de problemáticas que los números por sí solos no nos muestran. Al visualizar, queremos explorar y entender nuestros datos. Graficar, además, nos puede ayudar con la interpretación de patrones, tendencias, distribuciones, etcétera. No son pocos los ejemplos de cómo la visualización ayudó a detectar un problema en un momento dado. Es más, la visualización nos puede ayudar a generar una reacción en nuestro receptor, y muchas veces es esta reacción la principal razón por la cual buscamos otra forma de ilustrar nuestros datos. Figura 4.1: Estatista Florence Nightingale (1820-1910). Florence Nightingale (1820-1910) fue una enfermera y estadista que ayudó a reorganizar la administración de los hospitales cívicos y militares en Gran Bretaña. Ella, con ayuda de un equipo, logró hacer un registro de las muertes y enfermedades en los hospitales militares en la Guerra de Crimea. Para su sorpresa, la gran mayoría de las muertes eran evitables, y la gran causa de ellas eran las condiciones paupérrimas del sistema de salud. Uno de los reportes hechos para el gobierno de Gran Bretaña era el siguiente gráfico: Figura 4.2: Diagrama sobre causas de mortalidad de la armada. En el primer gráfico, podemos ver las causas principales de mortalidad de la armada británica. En rojo, muerte por heridas de guerra, en azul, enfermedades prevenibles y en negro, otro tipo de causas. Este gráfico no sólo nos entrega información cuantitativa de las muertes, también señala un problema sustantitivo del sistema de salud militar en ese entonces. Figura 4.3: Gráfico de barras diferenciando la mortalidad de los soldados británicos y los civiles. Ambos gráficos dieron a conocer el problema, lo cual fue el pie inicial para la formulación de reformas. Así, la visualización se convierte en una herramienta aplicable a todos los pasos de la investigación. Es importante en un primer momento para la exploración de los datos y entender cómo se relacionan entre ellos, sus distribuciones y frecuencias. Luego, la visualización nos es útil para mostrar al lector posibles tendencias o patrones en los datos. Finalmente, la visualización es una gran herramienta de difusión del conocimiento. Pero recuerda: un gran poder conlleva una gran responsabilidad, y las relaciones espurias dejan de ser entretenidas cuando las personas se las creen. Aun así, siempre es entretenido ver cómo existe una correlación entre el consumo de queso per cápita y el número de personas que mueren estranguladas por sus sábanas en Estados Unidos. James E. Monogan III, (2015) ya resumía de forma muy simple, y para cientistas sociales, por qué la visualización de datos es importante al momento de trabajar con datos cuantitativos. En la introducción del capítulo, Monogan en simples líneas nos habla de la importancia y la ventaja que conlleva trabajar con imágenes, desde la simple distribución de las variables, sus outliers o sesgos; hasta el cambio de ellas a través del tiempo. Por esta razón, la visualización de datos es una herramienta fundamental para cualquiera que trabaje con datos. No es, por ningún motivo, un “movimiento estético”; graficar es extremadamente útil. Aun así, para algunos, la visualización de datos es tanto un elemento funcional al análisis como un elemento estético por excelencia. Para Edward R. Tufte, (2006) visualizar datos de forma efectiva tiene un componente artístico inevitable. Con formación de estadista y Doctor en Ciencia Política de la Universidad de Yale, Edward Tufte se dedicó a entender y explicar cómo la ciencia y el arte tienen en común la observación a ojos abiertos que genera información empírica. Su libro Beautiful Evidence describe el proceso de cómo ver se transforma en mostrar, y cómo la observación empírica se transforma a su vez en explicaciones y evidencia. Figura 4.4: Portada de ‘Beautiful Evidence’. Tenemos que entender que la visualización de datos es un lenguaje como cualquier otro. Como emisores, tenemos que conocer a nuestra audiencia: quiénes son los receptores del mensaje, si es un público experto o simplemente gente de a pie. En cualquier circunstancia nosotros adecuaríamos nuestro mensaje al tipo de receptor. Lo mismo sucede cuando visualizamos datos. Los gráficos que hacemos tienen que adecuarse a nuestro público. Pero incluso con la gente más entendida no hay que abusar: no se trata de aplicar todo lo que sabemos a la vez, sino de entender qué estamos tratando de proyectar. Por lo tanto el código –entendido como los signos y reglas– tiene que tener un sentido e insertarse en un contexto específico. Entender las funciones del lenguaje es fundamental. Tanto el código como el canal tienen que ser adecuados para que todos entiendan lo que estoy intentando mostrar con mis datos. Por eso, vale preguntarse: ¿Mis datos están siendo bien representados? ¿Escogí el tipo de gráfico adecuado para mis datos? Este tipo de representación ¿es efectiva? ¿Estoy comunicando lo que quiero comunicar? Los elementos visuales ¿se entienden? ¿Se leen de forma correcta? El buen uso de las funciones del lenguaje no sólo sirve para rendir y aprobar exámenes escritos, sino que también es muy útil para entender que el mensaje no se construye sólo con código. Hay mucho más detrás de eso. En el siguiente ítem, hablaremos de cómo funciona ggplot2 para entenderlo un poco más. De ahí en adelante, empezaremos con lo práctico. Para empezar, los tipos más comunes de representaciones visuales como lo son el histograma, el gráfico de barras, de densidad, de línea, entre otros. Además, introduciremos otros paquetes de utilidades para hacer gráficos más sofisticados. Finalmente, veremos algunos otros paquetes que nos pueden ser útiles dentro de las ciencias sociales, y la ciencia política en particular, como lo son sf y ggparliament. Si quieres aprender más sobre visualización de datos, revisa Data Visualization: A Practical introduction de Kieran Healy, un libro disponible de forma gratuita, que es entretenido y didáctico para enseñar ggplot2 paso a paso. En este libro no sólo encontrarás la parte teórica, sino también la práctica. Por otro lado, la página From Data to Viz puede hacerte de ayuda para saber cómo quieres presentar tus datos, pero no sólo eso: ya sea que trabajes con R o Python, puedes encontrar los paquetes y códigos para su aplicación. 4.2 Primeros pasos Ahora que entendemos el proceso previo a la construcción del gráfico, tenemos que familiarizarnos con ggplot2. A Layered Grammar of Graphics de Hadley Wickham, explica de forma detallada el funcionamiento de esta nueva “gramática” para hacer gráficos. Si dominas inglés, te recomiendo que leas de primera fuente cómo se pensó este paquete y cómo entender el uso de las capas en la construcción de los gráficos. A pesar de que el uso de ggplot2 se expandió rápidamente, dentro de la comunidad de R hay constantes discusiones sobre la enseñanza de ggplot2 como primera opción, por sobre los gráficos de R base. Por ejemplo, David Robinson en su blog tiene diferentes entradas sobre este tema, en las cuales expone de forma contundente las ventajas de ggplot2 por sobre las otras opciones. Si recién estás empezando a familiarizarte con R, empezar con ggplo2 te brindará herramientas muy poderosas y la curva de aprendizaje no es tan elevada como lo requeriría R base. Algunas de las ventajas que menciona David Robinson en “Why I use ggplot2” son: ¡Leyendas! R base requiere de más conocimientos de los usuarios para poder hacer leyendas en los gráficos. Nuestro amigo ggplot2 las hace automáticamente. ¡Faceting! Suena extrañísimo, pero no encontré una traducción que le hiciera justicia. Básicamente, podemos crear subgráficos con terceras o cuartas variables que nos permita entender mejor el comportamiento de nuestros datos. Trabaja en conjunto con el tidyverse. Eso quiere decir que podemos hacer más por menos. Al final del capítulo entenderás a lo que me refiero. Hay atajos para todo. Estéticamente, es mejor. Ni hablar de las miles de opciones de paletas cromáticas, temas, fuentes, etc. Si no te gusta, existe una forma de cambiarlo. Teniendo esto en consideración, comencemos con lo práctico. 4.2.1 Las capas del multiverso ggplotidiano Entremos al tema que nos interesa: ¿Cómo funciona ggplot2? Este paquete viene incluido en el tidyverse, por lo que no es necesario cargarlo de forma separada. Además, usaremos las herramientas de ambos paquetes durante todo el capítulo. El primer paso entonces es cargar el paquete: library(tidyverse) La intuición detrás deggplot2 es muy simple. La construcción de los datos se hace en base a capas que contienen cierto tipo de información. 4.2.1.1 Datos La primera capa son los datos que utilizaremos. Para hacer esto un poco más demostrativo, cargaremos los datos que usaremos a través de todo el capítulo. library(paqueteadp) data(municipios_chile) Ahora la base se ha cargado en nuestra sesión de R: ls() ## [1] &quot;municipios_chile&quot; Estos datos corresponden a información sobre las municipalidades chilenas. Algunos son del Servicio Electoral y otros del Sistema Nacional de Información Municipal de Chile. En la primera base encontramos los resultados de las elecciones locales, regionales y nacionales del país; y en la segunda, encontramos características económicas, sociales y demográficas de los municipios chilenos. En este caso, contamos con los datos electorales comunales desde 1992 al 2012, y además con datos descriptivos como la población, los ingresos totales de la municipalidad, el gasto en asistencia social y el porcentaje de personas en situación de pobreza según el total comunal de la Encuesta de Caracterización Socioeconómica Nacional, CASEN. str(municipios_chile) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 1011 obs. of 6 variables: ## $ year : Factor w/ 6 levels &quot;1992&quot;,&quot;1996&quot;,..: 4 4 4 4 4 4 4 4 4 4 ... ## $ zona : Factor w/ 5 levels &quot;Norte Chico&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ comuna : chr &quot;Alto Hospicio&quot; &quot;Arica&quot; &quot;Camarones&quot; &quot;Camina&quot; ... ## $ genero : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 2 1 1 1 1 1 1 1 ... ## $ ingresos: int 1908611 12041351 723407 981023 768355 558068 768544 16768224 1390094 2396778 ... ## $ casen : num NA 23.5 10.6 37.3 58.3 ... Al mirar la base, encontramos que hay datos continuos (numéricos) y categóricos (factores). Saber qué tipo de variable manejamos es esencial para el siguiente paso. 4.2.1.2 Estética La segunda capa corresponde al mapeo de las variables dentro del espacio. En este paso, utilizamos mapping = aes(), el cual contendrá la variable que tendremos en nuestro eje x y nuestro eje y. Para aes() hay muchas más opciones que iremos viendo dentro del capítulo: algunas de ellas son, por ejemplo, fill, color, shape, y alpha. Todas estas opciones son un conjunto de signos que nos permitirán traducir de mejor manera lo que queremos decir a través de nuestro gráfico. Normalmente, a estas opciones se le llaman aesthetics o aes(). ggplot(data = municipios_chile, mapping = aes(x = year, y = casen)) El resultado muestra un cuadro vacío. Eso es porque no lo hemos dicho qué objeto geométrico es el que usaremos. 4.2.1.3 Objeto geométrico Suena extraño, pero cuando hablamos de objeto geométrico o geom, estamos hablando del tipo de gráfico que queremos hacer, ya sea un gráfico de línea, uno de barra, un histograma, un gráfico de densidad, o de puntos, o si queremos hacer un boxplot. Esta es la tercera capa. En este caso, como tenemos los datos de la encuesta CASEN, haremos un boxplot para ver cómo se distribuyen los municipios de nuestra muestra. ggplot(data = municipios_chile, mapping = aes(x = year, y = casen)) + geom_boxplot() Lo primero que notamos es la ausencia de datos para tres periodos. Lamentablemente, en el SINIM no hay datos anteriores al 2002, por lo que no hay registros para esos años. Por este motivo, parece una buena idea filtrar y dejar solo los años que tengan datos sobre la encuesta CASEN. Además de eso, nuestro gráfico no nos dice mucho sobre el porcentaje de pobreza y su distribución. Considerando la geografía chilena, sería una buena idea que vieramos la distribución por zona. 4.2.1.4 Faceting Ahora ocuparemos nuestras habilidades para hacer dos cosas: primero, ocuparemos filter para dejar sólo los años que nos interesan. Segundo, dividiremos los resultados por zona usando facet_wrap, que corresponde a la cuarta capa que podemos usar para armar un gráfico con ggplot. No siempre es necesaria, pero siempre es útil mostrar lo que puede lograr. Cuando utilizamos esta capa, lo que buscamos es organizar las geometrías que estamos utilizando en función de una variable categórica. En este caso, zona. Pero el faceting como acción es mucho más que esto. facet_wrap y facet_grid pueden tomar una serie de argumentos, donde el primero es el más importante. La sintaxis que usamos en este caso es la usada para fórmulas en R, y denotamos el primer argumento con el signo “~”. Con nrow y ncol podemos especificar cómo queremos ordenar nuestro gráfico. Finalmente, agregamos dos líneas de código, una para filtrar y otra para subdividir nuestra información. Esto es lo que logramos: ggplot(data = municipios_chile %&gt;% filter(year == c(2004, 2008, 2012)), mapping = aes(x = year, y = casen)) + geom_boxplot() + facet_wrap(~ zona, nrow = 1) Tanto con facet_wrap como con facet_grid podemos usar más de un argumento, pero los resultados son distintos. facet_grid no sólo ordena las geometrías, sino que es capaz de cruzarlas creando gráficos con dos o más dimensiones utilizando variables categóricas. Observen el siguiente ejemplo: facet_wrap ggplot(data = municipios_chile %&gt;% filter(year == c(2004, 2008, 2012)), mapping = aes(x = year, y = casen)) + geom_boxplot() + facet_wrap(zona ~ genero) facet_grid ggplot(data = municipios_chile %&gt;% filter(year == c(2004, 2008, 2012)), mapping = aes(x = year, y = casen)) + geom_boxplot() + facet_grid(zona ~ genero) Este gráfico nos muestra que, por zona, el porcentaje de pobreza ha variado considerablemente desde el 2004 al 2012 y que hay una alta variabilidad intraregional. Además, nos muestra cómo ggplot entrega resultados de calidad sin mayores complejidades. La función facet_wrap es una capa opcional dentro de las múltiples capas de “A Layered Grammar of Graphics”, pero es importante recordar que las otras tres capas deben estar presentes para cualquier tipo de resultado. 4.2.1.5 Transformaciones Otra capa que puedes utilizar es una capa que te permitirá hacer transformaciones de escala en tus variables. Normalmente aparecerá con el nombre scale_x_discrete, la cual va variando dependiendo de la estética que estemos utilizando dentro de nuestro mapeo. Así, podremos encontrarnos con scale_fill_continuos o scale_y_log10. Por ejemplo, podemos ver cómo se distribuye el ingreso de las municipalidades según el porcentaje de pobreza de nuestra muestra. Normalmente esto lo haríamos de la siguiente manera: ggplot(data = municipios_chile %&gt;% filter(year == c(2004, 2008, 2012)), mapping = aes(x = casen, y = ingresos)) + geom_point() Teóricamente, cuando ocupamos una variable que tiene relación con dinero, le aplicamos una transformación logarítmica. Pero ¿cómo se traduce eso en nuestra imagen? ggplot(data = municipios_chile %&gt;% filter(year == c(2004, 2008, 2012)), mapping = aes(x = casen, y = ingresos)) + geom_point() + scale_y_log10() Esto es de lo que hablamos cuando hablamos de escalas. 4.2.1.6 Sistema de coordenadas Normalmente, trabajaremos con un eje x y un eje y. Existen funciones en ggplot2 como coord_flip que nos permitirá cambiar el sentido de nuestro gráfico. Pero también podemos usar este tipo de capa cuando trabajamos con datos geográficos o cuando, por ejemplo, queremos hacer un gráfico de torta. Aunque, normalmente, no queremos hacer gráficos de torta. Entre más utilices ggplot2, más aprenderás de cada una de las opciones. 4.2.1.7 Temas Cuando mapeamos los datos, usamos opciones estéticas. Cuando queremos cambiar cómo luce el gráfico, cambiamos el tema. Esto se puede hacer a través de theme, el cual te permite modificar cuestiones que no se relacionan con el contenido del grafico. Por ejemplo, los colores del fondo o el tipo de letras de los ejes. También puedes cambiar dónde se ubicará la leyenda o la ubicación del título. También, puedes cambiar el título, el nombre de los ejes, agregar anotaciones, etc. Solo necesitas conocer labs, annotate y ggtitle. Ahora, a aplicar todo lo que al parecer entendemos. 4.3 Elecciones locales y visualización de datos Como habíamos mencionado, lo principal es entender que la visualización nos sirve para explorar nuestros datos y contestar preguntas sustantivas de nuestra investigación. Muchas veces las medias, desviaciones estándar u otro tipo de parámetro no nos dicen mucho. Esos mismos datos podemos expresarlos a través de la visualización. Por ejemplo, un boxplot puede ser útil para representar la distribución de los datos que tenemos y ver sus posibles outliers, mientras que un gráfico de barras nos ayudará a ver la frecuencia de nuestros datos categóricos, y un gráfico de línea nos sirve para entender cambios en el tiempo. Y esos son sólo algunos ejemplos dentro de una variada gama de posibilidades. En esta tercera parte, aprenderemos a visualizar diferentes tipos de gráficos con los datos de reelección municipal en Chile. Para contextualizar, en Chile la divisón político-administrativa más pequeña es la comuna o municipio que cada cuatro años escoge a sus autoridades locales: un alcalde y un concejo municipal. Desde 1992 al 2000, los alcaldes fueron electos de forma indirecta, y desde el 2004 en adelante empiezan a ser electos directamente por la ciudadanía. Ya que conocemos nuestros datos, podemos empezar por lo más simple. Una buena idea, por ejemplo, es ver la cantidad de mujeres electas como alcaldesas versus el número de hombres electos. Para eso, podemos usar un gráfico de barras. Como bien vimos en el ítem anterior, para armar cualquier tipo de gráfico necesitamos conocer la o las variables que queremos usar y cuál geometría o geom nos permite representar lo que queremos conocer. En este caso, usaremos geom_bar para ver cuántos hombres y cuántas mujeres han sido electos desde 1992. 4.3.1 Gráfico de barras plot_a &lt;- ggplot(municipios_chile, mapping = aes(x = genero)) plot_a + geom_bar() Como vemos, armar un gráfico de barras es muy simple. Podemos ver que, desde el 2004, han sido electos más de 800 hombres como alcaldes, una cifra que supera largamente al número de mujeres que han sido electas para el mismo cargo en la misma cantidad de tiempo. Pero, quizás, esta cifra ha variado en el tiempo y no lo podemos ver en un gráfico de este tipo. Esta parece ser una buena razón para usar facet_wrap. plot_a + geom_bar() + facet_wrap(~year, nrow = 1) Como podemos ver, el número de alcaldesas parece aumentar. Aunque es un aumento mucho menor al esperado. Esta parece ser una problemática sustantiva al momento de hacer un análisis de gobiernos locales en Chile. Las geometrías geom_bar o geom_col, geom_density y geom_histogram no suelen llevar el eje y explicitado en las estéticas, ya que son un conteo sobre el eje x. Aun así, uno puede modificar el eje y en estas geometrías aplicando algún tipo de transformación. Por ejemplo, al especificar y = ..prop.. como estética dentro del objeto geométrico, estamos ordenando el cálculo de la proporción, no la cuenta. Normalmente, usaremos aes() en conjunto con los datos en ggplot(), pero dependiendo de la preferencia que tengas, es posible usarlo también con los geom. Esto último es más común cuando ocupamos más de una base de datos o cuando queremos hacer algún tipo de transformación. Por ejemplo, nos podria interesar el número de autoridades locales por zona geográfica. Para eso, nos sería útil usar la proporción, ya que cada zona geográfica está compuesta por una diferente cantidad de comunas. De esta forma, comparar la situación entre zonas sería más simple. plot_a + geom_bar(mapping = aes(y = ..prop.., group = 1)) + facet_wrap(~zona, nrow = 1) Pero, ¿por qué ocupamos group = 1? Cuando queremos calcular la proporción con y = ..prop.., tenemos que tener un cuidado especial si estamos usando facet_wrap. Esta función no la calcula en base a la suma de ambos géneros por zona, sino en base a sí misma. Por ejemplo, esta función registra que hay 89 hombres electos y 13 mujeres electas en el Norte Grande y concluye que “en el Norte Grande los 89 hombres corresponden al 100% de hombres electos, y las 13 mujeres corresponden al 100% de mujeres electas”. Claramente, eso no es lo que queremos lograr representar en este grafico. Por esa razón, utilizamos group = 1. Intenta ver el resultado sin group = 1 para ver qué resulta. ¡Ahora sí lo logramos! Vemos que no hay grandes diferencias, siendo el “Norte Chico” el que cuenta con más mujeres en la alcaldía en relación a los hombres, algo que podría resultar hasta contraintuitivo. A pesar de esto, no hay grandes diferencias de zona a zona y parece que se replicaran los resultados que vimos en nuestro primer gráfico de barras. Ahora, podríamos arreglar la presentación del gráfico. Todo buen gráfico debe llevar, por ejemplo, un título explicativo, la fuente de los datos y el detalle de los ejes. plot_a + geom_bar(mapping = aes(y = ..prop.., group = 1)) + facet_wrap(~zona, nrow = 1) + labs(title = &quot;Proporción de mujeres y hombres electos alcaldes (2004-2012)\\nPor zonas económicas de Chile&quot;, x = &quot;Género&quot;, y = &quot;Proporción&quot;, caption = &quot;Fuente: base de elaboración propia con datos del SERVEL y SINIM (2018)&quot;) Ahora, sólo nos falta agregar las etiquetas del eje x. Eso lo podemos hacer fácilemente con scale_x_discrete. Tienes que tener en consideración qué estética de aes() modificarás, ya que eso cambiará el scale que necesites. Si estuviéramos viendo las etiquetas de fill, tendríamos que usar scale_fill_discrete, por ejemplo. También tienes que tener en consideración qué tipo de variable estás usando. Que scale_x_dicrete tenga “discrete” al final no es una decisión aleatoria. Como comprenderás, depende totalmente del tipo de variable que estemos manejando. plot_a + geom_bar(mapping = aes(y = ..prop.., group = 1)) + facet_wrap(~zona, nrow = 1) + scale_x_discrete(labels = c(&quot;Hombres&quot;, &quot;Mujeres&quot;)) + labs(title = &quot;Proporción de mujeres y hombres electos alcaldes (1992-2012)\\nPor zonas económicas de Chile&quot;, x = &quot;Género&quot;, y = &quot;Proporción&quot;, caption = &quot;Fuente: base de elaboración propia con datos del SERVEL y SINIM (2018)&quot;) Con labels podemos cambiar las etiquetas. Ten en consideración el número de breaks de tu variable categórica para que calcen a la perfección y no te sobre (o te falte) alguna categoría. 4.3.2 Gráfico de Línea En el gráfico final de la sección anterior, vimos que si bien la elección de mujeres alcaldesas en Chile ha aumentado, este aumento no parece ser significativo: en el 2012, sólo un 13% de los alcaldes electos eran mujeres. Quizás esto puede deberse a que los cambios socioeconómicos no han repercutido en la percepción sobre los roles de género en la sociedad. Tal vez, mirar los datos económicos de ingresos municipales o de porcentaje de pobreza según la CASEN nos ayuden a entender por qué no ha aumentado sustantivamente la elección de mujeres en las elecciones municipales. Para esto podemos usar geom_line, el objeto geométrico que nos permitirá ver la evolución en el tiempo de nuestro objeto de estudio. La intuición sería hacer la figura de esta manera: plot_b &lt;- ggplot(data = municipios_chile, mapping = aes(x = year, y = ingresos)) plot_b + geom_line() El problema es que no nos entrega el resultado esperado. La intuición es correcta, pero nosotros tenemos que ayudar a geom_line con ciertas especificaciones. geom_line agrupa las observaciones para crear el gráfico de línea. En este caso, las agrupa por lo que cree tiene más sentido: el año. Por esta razón, tenemos que especificar cuál es la variable que agrupa toda la información y, como sabemos, la información que tenemos está agrupada por municipio. Cuando agregamos esta información como geom_lines(aes(group = comuna)), el resultado cambia y se asemeja a lo que buscábamos: plot_b + geom_line(mapping = aes(group = comuna)) Uno de los problemas que surge a primera vista es que, considerando que Chile tiene 345 comunas, parece imposible tenerlas todas en un solo gráfico. Ahora, podemos separar el gráfico como lo habíamos hecho anteriormente. Se puede hacer por zona o por región, pensando en cuál resultado te interesa más. Ya que hemos visto diferentes resultados por zonas, sería interesante ver el ingreso de la misma manera. plot_b + geom_line(aes(group = comuna)) + facet_wrap(~zona, nrow = 1) Como son pocos los años que tenemos en la muestra, no podemos ver mucha variabilidad y a primera vista, parece que los ingresos de todos los municipios han incrementado considerablemente. Quizás, podemos seguir mejorando nuestro gráfico. Probablemente, no estés muy familiarizado con la notación científica y te sientes más cómodo leyendo números más grandes. Quizás sabes que es mejor trabajar todo tipo de variable monetaria con su transformación logarítmica, como nos han enseñado en diferentes cursos de métodología. Puede, también, que quieras agregar otro tipo de información a este gráfico, como por ejemplo, las medias. ¿Qué te parece el siguiente gráfico? medias &lt;- municipios_chile %&gt;% group_by(zona) %&gt;% summarise(mean = mean(ingresos, na.rm = T)) plot_b + geom_line(color = &quot;gray70&quot;, aes(group = comuna)) + geom_hline(aes(yintercept = mean), data = medias, color = &quot;dodgerblue3&quot;) + scale_x_discrete(expand = c(0,0)) + scale_y_log10(labels = scales::dollar) + facet_wrap(~ zona, nrow = 1) + labs(title = &quot;Ingresos municipales en años electorales (2004-2012)&quot;, y = &quot;Ingresos&quot;, x = &quot;Años&quot;) + theme(panel.spacing = unit(2, &quot;lines&quot;)) ¿Qué especificamos? Primero, creamos una base de datos (“medias”) que contiene los promedios de ingresos por cada zona. Esto lo hicimos utilizando group_by y summarise del tidyverse. municipios_chile %&gt;% group_by(zona) %&gt;% summarise(mean = mean(ingresos, na.rm = T)) ## # A tibble: 5 x 2 ## zona mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 Norte Chico 4816249. ## 2 Norte Grande 7167984. ## 3 Zona Austral 2609648. ## 4 Zona Central 7302625. ## 5 Zona Sur 3219110. Especificamos el color de geom_line. Agregamos a nuestro código geom_hline. Este objeto geométrico, como geom_vline o geom_abline, nos sirven para agregar líneas con información. En este caso, lo usé para agregar el promedio de ingresos de cada zona. Especificamos la variable que contiene los promedios yintercept = mean, de la base medias, y además, especificamos el color con color = &quot;dodgerblue3&quot;. Usamos scale_x_discrete para especificar la expansión de los paneles. Si antes se veía un espacio gris sin información, lo sacaremos. Esto es estético. Utilizamos scale_y_log10 para escalar nuestros datos. Como los presentábamos, no lográbamos ver más allá de aquellos municipios con un altísimo ingreso, mientras que las demás comunas quedaban apiladas en el fondo del panel. Esta es una transformación logarítmica que normalmente se hace cuando trabajamos modelos lineales que contienen datos monetarios. Además, cambiamos las etiquetas del eje y: ya no aparece la notación científica. Esto se hace con un paquete llamado scales. Aquí llamamos directamente la función con scales::dollar. Agregamos el título y los nombres del eje x y eje y con labs. Especificamos información del tema. Sin él, los años entre un panel y otro chocarían. Para eso, especificamos con panel.spacing = unit(2, &quot;lines&quot;) en la capa theme. 4.3.3 Boxplot Ya vimos que los ingresos de los municipios crecieron entre el 2004 y el 2012. Cuando observamos el gráfico sin la transformación funcional, notamos que habían algunas comunas que tenían ingresos muy por sobre el resto y sobresalían dentro de sus zonas. La intuición nos dice que posiblemente sean outliers. Podríamos ver esto más claramente con un boxplot, el cual nos sirve para graficar diferentes datos descriptivos de nuestras variables como son la mediana, el mínimo y el máximo. En este caso, lo utilizaremos para ver si nuestra intuición era acertada o no. Filtramos los datos como lo hicimos con el gráfico anterior. En nuestro eje x pondremos las zonas de Chile y en el eje y los ingresos. Además, ocuparemos otro tipo de estética: color, la cual usaremos para identificar de mejor manera cada zona. Propiedades estéticas como fill, color, size, cambian al ser utilizadas con variables discretas o continuas. Este es el resultado que trabajaremos: plot_c &lt;- ggplot(data = municipios_chile %&gt;% filter(year %in% c(2004, 2008, 2012)), mapping = aes(x = zona, y = ingresos, color = zona)) + geom_boxplot() + facet_wrap(~year, ncol = 1) plot_c Uno de los problemas que podríamos tener con este gráfico, es que no nos permite observar bien los outliers, ya que la expansión del eje y es muy pequeña. Para solucionar esto podemos usar coord_flip, una función que nos permite dar vuelta los ejes de nuestro gráfico: plot_c + coord_flip() Ahora ya podemos observar mejor algunos de los outliers presentes. Quizás, después de ver estos resultados, nos gustaría identificar qué comunas son las que reciben más ingresos totales. Para eso podemos usar otra estética, label dentro de geom_text. Para nombrar sólo los outliers, tenemos que hacer un subset de los datos. plot_c + coord_flip() + geom_text(data = municipios_chile %&gt;% filter(ingresos &gt; 50000000), mapping = aes(label = comuna)) Lamentablemente, las etiquetas están por encima de los puntos y, en algunos casos, se pierden cuando estos están muy juntos. Una de las soluciones es con el paquete ggrepel que tiene el elemento geométrico geom_text “mejorado” para que las etiquetas no choquen entre sí. También, cambiaremos el color de las letras para que sea posible leerlas sin mayor dificultad. Como ven, este color va afuera de la estética de geom_text_repel, ya que definimos el color para todo el objeto. Cuando va dentro de aes(), el color se modifica según la candidad de, por ejemplo, ingresos o por el tipo de, por ejemplo, zona. library(ggrepel) plot_c + coord_flip() + geom_text_repel(data = municipios_chile %&gt;% filter(ingresos &gt; 50000000), mapping = aes(label = comuna), color = &quot;black&quot;) El corte puede ser en los $50.000.000 o en números más grandes o más pequeños. Depende completamente de lo que queremos observar. Además, con geom_text o geom_text_repel no solo puedes modificar el color, sino también el el tipo de fuente de la letra, o si debe estar en negrita, cursiva o subrayada. Para ver más opciones, debes ingresar ?geom_text o llamar a help(&quot;geom_text&quot;). También podríamos agregar otro tipo de información o cambiar cómo está presentado lo que ya tenemos. plot_c + coord_flip() + geom_text_repel(data = municipios_chile %&gt;% filter(ingresos &gt; 50000000), mapping = aes(label = comuna), color = &quot;black&quot;, size = 3) + scale_y_continuous(labels = scales::dollar) + labs(title = &quot;Ingresos de las municipalidades según zona (2004-2012)&quot;, x = &quot;Ingresos&quot;, y = &quot;Zona&quot;, caption = &quot;Fuente: Elaboración propia en base a datos del SINIM (2018)&quot;) + guides(color = F) Otras especificaciones: Agregamos la información descriptiva del gráfico. Cambiamos el tamaño de la letra. Esto era importante por la cantidad de comunas que están por sobre los $50.000.000 en ingresos. Nuevamente, cambiamos las etiquetas del eje y con scales::dollar. Por último, con guides y especificando el aes() que buscamos afectar, escribimos el código color = F para eliminar la leyenda, ya que era información que se repetía dentro del gráfico que realizamos. Te invito a jugar con geom_text: cambia los colores, el tamaño, la fuente, etc. También, te insto a instalar paquetes que te permitirán personalizar aun más tus gráficos: ggthemes de jrnorl tiene temas para gráficos de programas y revistas conocidas como Excel o The Economist. Por otro lado, hrbrthemes de hrbrmstr ha elaborado algunos temas minimalistas y elegantes que harán que todos los gráficos que hagas luzcan mejor. Si tienes algo por los colores, puedes mirar el paquete wespalette de karthik una paleta cromática basada en las películas de Wes Anderson o crear tus propias paletas según imágenes con colorfindr. Puedes averiguar más sobre este último en el siguiente vínculo. 4.3.4 Histograma Según lo que pudimos observar en nuestro boxplot, muchas comunas –especialmente de la zona central–, están muy por encima de los ingresos medianos por zona. Podríamos ver la distribución de estos datos a través de un histograma. Hacer un histograma es muy fácil, y como lo había mencionado anteriormente, geom_histogram no lleva el eje y de forma explícita ya que cuenta la frecuencia de un evento dentro de cierto intervalo. Cuando creamos el histograma según nuestra intuición, este es el resultado: ggplot(data = municipios_chile, mapping = aes(x = ingresos)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 7 rows containing non-finite values (stat_bin). Como podemos observar, el gráfico nos tira un “Warning” que nos indica que hay “738 filas que contienen valores no-finitos”. Esta advertencia se ha repetido constantemente durante el capítulo, y no quiere decir nada más que “Hay valores 0 o desconocidos dentro de esta variable” y se debe, como sabemos, a que los primeros periodos no cuentan con información. Así que tranquilo, si filtráramos los datos con filter(!is.na(ingresos)) lo más seguro es que la advertencia desaparezca. También, la consola nos dice este mensaje: stat_bin() using bins = 30. Pick better value with binwidth. Simplemente, nos dice que es posible modificar el número de intervalos. Lo siguiente que haré será modificar el eje x. Nunca se me ha dado bien leer los números con la notación científica. Por otro lado, probaremos cambiando el número de intervalos con bins. ggplot(data = municipios_chile, mapping = aes(x = ingresos)) + geom_histogram(bins = 50) + scale_x_continuous(labels = scales::dollar) ## Warning: Removed 7 rows containing non-finite values (stat_bin). ¿Qué pasa cuando ponemos bins = 15 intervalos? Lo que haremos a continuación es hacer un subset de los datos. Teniendo en consideración el número de outliers que nos encontramos, eliminaremos los municipios que tienen ingresos mayores a los $50.000.000. También podemos ver la frecuencia por zona. Al igual que cuando ocupamos color con geom_boxplot, ocuparemos fill con geom_histogram. ggplot(data = municipios_chile %&gt;% filter(ingresos &lt; 50000000), mapping = aes(x = ingresos, fill = zona)) + geom_histogram(alpha = 0.5, bins = 50) + scale_x_continuous(labels = scales::dollar) + labs(title = &quot;Número de municipalidades según sus ingresos anuales (2004-2012)&quot;, x = &quot;Ingresos&quot;, y = &quot;Número de municipios&quot;, caption = &quot;Fuente: Elaboración propia en base a datos del SINIM (2018)&quot;) 4.3.5 Relación entre variables Probablemente una de tus mayores preocupaciones es si las dos variables que estás estudiando se relacionan de algún modo. Con ggplot esto es muy simple de comprobar. En este caso, tenemos dos variables continuas: el porcentaje de pobreza de la CASEN y los ingresos municipales. Según la teoría, debería existir un tipo de relación: a mayor ingreso municipal, menor debería ser el porcentaje de pobreza en la comuna. Creamos nuestros datos: plot_f &lt;- ggplot(data = municipios_chile, mapping = aes(x = casen, y = log(ingresos))) Para este tipo de gráfico, utilizamos geom_smooth. Con este objeto, puedes modificar la forma en que se relacionan las variables a través de method. Incluso puedes poner tu propia fórmula. Por defecto, viene especificada una relación lineal entre las variables, así que no es necesario escribirlo. plot_f + geom_smooth(method = &quot;lm&quot;, color = &quot;dodgerblue3&quot;) Se ve un poco vacío, ¿no? Normalmente, ocuparemos geom_smooth con otra figura geométrica geom_point, para indicar la posición de las comunas dentro del espacio. Ocuparemos alpha para que veamos la sobreposición de los puntos. Sin ser muchos, no hay problemas en ver cómo se distribuyen. plot_f + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, color = &quot;dodgerblue3&quot;) Ahora podríamos hacer dos especificaciones. Primero, pondremos el título y el nombre de los ejes. Segundo, en geom_x_continuous especificaremos donde tiene que empezar y terminar nuestro gráfico. Esto ya lo habíamos usado con geom_line. plot_f + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, color = &quot;dodgerblue3&quot;) + scale_x_continuous(expand = c(0,0)) + labs(title = &quot;Relación entre ingresos y porcentaje de pobreza CASEN, Chile (2004-2012)&quot;, x = &quot;Porcentaje de Pobreza CASEN&quot;, y = &quot;Ingresos&quot;, caption = &quot;Fuente: Elaboración propia en base a datos del SINIM (2018)&quot;) Claramente, hay una correlación negativa entre ambas variables. Era lo que esperábamos. Ahora, podemos calcular la correlación entre ambas para estar más seguros de los resultados obtenidos: cor(municipios_chile$casen, municipios_chile$ingresos, use = &quot;pairwise.complete.obs&quot;) ## [1] -0.27 La correlación entre ambas variables es de -0.27. Sería interesante agregar esta información al gráfico. Esto lo podemos realizar con annotate. Sólo tenemos que especificar el tipo de objeto geométrico que queremos generar. En este caso, lo que queremos crear es texto text, pero podría ser un cuadro resaltando un punto específico en el gráfico rect o una línea segment. Especificamos donde lo ubicaremos y, finalmente, anotamos lo que queremos anotar. plot_f + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, color = &quot;dodgerblue3&quot;) + scale_x_continuous(expand = c(0,0)) + labs(title = &quot;Relación entre ingresos y porcentaje de pobreza CASEN, Chile (2004-2012)&quot;, x = &quot;Porcentaje de Pobreza CASEN&quot;, y = &quot;Ingresos&quot;, caption = &quot;Fuente: Elaboración propia en base a datos del SINIM (2018)&quot;) + annotate(&quot;text&quot;, x = 50, y = 15, label = &quot;Correlación:\\n-0.27&quot;) 4.4 Para seguir aprendiendo Para visualizar tus datos, hay diferentes caminos. En esta entrada, pudiste conocer las principales funciones de ggplot2 un paquete del tidyverse, pero hay muchos paquetes que pueden ser de ayuda en otro tipo de visualizaciones. Si bien ggplot2 puede no tener todos los obetos geométricos que necesites, hay otros paquetes para visualizar otro tipo de datos que funcionan bajo ggplot y las capas que componen esta forma “gramatical”. 4.4.1 Otros paquetes: ####sf Permite visualizar elementos espaciales. Para ggplot actúa con geom_sf. Permite crear figuras geográficas con diferentes tipos de datos espaciales. En el capítulo 16 sobre datos espaciales, Andrea y Gabriel entregan las herramientas para trabajar con sf, sus principales funciones y directrices. Aquí puedes encontrar más detalles sobre cómo instalarlo y su funcionamiento dependiento de tu ordenador. 4.4.1.1 ggparliament Probablemente, todos los cientistas políticos deberíamos conocer este paquete. Este paquete permite hacer visualizaciones con la composición del poder legislativo. Soñado para quién trabaja con ese tipo de información. Te permite especificar el número de escaños, el color de cada uno de los partidos, y añadir diferentes características a tus gráficos. El siguiente es un ejemplo: Figura 4.5: Gráfico de barras diferenciando la mortalidad de los soldados británicos y los civiles. Aquí puedes encontrar más detalles sobre las herramientas de ggparliament. 4.4.1.2 ggraph Si estudias redes y conoces el funcionamiento de ggplot2, este paquete puede convertirse en tu nuevo mejor amigo. Está hecho para todo tipo de datos relacionales, y si bien funciona con la lógica de ggplot2, tiene su propio conjunto de objetos geométricos, facets, entre otros. Aquí puedes encontrar más información. En el capítulo de análisis de redes del libro, el 13, Ejercicios antes de continuar al próximo capítulo Ya sabes como hacer gráficos y, por básico que parezca, ahora cuentas con muchas herramientas para seguir trabajando. Pero hay algunas cosas que se quedaron en el tintero: Ya vimos cómo hacer un histograma, aun así, los gráficos de densidad suelen ser más usados para ver la distribución de una variable. Utilizando las mismas variables, haz un gráfico de densidad con geom_density(). Muchas veces los gráficos de barras suelen presentarse con la frecuencia o proporción dentro de la barra. Esto también lo podemos hacer con ggplot2. Usando geom_bar() y geom_text(), anota el número de alcaldes y alcaldesas por zona geográfica. Una pequeña ayuda: hay que hacer cálculos con tidyverse antes de agregar esa información al plot. Escogiendo un sólo año, haz un gráfico de línea con geom_smooth() que indique la relación de ingresos y el porcentaje de pobreza. Ahora, con annotate harás un recuadro que encerrará a las comunas con mayor porcentaje de pobreza y, sobre él, anotarás a las comunas que corresponden. "],
["carga.html", "Capítulo 5 Carga de bases 5.1 Introducción 5.2 Distintos formatos de bases 5.3 Múltiples bases de datos a la vez 5.4 Bases de datos tabulares grandes", " Capítulo 5 Carga de bases Por Soledad Araya y Andrés Cruz 5.1 Introducción Cargar bases de datos no es siempre una tarea fácil. Quienes crean bases de datos utilizan distintos formatos de archivo, tratando de optimizar diferentes parámetros (usabilidad por usuarios de ofimática, tamaño, etcétera). A veces, la información que necesitas está distribuida en múltiples bases de datos pequeñas. En otros casos, el problema es la magnitud de los datos, con bases de datos que, utilizando los métodos usuales de carga, atentan contra la integridad de tu computador personal. Las siguientes tres subsecciones esperan guiarte en la resolución de estos desafíos. A diferencia de los otros capítulos del libro, en este se requiere que descargues las bases de datos directamente en tu computador, sin usar nuestro paquete asociado. ¡La idea es que aprendas a cargar datos en el mundo real! Comienza creando una carpeta en tu computador, que será la carpeta del proyecto. Nosotros llamaremos a nuestra carpeta del proyecto, muy originalmente, carga-bases. Luego, descarga desde este link el archivo .zip con las bases de datos para el capítulo, y guárdalo en la carpeta que creaste antes. En Windows, por ejemplo, esto debería verse parecido a la siguiente imagen: Figura 5.1: Carpeta con archivo .zip Luego, debes descomprimir el contenido del archivo .zip. La forma exacta de hacer esto dependerá de tu sistema y sus programas, pero suele ser algo similar a Click Derecho &gt; Descomprimir aquí. Deberías tener ahora una subcarpeta llamada “datos”: Figura 5.2: Carpeta con subcarpeta ya descomprimida Revisa esta nueva subcarpeta. Están las bases de datos a utilizar en este capítulo. Como verás, aprenderemos a cargar y utilizar múltiples formatos comunes: Figura 5.3: Subcarpeta con archivos de datos ¡Casi terminamos! Como se justifica y explicamos en mayor detalle en el Capítulo 3, haremos uso de RStudio Projects para ordenar nuestro trabajo. A continuación, crea un proyecto de RStudio haciendo click en la parte superior derecha de RStudio, “Project…”, y luego “New Project”. Seleccionando “Existing Directory” en la siguiente ventana podrás vincular al proyecto la carpeta que creaste antes (en nuestro caso, carga-base): Figura 5.4: Crear nuevo proyecto de RStudio en carpeta existente. ¡Perfecto, ahora deberías tener tu nuevo proyecto para comenzar a trabajar ordenadamente, con los datos del capítulo! 5.2 Distintos formatos de bases En esta subsección aprenderás a cargar distintos formatos de bases de datos en R: separados por delimitadores, creados en R, creados con etiquetas en Stata o SPSS, y nativos de Excel. 5.2.1 Archivos separados por delimitadores (.csv y .tsv) Un archivo de texto plano (o flat file dataset en inglés) es un archivo formado sólo por texto o caracteres. Es un formato soportado por distintos programas, y muy fácil de trabajar. La única diferencia es en el tipo de separador que utilizan: (1) en el caso de .csv, el separador es una coma (,) y (2) en el caso de .tsv es separado por una tabulación (o topes) que se expresan con &lt;TAB&gt; o \\t. Por su masividad, es importante saber cómo tratar estos archivos dependiendo de sus características. 5.2.1.1 R base R base tiene funciones para leer este tipo de archivos. Quizás, más de alguna vez te has encontrado con read.table o read.csv, que es una función específica para leer archivos csv del paquete integrado utils de R. El primero debería ser suficiente para abrir cualquiera de estos archivos. Solo hay que tener especial preocupación con las características de cada uno. Como habíamos mencionado, las separaciones de estos archivos suelen ser diferentes, y esta función nos da la posibilidad de especificar el tipo de separación (“,”, “;”, “|” o &quot;) con el argumento sep =. Sin embargo, existen mejores opciones para cargar este tipo de bases de datos. Si bien read.table() puede resultar amigable cuando estás recién empezando, cuando necesitas trabajar con archivos mucho más grandes y pesados, no es suficiente. Una segunda desventaja de read.table() es que, normalmente, no suele funcionar adecuadamente con archivos que tienen caracteres especiales, ¡una gran desventaja para nosotros los hispanohablantes! Para esos casos, tenemos diferentes alternativas que presentaremos a continuación. 5.2.1.2 readr El paquete readr es el encargado de introducir una serie de funciones para leer distintos tipos de archivos. A primera vista, la diferencia entre las funciones de utils y las de readr es un guión bajo. Aun así, las funciones de readr suelen trabajar mejor con bases grandes, y su velocidad es mucho mayor al momento de leerlas. readr al igual que muchos de los paquetes ya presentados, es parte de los paquetes del núcleo de tidyverse. Así que puedes decidir si llamarlo por sí solo, o con las herramientas de su universo. library(tidyverse) ## ── Attaching packages ──────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0 ✔ purrr 0.3.1 ## ✔ tibble 2.0.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ─────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() # library(readr) # ¡esto también funcionaría, pero no cargaría lo demás! Comencemos presentando una de las fuentes de datos que ocuparemos en este capítulo, proveniente del proyecto Desiguales del PNUD, que buscó mapear las complejidades de la desigualdad en Chile ayudándose de una encuesta aplicada el año 2016. Utilizaremos una pequeña subsección de su base de datos, que contiene información para 300 encuestados y encuestadas a lo largo de 20 variables. Como pudiste revisar antes, contamos con la base de datos en seis formatos diferentes para el desarrollo de este capítulo: Figura 5.5: Subcarpeta con archivos de datos ¿Cómo cargar la base en formato .csv, entonces? Como sospecharás, comenzamos con la base más simple de cargar. Gracias a la función read_csv() de readr, solo necesitaremos poner la ruta del archivo dentro de nuestro proyecto: ## Parsed with column specification: ## cols( ## .default = col_double() ## ) ## See spec(...) for full column specifications. df_desiguales_csv &lt;- read_csv(&quot;datos/desiguales.csv&quot;) Comenzamos el nombre del objeto con “df” como abreviatura de dataframe. Para confirmar que nuestro archivo está en el ambiente como objeto, ocuparemos ls(). Con esta función puedes ver la lista de objetos que has creado o cargado. También puedes revisar esto en el panel de Environment de RStudio. Figura 5.6: Solapa de Environment, arriba a la derecha ls() ## [1] &quot;df_desiguales_csv&quot; Ahora que confirmamos que nuestro archivo cargó, necesitamos saber si es o no un objeto de clase dataframe. Como ya han visto en capítulos anteriores, en R podemos utilizar diferentes tipos de objetos (por ejemplo, vectores). Que no son siempre dataframes. Por eso es necesario revisar si lo que cargamos es o no el objeto que deseábamos. Para esto, ocuparemos la función class(). class(df_desiguales_csv) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Esta vez, comprobamos con class() que este archivo está en formato dataframe y tibble. Como seguiremos utilizando las herramientas propias del tidyverse, nos es útil trabajar con este formato. Es ideal conocer un poco más de la función para capear posibles obstáculos cuando cargas bases de datos en este formato. Para empezar, puede que tu archivo esté separado por otro tipo de separador. Esto lo puedes definir con delim = que cumple la misma función que sep = en read.table. También, cuando manejas archivos con algún tipo de digitación diferentes, resulta útil conocer locale para especificar separador de decimales y miles. Una segunda acotación: Probablemente al escribirread_csv en R vieron que también apareció sugerida la función read_csv2(). La única diferencia es el caracter con el que estamos delimitando nuestros datos: read_csv2() tiene como delimitador por defecto “;”, mientras que en read_csv() es “,”. Para archivos .tsv, está la función read_tsv(), función que no viene por defecto en el paquete utils. Las funciones más comunes para explorar las bases de datos son head(), str(), summary() y dim(). También la función glimpse() del paquete dplyr. Exploremos brevemente los datos que estamos manejando con glimpse. Las herramientas del paquete dplyr deberían estar disponibles ya, dado que ya cargaste el tidyverse antes. Ocuparemos glimpse para ver una breve panorámica de las primeras 10 columnas de nuestros datos: df_desiguales_csv %&gt;% select(1:10) %&gt;% glimpse() ## Observations: 300 ## Variables: 10 ## $ id &lt;dbl&gt; 34, 36, 70, 75, 99, 121, 122, 128, 160, 163, 166, 172,… ## $ sexo &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, … ## $ zona &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, … ## $ macrozona &lt;dbl&gt; 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, … ## $ region &lt;dbl&gt; 13, 13, 7, 7, 13, 7, 7, 13, 7, 7, 7, 5, 7, 13, 13, 13,… ## $ edad &lt;dbl&gt; 63, 52, 73, 78, 22, 51, 18, 21, 57, 41, 55, 64, 26, 70… ## $ p1_anyo &lt;dbl&gt; 1952, 1963, 1943, 1938, 1993, 1964, 1997, 1995, 1958, … ## $ p1_mes &lt;dbl&gt; 8, 7, 2, 2, 12, 11, 10, 1, 12, 4, 10, 12, 12, 10, 10, … ## $ p2 &lt;dbl&gt; 1, 1, 4, 7, 8, 1, 5, 5, 3, 3, 7, 4, 5, 1, 5, 1, 1, 5, … ## $ p3 &lt;dbl&gt; 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 1, 1, 1, 3, 1, 2, 3, … ¡Todo se ve bien! Sigamos explorando otras formas de cargar datos a R. 5.2.2 Archivos creados con R (.Rdata y .rds) Ahora centraremos nuestra atención en los archivos creados con R, que tienen por extensión .Rdata (.Rda también es válido) o .rds. Estos formatos de archivo permiten almacenar cualquier objeto de R, ya sea un vector, dataframe, matriz, lista, etcétera. Tras leer el archivo, R cargará el objeto tal como fue guardado. De esta forma, los archivos en este formato destacan por (a) su flexibilidad a la hora de qué almacenan, no estando limitados a bases de datos, y (b) su compatibilidad perfecta con R: por ejemplo, podrás estar seguro de que cada variable en una base de datos .rds será cargada en el formato que corresponde (los vectores numéricos como numéricos, los factores como factores, y así2). La diferencia entre un archivo .Rdata (o .Rda) y uno .rds es simple, pero importante: mientras que el primero puede contener cualquier número de objetos, el segundo está limitado a un objeto. A continuación aprenderás a cargar archivos creados con R, en cualquiera de estos dos formatos. 5.2.2.1 Archivos .Rdata (uno o más objetos) Los archivos .Rdata, crucialmente, pueden contener más de un objeto. Aunque esto suena conveniente, incluye una limitante: al cargar los objetos, estos automáticamente adoptarán el nombre con el que fueron creados. A modo de ejemplo, en el archivo .Rdata para la base de Desiguales hemos guardado dos objetos: el dataframe que vimos antes (ahora llamado df_desiguales_rdata) y un vector numérico con las edades de las personas encuestadas (llamado vector_edades). Para cargar el archivo basta con utilizar la función load(). load(&quot;datos/desiguales.Rdata&quot;) Como habíamos visto, puedes revisar con el comando ls() si los objetos se cargaron correctamente en la sesión de R. Si los dos objetos se cargaron apropiadamente, ambos deberían aparecer nombrados, sumándose al anterior dataframe originado desde un archivo .csv. ls() ## [1] &quot;df_desiguales_csv&quot; &quot;df_desiguales_rdata&quot; &quot;vector_edades&quot; 5.2.2.2 Archivos .rds (solo un objeto) Los archivos .rds están limitados a almacenar un solo objeto. Aunque suene menos prometedor que la flexibilidad de .Rdata, este formato destaca por su modularidad, ayudando a mantener el orden en tus archivos. Otra cualidad positiva de .rds es que la sintaxis para cargarlos es familiar, muy similar a la que ya usaste antes para cargar el archivo .csv. ¡Ahora sí puedes nombrar el objeto al crearlo! El comando es el siguiente, usando la función read_rds() (que cargamos antes con library(tidyverse)): df_desiguales_rds &lt;- read_rds(&quot;datos/desiguales.rds&quot;) Nuevamente, puedes utilizar el comando ls() o RStudio para asegurarte de que el objeto se creó sin problemas en la sesión: ls() ## [1] &quot;df_desiguales_csv&quot; &quot;df_desiguales_rdata&quot; &quot;df_desiguales_rds&quot; ## [4] &quot;vector_edades&quot; ¡Todo bien! Con la función class(), por cierto, puedes comprobar que el nuevo objeto df_desiguales_rds es, en efecto, un dataframe (¡y un tibble!). class(df_desiguales_rds) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; 5.2.3 Datos con etiquetas (Stata o SPSS) Los datos con etiquetas son comunes en las ciencias sociales, ampliamente utilizados en los archivos de SPSS (.sav) y Stata (.dta). La idea principal es relativamente simple: en el archivo se guarda información explicativa adicional a los valores de la base de datos. Por ejemplo, la variable p2 de la base de Desiguales, correspondiente a la segunda pregunta de la encuesta, podría tener asignada una etiqueta que describa la pregunta (“Estado civil”) o la registre explícitamente (“Independiente de si usted tiene o no tiene pareja actualmente, ¿podría decirme cuál es su estado conyugal o civil actual?”). Llamaremos a este tipo de etiquetas etiquetas de variable. Por otro lado, las etiquetas también pueden registrar información sobre los valores de las variables. Por ejemplo, en la misma pregunta p2, las siguientes respuestas eran posibles entre los y las encuestadas: Valor Etiqueta 1 Casado(a) por primera vez 2 Casado(a) por segunda vez o más 3 Casado(a) legalmente, pero separado de hecho 4 Divorciado 5 Soltero(a), nunca se ha casado 6 Soltero(a), pero con un matrimonio legalmente anulado 7 Viudo(a) 8 Conviviente o pareja 88 No Sabe 99 No Responde Una base de datos puede registrar simplemente los valores numéricos que registraron los y las encuestadoras, guardando en etiquetas la información explicativa. De esta forma, aunque la base de datos solo registra un “4” en la variable p2 para una encuestada, la etiqueta ayuda a identificar que identificó su estado civil como divorciada. Llamaremos a este tipo de etiquetas etiquetas de valores. A continuación, aprenderás a cargar bases de datos con etiquetas a R, provenientes de SPSS (.sav) o Stata (.dta). Esto te permitirá informar tu análisis por medios de las etiquetas, tanto de variables como de valores. Para esto necesitarás el paquete haven. Si es que ya instalaste el tidyverse, entonces tienes acceso a haven. Sin embargo, library(tidyverse) no bastará para cargarlo, debes hacerlo aparte3: library(haven) Los comandos para cargar datos con etiquetas son similares a los que utilizaste anteriormente para cargar archivos .csv y .rds. Mientras que para SPSS la función es read_spss(), para Stata es read_stata(). Carguemos inmediatamente ambas bases: df_desiguales_spss &lt;- read_spss(&quot;datos/desiguales.sav&quot;) df_desiguales_stata &lt;- read_stata(&quot;datos/desiguales.dta&quot;) Puedes, nuevamente, revisar que se crearon los objetos utilizando ls() o el panel “Environment” de RStudio. Además, puedes chequear que, en efecto, son dataframes y tibbles: ls() ## [1] &quot;df_desiguales_csv&quot; &quot;df_desiguales_rdata&quot; &quot;df_desiguales_rds&quot; ## [4] &quot;df_desiguales_spss&quot; &quot;df_desiguales_stata&quot; &quot;vector_edades&quot; class(df_desiguales_spss) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; class(df_desiguales_stata) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Las bases de datos con etiquetas se distinguen de otras no en la clase de objeto, sino que en sus variables. Utilizando herramientas que aprendiste en el capítulo de manejo de datos, puedes explorar las diez primeras variables de cualquiera de las bases (ambas darán el mismo resultado, por lo que de ahora en adelante nos centraremos en la proveniente de SPSS). df_desiguales_spss %&gt;% select(1:10) %&gt;% glimpse() ## Observations: 300 ## Variables: 10 ## $ id &lt;dbl&gt; 34, 36, 70, 75, 99, 121, 122, 128, 160, 163, 166, 172,… ## $ sexo &lt;dbl+lbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1,… ## $ zona &lt;dbl+lbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2,… ## $ macrozona &lt;dbl+lbl&gt; 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 2, 4, 4, 4, 4,… ## $ region &lt;dbl+lbl&gt; 13, 13, 7, 7, 13, 7, 7, 13, 7, 7, 7, 5, 7, 13, 13,… ## $ edad &lt;dbl&gt; 63, 52, 73, 78, 22, 51, 18, 21, 57, 41, 55, 64, 26, 70… ## $ p1_anyo &lt;dbl+lbl&gt; 1952, 1963, 1943, 1938, 1993, 1964, 1997, 1995, 19… ## $ p1_mes &lt;dbl+lbl&gt; 8, 7, 2, 2, 12, 11, 10, 1, 12, 4, 10, 12, 12, 10, … ## $ p2 &lt;dbl+lbl&gt; 1, 1, 4, 7, 8, 1, 5, 5, 3, 3, 7, 4, 5, 1, 5, 1, 1,… ## $ p3 &lt;dbl+lbl&gt; 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 1, 1, 1, 3, 1, 2,… Como puedes notar, la mayoría de las variables no son solo vectores numéricos (“dbl” o “double”4), sino que también incluyen etiquetas (+ “lbl” o “label”, etiqueta en inglés). Ocupando el comando head(), obtengamos los seis primeros valores y más información de nuestra variable etiquetada p2: head(df_desiguales_stata$p2) ## &lt;Labelled double&gt;: P2 - Â¿podria decirme cual es su estado conyugal o civil actual? ## [1] 1 1 4 7 8 1 ## ## Labels: ## value label ## 1 Casado(a) por primera vez ## 2 Casado(a) por segunda vez o mÃ¡s ## 3 Casado(a) legalmente, pero separado de hecho ## 4 Divorciado ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 6 rows ] Analicemos brevemente el resultado que reporta la consola. En primer lugar, se muestra nuevamente que la variable en cuestión es un vector numérico con etiqueta (“”). Luego, tras dos puntos, se registra la etiqueta de variable: “P2 - ¿podria decirme cual es su estado conyugal o civil actual?”. A continuación se encuentran los seis primeros valores de la variable, como se requirió a través de head(). Por último se hallan las etiquetas de valores, que entregan información sobre qué significa cada número en el contexto de la variable. De esta forma, obtuvimos toda la información que registran las etiquetas para esta base. Con los comandos que viste anteriormente puedes tener claridad de qué significa cada valor en tu base de datos etiquetados. Para terminar, nota que R, con la ayuda de haven, también mostrará las etiquetas en otros casos, cuando sea conveniente. Por ejemplo, a continuación veamos un resumen simple de las dos primeras variables de la base de datos. ¡Nota lo fácil que es de leer! df_desiguales_stata %&gt;% select(region, p2) ## # A tibble: 300 x 2 ## region p2 ## &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; ## 1 13 [Metropolitana de Santia… 1 [Casado(a) por primera vez] ## 2 13 [Metropolitana de Santia… 1 [Casado(a) por primera vez] ## 3 7 [Del Maule] 4 [Divorciado] ## 4 7 [Del Maule] 7 [Viudo(a)] ## 5 13 [Metropolitana de Santia… 8 [Conviviente o pareja] ## 6 7 [Del Maule] 1 [Casado(a) por primera vez] ## 7 7 [Del Maule] 5 [Soltero(a), nunca se ha casado] ## 8 13 [Metropolitana de Santia… 5 [Soltero(a), nunca se ha casado] ## 9 7 [Del Maule] 3 [Casado(a) legalmente, pero separado de … ## 10 7 [Del Maule] 3 [Casado(a) legalmente, pero separado de … ## # … with 290 more rows 5.2.4 Archivos de Excel Si bien la mayoría del tiempo utilizaremos bases de datos que vienen en los formatos anteriores, tenemos que tener en consideración que no todas las instituciones presentan su información así. Muchas veces tendrás que enfrentarte a formatos que generan dolores de cabeza. Por ejemplo, Excel. En Chile, muchas organizaciones gubernamentales aún trabajan con Excel y el problema en sí no es el formato, sino la estructura usual de las bases. La mayoría del tiempo nos enfrentaremos a algo así: Figura 5.7: Base de datos del Sistema Nacional de Información Municipal (SINIM) O así: Figura 5.8: Base de datos del Centro de Estudios y Análisis del Delito (CEAD) Es en ese momento donde empiezan los problemas. Para empezar, carguemos uno de los paquetes más usados para leer archivos de Excel (.xls o .xlsx) es readxl, un paquete del tidyverse5: library(readxl) Para el siguiente ejemplo, ocuparemos la base de datos del Centro de Estudias y Análisis del Delito (CEAD). Esta base es la que, por defecto, se puede bajar desde su página web. Para cargar el archivo, ocuparemos read_excel() de readxl. Sólo es necesario poner la ruta de acceso al archivo .xls o .xlsx: ## New names: ## * `` -&gt; `..3` ## * `` -&gt; `..4` ## * `` -&gt; `..5` ## * `` -&gt; `..6` ## * `` -&gt; `..7` df_cead_excel &lt;- read_excel(&quot;datos/cead.xls&quot;) Primero, revisamos que los datos se hayan cargado de forma correcta en el Environment con ls(). Luego, ocuparemos glimpse para tener un “vistazo” de las primeras observaciones en nuestra base de datos: ls() ## [1] &quot;df_cead_excel&quot; &quot;df_desiguales_csv&quot; &quot;df_desiguales_rdata&quot; ## [4] &quot;df_desiguales_rds&quot; &quot;df_desiguales_spss&quot; &quot;df_desiguales_stata&quot; ## [7] &quot;vector_edades&quot; df_cead_excel %&gt;% glimpse() ## Observations: 79 ## Variables: 7 ## $ Medida &lt;chr&gt; &quot;Tipo de Datos&quot;, NA, &quot;Unidad Territorial&quot;, &quot;Regiones&quot;… ## $ Frecuencia &lt;chr&gt; &quot;Casos Policiales&quot;, NA, NA, &quot;Región Metropolitana&quot;, &quot;… ## $ `..3` &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `..4` &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `..5` &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `..6` &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `..7` &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… Algo no se ve bien. Si miramos nuevamente la imagen de la base de datos del CEAD, nos daremos cuenta qué está ocurriendo. Para empezar, el archivo Excel tiene en sus filas iniciales información de la base, pero estas no son observaciones. Figura 5.9: Base de datos del Sistema Nacional de Información Municipal SINIM. Para solucionar esto, es necesario utilizar skip =. Este argumento nos ayudará a saltarnos las filas que no nos interesan, en este caso, 18: ## New names: ## * `` -&gt; `..1` df_cead_excel_v2 &lt;- read_excel(&quot;datos/cead.xls&quot;, skip = 18) Otra forma de hacer lo mismo, es delimitar la información que queremos cargar a través del rango. Así, demarcamos el rango con range y obtendremos sólo la información dentro del rectángulo que especifiquemos. El rango lo delimitamos con la letra de la columna y el número de fila del archivo .xls o .xlsx. ## New names: ## * `` -&gt; `..1` df_cead_excel_v3 &lt;- read_excel(&quot;datos/cead.xls&quot;, range = &quot;A20:G81&quot;) Verificamos que se haya cargado correctamente con ls() y luego vemos el tipo de archivo con class(). Para efectos del capítulo, ocuparemos sólo la última versión del archivo .xls cargado (df_cead_excel_v3). ls() ## [1] &quot;df_cead_excel&quot; &quot;df_cead_excel_v2&quot; &quot;df_cead_excel_v3&quot; ## [4] &quot;df_desiguales_csv&quot; &quot;df_desiguales_rdata&quot; &quot;df_desiguales_rds&quot; ## [7] &quot;df_desiguales_spss&quot; &quot;df_desiguales_stata&quot; &quot;vector_edades&quot; class(df_cead_excel_v3) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Como puedes observar, es muy fácil cargar este tipo de archivos después de practicar un poco. Por ahora, eso es lo que necesitas saber para cargar archivos .xls o .xlsx. Puede que tengas que lidiar con problemas de digitación, con nombres de variables imposibles o con errores de tipeo que pueden causar más de un problema cuando trabajas con skip (¿Por qué hay tantas variables si solo eran 15 columnas? ¡Alguien metió mal el dedo!). Pero ya manejas herramientas del tidyverseque te pueden ayudar con eso. Trataremos de hacer algunos cambios básicos a la base del CEAD: names(df_cead_excel_v3) ## [1] &quot;..1&quot; &quot;2013&quot; &quot;2014&quot; &quot;2015&quot; &quot;2016&quot; &quot;2017&quot; &quot;2018&quot; El primer problema es el nombre de la columna que indica el municipio. El segundo problema es que, al menos que queramos tener una base de datos ancha, ¡No nos sirve tener los años como columnas! Para el primer problema ocupamos rename y para el segundo gather. De esta forma, nos quedamos con una base larga. df_cead_excel_v2 &lt;- df_cead_excel_v3 %&gt;% rename(county = &quot;..1&quot;) %&gt;% gather(key = year, value = n_crime, -county) Otro tipo de problema al usar archivos de Excel es que la información puede estar dividida en distintas hojas. Como verás, read_excel() se hace cargo de esto de una manera muy simple. Para demostrar esto, utilizaremos la base del Sistema Nacional de Información Municipal SINIM: df_sinim_excel &lt;- read_excel(&quot;datos/sinim.xls&quot;, sheet = 2, skip = 2, na = &quot;No Recepcionado&quot;) Con el argumento sheet = seleccionamos la hoja que queremos cargar a R. Podemos cargarla tanto con el número de la posición como con el nombre de la hoja. Como ya vimos, con skip seleccionamos el número de filas que nos saltaremos al cargar la base, y con na damos a entender qué otra frase, palabra o situación queremos catalogar como NA aparte de las celdas en blanco. Una de las tareas principales cuando trabajas con Excel es la limpieza de la base de datos. Lamentablemente, estos dos ejemplos son muy comunes, y para lidiar con algunos de estos problemas está janitor. Normalmente, los nombres de las variables en Excel suelen venir de forma detallada y/o descriptiva: pueden tener espacios, caracteres especiales y mayúsculas que dificultan trabajar fácilmente con las variables de interés. Por ejemplo, los nombres de las columnas en esta base de datos tienen tienen todo lo mencionado: names(df_sinim_excel) ## [1] &quot;CODIGO&quot; &quot;MUNICIPIO&quot; ## [3] &quot;INGRESOS MUNICIPALES ($) 2017&quot; &quot;INGRESOS MUNICIPALES ($) 2016&quot; ¿Se imaginan tener que escribir esos nombres cada vez que queramos hacer algún tipo de análisis? Para eso tenemos la función clean_names() del paquete janitor. Este es un paquete creado para facilitar la limpieza de datos y, sin ser un paquete del tidyverse, puede usarse sin problemas con los pipes. Y, para nuestra suerte, está optimizado para funcionar con readr y readxl. La función clean_names() funciona de manera simple, luego de haber instalado janitor (con install.packages(&quot;janitor&quot;): df_sinim_excel_v2 &lt;- df_sinim_excel %&gt;% janitor::clean_names() Ahora, revisemos nuevamente los nombres de las variables: names(df_sinim_excel_v2) ## [1] &quot;codigo&quot; &quot;municipio&quot; ## [3] &quot;ingresos_municipales_2017&quot; &quot;ingresos_municipales_2016&quot; ¡Esto se ve mucho mejor! Esta función hace más simple y amigable el trabajo en R, independiente de tu nivel de conocimiento y lo que busques hacer. Trabajar con los nombres de las variables puede ser un dolor de cabeza, y esta herramienta te evitará trabajar demás y así concentrarte en lo sustantivo de tu análisis (que, definitivamente, no debería ser el nombre de las columnas). Con estas herramientas que te hemos entregado, deberías estar más que preparado para enfrentarte al terrible mundo de trabajar con Excel. 5.3 Múltiples bases de datos a la vez PENDIENTE 5.4 Bases de datos tabulares grandes Las bases de datos grandes cada vez se encuentran más al alcance de los y las cientistas sociales, de la mano de los avances en los procesadores y la velocidad de las conexiones a internet. Sin embargo, las herramientas usuales de manejo de datos tabulares a menudo no funcionan correctamente en bases de datos grandes, y es necesario buscar alternativas. Qué tan complejo es lidiar con el tamaño de una base de datos depende de múltiples factores (la naturaleza de los datos y las características del computador, por ejemplo), pero, en general, podemos aventurar que una base de datos tabular de más 1 GB de tamaño le dará problemas a R en un computador personal regular. ¿Qué hacer para lidiar con este tipo de base de datos? A continuación trabajarás con la base de Desiguales en formato .csv, como lo hiciste en la subsección 5.2 de este capítulo. Asumiremos que se podría tratar de una “base de datos grandes”, aunque sus escuálidos 15 KB de tamaño digan lo contrario. Por cierto, si quieres probar a hacer los siguientes análisis con bases de datos grandes reales de interés politológico, puedes encontrar varias para descargar en el Observatorio de Complejidad Económica, que registran comercio bilateral por distintas categorías productivas6. ¿Cómo lidiar con una base de datos grande, entonces? Una primera alternativa a evaluar es si se necesita hacer uso de la base completa en el análisis, o si esta se puede hacer más pequeña antes. Para comenzar un análisis exploratorio que ayude a clarificar esto, recomendamos utilizar el argumento n_max = en read_csv() y sus funciones hermanas (por ejemplo, read_tsv()). De la siguiente forma podemos solo leer las primeras cien observaciones de la base de datos en cuestión, haciendo el proceso computacional ostensiblemente menos costoso: ## Parsed with column specification: ## cols( ## .default = col_double() ## ) ## See spec(...) for full column specifications. df_desiguales_grande_100 &lt;- read_csv(&quot;datos/desiguales.csv&quot;, n_max = 100) Seguramente tu sistema podrá manejar con facilidad esta nueva base de datos reducida. Ahora bien, ¿qué pasa si, tras revisar los datos, te das cuenta de que solo necesitas un par de variables para tu análisis? Recortar la base de datos te permitirá hacer un uso más eficiente de tus recursos computacionales. Nota que ahora el argumento n_max = no servirá, pues esta recorta filas en vez de columnas. Por ejemplo, supongamos que de la base de Desiguales (“grande” en nuestro ejemplo) solo necesitas las variables edad y p2. Con la ayuda de una función de asistencia de nuestro paquete paqueteadp (cols_only_chr()), puedes hacer que read_rds() cargue la base solo con dichas variables, obviando todas las demás y el costo computacional que ellas significan: library(paqueteadp) df_desiguales_grande_2vars &lt;- read_csv(&quot;datos/desiguales.csv&quot;, col_types = cols_only_chr(c(&quot;edad&quot;, &quot;p2&quot;))) Ahora la nueva base df_desiguales_grande_2vars tendrá todas las observaciones, pero solo se habrán cargado las dos variables requeridas, como puedes chequear con un resumen simple: df_desiguales_grande_2vars ## # A tibble: 300 x 2 ## edad p2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 63 1 ## 2 52 1 ## 3 73 4 ## 4 78 7 ## 5 22 8 ## 6 51 1 ## 7 18 5 ## 8 21 5 ## 9 57 3 ## 10 41 3 ## # … with 290 more rows ¿Qué pasa si las variables a cargar son varias, y se hace tedioso añadir todos los nombres “a mano”? Supongamos que solo quieres las primeras 15 variables de la base para tu análisis. Los siguientes comandos solucionarán el problema, obteniendo primero los nombres de las variables (names()) a partir de tu base de análisis preliminar, df_desiguales_grande_100: nombres_variables &lt;- df_desiguales_grande_100 %&gt;% select(1:15) %&gt;% names() df_desiguales_grande_20vars &lt;- read_csv(&quot;datos/desiguales.csv&quot;, col_types = cols_only_chr(nombres_variables)) Ahora bien, es posible que algunos análisis no puedan prescindir de variables, o incluso que después de procedimientos como el anteriormente presentado las bases de datos sigan siendo demasiado pesadas. En estos casos, el ecosistema de R también provee de alternativas, que presentaremos brevemente a continuación. Una primera opción es la función fread() del paquete data.table. Esta función, optimizada para obtener velocidad, es casi siempre más rápida que read_csv() y asociados, aunque no tiene el mismo rango de opciones ni facilidad de uso de estas. Una vez que data.table esté instalado en nuestro sistema –es decir, después de install.packages(&quot;data.table&quot;)–, el siguiente comando nos permitirá cargar la base de datos: library(data.table) df_desiguales_grande_fread &lt;- fread(&quot;00-datos/carga/desiguales.csv&quot;) Nota que, aunque el objeto creado es un dataframe, también es de un tipo especial llamado data.table: class(df_desiguales_grande_fread) ## [1] &quot;data.table&quot; &quot;data.frame&quot; El paquete data.table tiene diversas funciones para lidiar con este tipo de objetos, que suelen ser más eficientes computacionalmente que las usuales –aunque, a menudo, menos intuitivas y legibles. Si es que quieres aprender más sobre data.table, puedes revisar las múltiples viñetas disponibles en la página del paquete, en inglés. Por último, si incluso data.table no es lo suficientemente eficiente como para lidiar con tus datos, tenemos otra opción para presentarte. El paquete ff, con más de diez años de existencia, provee una solución interesante al problema de las bases de datos grandes: en vez de cargarlas en la memoria RAM, como hacen R y los paquetes que hemos visto, ocupa directamente el disco duro (que usualmente tiene mucho más espacio disponible). Aunque esto haga que la mayoría de las operaciones tradicionales de R no funcionen, ff abre la puerta al uso de bases gigantes: el paquete aporta toda una nueva familia de funciones de análisis ad hoc, inspiradas en las de R. Puedes encontrar más información en inglés sobre ff, si lo requieres para un proyecto, en su sitio web y su archivo de ayuda oficial de R. Si primero quieres comprobar cómo funciona el cargar tu base con este paquete, puedes utilizar el siguiente comando7 –asumiendo que el paquete está instalado, con install.packages(&quot;ff&quot;): library(ff) df_desiguales_grande_ff &lt;- read.csv.ffdf(file = &quot;00-datos/carga/desiguales.csv&quot;) Ejercicios antes de continuar al próximo capítulo - Desde la página web de Latinobarómetro baja la edición de 2017 en formato SPSS (.sav) e impórtala a R. ¡Cuidado con las etiquetas! Ahora, repetí el proceso bajando la base en formato de Stata (.dta). Esto no es necesariamente cierto en un archivo .csv, por ejemplo, que no contiene esta información almacenada. Lo que R debe hacer para dicho formato –y otros –es inferir el tipo de archivo.↩ Aunque haven es parte del tidyverse, no es miembro del “núcleo” de este. El núcleo del tidyverse incluye unos pocos paquetes, los más utilizados. El resto de los paquetes, como haven, deben cargarse (pero no instalarse) por separado.↩ Esta es una forma de llamar a los números reales en computación↩ Al igual que haven, readxl es parte del tidyverse, pero no es miembro del “núcleo” de este. Por lo tanto, es necesario cargarlo por separado↩ Estas se encuentran en formato .tsv, por lo que deberás alterar ligeramente la sintaxis siguiente, como viste en la subsección para archivos .csv/.tsv (5.2).↩ Es importante aquí la inclusión del argumento file =, pues este no es el primer argumento que recibe la función en cuestión.↩ "],
["lineal.html", "Capítulo 6 Modelos lineales 6.1 Introducción 6.2 Aplicación en R 6.3 Modelo bivariado: regresión lineal simple 6.4 Modelo multivariado: regresión múltiple", " Capítulo 6 Modelos lineales Por Inés Fynn y Lihuen Nocetto Lecturas de referencia Angrist, Joshua, Jörn-Steffen Pischke. (2008). Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton University Press. Dunning, Thad. (2012). Natural Experiments in the Social Sciences. A design-based approach. Cambridge University Press. Gelman, Andrew, Jennifer Hill. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press. Wooldridge, Jeffrey.(2010). Introducción a la econometría. Un enfoque moderno. Ed. Cengage Lewis-Beck, C., &amp; Lewis-Beck, M. (2015). Applied regression: An introduction (Vol. 22). Sage publications. 6.1 Introducción Este es el primer capítulo donde veremos como hacer regresiones. Comenzaremos por regresiones lineales, comunmente usado para variables dependientes continuas. Aquí la función es lineal, es decir, requiere la determinación de dos parámetros: la pendiente y la ordenada en el origen. Cuando este análisis es multivariado, se complejiza más aún. Cubriremos como interpretar coeficientes, como crear tablas de regresión, visualizar valores predichos, y además nos detendremos a evaluar los supuestos de Minimos Cuadrados Ordinarios, para que puedas hacer diagnosticos respecto al buen ajuste de tus modelos. 6.2 Aplicación en R La base de datos con la que trabajaremos en este capítulo se nutre de dos bases de datos construidas por Evelyne Huber y John D. Stephens. Estas dos bases de datos son: Latin America Welfare Dataset, 1960-2014 (Evelyne Huber and John D. Stephens, Latin American Welfare Dataset, 1960-2014, University of North Carolina at Chapel Hill, 2014.): contiene variables sobre Estados de Bienestar en todos los países de América Latina y el Caribe entre los años 1960 y 2014. Latin America and Caribbean Political Data Set, 1945-2012 (Evelyne Huber and John D. Stephens, Latin America and the Caribbean Political Dataset, 1945-2012, University of North Carolina at Chapel Hill, 2012.): contiene variables políticas para todos los países de América Latina y el Caribe entre los años 1945 y 2012. La base de datos resultante contiene 1074 observaciones de 25 países entre los años 1970 y 2012 (se dejaron por fuera los datos de la década del 60 por tener demasiados valores perdidos). Primero cargamos el paquete tidyverse que incluye un conjunto amplio de funciones para el manejo de bases de datos library(tidyverse) Vamos a importar esta base de datos. library(paqueteadp) data(bienestar_la) Ahora la base se ha cargado en nuestra sesión de R: ls() ## [1] &quot;bienestar_la&quot; Como base para el análisis en este capítulo tomaremos el paper de Huber, E., Nielsen, F., Pribble, J., &amp; Stephens, J. D. (2006). Politics and inequality in Latin America and the Caribbean. American Sociological Review, 71(6), 943-963 donde se estiman los determinantes de la desigualdad en los países de América Latina y el Caribe. Trabajar sobre la base de este artículo nos permite estimar un modelo con varias variables de control que ya se ha probado son importantes para explicar la variación de la desigualdad en la región. Por tanto, la variable dependiente que nos interesa explicar es la desigualdad de ingresos en países de América Latina y el Caribe, operacionalizada a partir del Índice de Gini (gini_slc). Las variables independientes que incorporaremos al modelo son las siguientes: Dualismosectorial (refiere a la la coexistencia de un sector tradicional de baja productividad y un sector moderno de alta productividad) - s_dualism PBI - rgdpch Inversión Extranjera Directa (ingresos netos - % del PIB) - fdiingdp Diversidad étnica (variable dummy codificada 1 cuando al menos el 20 %, pero no más del 80 % de la población es étnicamente diversa) - ethnicdicot Democracia (tipo de régimen) - demrss Gasto en educación (como porcentaje del PBI) - cseduc Gasto en salud (comoporcentaje del PBI) - cshlth Gasto en seguridad social (comoporcentaje del PBI) - csssw Balance legislativo - legbal AutoritarismoRepresivo - repressauthor Durante este capítulo intentaremos estimar cuál es el efecto del gasto en educación sobre la desigualdad en los países de América Latina y el Caribe. De este modo, nuestra variable independiente de interés será Gasto en Educación (cseduc). 6.2.1 Estadísticos Descriptivos Antes de estimar un modelo con Mínimos Cuadrados Ordinarios (MCO, o OLS por su sigla en inglés), o con cualquier estimador, es recomendable reconocer la distribución de las variables de interés: la variable dependiente \\(y\\) (también llamada variable explicada o regresando) y la variable independiente de interés \\(x\\) (también llamada variable explicativa o regresor). Por lo general, en nuestros modelos tendremos, además de la variable independiente de interés, otras variables independientes (o explicativas) que les llamaremos “controles” pues su función será hacer el escenario ceteris paribus lo más creíble posible. Es decir, “mantener el resto de los factores constantes” para acercarnos lo más posible a un mundo experimental, en el que podemos controlar todas las variables que afectan \\(y\\) y observar cómo la variación en una sola variable independiente \\(x\\) afecta la variación de la variable dependiente (\\(y\\)). Entonces, antes de estimar el modelo, vamos a observar los estadísticos descriptivos de las variables que estarán incorporadas a dicho modelo (tanto de la dependiente como de las independientes). El objetivo es prestar atención a los siguientes puntos: Variación en \\(x\\): que las variables independientes (pero sobre todo la de interés) tengan variación en nuestra muestra. Pues si no hay variación de \\(x\\), no podremos estimar cómo esta variación afecta la variación de \\(y\\). Variación en \\(y\\): si la variable dependiente no varía, no vamos a poder explicar su variación en función de las \\(x\\). Unidad de medición de las variables: es en esta instancia donde evaluamos cómo están medidas nuestras variables (además de revisar los diccionarios que por lo general acompañan las bases de datos con las que trabajamos), para poder entender qué tipo de variables estas debieran ser (nominales, ordinales, continuas), y además para luego poder interpretar correctamente los resultados obtenidos. Tipos de variables: en la estimación por Mínimos Cuadrados Ordinarios las variables dependientes deben ser, generalmente, continuas (aunque es posible trabajar con variables dependientes dicotómicas). Por tanto, debemos asegurarnos que la variable dependiente sea continua y numérica. Además, es importante conocer el tipo de variables independientes y chequear que su tipo sea coherente con cómo está codificada (i.e si tenemos una variable independiente de “rangos de edad” la variable debe ser categórica o de factor, y no númerica), para que luego nuestras interpretaciones de los resultados sean correctas. Identificar valores perdidos: Si nuestras variables tienen demasiados valores perdidos debemos revisar a qué se debe esto y, eventualmente, imputar datos (como se explica en el Capítulo 11). 6.2.2 Estadísticos descriptivos y distribución de las variables del modelo Una primera visualización de nuestras variables de interés la podemos hacer utilizando el comando skmir que nos otorga no solo algunos estadísticos descriptivos sino también la distribución de las variables. library(skimr) skim(bienestar_la %&gt;% select(gini_slc, cseduc, fdiingdp, rgdpch, cseduc, cshlth, csssw, ethnicdicot, pop014wdi))%&gt;% skimr::kable() ## Skim summary statistics ## n obs: 1074 ## n variables: 8 ## ## Variable type: numeric ## ## variable missing complete n mean sd p0 p25 p50 p75 p100 hist ## ------------- --------- ---------- ------ --------- --------- --------- --------- --------- --------- --------- ---------- ## cseduc 325 749 1074 3.8 1.65 0.4 2.6 3.8 4.8 9.1 ▂▅▇▇▅▂▁▁ ## cshlth 325 749 1074 2.5 1.89 0.1 1.1 2.1 3.4 15 ▇▆▃▁▁▁▁▁ ## csssw 467 607 1074 4.1 3.57 0 1.5 3 5.7 16.8 ▇▆▃▂▁▁▁▁ ## ethnicdicot 370 704 1074 0.45 0.5 0 0 0 1 1 ▇▁▁▁▁▁▁▇ ## fdiingdp 198 876 1074 2.19 4.18 -55.24 0.48 1.43 3.29 39.81 ▁▁▁▁▇▂▁▁ ## gini_slc 649 425 1074 49.54 6.73 28.9 44 50.6 55 68.3 ▁▁▅▆▆▇▁▁ ## pop014wdi 129 945 1074 36.36 7.01 18.99 30.79 36.57 42.26 48.73 ▁▃▆▆▇▆▇▆ ## rgdpch 194 880 1074 7345.94 4981.75 1421.71 4046.04 6264.85 8667.02 29343.9 ▇▇▃▁▁▁▁▁ En el output se ordenan los resultados por tipo de variable, nos indica la cantidad de missing para cada una de ellas, su respectiva media, desvío estandar, los valores correspondientes a los percentiles y un pequeño histograma que nos muestra cómoestán distribuidas. Además, si quisieramos, podriamos realizar una tabla seleccionando solo algunos de los estadísticos que te interesan. Aquí por ejemplo, solo nos interesa ver la media y desviación estándar: # skim(bienestar_la %&gt;% select(cseduc, fdiingdp, rgdpch, cseduc, cshlth, csssw, ethnicdicot, pop014wdi))%&gt;% dplyr::filter(stat==c(&quot;mean&quot;,&quot;sd&quot;))%&gt;% skimr::kable() En este caso solo nos interesa observar su distribución en histogramas: # skim(bienestar_la %&gt;% select(cseduc, fdiingdp, rgdpch, cseduc, cshlth, csssw))%&gt;% dplyr::filter(stat==&quot;hist&quot;)%&gt;% skimr::kable() 6.2.3 Matriz de correlación de variables independientes Luego de reconocer todas las variables que incorporaremos al modelo, es recomendable observar cómo están relacionadas entre ellas. Para esto, realizamos una matriz de correlación de las variables independientes con el comando rcorr del paquete Hmisc donde podemos evaluar la correlaicón de Pearson entre todas las variables. Además, este comando también nos da el p value para cada correlación, donde podemos evaluar si la correlación observada es significativa. De todos modos, es importante recordar que la correlación no implica causalidad. Aquí simplemente queremos comprender si las variables del modelo están de algún modo relacionadas. Este paso es importante no solo para un reconocimiento de nuestros datos y variables, sino también porque queremos evitar que nuestro modelo tenga multicolinealidad perfecta (que hayan variables independientes que estén perfectamente corrrelacionadas) pues es uno de los supuestos centrales de OLS que revisaremos al final de este capítuo. variables &lt;- c(&quot;gini_slc&quot;,&quot;cseduc&quot;,&quot;s_dualism&quot;,&quot;fdiingdp&quot;,&quot;rgdpch&quot;,&quot;ethnicdicot&quot;,&quot;demrss&quot;,&quot;cshlth&quot;,&quot;csssw&quot;,&quot;legbal&quot;,&quot;repressauthor&quot;,&quot;pop014wdi&quot;) vind &lt;- bienestar_la[variables] Cargamos el paquete ´Ggally´que nos será útil para ver las correlaciones. # library(GGally) # ggcorr(vind, label = T) Ahora que conocemos todas las variables que incorporaremos al modelo, y cómo se relacionan entre sí, profundizaremos sobre las variables de interés: la dependiente y la independiente. 6.2.4 Distribución de las variables de interés Como mencionamos anteriormente, siempre nos interesa estimar cómo el cambio en una variable independiente (su variación) afecta la variación de una variable dependiente. Es decir, cómo cambia \\(y\\) cuando cambia \\(x\\). En este caso, supongamos que nos interesa estimar cómo varían los niveles de desigualdad de un país (medido a partir del Índice de Gini) en relación al gasto en la educación (medido como porcentaje del PBI destinado a la educación). De este modo, nuestra variable independiente de interés es el gasto en la educación, mientras que la variable dependiente es la desigualdad. Veamos cómose distribuyen estas variables en nuestra base de datos: ggplot(bienestar_la, aes(x=gini_slc, na.rm=TRUE))+ geom_histogram(binwidth = 1,color=&quot;white&quot;, fill=&quot;black&quot;) + labs(title = paste( &quot;Distribución de la Variable Dependiente&quot;), caption = paste (&quot;Fuente: Huber et al (2012)&quot;), x = &quot;Índice de Gini&quot;, y = &quot;Frecuencia&quot; ) ## Warning: Removed 649 rows containing non-finite values (stat_bin). Figura 6.1: La variable dependiente: Índice de Gini La variable independiente: Gasto en Educación (% de PBI) ggplot(bienestar_la, aes(x=cseduc, na.rm=TRUE))+ geom_histogram(binwidth = 1,color=&quot;white&quot;, fill=&quot;black&quot;) + labs(title = paste( &quot;Distribución de la Variable Independiente&quot;),subtitle = paste(&quot;% de PBI destinado a la Educación&quot;), caption = paste (&quot;Fuente: Huber et al (2012))&quot; ), x = &quot;Gasto en Educación&quot;, y = &quot;Frecuencia&quot; ) ## Warning: Removed 325 rows containing non-finite values (stat_bin). Figura 6.2: Distribución de la Variable Independiente: Gasto en Educación 6.2.5 Relación entre la variable dependiente e independiente Luego de observar cómo distribuyen las variables de interés, podemos ver gráficamente cómo se relacionan. Es decir, graficamos la correlación entre estas dos variables: en el eje de \\(x\\) (horizontal) ubicamos a la variable independiente, mientras que en el eje de \\(y\\) (vertical) la variable dependiente. Como resultado, cada “punto” del gráfico 6.3 representa una observación de nuestra muestra con un determinado valor de gasto en educación (\\(x\\)) y un determinado valor en el índice de Gini (\\(y\\)). ggplot(bienestar_la, aes(cseduc, gini_slc)) + geom_point()+ labs(x=&quot;Gasto en Educación (%de PBI)&quot;, y=&quot;Gini&quot;, caption = paste (&quot;Fuente: Huber and Stephens, 2012&quot;)) ## Warning: Removed 718 rows containing missing values (geom_point). Figura 6.3: Relación entre Gasto en Educación e Índice de Gini Esta es una primera visualización de la relación entre nuestras variables que nos permite observar si hay algún tipo de vínculo entre ellas. Aquí claramente se observa una relación positiva (a mayor gasto en educación, mayor Gini). De todos modos, hasta el momento no podemos decir nada concluyente sobre la relación entre gasto en educación y nivel de desigualdad, para esto es necesario estimar el modelo. Hasta ahora solo estuvimos conociendo nuestros datos, avancemos hacia la regresión. 6.3 Modelo bivariado: regresión lineal simple El modelo lineal simple asume que una variable aleatoria de respuesta \\(y\\) es una función lineal de una variable independiente \\(x\\) más un término de error \\(u\\). También decimos que la variable dependiente \\(y\\) es resultado de un Proceso de Generación del Dato (DGP por sus siglas en inglés) que puede escribirse \\[ Y = \\beta_0 + \\beta_1x + u \\] Entonces, el modelo lineal implica precisamente definir que \\(Y\\) está generada por una función lineal de \\(x_1\\), además de un término constante \\(\\beta_0\\) y la variable \\(u\\) que es inobservada. Dos supuestos son necesarios para derivar los estimadores de Mínimos Cuadrados Ordinarios. El primero refiere a que la esperanza de \\(u\\) es igual a 0 \\[E(u)=0\\]. Esto implica que a nivel poblacional, todos los factores inobservados promedian cero. El supuesto más importante refiere a que la media de \\(u\\) para cada valor de \\(x\\) es cero: \\[E(u|x)=0\\] Este supuesto se conoce en la literatura econométrica comomedia condicional cero, o independencia condicional. En la literatura experimentalista se conoce comoexogeneidad de la \\(x\\) o también que \\(x\\) y \\(u\\) son ortogonales. Todos estos términos implican que se asume que para cada valor de la variable independiente de interés \\(x\\) los factores inobservados promediarán cero. En otras palabras, conocer un valor determinado de la \\(x\\) no nos dice nada acerca de los inobservados. Cuando \\(E(u|x) = 0\\), entonces se cumple que \\[cov(x,u) = 0\\]. En definitiva, bajo el supuesto de independencia condicional, \\(x\\) y \\(u\\) no correlacionan y esto permite derivar los estimadores de Mínimos Cuadrados Ordinarios a través de las condiciones de primer orden. Las dos condiciones de primer orden son que \\[E(u)=0\\] y \\[E(u|x)=0\\] Entender que estas son las condiciones que permiten derivar el estimador de MCO es clave para entender porqué a partir de la estimación por MCO no podemos testear la independencia del error. Esto es, por construcción, los residuos (\\(\\hat{u}\\)) de la regresión siempre promediarán cero en la muestra y en cada valor de \\(x\\). Demandar exogeneidad implica poder argumentar que \\(u\\) es efectivamente ortogonal a las \\(x&#39;s\\), algo más creíble para experimentos y cuasi experimentos (ver Gerber y Green 2012, Dunning 2012, Glennester y Takavarsha 2013). Si damos por válido que \\(E(u|x)= 0\\),entonces por álgebra podemos escribir la esperanza de una variable Y dado X como \\[E(y|x)= \\beta_0+ \\beta_1x\\] Como se ve, el término inobservado \\(u\\) desaparece de la ecuación cuando se considera la esperanza de la distribución de \\(Y\\) (la esperanza de \\(u\\) es cero). Al término constante \\(\\beta_0\\) y al efecto de la \\(x_1\\) se lo conoce como “parte sistemática del modelo”&quot;, o también Función de Regresión Poblacional (FRP). Es clave entender que el supuesto de \\(E(u)=0\\) refiere al promedio de los factores inobservables y que por tanto, en promedio \\(u\\) no afecta a \\(Y\\). Sin embargo, debe recordarse que cada realización de la variable aleatoria Y está generada por la ecuación 1, y que por tanto, la variable Y de un individuo aleatoriamente seleccionado de esa población es \\[Y_i= \\beta_0+\\beta_1x_{1i}+u_i\\] Esto quiere decir que la realización de la variable aleatoria \\(Y\\) para el individuo \\(i\\) es una función de \\(x_1\\) y \\(u\\). O sea, del valor que toma la \\(x_1\\) en ese individuo y el valor que toma \\(u\\). Todos los factores inobservados que se anotan como\\(u\\) son los que explican que cada individuo se aleje de la FRP. 6.3.1 Estimando un modelo lineal en R Una vez que hemos definido un modelo poblacional como el descrito por la ecuación (1), nuestro trabajo es estimar el impacto de la variable independiente sobre la dependiente. La función lm que se encuentra en R base es la principal herramienta para estimar modelos lineales. La forma general que toma la función es lm(Y ~ 1 + X) De donde se entiende que se estima un linear model (lm) para una variable dependiente Y regresada (~) en una variable independiente X. El “1” no es necesario, pero por estilo, se agrega para denotar el intercepto (\\(\\beta_0\\)). Con base a la investigación de Huber y Stephens (2006) asumamos que la Desigualdad es una función lineal del Gasto en Educación más un término de error inobservado \\(u\\) y una constante \\(\\beta_0\\). Formalmente: \\[ Desigualdad = \\beta_0 + \\beta_1 GastoEducacion + u \\] Por el momento, vamos a asumir que la base de datos tiene 1074 observaciones independientes. En realidad, la estructura es de datos de panel: el mismo país tiene distintas observaciones a lo largo del tiempo. Sin embargo, por el momento no estamos en condiciones de abordar correctamente estos datos. El supuesto de observaciones independientes e idénticamente distribuidas es lo que permite escribir la realización del modelo para un individuo \\(i\\) aleatoriamente seleccionado como\\[Desigualdad_i= \\beta_0 + \\beta_1GastoEducacion_i + u_i\\] Para estimar el modelo, utilizaremos los datos de la base de Huber y Stephens (2006). La variable dependiente “Desigualdad” está medida a través del índice de Gini y el nombre de la variable es gini_slc. La variable independiente “Gasto en Educación” es cseduc. Como los datos están alojados en un data.frame, a la función le tenemos que indicar que traiga esos datos de la base correspondiente. Eso es lo que ocurre luego de la coma modelo_1 &lt;- lm(gini_slc~ 1 + cseduc , data=bienestar_la) #luego de la coma le indicamos el data.frame que contiene los datos. class(modelo_1) # verificamos que la clase del objeto es &quot;lm&quot; ## [1] &quot;lm&quot; En la primera línea del código se creó un objeto (modelo_1) en que se está guardando el resultado de la función lm. Esta función arroja objetos de clase “lm” que son vectores que incluyen los coeficientes estimados, los errores estándar, residuos, la bondad de ajuste, entre otros resultados de la estimación. Para ver los componentes del objeto, una forma rápida es utilizar la función summary summary( modelo_1) ## ## Call: ## lm(formula = gini_slc ~ 1 + cseduc, data = bienestar_la) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.04 -5.62 1.09 4.99 15.57 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 44.80 1.02 43.80 &lt; 2e-16 *** ## [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.7 on 354 degrees of freedom ## (718 observations deleted due to missingness) ## Multiple R-squared: 0.0642, Adjusted R-squared: 0.0615 ## F-statistic: 24.3 on 1 and 354 DF, p-value: 1.29e-06 Presentaciones más elegantes pueden obtenerse con la función screenregdel paquete texreg. Veamos la presentación de resultados con la función screenreg library(texreg) ## Version: 1.36.23 ## Date: 2017-03-03 ## Author: Philip Leifeld (University of Glasgow) ## ## Please cite the JSS article in your publications -- see citation(&quot;texreg&quot;). ## ## Attaching package: &#39;texreg&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract screenreg(modelo_1) ## ## ======================= ## Model 1 ## ----------------------- ## (Intercept) 44.81 *** ## (1.02) ## cseduc 1.23 *** ## (0.25) ## ----------------------- ## R^2 0.06 ## Adj. R^2 0.06 ## Num. obs. 356 ## RMSE 6.70 ## ======================= ## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 Podemos agregar nombre a las variables library(texreg) screenreg(modelo_1, custom.model.names = &quot;Modelo 1&quot;, custom.coef.names = c(&quot;Constante&quot;, &quot;Gasto en educacion&quot;)) ## ## ============================== ## Modelo 1 ## ------------------------------ ## Constante 44.81 *** ## (1.02) ## Gasto en educacion 1.23 *** ## (0.25) ## ------------------------------ ## R^2 0.06 ## Adj. R^2 0.06 ## Num. obs. 356 ## RMSE 6.70 ## ============================== ## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 Y podemos exportar la tabla en formato .doc para agregarla a nuestro trabajo. El archivo se guardará en nuestro directorio de trabajo htmlreg(list(modelo_1), file = &quot;modelo_1.doc&quot;, custom.model.names = &quot;Modelo 1&quot;, custom.coef.names = c(&quot;Constante&quot;, &quot;Gasto en educacion&quot;), inline.css = FALSE, doctype = TRUE, html.tag = TRUE, head.tag = TRUE, body.tag = TRUE) ## The table was written to the file &#39;modelo_1.doc&#39;. Como se ve, resulta más cómodo ver los resultados con texreg que con summary. Allí se ve claramente que la variable cseduc, gasto en educación, tiene un efecto positivo, de magnitud 1.233, estadísticamente significativo. En concreto, cuando el gasto en educación como porcentaje del PIB aumenta en una unidad, la desigualdad aumenta en un 1,23%. Esto es así ya que nuestra variable dependiente está medida del 1 al 100 al igual que la independiente. El efecto del Gasto en Educación es significativo al 99.9% de confianza. Sabemos eso porque al lado del coeficiente aparecen tres estrellas, que refieren a un nivel de significancia de 0,01 %. La significancia estadística es resultado de la prueba t. Esta nos indica la distancia estandarizada donde se encuentra el beta estimado en la distribución del estimador bajo la hipótesis nula de que \\(\\beta_1=0\\). El estimador tiene una distribución t-Student con grados de libertad igual a \\(n-k-1\\) donde \\(k\\) es el número de variables independientes y se le suma 1 por la estimación de la constante \\(\\beta_0\\). Una aproximación manual de la distancia del beta estimado en la distribución del estimador bajo la hipótesis nula \\(\\beta_1=0\\), la obtenemos cuando dividimos la estimación por su error estándar: 1.233 / 0.25 ## [1] 4.9 Este valor es el mismo que arroja la tercera columna de la sección “Coefficients” del summary del modelo_1. El valor t se interpreta como la distancia de la estimación de \\(\\hat\\beta_1\\) de la media de una distribución del estimador bajo \\(H_0=\\beta_1=0\\). En este caso, el valor 1.233 está a 4.93 desvíos estándar de la distribución del estimador cuando H0 es verdadera (la media de la distribución es 0). Como las distribuciones t colapsan sobre la normal a medida que aumentan los grados de libertad, y sabemos que aproximadamente hasta 2 desvíos estándar se encuentra el 95% de la probabilidad de una normal, entonces cuando el estadístico t supera el valor de 2 podemos saber que la probabilidad de observar nuestra estimación si H0 fuera cierta es menor a 0.05. Cuando esto sucede, rechazamos la hipótesis nula a un nivel de confianza del 95%. En este caso, el \\(\\hat\\beta_1\\) estimado está a más de 4.93 desvíos estándar de la media de la distribución bajo \\(H_0=\\beta_1=0\\) por lo que es muy poco probable haber observado un efecto de 1.23 si \\(H_0\\) es verdadera. La probabilidad precisa puede observarse en la cuarta columna del summarydel modelo, que puede solicitarse a R con el siguente comando. coef(summary( modelo_1))[, &quot;Pr(&gt;|t|)&quot;] ## (Intercept) cseduc ## 5.5e-145 1.3e-06 La probabilidad de observar una estimación de 1.23 si H0 es verdadera es de 0,00000128. Por tanto, podemos rechazar H0 aún a un nivel de confianza de 99.9%. #*****errores robustos 6.3.2 Representación gráfica Como vimos anteriormente, una de las maneras más fáciles de mostrar la relación entre dos variables es a través de gráficos. El paquete ggplot2es una herramienta de suma utilidad para realizar diverso tipo de representaciones. En el primer código se grafican todas las observaciones en función de sus valores de variable independiente y dependiente. library (ggplot2) ggplot(data = bienestar_la, #se especifica el origen de la base de datos aes(x = cseduc, y = gini_slc))+ #se seleccionan las variables independiente y dependiente geom_point() + #se plotean los valores observados geom_smooth(method = &quot;lm&quot;, # se superpone la línea de regresión se = FALSE, #no se plotea el área de error al 95% de confianza color = &quot;blue&quot;)+ #color de línea labs (x = &quot;Gasto Educación&quot;) + # título del eje X labs( y= &quot;Desigualdad&quot;) #título del eje Y ## Warning: Removed 718 rows containing non-finite values (stat_smooth). ## Warning: Removed 718 rows containing missing values (geom_point). #+ labs ( title =&quot;&quot;) #título del gráfico Figura 6.4: Relación lineal entre Gasto Educación y Desigualdad Usualmente es útil mostrar también una representación gráfica del error de la predicción de la recta. ggplot2nos permite editar un área sombreada donde podrían haber estado los valores predichos con un determinado nivel de significancia. Si bien el 95% de confianza es el valor que viene por defecto, también podemos editar este valor. El primer bloque muestra la línea de regresión y su error para un nivel de significancia estadística del 95%. ggplot(data = bienestar_la, #se especifica el origen de la base de datos aes(x = cseduc, y = gini_slc))+ geom_point() + geom_smooth(method = &quot;lm&quot;, se = T, #se agrega error de predicción color = &quot;blue&quot;)+ labs (x = &quot;Gasto Educación&quot;) + labs( y= &quot;Desigualdad&quot;) + labs ( title =&quot;Relación lineal entre Gasto Educación y Desigualdad&quot;) ## Warning: Removed 718 rows containing non-finite values (stat_smooth). ## Warning: Removed 718 rows containing missing values (geom_point). Figura 6.5: En esta figura adicionamos un intérvalo de confianza de 95% 6.4 Modelo multivariado: regresión múltiple Si bien suele interesar el efecto de una variable independiente sobre una dependiente, lo más común es estimar modelos en los que la \\(Y\\) es resultado tanto de una variable independiente de interés como de un conjunto de variables de control. Formalmente, \\[Y= \\beta_0+\\beta_1x_{1}+\\beta_1x_{2}+...+\\beta_jx_{j}+u\\] A diferencia de la regresión simple, ahora la variable aleatoria \\(Y\\) es una función de diversas variables más un término de error \\(u\\). Al igual que la regresión simple, la esperanza del error condicional en los valores de las \\(x_j\\) debe ser igual a cero. Formalmente, \\(E(u|x_1, x_2, ..., x_j)=0\\). Para estimar insesgadamente un modelo lineal múltiple no sólo se necesita el supuesto de media condicional cero, pero se presentarán todos los supuestos en una sección posterior. Por el momento estimaremos un modelo poblacional en que la desigualdad social (gini_slc) es una función lineal del gasto en educación como porcentaje del PIB (cseduc), de la inversión extranjera directa (fdiingdp), del gasto en salud como porcentaje del PIB (cshlth), del gasto en Seguridad Social como porcentaje del PIB (csssw), de la población joven (pop014wdi), del dualismo estructural de la economía (s_dualism), de la división étnica (ethnicdicot), del PIB per cápita real (rgdpch), del tipo de régimen (demrss), del balance entre los poderes del Estado (legbal) y del autoritarismo (repressauthor). Como se ve, se han incluido una multiplicidad de variables que se piensa que predicen la desigualdad (Huber et al, 2006). El análisis de regresión múltiple nos permitirá estimar hasta qué punto nuestro modelo es correcto. En primer término se debe correr la estimación de MCO. La función lmtambién estima modelos múltiples y la única diferencia es que deben sumarse las variables independientes. Antes de estimar el modelo, filtraremos la base de datos, eliminando todos los casos con valores perdidos (NAs) en nuestras variables de control. Hay mejores formas de lidiar con los valores perdidos, para esto puedes revisar el Capítulo 11 sobre imputación de valores perdidos. Aquí, por practicidad, simplemente nos quedaremos con aquellos casos (país/año) que están completos para las variables de nuestro modelo: bienestar_la_sinna&lt;- bienestar_la %&gt;% drop_na(gini_slc, cseduc , fdiingdp , cshlth , csssw , pop014wdi, s_dualism , ethnicdicot , rgdpch , demrss, legbal , repressauthor) Ahora si estamos en condiciones de estimar el modelo 2: modelo_2&lt;- lm(gini_slc~ 1+ cseduc + fdiingdp + cshlth + csssw + pop014wdi+ s_dualism + ethnicdicot + rgdpch + as.factor(demrss) + legbal + repressauthor,data = bienestar_la_sinna ) Recordemos que el 1 no es necesario, puedes probar rodando el modelo sin el, pero lo colocamos para recordarte que estamos también estimando el intercepto. Hemos indicado que la variable demres es categórica mediante as.factor. De esta manera, cada categoría de régimen se mide mediante un coeficiente ´dummy´. Al igual que el modelo simple, podemos visualizar e imprimir los resultados de la estimación con summaryo screenreg. screenreg(modelo_2) ## ## ============================== ## Model 1 ## ------------------------------ ## (Intercept) 85.94 *** ## (8.73) ## cseduc 1.59 *** ## (0.45) ## fdiingdp 0.24 ## (0.18) ## cshlth -0.83 ** ## (0.26) ## csssw -0.83 *** ## (0.20) ## pop014wdi -0.93 *** ## (0.17) ## s_dualism -0.17 *** ## (0.03) ## ethnicdicot 3.68 *** ## (1.04) ## rgdpch -0.00 ** ## (0.00) ## as.factor(demrss)2 -2.29 ## (4.75) ## as.factor(demrss)3 -2.90 ## (4.70) ## as.factor(demrss)4 -5.14 ## (4.62) ## legbal -10.40 *** ## (2.22) ## ------------------------------ ## R^2 0.59 ## Adj. R^2 0.56 ## Num. obs. 167 ## RMSE 4.52 ## ============================== ## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 Estas funciones también nos permiten comparar dos o más modelos. Al momento de presentar una investigación suele ser recomendable mostrar cómo cambian (o no) los resultados ante distintas especificaciones. Primeo guardamos los modelos en una lista. Al comando de screenreg le agregremos los nombres de las variables como ya hemos visto. En el caso que se trabaja aquí la comparación de modelos es modelos&lt;-list(modelo_1, modelo_2) screenreg(modelos, custom.model.names = c(&quot;Modelo 1&quot;, &quot;Modelo 2&quot;), custom.coef.names = c(&quot;Constante&quot;, &quot;Gasto en educacion&quot;, &quot;IED&quot;, &quot;Gasto en salud&quot;, &quot;Gasto en seg. social&quot;, &quot;Población jóven&quot;, &quot;Dualismo de economía&quot;, &quot;División étnica&quot;, &quot;PBI pc&quot;, &quot;Reg. democrático&quot;, &quot;Reg. mixto&quot;, &quot;Reg. autoritario&quot;, &quot;Balance entre poderes&quot;)) ## ## ============================================= ## Modelo 1 Modelo 2 ## --------------------------------------------- ## Constante 44.81 *** 85.94 *** ## (1.02) (8.73) ## Gasto en educacion 1.23 *** 1.59 *** ## (0.25) (0.45) ## IED 0.24 ## (0.18) ## Gasto en salud -0.83 ** ## (0.26) ## Gasto en seg. social -0.83 *** ## (0.20) ## Población jóven -0.93 *** ## (0.17) ## Dualismo de economía -0.17 *** ## (0.03) ## División étnica 3.68 *** ## (1.04) ## PBI pc -0.00 ** ## (0.00) ## Reg. democrático -2.29 ## (4.75) ## Reg. mixto -2.90 ## (4.70) ## Reg. autoritario -5.14 ## (4.62) ## Balance entre poderes -10.40 *** ## (2.22) ## --------------------------------------------- ## R^2 0.06 0.59 ## Adj. R^2 0.06 0.56 ## Num. obs. 356 167 ## RMSE 6.70 4.52 ## ============================================= ## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 Como se observa, la estimación puntual del efecto del gasto en Educación cambió ligeramente. Mientras en el modelo simple el efecto es de 1,23 en el modelo múltiple este efecto pasa a 1,59. En este caso, la interpretación es que cuando el gasto en educación aumenta en una unidad, la desigualdad aumenta en promedio 1,59 puntos porcentuales manteniendo todos los demás factores constantes. Esto es, una vez que se descuenta el efecto de las variables de control. Al igual que en el modelo 1, la variable sigue siendo significativa al 99,9% de confianza por lo que decimos que el efecto del Gasto en Educación es robusto a diferentes especificaciones. Cuando los investigadores incluyen nuevos controles al modelo y la principal variable de interés se mantiene significativa y con magnitudes relativamente estables se gana evidencia a favor del efecto de la misma. En otras palabras, cada vez es menos probable que el efecto observado en primera instancia fuese espurio. Otra de las contribuciones del modelo 2 es la incorporación de variables nominales. Las variables dicotómicas y las categóricas plantean un ligero desafío de interpretación. Obsérvese la variable Diversidad Étnica que es dicotómica donde el valor 1 implica que más del 20% de la población pertenece a una minoría étnica y 0 si no hay una minoría tan relevante. El coeficiente de “ethicdicot” es 3,7 significativo al 99,9%. ¿Cómo interpretarlo? Sencillamente, el valor predicho de la desigualdad es 3.7 puntos mayor cuando existe una minoría étnica, a cualquier valor de las otras \\(x&#39;s\\). Para interpretar estos coeficientes siempre debe conocerse la categoría base. Como ethicdicot es igual a 0 cuando no hay minorías étnicas, el coeficiente se interpreta como el pasaje hacia tener minoría étnica. En el caso de la variable s_dualism, dado que la categoría base es 0 para “sin dualismo”, el coeficiente se interpreta tal que tener una economía dual disminuye (coeficiente negativo) la desigualdad en aproximadamente 0.17 puntos. Como práctica exporta la tabla con los dos modelos a un archivo de Word, te esperamos. Se puede observar gráficamente la incidencia de las variables dicotómicas utilizando plot_model del paquete sjPlot library(sjPlot) En la gráfica que sigue se muestra el efecto del gasto en educación sobre la desigualdad para las distintas categorías de diversidad étnica, manteniendo todas las demás variables constantes. Se aprecia claramente que lo único que cambia entre las categorías es el intercepto. Los países que tienen una minoría étnica importante tienen aproximadamente 3,7 puntos más de desigualdad que los que no tienen minorías étnicas, a cualquier valor de gasto en educación, manteniendo todas las demás variables constantes. # plot_model(model = modelo_2, # type = &quot;pred&quot;, # terms = c(&quot;cseduc&quot;, &quot;ethnicdicot&quot;), # facet.grid = F, # show.ci = T, # title = &quot;Valores predichos de desigualdad según gasto en educación y etnicidad&quot;) 6.4.1 Ajuste del modelo La bondad de ajuste se define como la capacidad explicativa del modelo. Intuitivamente refiere a qué porción de la variación de la variable dependiente \\(y\\) es explicada por el modelo especificado. La medida de la bondad de ajuste es el \\(R^2\\) y se define como 1- SRC/STC, donde SRC es la Suma de los Residuos al Cuadrado y STC la Suma de los Totales Cuadrados. De manera simple SRC es una medida de todo lo que el modelo no explica, mientras que STC es la variabilidad de la \\(y\\). Un modelo que explique toda la variación de \\(y\\) tendrá un \\(R^2\\) de 1. Un modelo que no explique nada de la variabilidad de la variable dependiente tendrá un valor de 0. Por regla general, a medida que se aumenta el número de variables independientes el \\(R^2\\) nunca disminuye por lo que se suele utilizar el \\(R^2\\) ajustado como una medida que penaliza la inclusión de variables sin fundamento. Como se puede observar en la comparación de los modelos estimados previamente, el modelo lineal simple tiene un \\(R^2\\) de 0.06. Eso puede leerse como que el modelo 1 explica el 6% de la variabilidad de la desigualdad. El modelo múltiple 2 aumenta su capacidad explicativa al 59%. Algunos investigadores buscan aumentar la bondad de ajuste del modelo. Sin embargo, estimar el efecto de una variable en concreto no requiere aumentar la bondad de ajuste sino simplemente que se cumplan los supuestos del Modelo Lineal Clásico como la media condicional 0, la linealidad de los parámentros y demás supuestos que se describen en la sección correspondiente. 6.4.2 Inferencia en modelos lineales múltiples Al igual que en la regresión lineal simple, los estimadores de cada uno de los parámentros \\(\\beta_j\\) tienen una distribución t-Student por lo que puede realizarse inferencia acerca de las estimaciones puntuales de cada \\(\\hat{\\beta_j}\\) a través de una prueba t. Sin embargo, a veces se desea imponer restricciones lineales múltiples al modelo del tipo \\(H_0= \\beta_1= \\beta_2 = 0\\). Aquí se está sosteniendo que el efecto de dos variables \\(x_1\\) y \\(x_2\\) es igual a cero. Un caso típico que requiere este tipo de hipótesis nula refiere a las variables categóricas que ingresan al modelo como variables dicotómicas ficticias. La variable dicotómica “educación media” y la dicotómica “educación superior” son en realidad categorías de una única variable nominal “nivel educativo” que sólo puede entrar en un análisis de regresión en la forma de dummies ficticias. La prueba que permite hacer inferencia para restricción lineal múltiple es la prueba F. Esto implica que la \\(H_0\\) de una restricción múltiple distribuye F de Fisher. Aquí se muestran dos maneras de testear una restricción lineal múltiple. La primera muestra cada uno de los pasos del test mientras que la segunda es más breve. Supongamos que se quiere testear la hipótesis nula que $H_0= _1= _2 = _3 =0 $. Según esta hipótesis las variables \\(x_1\\), \\(x_2\\) y \\(x_3\\) no afectan a \\(Y\\) cuando se las considera en conjunto. La hipótesis alternativa es que al menos uno de los betas es distinto de 0. Si \\(H_0\\) es verdadera entonces un modelo que excluya estas variables debería explicar lo mismo que un modelo que las incluya, o sea, estas variables son redundantes. La manera de testear esta hipótesis es a través de un test F en el que se compara la suma de los residuos cuadrados del modelo completo y el modelo restringido. En términos simples, si las variables deben ser excluidas porque no explican la variabilidad de \\(y\\) la Suma de los Residuos Cuadrados de ambos modelos (otra manera es ver el \\(R^2\\)) no debe cambiar significativamente. Se utiliza el hecho que la comparación de los residuos cuadrados distribuye F \\[F= \\frac{(SRC_r-SRC_c)/q}{SRC_c/(n-k-1)}\\] Donde \\(SRC_r\\) es la Suma de los Residuos Cuadrados del modelo restringido, \\(SRC_c\\) es la Suma de los Residuos Cuadrados del modelo completo, \\(q\\) es la cantidad de variables excluidas y \\(n-k-1\\) son los grados de libertad del modelo completo. En R se puede utilizar la función anova para comparar los modelos. Por ejemplo, supongamos que un colega asegura que el balance del legislativo (legbal), el tipo de régimen (demrss) y la diversidad étnica (ethnicdicot) deben excluirse del modelo. Entonces debemos estimar un modelo restringido tal que modelo_2_restringido &lt;- lm(gini_slc ~ 1 + cseduc + fdiingdp + cshlth + csssw + pop014wdi+ s_dualism + rgdpch + repressauthor,data = bienestar_la_sinna) Como se ve, las variables mencionadas fueron excluidas de la sintaxis. Ahora se debe comparar el poder explicativo de cada modelo anova( modelo_2, modelo_2_restringido) ## Analysis of Variance Table ## ## Model 1: gini_slc ~ 1 + cseduc + fdiingdp + cshlth + csssw + pop014wdi + ## s_dualism + ethnicdicot + rgdpch + as.factor(demrss) + legbal + ## repressauthor ## Model 2: gini_slc ~ 1 + cseduc + fdiingdp + cshlth + csssw + pop014wdi + ## s_dualism + rgdpch + repressauthor ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 154 3148 ## [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La significancia de la última columna del test muestra con claridad que se rechaza la hipótesis nula por lo que esas variables no deben excluirse del modelo. 6.4.3 Supuestos de OLS El estimador de Mínimos Cuadrados Ordinarios será de utilidad (estimará insesgadamente el parámetro poblacional) si es que se cumplen los supuestos de Gauss-Markov que permiten que sea el Mejor Estimador Lineal Insesgado (MELI, o BLUE por sus siglas en inglés). Para profundizar sobre los supuestos es recomendable consultar Wooldrige (2006) y Stock y Watson (2012). Es importante evaluar que en nuestra estimación se estén cumpliendo estos supuestos. Como veremos a continuación, esta evaluación es teórica y, en algunos casos, se podrá aproximar empíricamente. Todos estos diagnósticos suelen ir incorporados a los artículos como anexos o en los archivos de replicación de datos, y no necesariamente en el cuerpo del texto. 6.4.3.1 1. Media condicional cero El supuesto central para utilizar el estimador de MCO. El postulado crucial de este supuesto es la independencia entre las variables indpendientes y el término de error, esto nos permite aislar de los factores no observables (contenidos en el término de error \\(u\\)) el efecto de las \\(x\\). Este supuesto no puede ser evaluado empíricamente porque, por definición, no conocemos los factores contenidos en el término de error. Por lo tanto, la defensa de este supuesto siempre será teórica. 6.4.3.2 2. Muestreo aleatorio Este es un supuesto sobre la generación de los datos. Se asume un muestreo aleatorio de tamaño \\(n\\) que implica que la muestra fue tomada de forma tal que todas las unidades poblacionales tuvieron la misma probabilidad de ser seleccionadas. Es decir, no hay un sesgo de selección muestral. 6.4.3.3 3. Linealidad en los Parámetros MCO asume que la variable dependiente (\\(y\\)) esta relacionada linealmente con la variable(s) independiente(s) y el término de error (\\(u\\)). Es decir, el aumento en una unidad de \\(x\\) implica un efecto constante en la variable dependiente \\(y\\). De aquí la forma funcional de la ecuación de regresión: \\[Y = \\beta_0 + \\beta_1x + u\\] Si la relación en realidad no es lineal, entonces estaremos ante un problema de especificación del modelo. Es decir, los valores predichos por nuestro modelo no se ajustarán a la realidad de nuestros datos y, en consecuencia, las estimaciones serán sesgadas. Por tanto, es clave evaluar si la relación que queremos estimar es lineal o si la forma funcional que caracteriza dicha relación es otra (por ejemplo, podría ser cuadrática, cúbica, logarítmica, etc). La buena noticia es que si tenemos motivos teóricos y empíricos para creer que la relación no es lineal, es posible realizar transformaciones a nuestras variables para lograr una mejor especificación del modelo. Un clásico ejemplo refiere a la relación parabólica entre la edad y el salario: a medida que aumenta la edad aumenta el salario hasta que llega un punto de inflexión donde el aumento de la edad se relaciona con menores niveles de ingreso, como una U invertida. En este caso lo recomnedable es realizar una transformación cuadrática a la variable edad para lograr una mejor especificación del modelo. Para evaluar la linealidad realizamos un gráfico de dispersión de los valores predichos contra los residuos \\(u\\). Lo que se intenta es evaluar si el promedio de los residuos tiende a ubicarse de manera aleatoria por encima y debajo del cero. Si los residuos muestran un patrón creciente o decreciente - o de cualquier otro tipo - entonces la forma funcional de alguna de las variables en cuestión no es lineal. Para esto utilizamos el comando plot: plot(y=modelo_1$residuals,x=modelo_1$fitted.values, xlab=&quot;Valores Predichos&quot;,ylab=&quot;Residuos&quot;) abline(0, 0) Figura 6.6: Test de linealidad en valores predichos Además, podemos hacer un gráfico de residuos parciales (o de componentes), donde se grafican cada una de las variables independientes del modelo contra los residuos. El objetivo es obtener un gráfico “parcial” para observar la relación entre la(s) variable(s) independiente(s) y la variable dependiente dando cuenta (controlando) de las demás variables del modelo. Una línea punteada nos muestra la predicción de OLS, y otra línea (rosada) nos muestra la relación “real”. Si observamos que alguna de nuestras variables no tiene una relación lineal podemos realizar transformaciones (a las variables!) para que la forma funcional se acerque a la empiria. Cabe destacar que, además de la justificación empírica, esta transformación lineal siempre debe ir acompañada de un argumento teórico de por qué la relación entre las dos variables toma esa forma. Una transformación que verás con regularidad en papers es la de transformaciones logaritmicas de variables. Estas se presentan tanto en la variable dependiente como en la independiente. Para ello te ofrecemos una tabla que te será de gran utilidad. Te permite saber como cambia la interpretación de los resultados cuando una de las variables (o ambas) es transformada. Figura 6.7: La tabla resume la interpretación de los coeficientes ante transformaciones logaritmicas Por ejemplo, si decidimos transformar nuestra variable dependiente de tal forma que modelo_1_log &lt;- lm(log(gini_slc)~ 1+ cseduc,data = bienestar_la) screenreg(modelo_1_log) ## ## ======================= ## Model 1 ## ----------------------- ## (Intercept) 3.78 *** ## (0.02) ## cseduc 0.03 *** ## (0.01) ## ----------------------- ## R^2 0.07 ## Adj. R^2 0.07 ## Num. obs. 356 ## RMSE 0.14 ## ======================= ## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 La interpretación sería: si aumentamos el gasto en salud en una unidad, esperaríamos que el Gini aumente un 3%, ceteris paribus. Para poder saber cuando transformar nuestras variables, veremos con un ejemplo como podemos diagnosticar un problema en la forma en que nuestras variables son medidas. library(car) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following object is masked from &#39;package:purrr&#39;: ## ## some crPlots(modelo_1) Figura 6.8: Test de linealidad La relación de nuestra variable de interés con la variable dependiente parece ser cuadrática creciente. Por tanto, podría ser razonable realizar una transformación cuadrática a la variable. Evaluemos esto gráficamente: bienestar_la_sinna &lt;- bienestar_la_sinna %&gt;% mutate(cseduc2 = cseduc*cseduc) modelo_1_cuadratico&lt;- lm(gini_slc ~ 1 + cseduc2 + cseduc, data=bienestar_la_sinna) crPlots(modelo_1_cuadratico) Figura 6.9: Test de linealidad alternativo A partir de un diagnostico visual, se observa una tendencia creciente en los residuos a medida que se avanza en los valores predichos. Además, se detectó una relación no lineal entre el gasto en educación y los niveles de desigualdad. La sospecha es que esta relación pueda ser cuadrática (parábola cuadrática creciente) y, de acuerdo al gráfico de residuales parciales parece ser que la variable transformada se acerca bastante más a la relación lineal que estima MCO (marcada por la línea punteada). Ayuda notar que la escala en la figura de la izquierda es de 0 a 15, mientras que la de la derecha es de 0 a 20, denotando una pendiente más pronunciada. Para confirmar las observaciones visuales, se suele utilizar un test estadístico para diagnosticar una mala especificación funcional del modelo: RESET Test de Ramsey. La idea es justamente evaluar si es que existe un error de especificación de la ecuación de regresión. Este test lo que hace es volver a estimar el modelo pero incorporando los valores predichos del modelo original con alguna transformación no lineal de las variables. Luego, a partir de un Test-F se evalúa si el modelo con la especificación no lineal tiene un mejor ajuste que el modelo original sin la transformación no lineal. La hipótesis nula postula que las nuevas variables (en este caso cseduc^2) no aportan significativamente para explicar la variación de la variable dependiente; es decir, que su coeficiente es igual a cero (\\(\\beta=0\\)). library(lmtest) ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric resettest(modelo_1, power=2, type=c(&quot;fitted&quot;), data=bienestar_la_sinna) ## ## RESET test ## ## data: modelo_1 ## RESET = 9, df1 = 1, df2 = 400, p-value = 0.004 De acuerdo al resultado del Test F, confirmamos lo observado gráficamente: el incorporar un término cuadrático del gasto en educación mejora el ajuste de nuestra estimación. A esta conclusión llegamos observando el valor-p del test RESET de Ramsey: a un nivel de significancia estadística del 5%, se rechaza la hipótesis nula de que la incorporación del término cuadrático no mejora el ajuste del modelo. Nota: Esta evaluación se realizó para un modelo de regresión simple (bivariado). Pero bien puede realizarse exactamente lo mismo para modelos multivariados. 6.4.3.4 Variación en las variables independientes y no colinealidad perfecta En primer lugar, es necesario que exista variación en la(s) variable(s) independiente(s). Una variable que no varía, ¡no es variable! Si no tengo variación, la estimación de los coeficientes será indeterminada. Además, mayor variación en las variables independientes me permitirá realizar estimaciones más precisas. Por otra parte, la no-colinealidad perfecta implica que las variables independientes no estén perfectamente correlacionadas linealmente. Es decir, si bien las variables independientes por lo general suelen tener alguna relación entre ellas, ¡no queremos que midan prácticamente lo mismo! Eso lo evaluaremos con tests de multicolinealidad. Problemas de la multicolinealidad: A. Pérdida de eficiencia, pues sus errores estándar serán infinitos. Aún si la multicolinealidad es menos que perfecta los coeficientes de regresión poseen grandes errores estándar, lo que hace que no puedan ser estimados con gran precisión. Repasemos la fórmula del error estándar de los coeficientes: \\[\\hat{\\sigma}{_\\hat{\\beta}{_1}} = \\frac{\\hat{\\sigma}} {\\sqrt{\\sum(X_j – \\bar{X})^2(1 - R^2_j)}}\\] \\(\\hat{\\sigma}\\) Es la varianza del término de error: \\(\\frac{\\sum\\hat{u}}{n-k-1}\\) \\(\\sum(X_j – \\bar{X})^2\\) Es la variabilidad de \\(x_j\\) (\\(STCx_j\\)) \\(1 - R^2_j\\) Es la porción de \\(x_j\\) que no es explicada por el resto de las x en el modelo (\\(R^2_j\\) indica la varianza de \\(x_j\\) que es explicada por el resto de las equis del modelo). Es por este término que la no colinealidad perfecta es tan importante! B. Las estimaciones de los coeficientes pueden oscilar demasiado en función de qué otras variables independientes están en el modelo. En una estimación OLS la idea es que puedes cambiar el valor de una variable independiente y no de las otras (de esto se trata ceteris paribus, es decir, manteniendo las otras covariables constantes). Sin embargo, cuando las variables independientes están correlacionadas, los cambios en una variable están asociados con los cambios en otra variable. Cuanto más fuerte es la correlación, más difícil es cambiar una variable sin cambiar otra. Se vuelve difícil para el modelo estimar la relación entre cada variable independiente y la variable dependiente manteniendo el resto constante porque las variables independientes tienden a cambiar simultáneamente. Repasemos la fórmula del estimación del coeficiente en una regresión múltiple: \\[\\hat{\\beta_1} = \\frac{\\sum(\\hat{r_{i1}}\\hat{y_i})}{\\sum(\\hat{r^2_{i1}})}\\] Donde: \\(\\hat{r_{i1}}\\) son los residuales de una regresión de \\(x_1\\) sobre el resto de las \\(x\\) en el modelo (osea la parte de \\(x_1\\) que no puede ser explicada - o que no está correlacionada - con el resto de las \\(x\\)) Por tanto, \\(\\hat{\\beta_1}\\) mide la relación muestral entre \\(y\\) y \\(x_1\\) luego de haber descontado los efectos parciales de \\(x_2\\), \\(x_3\\)…\\(x_k\\). Para evaluar la multicolinealidad, un primer paso es observar la matriz de correlación de las variables de nuestro modelo (tal como hicimos en la etapa de analizar los estadísticos descriptivos): library(GGally) ## ## Attaching package: &#39;GGally&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## nasa vind &lt;- bienestar_la[variables] ggcorr(vind, label = T) Figura 6.10: Matriz de correlación, donde diagnosticaremos problemas de multicolinearidad Vemos que algunas de nuestras variables tienen correlaciones fuertes, como el gasto en seguridad social csssw y la cantidad de población jóven que tiene el país pop014w, que tienen una correlacion negativa de 0.7. De todos modos, para detectar si la multicolinealidad es problemática es necesario realizar un test de vif (variance inflation factors) porque ver correlación de a pares no nos ayuda a dilucidar si más de dos variables tienen una correlación lineal. Lo que nos dice este test vif es qué tanto se “agrandan” los errores de cada coeficiente en presencia de las demás variables (qué tanto se incrementa la varianza del error). # modelo_2 &lt;- lm(gini_slc ~ 1 + cseduc + fdiingdp + cshlth + csssw + pop014wdi + s_dualism + ethnicdicot + rgdpch + demrss + legbal + repressauthor,data = bienestar_la_sinna ) # # vif(modelo_2) Luego, realizo una consulta sobre si la raíz cuadrada de vif para cada variable es menor que 2 (la raíz cuadrada porque lo que me interesa es el error estándar y no la varianza). Vif: debería ser menor a 2, si es mayor a dos quiere decir que la varianza es demasiado alta y por tanto hay problema de multicolinealidad. # sqrt(vif(modelo_2)) &gt; 2 De acuerdo a la consulta, parece ser que no tenemos un problema de multicolinealidad. Pero si lo tenemos, debríamos corregirlo? En general los manuales de econometría que te recomendamos coinciden en que la necesidad de reducir la multicolinealidad depende de su gravedad y de cuál es el objetivo principal del modelo de regresión. Hay que tener en cuenta los siguientes tres puntos: La gravedad de los problemas aumenta con el grado de multicolinealidad. Por lo tanto, si la multicolinealidad es moderada, es posible que no necesitemos resolverla. La multicolinealidad afecta solo a las variables independientes específicas que están correlacionadas. Por lo tanto, si la multicolinealidad no está presente para las variables independientes de interés, es posible que no necesitemos resolverla. La multicolinealidad afecta los coeficientes y los valores-p, y el error estándar, pero no influye directamente en los valores predichos del modelo, la precisión de estas predicciones y las estadísticas de bondad de ajuste. Si el objetivo principal es hacer predicciones, y no necesitamos comprender el papel de cada variable independiente, no necesitamos reducir la multicolinealidad. Soluciones a la multicolinealidad Remover una de las variables indpendientes que esté altamente correlacionada. Esto constituye un trade-off, y se tiene que justificar teóricamente por qué se mantiene una variable y no la otra, además de hacer evidente el alto grado de correlación. Puedo combinar las variables que estén altamente correlacionadas, hacer un índice por ejemplo como enseñamos en el Capítulo 10. Hasta ahora hemos visto cuatro supuestos, que permiten derivar que nuestros estimadores por MCO no son sesgados. Es decir, nos permiten confiar en que la esperanza de la estimación realizada a través de MCO será igual al promedio poblacional: \\(E(\\hat\\beta)=\\beta\\) 6.4.3.5 5. Homocedasticidad El quinto supuesto tiene que ver con la eficiencia. Esto es, con la varianza del término de error de nuestra estimación. La varianza del término de error es constante. Es decir, dado cualquier valor de las variables explicativas, el error tiene la misma varianza: \\(Var(u\\mid{x})=\\sigma^2\\), es decir \\(Var(u)=\\sigma^2\\) De este modo, la varianza del error no observable, \\(u\\), condicional sobre las variables explicativas, es constante. Como mencionamos anteriormente, este supuesto no afecta el sesgo del estimador (es decir, que la distribución muestral de nuestra estimación \\(_\\hat{\\beta_1}\\) esté centrada en \\(\\beta_1\\)), sino su eficiencia (qué tanta dispersión hay en torno a la estimación \\(_\\hat{\\beta_1}\\) del parámetro \\(\\beta_1\\)). Este supuesto es central para poder calcular la varianza de los estimadores de MCO, y es el que permite que sea el estimador de mínima varianza entre los estimadores lineales insesgados. Si evaluamos la fórmula del error estándar de los coeficientes, se hace evidente la necesidad del supuesto: \\[\\hat{\\sigma}{_\\hat{\\beta}{_1}} = \\frac{\\hat{\\sigma}} {\\sqrt{\\sum(X_j – \\bar{X})^2(1 - R^2_j)}}\\] \\(\\hat{\\sigma}\\) Es la varianza del término de error: \\(\\frac{\\sum\\hat{u}}{n-k-1}\\) Para poder aplicar esta fórmula, necesitamos que \\({\\sigma^2}\\) sea constante. Cuando este supuesto no se cumple, es decir el término de error no se mantiene constante para distintos valores de \\(x\\), estamos ante un escenario de heterocedasticidad. Es bastante frecuente tener heterocedasticidad. La buena noticia es que esto no imposibilita la utilización del estimador OLS: ¡hay una solución! A. Evaluando el supuesto Para la evaluación de este supuesto se suelen seguir dos pasos: Diagnóstico visual: Lo que buscamos es observar si los residuales (distancia entre los puntos y la línea de regresión) son constantes para distintos valores de equis. En primer lugar, hacemos un simple diagrama de dispersión entre la variable independiente que nos interesa y la variable dependiente: ggplot(bienestar_la_sinna, aes(cseduc, gini_slc)) + geom_point() + theme_bw()+ geom_smooth(method=lm) Figura 6.11: Evaluación visual del supuesto de homocedasticidad Otra manera de hacer lo mismo y donde es más evidente: car::scatterplot(bienestar_la_sinna$cseduc,bienestar_la_sinna$gini_slc) Figura 6.12: Evaluación alternativa del supuesto de homocedasticidad Parece ser que en los niveles bajos de gasto en educación la variabilidad de los niveles de desigualdad es bastante más alta que a niveles más elevados de gasto en educación. Podemos hacer un mejor diagnóstico visual si utilizamos el modelo estimado (y no solo la relación entre las dos variables) y graficamos los residuos. Primero lo hacemos para el modelo bivariado: par(mfrow=c(2,2)) plot(modelo_1) Figura 6.13: Evaluación del supuesto de homocedasticidad incluyendo covariables Luego para el modelo multivariado: plot(modelo_2) ## Warning: not plotting observations with leverage one: ## 123 ## Warning: not plotting observations with leverage one: ## 123 par(mfrow=c(1,1)) Figura 6.14: Evaluación recomendada del supuesto de homocedasticidad Los dos gráficos que nos interesa analizar son los de la izquierda: donde se grafican los valores predichos y los residuos. Recordemos que bajo el supuesto de homocedasticidad, como la \\(Var(u\\mid{x})=\\sigma^2\\) , entonces la \\(Var(Y\\mid{x})=\\sigma^2\\). En otras palabras, la varianza de los residuos de los valores predichos a partir de las equis debiera ser constante. Por tanto, si no hay absolutamente ninguna heterocedasticidad (osea si estamos ante un escenario de homocedasticidad), deberíamos ver una distribución de puntos completamente aleatoria e igual en todo el rango del eje X y una línea roja constante. Sin embargo, claramente se observa que los residuos no son constantes para distintos valores de la variable de gasto en educación. Estamos frente a un caso de heterocedasticidad. Podemos también evaluar cada una de las variables del modelo y, así, identificar para qué variables específicas hay heterocedasticidad. Nuevamente, lo que esperamos es que la línea roja coincida con la línea punteada (en cero). car::residualPlots(modelo_2) ## Test stat Pr(&gt;|Test stat|) ## cseduc -1.80 0.074 . ## fdiingdp -0.09 0.928 ## [ reached getOption(&quot;max.print&quot;) -- omitted 10 rows ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Figura 6.15: Análisis de residuos para cada covariable Diagnóstico estadístico: En un segundo paso realizamos un diagnóstico estadístico. Hay distintas maneras de evaluar la homocedasticidad, pero exisrw el test de Breusch-Pagan es el que se utiliza con más frecuencia. La lógica que está por detrás de este test es la siguiente: se realiza una regresión donde la variable dependiente son los residuos al cuadrado, para evaluar si las variables independientes del modelo tienen relación con \\(u\\). Lo que se quiere encontrar es que ese efecto sea 0, porque si la varianza del error es constante, el error (residuos) no debería variar según los valores de las \\(x&#39;s\\). En definitiva, ¡no se quiere rechazar la hipótesis nula! bptest(modelo_2,studentize=T) ## ## studentized Breusch-Pagan test ## ## data: modelo_2 ## BP = 30, df = 10, p-value = 0.001 Como el p-value es menor a 0.05, se rechaza la hipótesis nula y, por tanto, estamos en un escenario de heterocedasticidad. B. Soluciones a la Heterocedasticidad Una vez que identificamos que tenemos heterocedasticidad, es necesario solucionarla. Una primera alternativa es corregir la forma funcional. Claramente podes estar ante el caso que la no constancia del término de error se deba a que la relación entre las variables no es lineal y oara esto ya vimos posibles soluciones a la no linealidad como exponenciar las variables. La segunda alternativa se da frecuentemente, cuando la naturaleza empírica de la relación hace que el error no sea constante. Sabemos que no podemos calcular los errores estándar de los estimadores como lo hacemos siempre en OLS: como la varianza del error no es constante es necesario modificar la forma en la que calculamos los errores. Entonces, para poder hacer inferencia necesitamos ajustar la estimación del error de forma tal de hacer una estimación válida en presencia de heterocedasticidad de la forma desconocida. Esto es, aunque no sepa el tipo de heterocedasticidad que tengo, puedo mejorar mi precisión y, además, hacer inferencia estadística válida. La fórmula habitual del error estándar del estimador es: \\[\\hat{\\sigma}{_\\hat{\\beta}{_1}} = \\frac{\\sum_{i=1}^{n}(x_{i}-\\overline{x})^2\\hat{\\sigma}} {\\sqrt{\\sum(X_j – \\bar{X})^2(1 - R^2_j)}}\\] Lo que pasa es que cuando tenemos homocedasticidad, lo que está en el nominador: \\(\\sum_{i=1}^{n}(x_{i}-\\overline{x})^2\\hat{\\sigma}=\\hat\\sigma\\). Como ahora \\(\\hat{\\sigma}\\) ya no es constante, esa igualdad ya no se mantiene! Esto porque el valor que adquiere \\(\\hat{\\sigma}\\) si va a depender de los distintos valores de \\(x\\). Además, recordemos que al estimar una regresión múltiple, en la estimación del error estándar es necesario descontar la variación de la \\(x_1\\) que es explicada por el resto de las \\(x_k\\) del modelo. De este modo, en una regresión múltiple, un estimador válido de \\(\\hat{\\sigma}{_\\hat{\\beta}{_1}}\\) bajo heterocedasticidad, será: \\[\\hat{\\sigma}{_\\hat{\\beta}{_1}} = \\frac{\\sum_{i=1}^{n}r_{ij}^2\\hat{u}^2}{\\sqrt{\\sum(X_j–\\bar{X})^2(1 - R^2_j)}}\\] Donde: \\(r_{ij}^2\\) Representa los residuos cuadrados de la regresión del resto de las variables independientes sobre la variable independiente \\(j\\). Representa la Varianza Total de equis luego de haber descontado el efecto del resto de las equis. A esta forma de estimar los errores estándar se la denomina “errores estándares robustos” o también le decimos “robustecer” el error, que no es otra cosa que dar cuenta y permitir la heterocedasticidad, volviendo los errores más exigentes. 6.4.4 Errores estándares robustos Si bien hay varias formas de robustecer los errores (incluso se podría hacer a mano), R nos permite calcularlos muy fácilmente con el comando coeftest del paquete lmtest. Además, el paquete sandwich con su función vcovHC nos permite incorporar la especificación de la matriz de varianza-covarianza robusta. HC0 = es la original de White (Wooldrige,2006) HC1= Es la que utiliza el software de Stata HC3 = Es la más conservadora y, por tanto, se suele ser altamente recomendada library(lmtest) library(sandwich) modelo_2_robusto_3=coeftest(modelo_2, vcov = vcovHC(modelo_2, &quot;HC3&quot;)) modelo_2_robusto_1=coeftest(modelo_2, vcov = vcovHC(modelo_2, &quot;HC1&quot;)) modelo_2_robusto_0=coeftest(modelo_2, vcov = vcovHC(modelo_2, &quot;HC0&quot;)) modelos_robustos&lt;-list(modelo_2, modelo_2_robusto_0, modelo_2_robusto_1, modelo_2_robusto_3) screenreg(modelos_robustos, custom.model.names = c(&quot;sin ES robustos&quot;, &quot;robusto HC0&quot;, &quot;robusto HC1&quot;, &quot;robusto HC3&quot;)) ## ## ========================================================================== ## sin ES robustos robusto HC0 robusto HC1 robusto HC3 ## -------------------------------------------------------------------------- ## (Intercept) 85.94 *** 85.94 *** 85.94 *** 85.94 ## (8.73) (8.77) (9.14) ## cseduc 1.59 *** 1.59 ** 1.59 ** 1.59 ## (0.45) (0.50) (0.52) ## fdiingdp 0.24 0.24 0.24 0.24 ## (0.18) (0.14) (0.14) ## cshlth -0.83 ** -0.83 *** -0.83 *** -0.83 ## (0.26) (0.22) (0.23) ## csssw -0.83 *** -0.83 ** -0.83 ** -0.83 ## (0.20) (0.25) (0.26) ## pop014wdi -0.93 *** -0.93 *** -0.93 *** -0.93 ## (0.17) (0.20) (0.21) ## s_dualism -0.17 *** -0.17 *** -0.17 *** -0.17 ## (0.03) (0.03) (0.03) ## ethnicdicot 3.68 *** 3.68 *** 3.68 *** 3.68 ## (1.04) (0.92) (0.96) ## rgdpch -0.00 ** -0.00 * -0.00 * -0.00 ## (0.00) (0.00) (0.00) ## as.factor(demrss)2 -2.29 -2.29 -2.29 -2.29 ## (4.75) (1.36) (1.41) ## as.factor(demrss)3 -2.90 -2.90 * -2.90 * -2.90 ## (4.70) (1.16) (1.20) ## as.factor(demrss)4 -5.14 -5.14 *** -5.14 *** -5.14 ## (4.62) (0.84) (0.88) ## legbal -10.40 *** -10.40 *** -10.40 *** -10.40 ## (2.22) (2.29) (2.38) ## -------------------------------------------------------------------------- ## R^2 0.59 ## Adj. R^2 0.56 ## Num. obs. 167 ## RMSE 4.52 ## ========================================================================== ## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 Todas las alternativas nos dan errores robustos similares. Las diferencias están dadas por distintas especificaciones sobre la matriz de varianza-covarianza robusta (HC). 6.4.5 Un caso especial de Heterocedasticidad: la varianza del error asociada a clusters Sabemos que hay observaciones que pueden estar relacionadas entre sí dentro de determinados grupos (o clusters). Por ejemplo, los países de América Latina podrían estar relacionados por pertenecer a regiones similares (América del Sur versus América Central o Caribe, regiones Andinas versus no Andinas, etc). Así, sus errores podrían estar correlacionados en base a la región que pertenecen. Entonces, tengo que la varianza del error condicionada por región no es constante. Cuando trabajamos con datos de panel, como es nuestro caso, esto es bastante más claro. Al contar con gasto en educación por país para varios años existe una auto-correlación del error entre observaciones de un mismo país. Es decir, los errores se encuentran correlacionados entre las observaciones de un mismo país para cada año (lo que se gasta en un año particular, probablemente esté relacionado con lo que se gastó en el año anterior). Entonces, cuando mis observaciones pertenecen a clusters y tengo motivos teóricos para pensar que sus errores estarán correlacionados dentro del cluster, la corrección supondrá clusterizar los errores estándar: cluster standard errors. Lo que estamos haciendo cuando clusterizamos los errores estándar es permitir que exista correlación del error dentro de los clusters (se relaja el supuesto de homocedasticidad). Así, permitimos que la varianza del error no sea constante, sino que sea diferente según los clusters. La selección de cuáles son los clusters relevantes estará dada teóricamente. En nuestro caso, hace sentido pensar que los clusters son los países. Recordemos que nuestro interés era estimar el el efecto del gasto en educación sobre el índice de Gini en países de América Latina. Observemos esta relación en la región para evaluar si, a primera vista, parece haber clusterización: library(ggplot2) ggplot(bienestar_la_sinna, aes(cseduc, gini_slc)) + geom_point() + facet_wrap(~country) Figura 6.16: Relación entre educación y Gini por país Parece ser que si existe cierta clusterización por país. Es deir, el gasto en educación por país suele mantenerse dentro de un rango que varía por poco. Cuando lo vemos así no queda tan claro porque son demasiados países, pero aún así pareciera haber cierta clusterización por país (las observaciones se agrupan por país; no parecen ser independientes). ggplot(bienestar_la_sinna, aes(cseduc, gini_slc, color=country)) + geom_point() + theme_bw() Figura 6.17: Condensamos la figura anterior en una sola faceta Para realizar la estimación por MCO con el error clusterizado, utilizamos el comando lm.cluster del paquete miceadds. Este comando lo que hace es clusterizar los errores estándar según la variable de clusterización indicada. En definitiva, lo que estamos haciendo es permitir que exista correlación del error dentro de los clusters, en este caso países (relajando el supuesto de homocedasticidad). Los errores estándar robustos por cluster pueden aumentar o disminuir los errores estándar. Es decir, los errores estándar clusterizados pueden ser más grandes o más pequeños que los errores estándar convencionales. La dirección en la que cambiarán los errores estándar depende del signo de la correlación del error intragrupo. Es necesario para hacer este paso que instales los paquetes miceadds y multiwayvcov. Mediante la opción “cluster” indicamos que variable será la que agrupará los errores: # install.packages(&quot;miceadds&quot;) library(miceadds) ## Loading required package: mice ## Loading required package: lattice ## ## Attaching package: &#39;mice&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## complete ## The following objects are masked from &#39;package:base&#39;: ## ## cbind, rbind ## * miceadds 3.0-16 (2018-12-11 10:39:14) ## ## Attaching package: &#39;miceadds&#39; ## The following objects are masked from &#39;package:GGally&#39;: ## ## mean0, min0 modelo_2_cluster &lt;- miceadds::lm.cluster( data=bienestar_la_sinna, formula=gini_slc ~ 1 + cseduc + s_dualism + fdiingdp + rgdpch + ethnicdicot + demrss + cshlth + csssw + legbal + repressauthor, cluster=&quot;country&quot;) summary(modelo_2_cluster) ## R^2= 0.51 ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.1e+01 3.96616 12.86 7.6e-38 ## cseduc 1.2e+00 0.63271 1.88 6.0e-02 ## [ reached getOption(&quot;max.print&quot;) -- omitted 9 rows ] Al usar clusters, el coeficiente de nuestra variable independiente cseduc bajó de 1,56 a 1,19 pero se mantuvo con significancia estadística alta (t valor de &gt; 12). 6.4.6 Un supuesto adicional para poder realizar inferencia Hasta aquí hemos repasado y evaluado empíricamente - en la medida de lo posible - los cinco supuestos del teorema de Gauss-Markov que aseguran que el estimador de MCO sea MELI (Mejor Estimador Lineal Insesgado). Sin embargo, estos no son suficientes para poder realizar inferencia estadística. Para esto, debemos asumir un supuesto adicional: 6.4.6.1 6. Normalidad en la distribución del error Como hemos visto anteriormente, para testear una hipótesis de significancia individual de un coeficiente estimado por MCO se utilizan los estadísticos \\(t\\) que permiten contrastar el valor \\(t\\) empírico contra un valor \\(t\\) teórico (llamado “valor crítico”) dado un nivel de significancia determinado (\\(\\alpha\\)), comúnmente se utiliza un alpha del 5% (por esto se habla de significancia estadística al 95% de confianza). Sin embargo, para poder realizar esta prueba de hipótesis y, así, hacer inferencia estadística, es necesario asumir que el coeficiente (\\(\\beta\\)) sigue una distribución T-Student. Sólo así podemos realizar la prubea de hipótesis utilizando el estadístico \\(t\\). El supuesto que permite esto es el de normalidad en la distribución del error. Como el estimador MCO (\\(\\beta\\)) es una combinación lineal de los errores (\\(Y = \\beta_0 + \\beta_1x + u\\)), al asumir distribución normal del error (\\(u\\)) podemos asumir distribución normal del estimador MCO. Sin embargo, como el error y su varianza son desconocidos, se estiman utilizando los residuos de la regresión (\\(\\hat{u}\\)), obteniendo así el error estándar de la estimación. Sin embargo, las estimaciones implican una pérdida de grados de libertad (por cada parámetro estimado pierdo un grado de libertad: n-k-1, n=tamaño muestral, k=cantidad de parámetros estimados - variables del modelo-, 1=la estimación del intercepto, \\(\\beta_0\\)) y, por tanto, la distribución del error estándar y, por tanto, del coeficiente, ya no distribuye normal sino T-Student (\\(\\hat\\beta \\sim t_{n-k-1}\\)). Los siguientes dos comandos nos permiten verificar que los residuos del modelo estimado a través de MCO siguen una distribución T-Student (aproximadamente normal). El comando qqplot viene por default en R y genera un gráfico de probabilidad normal que muestra la distribución de los datos contra una distribución normal teórica esperada. Por tanto, lo que es importante mirar del gráfico es que las observaciones (que son los residuos) no se salgan de las líneas punteadas (que delimitan la distribución normal). qqPlot(modelo_2$residuals) ## [1] 160 118 Figura 6.18: Normalidad de los residuos. Note que los países 160 y 118 son valores extremos El comando ggpubr del paquete ggpubr permite construir gráficos de densidad. De este modo, puedo graficar los residuos para evaluar visualmente si siguen una distribución aproximadamente normal. library(ggpubr) ## Loading required package: magrittr ## ## Attaching package: &#39;magrittr&#39; ## The following object is masked from &#39;package:texreg&#39;: ## ## extract ## The following object is masked from &#39;package:purrr&#39;: ## ## set_names ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract ggdensity(modelo_2$residuals, main = &quot;Gráfico de Densidad de los Residuos&quot;) Figura 6.19: Test de normalidad de los residuos Luego de la evaluación de los supuestos y buscar soluciones (cuando es necesario), podemos tener mayor confianza en nuestra estimación y la relación encontrada entre las variables. Aún así, una explicación completa de nuestro hallazgo implica profundizar en por qué y cómo se relacionan estas dos variables. Todo lo que hemos visto te será muy útil en el Capítulo 11 de selección de casos de estudio. En el próximo capítulo pasaremos a estimar modelos con variables dependientes binarias por Estimación de Maxima Verosimilitud. Ejercicios antes de continuar al próximo capítulo Haga un scatterplot entre la relación de la variable gini_slc y la variable fdiingdp. Agregue el nombre del país a cada observación. Al modelo 1 agreguele como control la variable fdiingdp e interprete su coeficiente. Haga los tests correspondientes para asegurarse de que los supuestos de OLS no son violados. Usando htmlregexporte la tabla de la regresión a Word. "],
["logit.html", "Capítulo 7 Modelos logísticos 7.1 Nociones básicas 7.2 Aplicación en R", " Capítulo 7 Modelos logísticos Por Francisco Urdinez Lecturas de referencia Box-Steffensmeier, J. M., Brady, H. E., &amp; Collier, D. (Eds.). (2008). The Oxford Handbook of Political Methodology (Vol. 10). Oxford Handbooks of Political Science. Oxford: Oxford University Press. Cap. 22 – Discrete Choice Methods. Agresti, A. (2007). An Introduction to Categorical Data Analysis, 2nd Ed. Hoboken: Wiley. Cap. 3, 4 y 5 – Generalized Linear Models; Logistic Regression; Building and Applying Logistic Regression Models. Greenhill, B., Ward, M. D., &amp; Sacks, A. (2011). The separation plot: A new visual method for evaluating the fit of binary models. American Journal of Political Science, 55(4), 991-1002. En el capítulo anterior vimos cómo hacer regresiones lineares cuando tenemos variables dependientes contínuas de una manera simple y cubriendo los paquetes más útiles a nuestro alcance. En este capítulo veremos cómo hacer lo mismo cuando tenemos variables dependientes dicotómicas (también llamadas binarias, o dummy), aquellas que asumen una de dos posibilidades, comunmente ‘0’ y ‘1’. Al igual que en los capítulos previos, no cubriremos aspectos sustanciales de la teoría por tras de cada modelo, ni desglosaremos en detalle las fórmulas. Para ello sugerimos referencias que van a ayudarte a acompañar lo que describimos, si es que nunca has leído al respecto. 7.1 Nociones básicas Los modelos para variables dependientes dicotómicas son utilizados para estimar la probabilidad de ocurrencia de un evento. En nuestra base de datos codificamos como ‘1’ los casos en que el evento sucede, y ‘0’ cuando no. Por ejemplo, si mi variable es “países con Acuerdos de Libre Comercio con Estados Unidos”, Chile será codificada con un ‘1’, Argentina con un ‘0’. Estos modelos estiman probabilidades, es decir, cual es la probabilidad de que se observe un ‘1’ dada ciertas caracteristicas de las observaciónes en nuestra muestra. Toda la discusión respecto a cómo estimar una probabilidad de una muestra es muy intersante, y puedes leerla en los capítulos 3, 4 y 5 de Agresti (2007). Es importante remarcar que en inglés existen dos conceptos diferentes que en español y portugués se traducen como una única palabra: probability y likelihood se traducen como probabilidad en diccionarios comúnes (¡haz la búsqueda si no nos crees!) a pesar de que la distinción entre ambos es vital para comprender como funcionan los modelos dicotómicos estimados por Máxima Verosimilitud (Maximum Likelihood en inglés). Aunque no vamos a ahondar en su distinción, es importante comprender que una probabilidad se estima a partir de una “población” de la cual conocemos sus “parámetros”, mientras que la verosimilitud recorre el camino inverso, es decir, estima los valores de los parámetros para los cuales el resultado observado mejor se ajusta a ellos (ver Figura 7.1). Figura 7.1: El camino de doble vía de probabilidad y verosimilitud Cuando tenemos una variable dependiente dicotómica que queremos modelar, se asume que la misma tiene una distribución de Bernoulli con una probabilidad de \\(Y=1\\) que desconocemos. Así, estimamos por medio de Máxima Verosimilitud nuestra probabilidad desconocida, dada una determinada combinación linear de variables independientes de nuestra elección (ver Figura 7.2). Un muy buen ejercicio para comprender como se estima un parámetro cuya distribución es binomial por medio de Máxima Verosimilitud es ofrecida por RPubs. Figura 7.2: Distribución de Bernoulli Los modelos logísticos han crecido enormemente en popularidad en Ciencia Política. Google Scholar reporta, de la búsqueda de “logit”+“political science” un total de 17100 referencias publicadas entre 2008 y 2018. La década previa (1998-2007) reporta 8900 artículos, y la anterior (1988-1997), 2100. Claro que la literatura en español y portugués es mucho más escasa. desde 1988 a 2018 hay 1060 resultados para la combinación “logit”+“ciencia política” y 355 para “logit”+“ciência política”. La Ciencia Política ha hecho extensivo el uso de modelos Logit, por sobre los modelos Probit, en buena medida debido a que los primeros permiten el cálculo de razones de oportunidades (odds ratios). Casi todos los manuales econométricos discuten las diferencias y similitudes entre ambos, aunque a los fines prácticos de estimar un modelo, ambos son muy similares y reportan coeficientes casi idénticos. Por ello, siendo que son métodos que llevan a resultados muy similares, sólo utilizaremos Logit en este capítulo. Los modelos Logit y Probit se diferencian es en su función de enlace (en la literatura recomendada se denominan link functions). Logit lleva su nombre debido a que su función está dada por el logaritmo natural de las razones de oportunidad (“log odds” \\(\\rightarrow\\) logit!). \\[ ln(odds) = ln(\\frac {p}{1 - p})\\] Despejando los términos podemos calcular la inversa de su función, de tal forma que tendremos \\[ logit^{-1}(\\alpha) = \\frac {1}{1+e^{-\\alpha}} = \\frac {e^\\alpha}{1+e^\\alpha}\\] Donde \\(\\alpha\\) es la combinación linear de las variables independientes y sus coeficientes. La inversa del Logit nos dará la probabilidad de la variable dependiente ser igual a ‘1’ dada una cierta combinación de valores para nuestras variables independientes. Figura 7.3: Inversa de la función logit Si profundizas en los textos recomendados, notarás que la función es indefinida en 0 y en 1, es decir, que la probabilidad se aproxima infinitamente al límite sin nunca tocarlo. Si modelaramos las probabilidades por medio de Mínimos Cuadrados Ordinarios, obtendríamos probabilidades mayores a 100% y menores a 0%, lo que es conceptualmente imposible. La función en forma de sigmoide de logit impide que esto suceda. Lo que estaremos estimando es la probabilidad de que, dada una cierta combinación de atributos (variables independientes y controles), cierta observación asuma que la variable dependiente sea igual a 1. Dicha probabilidad puede ser comparada al valor real que la variable dependiente ha asumido. Por eso mismo, muchos manuales se refieren a los modelos logit como modelo de “clasificación”, pues lo que estimamos es la probabilidad de cada observación ser clasificada como \\(Y=1\\). 7.2 Aplicación en R Como mencioné antes, los modelos probabilísticos han ganado enorme preeminencia en la Ciencia Política en los últimos años, y por ello es probable que estés buscando una guía aplicada para saber qué hacer y qué no hacer cuando tengas una variable dependiente dicotómica. Vamos a ilustrar un paso a paso en R utilizando como ejemplo la base de datos del libro “Democracies and Dictatorships in Latin America: Emergence, Survival, and Fall” (2013) de Scott Mainwaring y Aníbal Perez-Liñan. A lo largo del libro los autores analizan cuales variables independientes ayudan a explicar por qué ocurrieron quiebres democráticos en América Latina durante todo el siglo XX y comienzos del XXI. En el capítulo 4, los autores se preguntan qué factores explican la supervivencia de los regímenes políticos. Si bien prueban varios modelos complejos, algunos logísticos y otros de supervivencia (desarrollados en este libro en el capítulo siguiente), haremos un ejemplo muy sencillo para que nos acompañes desde tu computador en el paso a paso. Para cargar la base de datos del capítulo, usa nuestro paquete…(cargar paquete) # Suponiendo que la variable dependiente asume el valor ‘1’ si el país sufre el quiebre de su régimen político democrático y ‘0’ si no, ¿qué efecto tiene sobre la probabilidad de un quiebre democrático que en la constitución nacional se le otorguen grandes poderes constitucionales al poder ejecutivo? Y si esos poderes son pequeños, ¿la probabilidad de un quiebre democrático es mayor o menor? Como argumentan los autores, se puede medir estos poderes por medio de un índice creado por Shugart y Carey (1992) de poder presidencial que los autores incluyen en su base de datos (en el capítulo 13 explicamos cómo crear índices). Luego, cargamos la base de datos, llamada quiebres_dem_la. library(tidyverse) library(paqueteadp) data(quiebres_dem_la) Ahora la base se ha cargado en nuestra sesión de R: ls() ## [1] &quot;quiebres_dem_la&quot; Queremos ver, primeramente, cuantos países han sufrido un quiebre democrático en la muestra. ¿Recuerdas lo que se explicó en el capítulo 4? Comenzamos cargando el tidyverse. El país con más quiebres democráticos es Perú con 6, seguido de Argentina y Panamá con 5 cada uno. library(tidyverse) quiebres_dem_la %&gt;% filter(breakdown == 1) %&gt;% count(country) ## # A tibble: 18 x 2 ## country n ## &lt;chr&gt; &lt;int&gt; ## 1 Argentina 5 ## 2 Bolivia 2 ## 3 Brazil 1 ## 4 Chile 3 ## 5 Colombia 1 ## 6 Costa Rica 3 ## 7 Cuba 1 ## 8 Dominican Republic 1 ## 9 Ecuador 4 ## 10 Guatemala 2 ## 11 Haiti 1 ## 12 Honduras 4 ## 13 Mexico 1 ## 14 Nicaragua 1 ## 15 Panama 5 ## 16 Peru 6 ## 17 Uruguay 3 ## 18 Venezuela 2 El primer modelo que vamos a probar usa la variable dependiente de quiebre democrático (breakdown) siendo predicha por el índice de Shugart y Carey de poderes del ejecutivo (shugart). La función glm exige que definamos nuestros datos, y el modelo, pues con la misma función para Generalized Linear Models podría usar Probit, Poisson y otras funciones menos comúnes en Ciencia Política. modelo_1 &lt;- glm(breakdown ~ shugart, data = quiebres_dem_la, family = binomial(&quot;logit&quot;)) Como vimos en capítulos anteriores, la función summary nos permite ver de forma rápida los resultados de un modelo de regresión: summary(modelo_1) ## ## Call: ## glm(formula = breakdown ~ shugart, family = binomial(&quot;logit&quot;), ## data = quiebres_dem_la) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.727 -0.295 -0.269 -0.223 2.792 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.2393 0.9638 -0.25 0.8039 ## [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 217.84 on 643 degrees of freedom ## Residual deviance: 209.56 on 642 degrees of freedom ## (1572 observations deleted due to missingness) ## AIC: 213.6 ## ## Number of Fisher Scoring iterations: 6 El coeficiente para la variable shugart está negativamente asociado a la probabilidad de ocurrencia de un quiebre de régimen (-0.191), y es estadísticamente significativo (p=0.00312). Ahora bien, a diferencia de los modelos por Mínimo Cuadrados del capítulo anterior, donde podíamos interpretar directamente el efecto de la variable independiente sobre la dependiente a partir de los coeficientes de la regresión, en el caso de regresiones logísticas esto no es tan sencillo. Debemos transformar los coeficientes en probabilidades u oportunidades. Si partimos de que la función de enlace de logit es el logaritmo de las razones de oportunidades, tenemos que: \\[ln(\\frac {p}{1 - p}) = \\beta_{0} + \\beta_{1}x_{1}\\] Despejando \\(ln\\), tenemos que: \\[(\\frac {p}{1 - p}) = e^{\\beta_{0}+\\beta_{1}x_{1}}\\] Y despejando los términos nuevamente tenemos que: \\[\\hat{p} = \\frac {e^{\\beta_{0}+\\beta_{1}x_{1}}}{1 + e^{\\beta_{0}+\\beta_{1}x_{1}}}\\] Lo que queremos, entonces, es transformar los coeficientes tal y como los reporta R en una probabilidad asociada a que la variable dependiente asuma el valor ‘1’. Sabemos que la variable independiente shugart es un índice que a mayor valor, mayor concentración de poder del ejecutivo vis a vis el legislativo, por lo tanto el coeficiente de la regresión nos indica que a menor concentración de poder del ejecutivo, mayor la probabilidad de un quiebre de régimen. Esto resultará intuitivo a todo aquel que esté familiarizado con la literatura. La muestra del libro cubre 20 países latinoamericanos entre 1900 y 2010, y el índice oscila entre un mínimo de 5 (para Haití, en varios años) y un máximo de 25 (Brasil en 1945) (ver Figura 7.4. Cuando anteponemos un (-) a la variable dentro de la función arrange, la ordenamos de mayor a menor. quiebres_dem_la %&gt;% select(country, year, shugart)%&gt;% arrange(shugart) ## # A tibble: 2,216 x 3 ## country year shugart ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Haiti 1987 5 ## 2 Haiti 1988 5 ## 3 Haiti 1989 5 ## 4 Haiti 1990 5 ## 5 Haiti 1991 5 ## 6 Haiti 1992 5 ## 7 Haiti 1993 5 ## 8 Haiti 1994 5 ## 9 Haiti 1995 5 ## 10 Haiti 1996 5 ## # … with 2,206 more rows quiebres_dem_la %&gt;% select(country,year, shugart)%&gt;% arrange(-shugart) ## # A tibble: 2,216 x 3 ## country year shugart ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Brazil 1945 25 ## 2 Chile 1981 24 ## 3 Chile 1982 24 ## 4 Chile 1983 24 ## 5 Chile 1984 24 ## 6 Chile 1985 24 ## 7 Chile 1986 24 ## 8 Chile 1987 24 ## 9 Chile 1988 24 ## 10 Haiti 1950 24 ## # … with 2,206 more rows Podemos ver la distribución de la variable por medio de un histograma. Casi todas las observaciones tienen valores de 16 en el índice, y asume una distribución normal. ggplot(quiebres_dem_la, aes(shugart)) + geom_histogram() Figura 7.4: Histograma del índice de Shugart &amp; Carey ¿Cómo puedo saber en qué magnitud se afecta la probabilidad de un quiebre democrático si el nivel de concentración de poder del ejecutivo pasa de un puntaje de 5 (mínimo) a uno de 25 (máximo) cuando no controlamos por nada más en la regresión? Para ello podemos reemplazar los valores de nuestra última fórmula, en la que hemos aislado en el lado izquierdo de la fórmula a \\(\\hat {p}\\). Primero debemos calcular cuál es la probabilidad de sufrir un quiebre de régimen en un nivel de shugart de 5 y en un nivel 25, respectivamente, para luego calcular la diferencia. Así tenemos que \\[\\hat{p} = \\frac {e^{(0+(-0.019*5))}}{1 + e^{(0+(-0.019*5))}}\\] Notarás que el valor correspondiente al intercepto es igual a 0 pues ese coeficiente no ha resultado estadísticamente significativo. Sabemos que para un índice de Shugart y Carey de 5, luego de hacer el cálculo en la fórmula arriba, la probabilidad es igual a 0.47 o 47%. Te recomiendo que te detengas en la lectura y hagas el cálculo de la función en lápiz y papel. Si repetimos el proceso para un valor de shugart de 25, la probabilidad cae a 38%. Con las probabilidades podemos calcular oportunidades, que son simplemente \\(\\frac {p}{1-p}\\). De esta manera, la oportunidad (odd en inglés) para un valor 5 del índice de Shugart y Carey es de 0.90 mientras que para un índice de Shugart y Carey de 25 es de 0.62. \\[oportunidad_1 = \\frac {0.47}{1 - 0.47}\\] \\[oportunidad_2 = \\frac {0.38}{1 - 0.38}\\] La utilidad de las oportunidades es que permite calcular razones de probabilidades (odds ratios) ¿Cuál es la gracia de calcular una razón de probabilidades? Veamos. Cuando calculo la probabilidad de un cambio en el índice de Shugart y Carey de 23 a 24, la magnitud será diferente a si calculamos un cambio en la probabilidad si el índice pasa de 12 a 13, por ejemplo. Si no me crees, puedes probarlo. Esto se debe a que los efectos de la variable independiente sobre la probabilidad de que la variable dependiente sea =1 no son lineares (recuerde la función en “S” de la Figura 7.3). Sin embargo, las razones de probabilidades tienen la útil propiedad de poder reflejar cambios independientemente de la curvatura de la función, es decir, son cambios “constantes”. Así, podemos expresar el efecto de la variable sin tener que especificar un valor determinado para ella. Por ello, verás en muchos artículos los coeficientes expresados como odds ratios. Veamos cómo sería el cálculo de razones de probabilidad siguiendo el ejemplo que acabamos de crear con la base de datos de Mainwaring y Perez-Liñan. Dijimos que la oportunidad está dada por \\(\\frac {p}{1 - p}\\). Una razón de oportunidades se expresaría, entonces, como \\(\\frac {\\frac {p_1}{1-p_1}}{\\frac {p_2}{1-p_2}}\\). Supongamos que en un país el año 1992 tenía un índice de Shugart de 15, y que en el año 1993 ese índice subió a 16. ¿Cuánto cambia la probabilidad de un quiebre democrático? \\[ Pr(quiebredemocratico){_{país,1992}} = \\frac {e^{(0+(-0.019*15))}}{1 + e^{((0+(-0.019*15))}} = 0.42\\] \\[ Pr(quiebre democratico){_{país,1993}} = \\frac {e^{(0+(-0.019*16))}}{1 + e^{(0+(-0.019*16))}} = 0.43\\] La probabilidad difiere poco y cae en un 2.4% lo que parece ser un efecto pequeño. La razón de oportunidades se calcula como el cociente de ambas oportunidades, así tenemos que: \\[\\frac {0.42}{0.43}=0.97\\] De esta manera, toda razón de oportunidades mayor a 1 expresa un cambio positivo, mientras que todo valor menor a 1 (entre 0 y 1) representa un cambio negativo en las probabilidades estimadas. Si hiciéramos el mismo ejercicio para otros valores del índice de Shugart y Carey, por ejemplo, un cambio de 3 a 4 o de 23 a 24, el cociente de las oportunidades daría 0.97. Ahora, haz el cálculo para el valor real de Chile en 1992 y 1993 como práctica, te esperamos. R ofrece paquetes para que este análisis sea fácil de hacerse. Podemos visualizar fácilmente las razones de oportunidades utilizando el paquete sjPlot. Podemos calcular probabilidades predichas, y además podemos hacer tests para saber la capacidad explicativa de nuestros modelos. Utilizando la misma base de datos haremos un ejemplo de una rutina típica, que puedes reproducir en tu computador utilizando tus propios datos. Los pasos a seguir son (a) estimar los modelos, (b) crear tablas formateadas para incluir en nuestros procesadores de texto, (c) crear figuras para visualizar la magnitud de los coeficientes por medio de cociente de oportunidades, (d) visualizar probabilidades predichas para variables de interés, y (e) calcular capacidad explicativa de los modelos (porcentaje correctamente predicho, AIC, BIC, curvas ROC, Brier scores o separation plots, que explicaremos a continuación). 7.2.1 Estimar los modelos Para ejemplificar este paso lo que haremos es agregar al modelo 1 dos modelos más: El modelo 2 tendrá como variables independientes al índice de Shugart y Carey más la variable age que mide en años la edad del régimen político. ggplot(quiebres_dem_la, aes(age)) + geom_histogram() modelo_2 &lt;- glm(breakdown ~ shugart + age, data = quiebres_dem_la, family = binomial(&quot;logit&quot;)) Luego de ejecutar la regresión, con summary vemos los coeficientes: summary(modelo_2) ## ## Call: ## glm(formula = breakdown ~ shugart + age, family = binomial(&quot;logit&quot;), ## data = quiebres_dem_la) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.734 -0.302 -0.266 -0.218 2.828 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.22220 0.96764 -0.23 0.8184 ## [ reached getOption(&quot;max.print&quot;) -- omitted 2 rows ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 217.84 on 643 degrees of freedom ## Residual deviance: 209.52 on 641 degrees of freedom ## (1572 observations deleted due to missingness) ## AIC: 215.5 ## ## Number of Fisher Scoring iterations: 6 El modelo 3 agrega a las dos variables del modelo 2 una tercer variable llamada fh que corresponde al Freedom House score de democracia. Cuanto mayor el score, mejor el funcionamiento de la democracia de un país. ggplot(quiebres_dem_la, aes(fh)) + geom_histogram() Figura 7.5: Histograma del índice de democrácia de FH Quizás también querramos ver por medio de una matriz de correlación como estan relacionadas nuestras variables independientes antes de incluirlas en el modelo. Para ello, podemos usar el paquete GGally que permite crear matrices de correlación con ggplot2. Vemos que la correlación entre age, fh y shugart es muy baja. library(GGally) correlacion &lt;- quiebres_dem_la %&gt;% dplyr::select(age, fh, shugart) ggcorr(correlacion, label = T) El modelo 3 entonces se verá de la siguiente manera: modelo_3 &lt;- glm(breakdown ~ shugart + age + fh, data = quiebres_dem_la, family = binomial(&quot;logit&quot;)) summary(modelo_3) ## ## Call: ## glm(formula = breakdown ~ shugart + age + fh, family = binomial(&quot;logit&quot;), ## data = quiebres_dem_la) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7864 -0.0008 -0.0001 0.0000 1.8940 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 15.360 6.584 2.33 0.020 * ## [ reached getOption(&quot;max.print&quot;) -- omitted 3 rows ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 71.271 on 421 degrees of freedom ## Residual deviance: 12.113 on 418 degrees of freedom ## (1794 observations deleted due to missingness) ## AIC: 20.11 ## ## Number of Fisher Scoring iterations: 12 Ahora la variable shugart ha dejado de ser estadisticamente significativa pero lo son fh y el intercepto. 7.2.2 Crear tablas ¿Recuerde el ADP? Una de las funciones que hemos facilitado es la de creación de tablas editables para artículos académicos utilizando la función texreg. Si utilizas nuestro paquete te ahorrarás muchos pasos que son engorrosos. Basicamente la función contiene tres paquetes, uno para exportar tablas a html (htmlreg), uno para exportar tablas a \\(\\LaTeX\\), y un tercero para verlas en RStudio (llamado screenreg). Por medio de htmlreg podemos exportar nuestras tablas formateadas en html para poder incorporarlas en nuestros artículos directamente. Una vez que estimamos los modelos de interés, los agrupamos en una lista por medio de la función list. Esto ahorra trabajo, pues en vez de tener que escribir el nombre de los tres modelos, simplemente nos referiremos al conjunto mp_modelos: library(texreg) ## Version: 1.36.23 ## Date: 2017-03-03 ## Author: Philip Leifeld (University of Glasgow) ## ## Please cite the JSS article in your publications -- see citation(&quot;texreg&quot;). ## ## Attaching package: &#39;texreg&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract mp_modelos &lt;- list(modelo_1, modelo_2, modelo_3) Para exportar la tabla a html debemos definir la opción file y un nombre para el archivo html. Yo la he llamado “tabla_1”. También voy a usar dos opciones de la función para mejorar aún más la tabla. La primera es custom.model.names que me permite ponerle nombres a los modelos y la segunda es custom.coef.names que me permite ponerle nombre a los coeficientes. Así el comando sería: htmlreg(mp_modelos, custom.model.names = c(&quot;Modelo 1&quot;, &quot;Modelo 2&quot;, &quot;Modelo 3&quot;), custom.coef.names = c(&quot;Intercepto&quot;, &quot;Indice de Shugart &amp; Carey&quot;, &quot;Edad del régimen&quot;, &quot;Ind. Freedom House&quot;), file=&quot;tabla_1.html&quot;) Esta línea de comando crea un archivo de html en la carpeta de su proyecto. Si haces clic derecho sobre el archivo, puedes abrirlo en Word o cualquier procesador de textos. También podemos incluir una versión en pantalla usando screenreg. En este caso, veamos como se ve sin la opción custom.coef.names. A este comando uno puede usarlo en sus proyectos de trabajo, y una vez que se decide por la mejor tabla, la exporta con htmlreg. screenreg(mp_modelos, custom.model.names = c(&quot;Modelo 1&quot;, &quot;Modelo 2&quot;, &quot;Modelo 3&quot;) ) ## ## ================================================ ## Modelo 1 Modelo 2 Modelo 3 ## ------------------------------------------------ ## (Intercept) -0.24 -0.22 15.36 * ## (0.96) (0.97) (6.58) ## shugart -0.19 ** -0.19 ** -0.22 ## (0.06) (0.07) (0.17) ## age -0.00 0.17 ## (0.02) (0.11) ## fh -3.75 * ## (1.51) ## ------------------------------------------------ ## AIC 213.56 215.52 20.11 ## BIC 222.50 228.92 36.29 ## Log Likelihood -104.78 -104.76 -6.06 ## Deviance 209.56 209.52 12.11 ## Num. obs. 644 644 422 ## ================================================ ## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 Observemos como shugart deja de ser estadísticamente significativa cuando controlamos por fh que, además, pasa a ser la única variable estadísticamente significativa del tercer modelo.También vemos como el número de observaciones cae significativamente al incluir la variable fh lo que hace difícil comparar los modelos. La función skim nos será muy útil para ver missing values. Las tres variables tienen niveles de missing muy altos, para fh de casi \\(2/3\\) del total de observaciones. library(skimr) vars_modelo_3 &lt;- quiebres_dem_la %&gt;% dplyr::select(age, fh, shugart) skim(vars_modelo_3) ## Skim summary statistics ## n obs: 2216 ## n variables: 3 ## ## ── Variable type:numeric ──────────────────────────────── ## variable missing complete n mean sd p0 p25 p50 p75 p100 hist ## [ reached getOption(&quot;max.print&quot;) -- omitted 3 rows ] Jane Miller pone mucho énfasis en su libro The Chicago Guide to Writing about Multivariate Analysis (2013), respecto a la diferencia entre significancia estadística y significancia substantiva a la hora de interpretar regresiones: no por ser una variable estadísticamente significativa la magnitud del efecto será el esperado, ni tampoco significa que el hallazgo es relevante cientificamente. Para explorar las magnitudes de los coeficientes vamos a concentrarnos en el tercer modelo. Además, aprovecho aquí para mostrarte como puedes cambiar los coeficientes que obtienes directamente de la regresión logística, que son log odds, y reemplazarlos por odds ratios. Para ello, puedes ayudarte de los argumentos override.coef, override.se y override.pvalues de screenreg(). Los odds ratios, como vimos antes, son simplemente los coeficientes exponenciados. Calcular errores estándar y valores-p, en tanto, es un poco más complejo: necesitamos acceder a la matriz varianza-covarianza del modelo. Por suerte esto ya lo hizo Andrew Heiss en R, a quien agradecemos por compartir la función. Nosotros adaptamos levemente sus funciones, que podrás ingresar fácilmente en tus análisis siempre y cuando hayas cargado el paquete del libro, paqueteadp (como hicimos antes). screenreg(modelo_3, custom.model.names = &quot;Modelo 3&quot;, override.coef = exp(coef(modelo_3)), # las siguientes funciones de ayuda, odds_*, están en el paquete del libro override.se = odds_se(modelo_3), override.pvalues = odds_pvalues(modelo_3), # adicionalmente, omitiremos el coeficiente omit.coef = &quot;Inter&quot; ) ## ## ========================== ## Modelo 3 ## -------------------------- ## shugart 0.80 *** ## (0.13) ## age 1.18 *** ## (0.13) ## fh 0.02 ## (0.04) ## -------------------------- ## AIC 20.11 ## BIC 36.29 ## Log Likelihood -6.06 ## Deviance 12.11 ## Num. obs. 422 ## ========================== ## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 Al obtener una tabla como la que acabamos de crear tenemos dos desafíos que abordaremos en los proximos subtitulos: primero, saber si la magnitud de los efectos es substantiva desde un punto de vista científico. Por ejemplo, si la variable fh resulta estadísticamente significativa pero la probabilidad de un quiebre de régimen cae en 0.03% si un país pasa del peor score de fh al mejor, entonces diríamos que, a pesar de estadísticamente significativa, nuestra variable carece de significancia substantiva. Segundo, comparar los modelos para saber cuál tiene mejor ajuste, es decir, cual tiene mejor capacidad explicativa. A diferencia de OLS no contamos con \\(R^2\\), por lo que se recurre a otras medidas. Veamos! 7.2.3 Visualización de resultados Podemos representar visualmente la última tabla con la función plot_model() del paquete sjPlot. Una figura es mucho más intuitiva que una tabla. La figura 7.6 muestra la probabilidad predicha de un quiebre democrático para para cada valor de poderes presidenciales. library(sjPlot) plot_model(modelo_2, type = &quot;pred&quot;, show.ci = TRUE, terms = &quot;shugart&quot;, title = &quot;Probabilidad predicha para Shugart (poderes presidenciales)&quot;) Figura 7.6: Modelo 2 en base Mainwaring y Perez Liñan (2013), prob. predicha de Shugart con otras variables en sus medias También podemos graficar los coeficientes como odds ratios: En azul obtenemos coeficientes positivos, en rojo coeficientes negativos (recuerde que odds ratios menores a 1 son negativos). El coeficiente se expresa como su valor promedio y su intérvalo de confianza del 95%. Si el coeficiente es estadisticamente significativo, su intérvalo de confianza no tocará la línea en 1. Si, por el contrario no son significativos, el efecto cruzará la línea. Aquellos coeficientes cuyo efecto es significativo, además, son señalados con un * (esto se logra con la opcion show.p = T). Quizás te preguntes por qué la variable shugart no tiene intérvalo de confianza. En realidad, lo que sucede es que la escala es mucho menor para esta variable que para fh. Al estar la escala en base a fh no se nota el intérvalo para shugart. # plot_model(modelo_3, show.ci = T, show.p = T) Es cada vez más frecuente encontrar este tipo de figuras en vez de tablas y personalmente pienso que son preferibles. Un precursor en la disciplina en el uso de figuras y gráficos informativos fue Edward Tufte. La ciencia política, sin embargo, no prestó demasiada atención a la presentación de resultados por medio de figuras hasta hace unas dos décadas, pero hoy la tendencia en la disciplina es a prescindir de tablas cuando estas no sean esenciales. Paquetes como sjPlot han facilitado esta tarea. Con el argumento type = &quot;pred&quot; en plot_model() podemos apreciar cómo es la relación entre cada variable independiente y la probabilidad de que la variable dependiente ocurra, cuando las demás variables independientes están en sus medias (en este caso fhcon agey shugarten valores medios). Vemos que la relación entre la probabilidad de ocurrencia de un quiebre democrático y el score de calidad institucional de Freedom House, es negativa. Cuando un país tiene un score de Freedom House de 2.5 (recuerdo, con las otras variables independientes en sus medias), la probabilidad de un quiebre oscila entre 58 y 100% con un intérvalo de confianza de 95%. Cuando el score es de 7.5, la probabilidad de un quiebre es casi nula. Las probabilidades altas de ocurrencia de un quiebre se registran en scores de Freedom House menores a 3.5, aproximadamente. Para cada valor de fh, además, vemos una nuve de puntos, que representa la cantidad de observaciones en cada valor. Así, sabemos que en el caso de fh la mayoría de las observaciones están entre los valores 6 y 10, que es donde el efecto es menor en mangnitud. Caer a un fh menor a 5 dispara las probabilidades de quiebre democrático. # plot_model(modelo_3, # type = &quot;pred&quot;, # show.ci = TRUE, # terms = &quot;fh&quot;, # title = &quot;Modelo 3 en base Mainwaring y Perez Liñan (2013), pr. predichas con otras variables en sus medias&quot;) 7.2.4 Ajuste de los modelos Una vez que uno ha analizado la significancia substantiva de los modelos por medio de figuras, graficando las probabilidades predichas y los efectos marginales, podemos explorar el ajuste de los modelos. Así como en MCO se usa \\(R^2\\) y el Mean Root Square Error ,existe una serie de estadísticas diseñadas para saber cuál de los modelos logísticos tiene mejor ajuste. 7.2.4.1 Pseudo-\\(R^2\\) Para entender como se interpreta el Pseudo-\\(R^2\\) (normalmente se usa el de McFadden) es importante compreender como se diferencia de un \\(R^2\\) de regresión lineal (puedes usar este link para acompañar su interpretación en el el capítulo de OLS). La fórmula, en este caso es \\(psR^2= 1-\\frac {ln \\hat{L}(Modelo completo)}{ln \\hat{L}(Modelo sólo con intercepto)}\\) Donde \\(\\hat{L}\\) es la verosimilitud estimada por el modelo. Básicamente, lo que la fórmula está haciendo es comparar el modelo con todas nuestras covariables al modelo que apenas tiene el intercepto, para ver cuanto mejora la capacidad explicativa del mismo si incluimos variables independientes. Como \\(L\\) está entre 0 y 1, su log es menor o igual a 0. Así, cuanto menor la razón, mayor la diferencia entre el modelo elegido y el modelo con apenas el intercepto. La función es tomada del paquete pscl. El modelo 1 y 2 tienen casi la misma capacidad explcativa, pese a que se ha incorporado una variable más en el segundo modelo. En cambio, al agregar fh al Modelo 3, nuestro Pseudo-\\(R^2\\) se dispara a 0.96. library(pscl) pR2(modelo_1)[[&quot;McFadden&quot;]] ## [1] 0.43 pR2(modelo_2)[[&quot;McFadden&quot;]] ## [1] 0.43 pR2(modelo_3)[[&quot;McFadden&quot;]] ## [1] 0.97 También se podría implementar un \\(psR^2\\) ajustado, es decir, una versión que penalice por cantidad de covaraibles. Al igual que en el \\(R^2\\) ajustado, incorporamos a la fórmula el término \\(c\\), que es cantidad de covariables, y así tenemos que \\(Pseudo-R^2= 1-\\frac {ln \\hat{L}(Modelo completo)-c}{ln \\hat{L}(Modelo sólo con intercepto)}\\) Si bien el \\(R^2\\) siempre aumenta al adicionar covariables al modelo, si utilizamos medidas ajustadas, estaremos teniendo en cuenta este aspecto. library(DescTools) PseudoR2(modelo_1, c(&quot;McFadden&quot;)) ## McFadden ## 0.43 PseudoR2(modelo_2, c(&quot;McFadden&quot;)) ## McFadden ## 0.43 PseudoR2(modelo_3, c(&quot;McFadden&quot;)) ## McFadden ## 0.97 7.2.4.2 AIC Muchas veces las tablas con modelos de regresión logística reportan AIC (Akaike Information Criterion). Estos permiten comparar entre modelos diferentes para una misma muestra, y decidir cual tienen mejor ajuste. El AIC,como el \\(Pseudo-R^2\\), usa información de \\(ln(\\hat {L})\\). El AIC lo que hace es medir la “distancia” que existe entre los verdaderos parámetros y los estimadores del modelo, por medio de una distancia matemática llamada divergencia de Kullback-Leibler. Cuanto menor sea esta distancia, mejor el modelo. Es muy útil a la hora de comparar diferentes modelos y se calcula como \\(AIC = 2p-2ln(\\hat {L})\\) Donde \\(p\\) es la cantidad de regresores incluyendo al intercepto, y \\(\\hat{L}\\) es la verosimilitud estimada por el modelo. De los tres modelos, claramente el que mejor ajuste posee bajo este criterio es el tercero. AIC(modelo_1) ## [1] 214 AIC(modelo_2) ## [1] 216 AIC(modelo_3) ## [1] 20 7.2.4.3 BIC El BIC (Bayesian information criterion), al igual que AIC, es un criterio de comparación de modelos según su ajuste. A los fines prácticos, y para no entrar en las diferencias entre AIC y BIC, es importante saber que BIC penaliza de manera más rigurosa que AIC la complejidad del modelo, siendo que su fórmula es \\(BIC=ln(n)p-2ln(\\hat {L})\\) donde agrega a la formula \\(n\\) que es el número de observaciones en la muestra. Los resultados son similares a los que obtubimos mediante AIC, es decir que el mejor ajuste lo muestra el tercer modelo. BIC(modelo_1) ## [1] 222 BIC(modelo_2) ## [1] 229 BIC(modelo_3) ## [1] 36 Se recomienda usar AIC o BIC pero no es necesario reportar ambos en una tabla con varios modelos. 7.2.4.4 Porcentaje de predicciones correctas Para entender el porcentaje de predicciones correctas en un modelo es importante tener en claro que un modelo produce cuatro combinaciones posibles: Figura 7.7: Tabla de clasificación a partir de la cual se calcula el porcentaje de predicciones correctas Toda observación será clasificada como “correcta” si corresponde a la casilla superior izquierda (verdadero positivo) o a la inferior derecha (verdadero negativo). El porcentaje de observaciones que pertenecen a estas dos casillas determina el porcentaje de predicciones correctas en el modelo. Como criterio estándar, si la probabilidad estimada para una observación es mayor o igual a 50% se estima que es una probabilidad positiva, y si es menor a 50% será una probabilidad negativa. Para poder calcular las predicciones correctas necesitamos hacer varios pasos, pero si usas los comandos que te damos a continuación, vas a poder hacerlo con tu propia base de datos cuando trabajes solo. Primero que nada necesitamos usar el paquete broom, que transforma los modelos que tenemos guardados (modelos 1, 2 y 3) en bases de datos en formato tidy como vimos en el Capítulo 6. Voy a calcular los valores predichos del modelo 3 como ejemplo que puede repetirse para los otros modelos. Lo primero que necesitamos son los valores predichos, es decir, la probabilidad asignada a cada obervación de que Y=1. En el tidy que obtenemos usando broom esta variable se llama .fitted. library(broom) pred_modelo_3 &lt;- augment(modelo_3, type.predict = &quot;response&quot;) pred_modelo_3 ## # A tibble: 422 x 11 ## .rownames breakdown shugart age fh .fitted .resid .std.resid ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 75 0 19 1 8 8.47e- 9 -1.30e-4 -1.30e-4 ## 2 76 0 19 2 8 10.00e- 9 -1.41e-4 -1.41e-4 ## 3 77 1 19 3 3 6.19e- 1 9.79e-1 1.31e+0 ## 4 85 0 19 1 10 4.70e-12 -3.07e-6 -3.07e-6 ## 5 86 0 19 2 10 5.55e-12 -3.33e-6 -3.33e-6 ## 6 87 0 19 3 11 1.55e-13 -5.56e-7 -5.56e-7 ## 7 88 0 19 4 11 1.83e-13 -6.04e-7 -6.04e-7 ## 8 89 0 19 5 11 2.16e-13 -6.57e-7 -6.57e-7 ## 9 90 0 19 6 11 2.55e-13 -7.14e-7 -7.14e-7 ## 10 91 0 19 7 10 1.28e-11 -5.05e-6 -5.05e-6 ## # … with 412 more rows, and 3 more variables: .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, ## # .cooksd &lt;dbl&gt; La función dplyr nos permite transformar nuestra nueva base obtenida por medio de broom. Una de las funciones básicas de dplyr es select, que nos permite escoger variables por sus nombres. Necesitamos identificar los valores predichos y la variable dependiente (en este caso breakdown) para poder comparar la probabilidad asignada por el modelo con el valor real de la varible. La función mutate de dplyr nos permitirá crear una variable binaria para saber si el modelo ha predicho correctamente cada observación. El punto de corte es de 0.5, es decir, si la probabilidad estimada es igual o superior a 0.5 la se considera que el modelo predijo la ocurrencia del evento, y si es menor a ese valor se considera que predijo la no ocurrencia del evento. Trabajar con valores arbitrarios de corte tiene desventajas, que veremos a continuación que son resueltas por los ROC. Al fin creamos una variable que llamamos pcp (porcentaje correctamente predicho) que refleja la proporcion de verdaderos positivos y verdaderos negativos estimados por el modelo 3. El resultado muestra que de las 422 observaciones del modelo 3, 99.5% han sido correctamente predichas. pred_modelo_3 %&gt;% dplyr::select(.rownames, breakdown, .fitted) %&gt;% mutate(predicho_binario = if_else(.fitted &gt;= 0.5, 1, 0), predicho_correcto = if_else(breakdown == predicho_binario, 1, 0)) %&gt;% summarize(pcp = mean(predicho_correcto)) ## # A tibble: 1 x 1 ## pcp ## &lt;dbl&gt; ## 1 0.995 7.2.4.5 Brier Score Ésta es otra medida de ajuste, menos frecuente en Ciencia Política. Cuanto más próximo el score de Brier está de 0, mejor el ajuste del modelo. En general uno no utiliza todas las medidas que estamos repasando (AIC, BIC, Brier, etc.) sino que elige dos o tres que sean de su agrado. Creo que situaciones en que se quiere “castigar” mucho las predicciones erróneas, ésta es una alternativa ideal ya que su fórmula viene dada por \\[BS = \\frac {1}{N} \\sum(\\hat{p}-x)^2\\], donde \\(N\\) es el número de observaciones, \\(\\hat{p}\\) es la probabilidad predicha para cada observación, y \\(x\\) es el valor real de la observación en nuestra base de datos. El score es el promedio para todas las observaciones de la muestra. ¿Cuál de los tres modelos tiene menor score? BrierScore(modelo_1) ## [1] 0.038 BrierScore(modelo_2) ## [1] 0.038 BrierScore(modelo_3) ## [1] 0.0048 7.2.4.6 ROC plot Las curvas de ROC tienen la ventaja de no definir un límite arbitrario a partir del cual se decide si la observación ha sido correcta o incorrectamente clasificada. Su desventaja es que es una figura extra que deberás incluir en tu artículo, que quizás puedas incluir en el apéndice. Para interpretar estas figuras, lo que nos interesa es el área debajo de la curva diagonal que cruza la figura. A mayor el área bajo la curva, mejor el ajuste del modelo. Si quieren leer más al respecto, el área conforma un score que se denomina AUC score (que viene de Area Under the Curve). Vamos a construirlo con la función geom_roc() del paquete plotROC. Lo que haré primero es crear una base de datos con los resultados de los tres modelos de regresión. A la base la llamaré pred_modelos. library(plotROC) pred_modelos &lt;- bind_rows(augment(modelo_1, response.type = &quot;pred&quot;) %&gt;% mutate(modelo = &quot;Modelo 1&quot;), augment(modelo_2, response.type = &quot;pred&quot;) %&gt;% mutate(modelo = &quot;Modelo 2&quot;), augment(modelo_3, response.type = &quot;pred&quot;) %&gt;% mutate(modelo = &quot;Modelo 3&quot;)) Una vez creada la base con la información de los tres modelos, procedo a crear la figura usando gglpot, como vimos en el CAP XXX. En el eje vertical tenemos la sensibilidad del modelo mientras que en el eje horizontal tenemos (1-especificidad) del modelo. La sensibilidad es la razón entre los verdaderos positivos (o sea, aquellas observaciones predichas como “1”, que realmente eran “1” en la base de datos), y la suma de los verdaderos postivos más los falsos negativos (aquellos preichos como “0” que en verdad eran “1”). La especificidad es la razón entre los verdaderos negativos (aquellas observaciones predichas como “0” que eran “0” en la base de datos) y la suma de los falsos positivos (aquellas observaciones predichas como “1” que en verdad eran “0”) sumado a los verdaderos negativos. roc &lt;- ggplot(pred_modelos, aes(d = breakdown, m = .fitted, color = modelo)) + geom_roc(n.cuts = 0) + geom_abline(slope = 1)+ xlab(&quot;1 - Especificidad&quot;)+ ylab(&quot;Sensibilidad&quot;) roc Figura 7.8: Roc Plot de nuestros modelos. Para poder ver el AUC, utilizamos la función calc_auc sobre el objeto que hemos llamado roc. calc_auc(roc) ## PANEL group AUC ## 1 1 1 0.64 ## 2 1 2 0.64 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 1 rows ] 7.2.4.7 Separation plots El separation plot fue propuesto por tres politólogos en este paper publicado en 2011 y remite a la idea que mencionamos al inicio del capítulo de que los modelos logísiticos son modelos de clasificación, y retoma la utilidad tanto de los porcentajes correctamente predichos como de los ROC. Los separation plots son figuras que permiten ver la información de una tabla de clasificación, pero donde podemos identificar qué observaciones son las peor predichas. Una crítica que puede hacersele a esta linda herramienta visual es que cuanto mayor es el número de observaciones en nuestra muestra, más dificil se vuelve interpretar la figura ya que la gracia del separation plot es que cada observación es incluida en la figura en la forma de una línea vertical, que será amarilla si la observación asume breakdown=0 y roja si breakdown=1. Nótese cómo ocupamos el argumento type = &quot;line&quot; en las opciones, para que cada observación sea incluida como línea. Si las observaciones fueran más de 1000, por decir un número no arbitrario, sería recomendable usar el arguemnto type = &quot;band&quot;, que facilita la lectura para modelos con muestras muy grandes. ¿Cómo interpretamos la figura? Si el modelo predijiera perfectamente nuestra variable dependiente, todas las observaciones en amarillo estarían del lado izquierdo, y todas las rojas del lado derecho, ya que en el eje horizontal de la figura tenemos la probabilidad predicha para cada obervación. library(separationplot) separationplot(pred = predict.glm(modelo_1, type = &quot;response&quot;), actual = as.vector(modelo_1$y), type = &quot;line&quot;, show.expected = T, newplot = F, line = F, heading = &quot;Separation plot - Modelo 1 en base a Mainwaring y Perez Liñan (2013)&quot;) separationplot(pred = predict.glm(modelo_3, type = &quot;response&quot;), actual = as.vector(modelo_3$y), type = &quot;line&quot;, show.expected = T, newplot = F, line = F, heading = &quot;Separation plot - Modelo 3 en base a a Mainwaring y Perez Liñan (2013)&quot;) Figura 7.9: Separation plots del modelo 1 y 3. El triangulo negro que vemos debajo de la figura es el punto a partir del cual las obsevaciones en rojo deberían comenzar si estuviesen perfectamente clasificadas de las amarillas. Si te fijas en el separation plot del modelo 1, verás que hay líneas rojas del lado izquierdo de la figura. Estas son observaciones para los cuales breakdown=1 y sin embargo se les ha estimado una probabilidad de quiebre democrático muy baja. El modelo 1, apenas con una variable dependiente (shugart) predice bastante mal la realidad de quiebres democráticos en América Latina. Notarás que el modelo 3 ha clasificado mucho mejor que el modelo 1 a las observaciones. De hecho, el modelo 3, apenas con 3 variables independientes, ha podido clasificar la variable dependiente casi perfectamente. Ejercicios antes de continuar al próximo capítulo Con lo aprendido en el Capítulo 4, de visualización de datos, mejora las figuras, agregando nombres a los ejes y título. Agrega dos variables independientes al modelo que hemos repasado en el capítulo e interprete los coeficientes como odds ratios. Grafique estos coeficientes usando sjPlot. Diagnostique el ajuste del modelo con un ROC y un Separation Plot. "],
["surv.html", "Capítulo 8 Modelos de supervivencia 8.1 Nociones básicas 8.2 El modelo Cox de riesgos proporcionales 8.3 Aplicación en R", " Capítulo 8 Modelos de supervivencia Lecturas de referencia Box-Steffensmeier, J. M. &amp; Jones, B. S. (2004). Event history modeling: A guide for social scientists. Cambridge: Cambridge University Press. Box-Steffensmeier, J. M., &amp; Jones, B. S. (1997). Time is of the essence: Event history models in political science. American Journal of Political Science, 41(4), 1414-1461. Allison, P. D. (2014). Event history and survival analysis: Regression for longitudinal event data (2nd edition). Thousand Oaks, CA: SAGE publications. Box-Steffensmeier, J. M., Brady, H. E., &amp; Collier, D. (Eds.). (2008). The Oxford handbook of political methodology (Vol. 10). Oxford: Oxford University Press. Hay una serie de preguntas recurrentes al análisis de datos políticos que aún no hemos cubierto. Muchas veces nos interesa saber por qué ciertos eventos duran lo que duran, o porqué tardan más que otros en ocurrir ¿Por qué la paz es tan duradera entre algunos países mientras que otros guerrean con frecuencia? ¿Cuál era la probabilidad de que Turquía ingresara a la Unión Europea en 2018? ¿Por qué algunos legisladores permanecen en sus cargos por varios periodos consecutivos mientras que otros no logran reelegirse tan solo una vez? ¿Cuánto demora un sindicato en entrar en huelga durante una crisis económica? 8.1 Nociones básicas Todas estas preguntas son sobre la duración de un evento. El momento de ocurrencia de un evento es parte de la respuesta que buscamos, necesitamos de un modelo que nos permita llegar a esta respuesta. Janet Box-Steffensmeier, una de las principales referencias en Ciencia Política de este método, se refiere a ellos como “modelos de eventos históricos” aunque buena parte de la literatura los llama modelos de supervivencia o modelos de duración. Si bien en la Ciencia Política no son modelos tan utilizados como uno creería (en el fondo, casi todas las preguntas que nos hacemos pueden ser reformuladas en una pregunta sobre la duración del evento), las ciencias médicas han explorado estos métodos en profundidad, y muchas las referencias que uno encuentra en R sobre paquetes accesorios a estos modelos son de departamentos bioestadísticos y médicos. De allí que “modelos de supervivencia” sea el nombre más frecuentemente utilizado para estos modelos, ya que en medicina comenzó a utilizárselos para modelar qué variables afectaban la sobrevida de sus pacientes enfermos. Podemos tener dos tipos de bases de datos para estos problemas. Por un lado podemos tener una base en formato de panel en el que para un momento dado nuestra variable dependiente codifica si el evento ha ocurrido (\\(=1\\)) o no (\\(=0\\)). Así, por ejemplo, podemos tener una muestra de veinte países para cincuenta años (1965-2015) en los que nuestra variable de interés es si el país ha implementado una reforma constitucional. La variable independiente asumirá el valor 1 para el año 1994 en Argentina, pero será 0 para el resto de los años en este país. Por otro lado, podemos tener una base de datos transversal en la que cada observación aparece codificada apenas una vez. En este caso necesitamos, además de la variable que nos dirá si en el periodo de interés el evento ocurrió o no para cada observación (por ejemplo, Argentina debería ser codificada como “1”), una variable extra que codifique el tiempo de “supervivencia” de cada observación, es decir, cuánto tiempo pasó hasta que finalmente el evento sucedió. Para el caso de Argentina, esta variable codificará 29 (años), que es lo que demoró en implementarse una reforma constitucional desde 1965. La elección del año de partida, como podrá sospechar, es decisión del investigador, pero tiene un efecto enorme sobre nuestros resultados. Además, muchas veces la fecha de inicio acaba determinada por la disponibilidad de datos y se alejan del ideal que quisieramos modelar. Supongamos que nos hacemos la pregunta que se hizo David Altman: “¿Por qué algunos países demoran menos que otros en implementar instancias de democracia directa?”. Para ello tenemos una base de datos en formato de panel que parte del año 1900 y que llega a 2016 para 202 países (algunas observaciones, como la Unión Soviética se transforman en otras observaciones a partir de un determinado año en que dejan de existir). Al observar sus datos uno nota algo que probablemente también te suceda en tu base de datos. Para el año 2016 apenas un pequeño porcentaje de países había implementado este tipo de mecanismos (27% para ser más precisos) pero la base está censurada ya que a partir de ese año no sabemos que ha ocurrido con los países que aún no han implementado mecanismos de democracia directa. No todas las observaciones han “muerto” aún, ¿cómo saber cuándo lo harán? Ésta es una pregunta válida, que podremos responder con este tipo de modelos, ya que podemos calcular el tiempo que demorará cada uno de los países censurados en nuestra muestra (con la información que le damos al modelo, que siempre es incompleta). En nuestra base de datos tendremos, al menos, cuatro tipos de observaciones (ver figura 8.1): (a) aquellas que, para el momento en que tenemos datos ya estaban en la muestra, aunque no siempre sabremos hace cuanto que “existen”. Son, en la figura, las observaciones B y C. En la base de datos de Altman, por ejemplo, México ya existía como entidad política en 1900, cuando su base de datos parte (sabemos que la Primera República Federal existió como entidad política desde octubre de 1824, por lo que México sería codificado como existente a partir de esa fecha). También sabemos que en 2012, por primera vez, México implementó una iniciativa de democracia directa, lo que define como positiva la ocurrencia del evento que nos interesa medir. Así, México sería como la observación B de la figura; (b) Algunas observaciones estarán desde el comienzo de la muestra, y existirán hasta el último momento sin haber registrado el evento de interés. Tal es el caso, de la observación C en la figura. En la muestra de Altman un ejemplo sería Argentina, que desde 1900 está registrado en la base (ya había “nacido”), y hasta el último año de la muestra no había registrado instancias de democracia directa (no “murió”), lo que la transforma en una observación censurada. A los fines prácticos no cambia saber qué ocurrió a partir del año en que nuestra base termina. Por ejemplo, en la figura, nuestra base cubre hast \\(t_7\\), y sabemos que en \\(t_8\\) la observación C aún no había muerto, y la observación D lo había hecho en \\(t_8\\). En nuestra base, C y D serán ambas observaciones censuradas en \\(t_7\\); (c) Algunas observaciones pueden entrar “tarde” en la muestra, como es el caso de las observaciones A y D. Por ejemplo, Eslovenia entra a la muestra de Altman en 1991, que es cuando se independiza de Yugoslavia y “nace” como país; (d) Algunas observaciones, independientemente de cuando entren a la muestra, “moriran” durante el periodo analizado. Por ejemplo, A y B mueren dentro del periodo que hemos medido entrte \\(t_1\\) y \\(t_7\\). Ya para la observación D, no registramos su muerte. Hay un caso no considerado en el ejemplo, de observaciones que nacen y mueren sucesivamente a lo largo del periodo de estudio. Para ellas, deberemos decidir si las tratamos como observaciones independientes, o si modelamos la posibilidad de morir más de una vez. Si es así, la probabilidad de morir por segunda vez deberá estar condicionada por la probabilidad de haber muerto (y cuando!) por primera vez. Este es un tipo de caso algo más complejo que no cubriremos en este caoítulo. Figura 8.1: Ejemplos de observaciones presentes en una base de datos de supervivencia Los modelos de supervivencia se interpretan a partir de la probabilidad de que en un momento dado el evento de interés ocurra siendo que que no ha ocurrido aun. Esta probabilidad recibe el nombre de tasa de riesgo. Partimos sabiendo que tenemos una variable, que llamaremos \\(T\\), y que representa un valor aleatorio positivo y que tiene una distribución de probabilidades (correspondiente a la probabilidad del evento ocurrir en cada uno de los momentos posibles) que llamaremos \\(f(t)\\). Esta probabilidad se puede expresar de manera acumulada, como una densidad acumulada \\(F(t)\\). Com, en la que vemos que \\(F(t)\\) viene dada por la probabilidad de que el tiempo de supervivencia \\(T\\) sea menor o igual a un tiempo específico \\(t\\) : \\[F(t)=\\int\\limits_0^t f(u)d(u)=Pr(T)\\leq t)\\] La función de supervivencia \\(\\hat S(t)\\), que es un concepto clave en estos modelos, está relacionada a \\(F(t)\\), ya que \\[\\hat S(t)= 1-F(t)=Pr(T\\geq t)\\] Es decir, la función de supervivencia es la probabilidad inversa de \\(F(t)\\), pues dice respecto a la probabilidad de que el tiempo de supervivencia \\(T\\) sea mayor o igual un tiempo \\(t\\) de interés. Para el ejemplo concreto de Altman, uno podría preguntarse cuál es la probabilidad de que un país no implemente un mecanismo de democracia directa (lo que sería equivalente a “sobrevivir” a dicha implementación) siendo que ya ha sobrevivido a los mismos por 30 años. A medida que más y más países en la muestra van implementando iniciativas de democracia directa, la probabilidad de supervivencia va disminuyendo. Los coeficientes de los modelos de supervivencia se suelen interpretar como tasas de riesgo (o “hazard rates” en inglés), que es el cociente de la probabilidad de que el evento suceda y la función de supervivencia \\[h(t)=\\frac{f(t)}{S(t)}\\] Así, la tasa de riesgo indica la tasa a la que las observaciones “mueren” en nuestra muestra en el momento \\(t\\), considerando que la observación ha sobrevivido hasta el momento \\(t\\). Veremos más adelante como en el ejemplo de Altman podemos interpretar los coeficientes de nuestras regresiones como tasas de riesgo. En definitiva, la tasa de riesgo \\(h(t)\\) es el riesgo de que el evento ocurra en un intervalo de tiempo determinado, que viene dado por \\[f(t)=\\lim_{\\bigtriangleup x \\to 0} \\frac {P(t+\\bigtriangleup t &gt; T \\geq t)}{\\bigtriangleup t}\\] 8.2 El modelo Cox de riesgos proporcionales Hay dos tipos de modelos de supervivencia, los llamados modelos paramétricos y los llamados semi-parametricos. Los primeros son aquellos que hacen supuestos sobre las características de la población a la que la muestra pertenece. En este caso, los supuestos son sobre el “baseline hazard”, es decir, sobre el riesgo de que el evento ocurra cuando todas nuestras variables independientes sean iguales a cero. El tipo de modelo de surpervivencia más común para esta categoría es el modelo de Weibull. Por otro lado, los modelos semi-parametricos no hacen ningún tipo de supuestos sobre la función de base, ya que ésta es estimada a partir de los datos. El ejemplo más famoso de ésta especificación es la del modelo de Cox. El Oxford Handbook sobre metodología política dedica un capítulo entero a discutir modelos de supervivencia, y en el se toma una posición fuerte en favor de los modelos semi-parametricos. Aquí seguiremos dicha recomendación ya que las ventajas son varias. Por un lado, como no se hacen presupustos sobre la función del riesgo de base, su estimación es mucho más precisa. En una estimación paramétrica, elegir un “baseline hazard” equivocado siginificará que todo nuestro trabajo analítico estará sesgado. La decisión de la forma que adopta la curva de base en un modelo de Weibull debería estar orientado por razones teóricas de cuál es el efecto de nuestra variable independiente sobre la probabilidad de supervivencia de la observación (ver figura 8.2). Sin embargo, no siempre nuestra teoría define tales presupuestos. Elegir una especificación por Cox nos ahorra tomar una decisión tan costosa. Figura 8.2: Diferentes riesgos de base en el modelo de Weibull Una segunda ventaja de los modelos semi-parametricos sobre los paramétricos tiene que ver con el presupuesto de riesgos proporcionales. Ambos, modelos paramétricos y semi-parametricos, asumen que los riesgos entre dos individuos cualquiera de la muestra se mantienen constantes a lo largo de todo su periodo de supervivencia. Es decir, se asume que la curva de riesgo de cada individuo sigue la misma curva en el tiempo. Este es un presupuesto fuerte para trabajos en ciencia política, ya que las observaciones cambian en el tiempo y se diferencian unas de otras. Piénsense en el trabajo de Altman, por ejemplo. Uno puede teorizar que la probabilidad de una iniciativa de democracia directa en un determinado año en un determinado país estará afectada por el nivel de solidez de las instituciones democráticas, que podemos medir con algún tipo de variable estándar como los 21 puntos de Polity IV o la más reciente medición de V-Dem. Podemos, entonces, teorizar que a mayor solidez institucional mayor probabilidad de implementar mecanismos de democracia directa. Sin embargo, los valores de estas variables no solo difieren ente países, sino que a lo largo del tiempo estas variables cambian mucho para un mismo país. Piénsese en Colombia, por ejemplo, en que la variable de V-Dem “v2x_polyarchy” sufrió avances y retrocesos entre 1900 y 2016 (ver figura 3). Cada vez que el valor de esta variable cambia, necesariamente cambia la tasa de riesgo de democracia directa para Colombia, rompiendo el presupuesto de proporcionalidad de los riesgos. Figura 8.3: Valores de poliarquía para Colombia según V-Dem La ventaja del modelo de Cox sobre sus contrapartes paramétricas es que existen tests para saber si alguna variable de nuestro modelo rompe el presupuesto de proporcionalidad de los riesgos, y de esa forma podremos corregirlo generando interacciones entre estas variables y variables temporales. De esta forma, permitimos que en nuestro modelo haya dos tipos de coeficientes: coeficientes constantes en el tiempo, y coeficientes cambiantes en el tiempo. Por ejemplo, podemos imaginar que ante un aumento brusco en la calidad de las instituciones democráticas de un país la tasa de riesgo de implementar democracia directa se dispare, pero que dicho efecto de desvanezca en el lapso de cuatro o cinco años. Cuando definas tu modelo, es importante que reflexiones sobre qué variables puede asumirse que permanezcan constantes en los riesgos y cuales no. La recomendación dada por el Oxford Handbook para una buena implementación de modelos de supervivencia es la siguiente: (a) Primero, dada las ventajas de los modelos semi-paramétricos sobre los paramétricos, se recomienda el uso de Cox por sobre Weibull u otro modelo paramétrico. (b) Una vez que hemos definido nuestra variable dependiente (el evento), el tiempo de “nacimiento” y de “muerte” de cada observación, podemos especificar nuestro modelo. (c) Los coeficientes deben ser interpretados en tasas de riesgo (hazard rates), lo que exige exponenciar los coeficientes brutos que obtenemos en R. (d) Una vez que tenemos el modelo que creemos correcto, en función de nuestras intuiciones teóricas, es necesario testear que ninguno de los coeficientes viole el presupuesto de proporcionalidad de los riesgos. Para ello ejecutamos un test de Grambsch y Therneau, o mediante el análisis de los residuos de Schoenfeld. (e) Una vez identificados los coeficientes problemáticos, permitimos que estos interactúen con el logaritmo natural de la variable que mide la duración del evento. De esta forma, permitimos que haya coeficientes cuyo efecto se desvanece o se potencia con el tiempo. Una vez corregidos los coeficientes problemáticos, podemos si, proceder a interpretar nuestro modelo y la función de supervivencia del modelo. 8.3 Aplicación en R Volvamos, entonces, a la pregunta que se hizo David Altman en el capítulo 3 de Citizenship and Contemporary Direct Democracy (2019), “Catching on: waves of adoption of citizen-initiated mechanisms of direct democracy since World War I”: “¿Por qué algunos países demoran menos que otros en implementar instancias de democracia directa?”. Para poder responder a esta pregunta, uno debe correr modelos de supervivencia. Comencemos por cargar el tidyverse y nuestra base (esta última, desde nuestro paquete paqueteadp): library(tidyverse) library(paqueteadp) data(dem_directa) Ahora la base se ha cargado en nuestra sesión de R: ls() ## [1] &quot;dem_directa&quot; Las variables que tenemos en la base son las siguientes: + Variable dependiente. Registra la ocurrencia del evento, que en este caso es la adopción de un mecanismo de democracia directa - cic_dummy + Año - year + Nombre del país - country_name + El país sufre un proceso de rápida democratización - dem_positive + El país sufre un proceso rápido de deterioro de la democracia - dem_negative + Memoria de instancias previas de democracia directa - memory + Score de democracia del país - v2x_polyarchy + Efecto de la difusion de capacidades - capabilities_geo_ide + Efecto de la difusión de ocurrencias - occur_geo_ide + Logaritmo natural de la población total del país - log_pop + Dummy para ex colonias británicas - c_gbr + Dummy para ex miembros de la URSS - c_ussr A lo largo del ejemplo usaremos los paquetes skimr, countrycode, survival, rms, survminer, ggalt, tidyversey texreg, pero los iremos cargando uno a uno para que veas para que sirven. Si utilizamos skim, como ya hicimos en otros capítulos, podemos ver que es una base en formato de panel balanceado. Es decir, tenemos una variable “país” (country_name), que se repite a lo largo de una variable “año” (year). skim(dem_directa) ## Skim summary statistics ## n obs: 13885 ## n variables: 23 ## ## ── Variable type:character ────────────────────────────── ## variable missing complete n min max empty n_unique ## country_name 0 13885 13885 4 32 0 202 ## ## ── Variable type:numeric ──────────────────────────────── ## variable missing complete n mean sd p0 ## p25 p50 p75 p100 hist ## [ reached getOption(&quot;max.print&quot;) -- omitted 22 rows ] Los países &quot;“entran” a la base cuando comienzan a existir como países independientes. Veamos el caso de Albania, por ejemplo, que nace como país en 1912 luego de las Guerras los Balcanes: dem_directa %&gt;% filter(country_name == &quot;Albania&quot;) ## # A tibble: 105 x 23 ## country_name year cic_dummy dem_positive dem_negative memory ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 1912 0 0 0 0 ## 2 Albania 1913 0 0 0 0 ## 3 Albania 1914 0 0 0 0 ## 4 Albania 1915 0 0 0 0 ## 5 Albania 1916 0 0 0 0 ## 6 Albania 1917 0 0 0 0 ## 7 Albania 1918 0 0 0 0 ## 8 Albania 1919 0 0 0 0 ## 9 Albania 1920 0 0 0 0 ## 10 Albania 1921 0 0 0 0 ## # … with 95 more rows, and 17 more variables: v2x_polyarchy &lt;dbl&gt;, ## # capabilities_geo_ide &lt;dbl&gt;, occur_geo_ide &lt;dbl&gt;, ## # c_pos_capabilities &lt;dbl&gt;, c_pos_occurrences &lt;dbl&gt;, c_pos_memory &lt;dbl&gt;, ## # log_pop &lt;dbl&gt;, c_gbr &lt;dbl&gt;, c_fra &lt;dbl&gt;, c_ussr &lt;dbl&gt;, c_spa &lt;dbl&gt;, ## # c_usa &lt;dbl&gt;, c_ned &lt;dbl&gt;, c_prt &lt;dbl&gt;, c_bel &lt;dbl&gt;, c_ahe &lt;dbl&gt;, ## # c_ote &lt;dbl&gt; Para que los modelos funcionen correctamente en R, los países deberían salir del análisis (¡y de la base!) cuando “mueren”. En este caso la muerte se da cuando los países adoptan mecanismos de democracia directa. Albania, siguiendo el ejemplo, debería dejar de existir en 1998, y no perdurar en la base hasta 2016 como sucede ahora. Entonces crearemos una segunda versión de nuestra base de datos donde esto ya ha sido corregido: Si tu base de datos está en este formato desde el comienzo, entonces podrás saltarte este paso. dem_directa_b &lt;- dem_directa %&gt;% group_by(country_name) %&gt;% # que la suma acumulada de la dummy sea como máximo 1 filter(cumsum(cic_dummy) &lt;= 1) %&gt;% ungroup() Lo que estamos haciendo es filtrar de tal forma que al registrar el primer evento de interés (en este caso es la variable cic_dummy) el resto es dejado a un lado. Si comparamos el caso de Albania para la base original y para la base actual veremos la diferencia: dem_directa %&gt;% filter(country_name == &quot;Albania&quot; &amp; cic_dummy == 1) ## # A tibble: 19 x 23 ## country_name year cic_dummy dem_positive dem_negative memory ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 1998 1 0 0 1 ## 2 Albania 1999 1 0 0 1 ## 3 Albania 2000 1 0 0 1 ## 4 Albania 2001 1 0 0 1 ## 5 Albania 2002 1 0 0 0.94 ## 6 Albania 2003 1 0 0 0.88 ## 7 Albania 2004 1 0 0 0.82 ## 8 Albania 2005 1 0 0 0.76 ## 9 Albania 2006 1 0 0 0.7 ## 10 Albania 2007 1 0 0 0.64 ## 11 Albania 2008 1 0 0 0.580 ## 12 Albania 2009 1 0 0 0.52 ## 13 Albania 2010 1 0 0 0.46 ## 14 Albania 2011 1 0 0 0.4 ## 15 Albania 2012 1 0 0 0.34 ## 16 Albania 2013 1 0 0 0.28 ## 17 Albania 2014 1 0 0 0.22 ## 18 Albania 2015 1 0 0 0.16 ## 19 Albania 2016 1 0 0 0.1 ## # … with 17 more variables: v2x_polyarchy &lt;dbl&gt;, ## # capabilities_geo_ide &lt;dbl&gt;, occur_geo_ide &lt;dbl&gt;, ## # c_pos_capabilities &lt;dbl&gt;, c_pos_occurrences &lt;dbl&gt;, c_pos_memory &lt;dbl&gt;, ## # log_pop &lt;dbl&gt;, c_gbr &lt;dbl&gt;, c_fra &lt;dbl&gt;, c_ussr &lt;dbl&gt;, c_spa &lt;dbl&gt;, ## # c_usa &lt;dbl&gt;, c_ned &lt;dbl&gt;, c_prt &lt;dbl&gt;, c_bel &lt;dbl&gt;, c_ahe &lt;dbl&gt;, ## # c_ote &lt;dbl&gt; dem_directa_b %&gt;% filter(country_name == &quot;Albania&quot; &amp; cic_dummy == 1) ## # A tibble: 1 x 23 ## country_name year cic_dummy dem_positive dem_negative memory ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 1998 1 0 0 1 ## # … with 17 more variables: v2x_polyarchy &lt;dbl&gt;, ## # capabilities_geo_ide &lt;dbl&gt;, occur_geo_ide &lt;dbl&gt;, ## # c_pos_capabilities &lt;dbl&gt;, c_pos_occurrences &lt;dbl&gt;, c_pos_memory &lt;dbl&gt;, ## # log_pop &lt;dbl&gt;, c_gbr &lt;dbl&gt;, c_fra &lt;dbl&gt;, c_ussr &lt;dbl&gt;, c_spa &lt;dbl&gt;, ## # c_usa &lt;dbl&gt;, c_ned &lt;dbl&gt;, c_prt &lt;dbl&gt;, c_bel &lt;dbl&gt;, c_ahe &lt;dbl&gt;, ## # c_ote &lt;dbl&gt; En resumen, ahora tenemos un panel desbalanceado, en el que los países entran a la base cuando comienzan a existir como tales y salen, o bien cuando adoptan mecanismos de democracia directa, o bien cuando la base termina en su extensión temporal (en 2016). De esta forma nuestra base se acerca mucho a la figura 1 con la que ejemplificamos los distintos tipos de observaciones. ¿Qué tal si probamos hacer algo similar a la figura 1 pero con los datos de David Altman? Este tipo de figuras se llaman gráficos de Gantt, y pueden recrearse con ggplot2 aunque es justo decir que hay que seguir unos cuantos pasos, y puede ser algo dificil. Ojalá que con este ejemplo puedas recrearlo con tus propios datos porque una figura así es de mucha utlidad para el lector. Primero debemos crear una base de datos que, para cada país, registre el año de entrada y el año de salida. También nos interesa por qué sale el país: ¿adopta democracia directa o la base termina? Vamos a crear un subonjunto que llamaremos gantt_plot_df donde nos quedamos solamente con tres variables de la base, que son el nombre del país country_name, el año year, y la variable dependiente cic_dummy. También quitaremos de la base aquellas observaciones que para el primer año de la base ya han “muerto”. Por ejemplo, Suiza había implementado mecanismos de democracia directa mucho antes que 1900, así que desde el primer año de la base hasta el último la variable dependiente será “1”: gantt_plot_df &lt;- dem_directa_b %&gt;% # las variables que nos interesan dplyr::select(country_name, year, cic_dummy) %&gt;% group_by(country_name) %&gt;% filter(year == min(year) | year == max(year)) %&gt;% # debemos sacar las observaciones para países que &quot;nacen&quot; (para nuestra base) con democracia directa: filter(!(year == min(year) &amp; cic_dummy == 1)) %&gt;% summarise(year_enters = min(year), year_exits = max(year), exits_bc_dd = max(cic_dummy)) %&gt;% ungroup() gantt_plot_df ## # A tibble: 194 x 4 ## country_name year_enters year_exits exits_bc_dd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1919 2016 0 ## 2 Albania 1912 1998 1 ## 3 Algeria 1962 2016 0 ## 4 Andorra 1900 2016 0 ## 5 Angola 1975 2016 0 ## 6 Antigua and Barbuda 1981 2016 0 ## 7 Argentina 1900 2016 0 ## 8 Armenia 1991 2016 0 ## 9 Australia 1901 2016 0 ## 10 Austria 1918 2016 0 ## # … with 184 more rows Los países que salen por democracia directa (“mueren”) son: gantt_plot_df %&gt;% filter(exits_bc_dd == 1) ## # A tibble: 49 x 4 ## country_name year_enters year_exits exits_bc_dd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 1912 1998 1 ## 2 Belarus 1991 1995 1 ## 3 Belize 1981 2008 1 ## 4 Bolivia 1900 2004 1 ## 5 Bulgaria 1908 2009 1 ## 6 Cape Verde 1975 1992 1 ## 7 Colombia 1900 1991 1 ## 8 Costa Rica 1900 2002 1 ## 9 Croatia 1992 2001 1 ## 10 Ecuador 1900 1998 1 ## # … with 39 more rows Podemos identificar en una nueva variable la región geopolítica de cada país, gracias a la función countrycode::countrycode() (Esto lo explicamos en detalle en el Capítulo 9). Este paquete es de gran utilidad para quienes hacen política comparada o relaciones internacionales porque facilita mucho darle códigos a los países. Lo que nos permite el paquete es asignar a cada país su región de pertenencia de manera casi automática: library(countrycode) gantt_plot_df_region &lt;- gantt_plot_df %&gt;% mutate(region = countrycode(country_name, origin = &quot;country.name&quot;, dest = &quot;region&quot;)) ## Warning in countrycode(country_name, origin = &quot;country.name&quot;, dest = &quot;region&quot;): Some values were not matched unambiguously: Kosovo, South Yemen, Vietnam, Democratic Republic of, Vietnam, Republic of gantt_plot_df_region ## # A tibble: 194 x 5 ## country_name year_enters year_exits exits_bc_dd region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan 1919 2016 0 Southern Asia ## 2 Albania 1912 1998 1 Southern Europe ## 3 Algeria 1962 2016 0 Northern Africa ## 4 Andorra 1900 2016 0 Southern Europe ## 5 Angola 1975 2016 0 Middle Africa ## 6 Antigua and Barb… 1981 2016 0 Caribbean ## 7 Argentina 1900 2016 0 South America ## 8 Armenia 1991 2016 0 Western Asia ## 9 Australia 1901 2016 0 Australia and New … ## 10 Austria 1918 2016 0 Western Europe ## # … with 184 more rows Como dice el warning, algunos países no fueron encontrados. Estos son los países que countrycode no pudo encontrar, seguramente porque los países están escritos de otra manera: gantt_plot_df_region %&gt;% filter(is.na(region)) ## # A tibble: 4 x 5 ## country_name year_enters year_exits exits_bc_dd region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Kosovo 2008 2016 0 &lt;NA&gt; ## 2 South Yemen 1967 2016 0 &lt;NA&gt; ## 3 Vietnam, Democratic Republic of 1945 2016 0 &lt;NA&gt; ## 4 Vietnam, Republic of 1949 1975 0 &lt;NA&gt; Podemos corregir esto a mano ex-post o correr countrycode::countrycode() de nuevo, pero con el argumento custom_match: gantt_plot_df_region &lt;- gantt_plot_df %&gt;% mutate(region = countrycode(country_name, origin = &quot;country.name&quot;, dest = &quot;region&quot;, custom_match = c(&quot;Korea, North&quot; = &quot;Eastern Asia&quot;, &quot;Kosovo&quot; = &quot;Eastern Europe&quot;, &quot;South Yemen&quot; = &quot;Western Asia&quot;, &quot;Vietnam, Democratic Republic of&quot; = &quot;South-Eastern Asia&quot;, &quot;Vietnam, Republic of&quot; = &quot;South-Eastern Asia&quot;))) gantt_plot_df_region ## # A tibble: 194 x 5 ## country_name year_enters year_exits exits_bc_dd region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan 1919 2016 0 Southern Asia ## 2 Albania 1912 1998 1 Southern Europe ## 3 Algeria 1962 2016 0 Northern Africa ## 4 Andorra 1900 2016 0 Southern Europe ## 5 Angola 1975 2016 0 Middle Africa ## 6 Antigua and Barb… 1981 2016 0 Caribbean ## 7 Argentina 1900 2016 0 South America ## 8 Armenia 1991 2016 0 Western Asia ## 9 Australia 1901 2016 0 Australia and New … ## 10 Austria 1918 2016 0 Western Europe ## # … with 184 more rows Ahora no tenemos NA! Hemos logrado asignar una región a cada país de la muestra. gantt_plot_df_region %&gt;% filter(is.na(region)) ## # A tibble: 0 x 5 ## # … with 5 variables: country_name &lt;chr&gt;, year_enters &lt;dbl&gt;, ## # year_exits &lt;dbl&gt;, exits_bc_dd &lt;dbl&gt;, region &lt;chr&gt; Con nuestra base ya armada, podemos hacer el plot con facilidad, gracias a ggalt::geom_dumbbell(): library(ggalt) gantt_plot &lt;- ggplot(data = gantt_plot_df_region, mapping = aes(x = year_enters, xend = year_exits, y = fct_rev(country_name), color = factor(exits_bc_dd))) + geom_dumbbell(size_x = 2, size_xend = 2) gantt_plot Figura 8.4: Diagrama de Gantt para todas las observaciones En el eje vertical tenemos los países ordenados alfabeticamente, y en el eje x hay dos variables que informan al gráfico, por un lado el comienzo de la línea (year_enters) y por otro su fin (year_exits). Además, hay una tercer variable informativa que es el color de la línea, que denota si el país implementó o no una instancia de democracia directa (exits_bc_dd). Los países en azul son los que implementaron dicha instancia entre 1900 y 2016. Si bien la figura es de una enorme utilidad visual, hay que reconocer que son demasiados países para incluirla en el cuerpo de un artículo. Un enfoque posible es concentrarnos en ciertas regiones. Recordarás la función filterdel Capítulo 3. Filtremos Sudamérica, por ejemplo: gantt_plot_sa &lt;- ggplot(data = gantt_plot_df_region %&gt;% filter(region == &quot;South America&quot;), mapping = aes(x = year_enters, xend = year_exits, y = fct_rev(country_name), color = fct_recode(factor(exits_bc_dd)))) + geom_dumbbell(size_x = 2, size_xend = 2) gantt_plot_sa Figura 8.5: Diagrama de Gantt para América del Sur Podemos agregarle los años como texto para mejorar aún más la lectura de la figura: gantt_plot_sa &lt;- gantt_plot_sa + geom_text(aes(label = year_enters), vjust = -.4) + geom_text(aes(x = year_exits, label = year_exits), vjust = -.4) gantt_plot_sa Figura 8.6: Diagrama de Gantt para América del Sur con todas las mejoras Finalmente, algunos retoques estéticos, con todo lo que hemos aprendido en el Capítulo 4: library(ggplot2) gantt_plot_sa &lt;- gantt_plot_sa + labs(x = &quot;año&quot;, y = &quot;&quot;, title = &quot;Años de entrada y salida, Sudamérica&quot;, color = &quot;¿Adopta democracia directa?&quot;) + theme(axis.text.x = element_blank()) gantt_plot_sa Figura 8.7: Diagrama de Gantt para todas las observaciones Además de gráficos de Gantt, es muy común que quien trabaja con modelos de supervivencia muestre gráficos con las curvas de supervivencia comparando dos grupos de interés. Por ejemplo, David Altman se pregunta si hubo una diferencia en el siglo XX entre países que se democratizaron rápidamente y aquellos que demoraron décadas en hacerlo respecto a la rapidez con que implementaron mecanismos de democracia directa. Este tipo de figuras no tiene valor inferencial, pero si gran valor decriptivo. Tenemos que estimar una curva de supervivencia no paramétrica, usando el método de Kaplan-Meier. A partir de este punto debemos hacer una modificación más a nuestra base. Seguramente tu también debas hacerlo con tus propios datos, así que presta atención. Los modelos de supervivencia no trabajan con datos de panel tradicionales, como el de nuestra base. Tenemos que convertirlos a “tiempo en riesgo”. ¿Qué quiere decir esto? Quiere decir que necesitamos dos variables nuevas, una que nos diga el tiempo que la observación lleva en riesgo de morir al inicio de cada t risk_time_at_starty otra variable que haga lo mismo al final de cada t risk_time_at_end. Estas dos variables serán utilizadas a partir de ahora en los scripts del análisis de supervivencia: dem_directa_c &lt;- dem_directa_b %&gt;% group_by(country_name) %&gt;% # vamos a eliminar el primer año para cada país. no está en riesgo, por definición! filter(year != min(year)) %&gt;% mutate(risk_time_at_end = c(1:n()), risk_time_at_start = c(0:(n() - 1))) %&gt;% ungroup() %&gt;% dplyr::select(country_name, year, risk_time_at_start, risk_time_at_end, everything()) dem_directa_c ## # A tibble: 12,014 x 25 ## country_name year risk_time_at_st… risk_time_at_end cic_dummy ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan 1920 0 1 0 ## 2 Afghanistan 1921 1 2 0 ## 3 Afghanistan 1922 2 3 0 ## 4 Afghanistan 1923 3 4 0 ## 5 Afghanistan 1924 4 5 0 ## 6 Afghanistan 1925 5 6 0 ## 7 Afghanistan 1926 6 7 0 ## 8 Afghanistan 1927 7 8 0 ## 9 Afghanistan 1928 8 9 0 ## 10 Afghanistan 1929 9 10 0 ## # … with 12,004 more rows, and 20 more variables: dem_positive &lt;dbl&gt;, ## # dem_negative &lt;dbl&gt;, memory &lt;dbl&gt;, v2x_polyarchy &lt;dbl&gt;, ## # capabilities_geo_ide &lt;dbl&gt;, occur_geo_ide &lt;dbl&gt;, ## # c_pos_capabilities &lt;dbl&gt;, c_pos_occurrences &lt;dbl&gt;, c_pos_memory &lt;dbl&gt;, ## # log_pop &lt;dbl&gt;, c_gbr &lt;dbl&gt;, c_fra &lt;dbl&gt;, c_ussr &lt;dbl&gt;, c_spa &lt;dbl&gt;, ## # c_usa &lt;dbl&gt;, c_ned &lt;dbl&gt;, c_prt &lt;dbl&gt;, c_bel &lt;dbl&gt;, c_ahe &lt;dbl&gt;, ## # c_ote &lt;dbl&gt; Corramos la curva de supervivencia no paramétrica, usando el método de Kaplan-Meier: library(survival) km &lt;- survfit(Surv(time = risk_time_at_start, time2 = risk_time_at_end, event = cic_dummy) ~ dem_positive, type = &quot;kaplan-meier&quot;, conf.type = &quot;log&quot;, data = dem_directa_c) Ahora podemos armar nuestro plot con survminer::ggsurvplot() library(survminer) ## Loading required package: ggpubr ## Loading required package: magrittr ## ## Attaching package: &#39;magrittr&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## set_names ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract ggsurvplot(km, conf.int = T, legend.title = &quot;&quot;, break.x.by = 20, legend.labs = c(&quot;Fast democratization = 0&quot;, &quot;Fast democratization = 1&quot;), data = dem_directa_c) + labs(title = &quot;Kaplan-Meier Survival estimates&quot;) Figura 8.8: Curva de Kaplan-Meier Altman tiene la hipótesis de que países que sufrieron “shocks” democratizadores fueron mucho más rápidos en implementar mecanismos directos. La figura confirma su intuición, pues vemos que la probabilidad de supervivencia de un país (léase como la probabilidad de que persista sin implementar mecanismos de democracia directa) se reduce a la mitad en los primeros cuatro años que siguien al shock democratizador. Por el contrario, países que se democratizan de a poco, no sufren este efecto. 8.3.1 Modelos de supervivencia e interpretación sus coeficientes como Hazard Ratios. Vamos a usar la base de Altman para estimar algunos modelos como ejemplo y correr los tests de Grambsch y Therneau para testear que ninguno de los coeficientes viole el presupuesto de proporcionalidad de los riesgos. No estamos replicando los modelos de su capítulo porque son algo más complejos (incluyen varias interacciones), simplemente usamos su base como referencia. Aquí vale la pena hacer una aclaración respecto a replicabilidad: Si el autor utiliza Stata (por ejemplo con el comando stcox) van a haber algunas diferencias menores en los resultados obtenidos utilizando R. Primer modelo: cox_m1 &lt;- coxph(Surv(risk_time_at_start, risk_time_at_end, cic_dummy) ~ dem_positive + dem_negative + memory + v2x_polyarchy, data = dem_directa_c, robust = T, method =&quot;breslow&quot;) Su test de riesgos proporcionales: cox.zph(cox_m1) ## rho chisq p ## dem_positive -0.0786 0.310 0.578 ## dem_negative 0.0482 0.108 0.743 ## memory -0.1065 0.785 0.376 ## v2x_polyarchy -0.2978 3.170 0.075 ## GLOBAL NA 4.909 0.297 Miremos el valor global del test. Su p-valor es superior al punto de corte de 0.05. Segundo modelo: cox_m2 &lt;- coxph(Surv(risk_time_at_start, risk_time_at_end, cic_dummy) ~ dem_positive + dem_negative + memory + v2x_polyarchy + capabilities_geo_ide, data = dem_directa_c, robust = TRUE, method =&quot;breslow&quot;) Su test de riesgos proporcionales: cox.zph(cox_m2) ## rho chisq p ## dem_positive -0.0549 0.1514 0.6972 ## dem_negative 0.0445 0.0979 0.7544 ## memory -0.0801 0.4505 0.5021 ## v2x_polyarchy -0.2974 3.2226 0.0726 ## capabilities_geo_ide -0.2892 4.9878 0.0255 ## GLOBAL NA 10.3957 0.0648 En este caso, el test global no llega al valor p de corte de 0.05. Sin embargo, hay una variable significativa en su Chi cuadrado:capabilities_geo_ide. El tercer modelo: cox_m3 &lt;- coxph(Surv(risk_time_at_start, risk_time_at_end, cic_dummy) ~ dem_positive + dem_negative + memory + v2x_polyarchy + capabilities_geo_ide + occur_geo_ide, data = dem_directa_c, robust = TRUE, method =&quot;breslow&quot;) Su test de riesgos proporcionales: cox.zph(cox_m3) ## rho chisq p ## dem_positive -0.1234 0.7250 0.3945 ## dem_negative 0.0147 0.0106 0.9178 ## memory -0.1059 0.7539 0.3852 ## v2x_polyarchy -0.2826 3.1303 0.0769 ## capabilities_geo_ide -0.1881 2.6878 0.1011 ## occur_geo_ide 0.0919 0.6365 0.4250 ## GLOBAL NA 7.8623 0.2484 Aquí, el test revela un escenario similar al del primer modelo, sin grandes problemas. El cuarto modelo: cox_m4 &lt;- coxph(Surv(risk_time_at_start, risk_time_at_end, cic_dummy) ~ dem_positive + dem_negative + memory + v2x_polyarchy + capabilities_geo_ide + occur_geo_ide + log_pop, data = dem_directa_c, robust = TRUE, method = &quot;breslow&quot;) Su test de riesgos proporcionales: cox.zph(cox_m4) ## rho chisq p ## dem_positive -0.1132 0.601 0.4383 ## dem_negative 0.0602 0.187 0.6655 ## memory -0.1051 0.740 0.3898 ## v2x_polyarchy -0.3694 5.586 0.0181 ## capabilities_geo_ide -0.1936 2.858 0.0909 ## occur_geo_ide 0.0973 0.718 0.3967 ## log_pop -0.3457 5.104 0.0239 ## GLOBAL NA 12.859 0.0756 Note como el p-valor global está muy cerca del punto de corte y las variables v2x_polyarchy y log_pop están violando el presupuesto de proporcionalidad de los riesgos. El quinto modelo: cox_m5 &lt;- coxph(Surv(risk_time_at_start, risk_time_at_end, cic_dummy) ~ dem_positive + dem_negative + memory + v2x_polyarchy + capabilities_geo_ide + occur_geo_ide + log_pop + c_gbr, data = dem_directa_c, robust = TRUE, method =&quot;breslow&quot;) Su test de riesgos proporcionales: cox.zph(cox_m5) ## rho chisq p ## dem_positive -0.0910 0.415 0.5196 ## dem_negative 0.0722 0.272 0.6023 ## memory -0.0915 0.739 0.3900 ## v2x_polyarchy -0.3369 4.451 0.0349 ## capabilities_geo_ide -0.1889 2.499 0.1139 ## occur_geo_ide 0.0860 0.545 0.4602 ## log_pop -0.3091 4.599 0.0320 ## c_gbr -0.1168 0.863 0.3529 ## GLOBAL NA 11.731 0.1636 El quinto modelo presenta el mismo escenario que el cuarto modelo. Tenemos dos variables violando el presupuesto de proporcionalidad. El test global tiene un p-valor de 0.16 por lo que no deberíamos preocuparnos por resolver la violación. Sin embargo, caso que en tu trabajo tengas un p-valor global menor a 0.05 te mostramos como abordar el problema tal como recomienda el texto del Oxford Handbook sobre el que basamos la discusión teórica al inicio del capítulo. Una forma de resolverlo es interactuando las variables problemáticas con el logaritmo natural de la variable temporal que creamos anteriormente. El quinto modelo corregido se vería así: cox_m5_int &lt;- coxph(Surv(risk_time_at_start, risk_time_at_end, cic_dummy) ~ dem_positive + dem_negative + memory + v2x_polyarchy + capabilities_geo_ide + occur_geo_ide + log_pop + c_gbr + v2x_polyarchy:log(risk_time_at_end) + log_pop:log(risk_time_at_end), data = dem_directa_c, robust = TRUE, method =&quot;breslow&quot;) Verás que el test ya no muestra problemas con la proporcionalidad de los riesgos. cox.zph(cox_m5_int) ## rho chisq p ## dem_positive -0.1020 0.6613 0.4161 ## dem_negative 0.0469 0.1141 0.7355 ## memory -0.0289 0.0808 0.7762 ## v2x_polyarchy 0.1570 1.2161 0.2701 ## capabilities_geo_ide -0.2036 3.4194 0.0644 ## occur_geo_ide 0.1072 0.9180 0.3380 ## log_pop 0.1627 1.3982 0.2370 ## c_gbr -0.0765 0.4057 0.5242 ## v2x_polyarchy:log(risk_time_at_end) -0.1466 1.1154 0.2909 ## log_pop:log(risk_time_at_end) -0.1891 1.9723 0.1602 ## GLOBAL NA 6.8948 0.7353 Veamos todos los modelos juntos con texreg: library(texreg) Para obtener hazard ratios en R necesitamos exponenciar los coeficientes y luego calcular los errores estándar y valores-p a partir de una transformación de la matriz varianza-covarianza del modelo. En el Capítulo 7 vimos cómo hacer esto para modelos logísticos cuando queremos odds ratios (utilizando las opciones override.coef, override.se y override.pvalues de texreg). Para los modelos de supervivencia este paso es idéntico. La única diferencia para nuestro caso actual es la siguiente: ahora tenemos varios modelos para los que queremos hazard ratios, por lo que utilizaremos la función de iteración map() para que las transformaciones de coeficientes, errores estándar y valores-p se apliquen en cada modelo: lista_modelos &lt;- list(cox_m1, cox_m2, cox_m3, cox_m4, cox_m5, cox_m5_int) screenreg(l = lista_modelos, custom.model.names = c(&quot;Modelo 1&quot;, &quot;Modelo 2&quot;, &quot;Modelo 3&quot;, &quot;Modelo 4&quot;, &quot;Modelo 5&quot;, &quot;Modelo 5.b&quot;), override.coef = map(lista_modelos, ~ exp(coef(.x))), override.se = map(lista_modelos, ~ odds_se(.x)), override.pvalues = map(lista_modelos, ~ odds_pvalues(.x)) ) ## ## ===================================================================================================================== ## Modelo 1 Modelo 2 Modelo 3 Modelo 4 Modelo 5 Modelo 5.b ## --------------------------------------------------------------------------------------------------------------------- ## dem_positive 6.24 ** 6.20 ** 7.00 ** 6.83 ** 6.20 ** 4.57 * ## (2.26) (2.28) (2.53) (2.44) (2.17) (1.81) ## dem_negative 2.33 2.45 2.42 2.41 2.26 2.35 ## (1.67) (1.80) (1.77) (1.78) (1.66) (1.75) ## memory 5.32 ** 4.92 ** 5.11 ** 5.06 ** 4.58 ** 4.69 * ## (1.95) (1.81) (1.85) (1.83) (1.76) (1.82) ## v2x_polyarchy 4.15 3.16 3.91 4.20 5.67 395.49 ## (2.24) (1.75) (2.17) (2.46) (3.38) (595.13) ## capabilities_geo_ide 1.59 *** 2.98 ** 2.98 ** 2.78 ** 2.71 ** ## (0.35) (0.99) (0.99) (0.90) (0.92) ## occur_geo_ide 0.03 0.03 0.02 0.04 ## (0.05) (0.05) (0.04) (0.07) ## log_pop 1.04 *** 1.03 *** 1.61 *** ## (0.09) (0.09) (0.33) ## c_gbr 0.43 * 0.47 * ## (0.19) (0.22) ## v2x_polyarchy:log(risk_time_at_end) 0.26 * ## (0.11) ## log_pop:log(risk_time_at_end) 0.88 *** ## (0.05) ## --------------------------------------------------------------------------------------------------------------------- ## AIC 386.32 382.78 379.23 380.98 379.30 373.28 ## R^2 0.00 0.00 0.01 0.01 0.01 0.01 ## Max. R^2 0.04 0.04 0.04 0.04 0.04 0.04 ## Num. events 48 48 48 48 48 48 ## Num. obs. 11460 11285 11276 11245 11245 11245 ## Missings 554 729 738 769 769 769 ## PH test 0.30 0.06 0.25 0.08 0.16 0.74 ## ===================================================================================================================== ## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 Si bien no estamos replicando la especificación de Altman, a simple vista nuestros resultados confirmarían la hipótesis de que países que sufrieron “shocks” democratizadores fueron mucho más rápidos en implementar mecanismos directos de democracia. Hemos terminado el ejercicio, ahora te invitamos a que hagas la lista de ejercicios antes de pasar al próximo capítulo. Ejercicios antes de continuar al próximo capítulo - Utilizando survminergrafique una curva de Kaplan-Meier para la variable c_gbr. La variable c_ussrindica aquellos países que fueron parte de la Unión Soviética. Grafique una curva de Kaplan-Meier para la variable. Incorpore esta variable a un sexto modelo, haga su test de Grambsch y Therneau y rehaga la tabla de los modelos con texreg ¿Tenés tu propia base de datos de supervivencia? Sería genial que repitieras todo el ejercicio usando tus datos y compartas dudas en nuestro GitHub. "],
["manejo-av.html", "Capítulo 9 Manejo avanzado de datos políticos 9.1 Introducción 9.2 ¿Cómo aprovechar countrycode? 9.3 ¿Cómo imputar datos faltantes? 9.4 Imputar datos usando Amelia 9.5 Regresiones luego de imputar datos", " Capítulo 9 Manejo avanzado de datos políticos Por Andrés Cruz Labrín y Francisco Urdinez Lecturas de referencia Lall, R. (2016). How multiple imputation makes a difference. Political Analysis, 24(4), 414-433. Honaker, J., King, G., &amp; Blackwell, M. (2011). Amelia II: A program for missing data. Journal of Statistical Software, 45(7), 1-47. Allison, P. D. (2001). Missing data (Vol. 136). Sage publications. Graham, J. W. (2009). Missing data analysis: Making it work in the real world. Annual Review of Psychology, 60, 549-576. 9.1 Introducción En este capítulo tratamos dos problemas que son muy recurrente entre quienes usamos datos de países tomados de diferentes fuentes (World Bank Data, V-Dem, World, Governance Indicators, Correlates of War, etc). El primero de ellos es el de la estandarización de los “códigos” que se utiliza para cada país. Estos códigos son importantes, porque a partir de estos es que unimos dos o más bases de datos de diferentes fuentes. Piense por ejemplo el siguiente escenario: su base tiene una variable llamada “país” donde Brasil es codificado en mayúsculas como “BRASIL”. Luego toma datos del Banco Mundial y los datos de Brasil están como “Brazil”, y luego toma datos de Correlates of War donde Brasil es el código 140. Si este problema se repite para los más de 200 países de su muestra, ¿cómo resuelve de manera rápida este rompecabezas? El segundo problema es que muchas veces nuestros datos tienen datos faltantes, o como se les llama comunmente, “missing values”. Cuando hay datos faltantes en una regresión R simplemente elimina la observación que tiene el dato faltante ya sea en su variable dependiente, independiente o controles (a esta eliminación se le llama listwise deletion). Imagine que queremos comparar diez países de América Latina en la evolución de su tasa de desempleo entre 2008 y 2018 y, sin embargo, uno de ellos no tiene datos para el trienio 2010-2013. ¿Podemos rellenar estos valores “adivinando” los valores que no observamos? A este proceso se le llama imputación. Decidir si imputar o no es una una decisión del investigador. Que la imputación sea adecuada dependerá de si los datos sean faltantes de manera aleatoria. Este dilema se presenta cuando estos datos son usados para crear gráficos y no queremos que queden incompletos. También, en algunos modelos avanzados, como los espaciales, tener valores faltantes puede impedirnos ejecutar los comandos. Para estos casos, podemos considerar imputar estos datos, es decir, completar los datos faltantes a partir de la información que tenemos para los otros países. Hacer esto tiene un costo que discutiremos en breve, ya que, como toda solución, no es perfecta. Para ejemplificar este ejercicio vamos a usar la base de datos sobre tratados internacionales creada por Carsten Schulz en base al repositorio de Naciones Unidas con todos los tratados internacionales vigentes. En este repositorio se alojan todos los tratados internacionales celebrados entre estados, con sus textos, e informaciones sobre los firmantes. La base del ejemplo está simplificada, apenas tenemos dos tratados internacionales en vez de las decenas que originalmente utiliza Schulz para estudiar qué lleva a un país a unirse a un acuerdo internacional. Los tratados del ejemplo son el Tratado de Prohibición Completa de los Ensayos Nucleares de 1996 y El Estatuto de Roma de 1998, que es el instrumento constitutivo de la Corte Penal Internacional y sólo tenemos los países del continente americano. Empezamos por cargar su base de datos, desde nuestro paquete paqueteadp: library(tidyverse) ## ── Attaching packages ──────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0 ✔ purrr 0.3.1 ## ✔ tibble 2.0.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ─────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(paqueteadp) Puede demorar uno o dos minutos la carga de los datos. data(tratados_int) Ahora la base se ha cargado en nuestra sesión de R. ls() ## [1] &quot;tratados_int&quot; Cada tratado viene acompañado de información del proceso de incorporación doméstica del tratado, es decir, fecha de firma, aceptación, ratificación y delegación. Estas acciones están categorizadas en la variable action_type_string y además son acompañadas de la variable action_date que registra la fecha de cada acción. La variable location registra la ciudad donde se firmó el tratado internacional, y country name el nombre de cada país. skimr::skim(tratados_int) ## Skim summary statistics ## n obs: 300 ## n variables: 6 ## ## ── Variable type:character ────────────────────────────── ## variable missing complete n min max empty n_unique ## action_type_string 0 300 300 8 23 0 7 ## [ reached getOption(&quot;max.print&quot;) -- omitted 3 rows ] ## ## ── Variable type:Date ─────────────────────────────────── ## variable missing complete n min max median ## action_date 211 89 300 1996-09-24 2016-03-03 1999-03-23 ## n_unique ## 62 ## [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ] 9.2 ¿Cómo aprovechar countrycode? Si leíste el Capítulo 8 sobre modelos de supervivencia, seguramente recuerdes que utilizamos la función countrycode para poder crear el gráfico de Gantt con el tiempo que demoró cada país en implementar mecanismos de democracia directa. Como en ese entonces lo vimos bastante por encima, ¡hagamos otro ejemplo! Teniendo la base de datos de Schulz, primero queremos agregarle a cada país el índice de capacidades materiales que codifica Correlates of War, el famoso índice CINC. Este índice es un proxy de poder nacional tal como lo entiende la escuela del realismo, y combina seis indicadores de poder duro, a saber, consumo primario de energía, población total, población urbana, producción de acero y hierro, gasto militar y número de tropas militares. El índice varía de 0 a 1 pues representa la proporción que cada país representa del poder mundial total en un año determinado. Una vez que hemos agregado estos datos a la base, queremos analizarlos pero únicamente para la región de América Latina. El paquete countrycode nos permitirá hacerlo de manera simple: lo que hace la función es estandarizar los nombres de los países en base a códigos preexistentes como el de ISO (International Organization for Standardization), el Fondo Monetario Internacional, Naciones Unidas, el proyecto Varieties of Democracy (V-Dem), el Banco Mundial, solo por mencionar algunos. Si logramos que la base de Schulz y la base de Correlates of War identifiquen a cada país con códigos equivalentes, nos aseguraremos que al unir las dos bases de datos no perderemos información. Cargamos el paquete countrycode y le preguntamos cuáles son los estándares que reconoce en ?codelist. Los estándares aparecen en el cuadrante derecho de RStudio. library(countrycode) ?codelist #lista todos los formatos de origen y destino que soporta el paquete Por la forma en que están registrados los países daría la sensación de que es un.name.es, es decir, los países están registrados con su nombre entero (que en este caso es el nombre oficial que Naciones Unidas da a cada país). Como vamos a unirlo a la base de Correlates of War querremos crear una variable que codifique cada país con la nomenclatura que ellos usan, que se llama cown (Correlates Of War Numbers). tratados_int_estandar &lt;- tratados_int %&gt;% mutate(ccode = countrycode(country_name_es, origin = &quot;un.name.es&quot;, dest = &quot;cown&quot;, custom_dict = codelist)) ## Warning in countrycode(country_name_es, origin = &quot;un.name.es&quot;, dest = &quot;cown&quot;, : Some values were not matched unambiguously: Venezuela, República Bolivariana de La variable ‘country_name_es’ de Schultz le da nombre a cada país, pero no podemos saber a priori si todos los países se adaptan al estándar que usamos (un.names.es). Veamos si tenemos algún missing value después de haber creado la variable de códigos de Correlates of War ‘ccode’. tratados_int_estandar %&gt;% filter(is.na(ccode)) ## # A tibble: 10 x 7 ## treaty_name adoption_date location country_name_es action_type_str… ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Rome Statu… 1998-07-17 Rome Venezuela, Rep… Acceptance ## 2 Rome Statu… 1998-07-17 Rome Venezuela, Rep… Accession ## 3 Rome Statu… 1998-07-17 Rome Venezuela, Rep… Approval ## 4 Rome Statu… 1998-07-17 Rome Venezuela, Rep… Ratification ## 5 Rome Statu… 1998-07-17 Rome Venezuela, Rep… Signature ## 6 Rome Statu… 1998-07-17 Rome Venezuela, Rep… Succession ## 7 Comprehens… 1996-09-10 New York Venezuela, Rep… Ratification ## 8 Comprehens… 1996-09-10 New York Venezuela, Rep… Signature ## 9 Comprehens… 1996-09-10 New York Venezuela, Rep… Succession ## 10 Comprehens… 1996-09-10 New York Venezuela, Rep… Succession to s… ## # … with 2 more variables: action_date &lt;date&gt;, ccode &lt;int&gt; Vemos que Venezuela no ha sido unida a la base porque su ccode aparece como NA. Esto puede ser por dos razones: o bien el nombre estaba mal escrito y por lo tanto countrycode no supo identificarlo, o bien que Correlates of War no tiene códigos para este país. En este caso, la respuesta es la primera opción. Schulz registra al país como “Venezuela, República Bolivariana de” y el estándar en español de Naciones Unidas un.names.es lo registra como “Venezuela (República Bolivariana de)”. Esta pequeña diferencia hizo con que a Venezuela no se le adjudique código. Cuando trabajes con nombres de países es casi seguro que te enfrentes a una situación como esta. Para comprobar los nombres puedes chequear en los nombres oficiales que Naciones Unidas da a cada país en todos los idiomas oficiales que se obtienen en este link Como countrycode no logró unir automáticamente a Venezuela, hay que resolver este tipo de situaciones manualmente con la opción custom_match. tratados_int_estandar &lt;- tratados_int %&gt;% mutate(ccode = countrycode(country_name_es, origin = &quot;un.name.es&quot;, dest = &quot;cown&quot;, #cown es Correlates of War numérico custom_dict = codelist, custom_match = c(&quot;Venezuela, República Bolivariana de&quot; = &quot;Venezuela (República Bolivariana de)&quot;))) Ahora veamos si se ha resuelto el problema. tratados_int_estandar %&gt;% filter(is.na(ccode)) ## # A tibble: 0 x 7 ## # … with 7 variables: treaty_name &lt;chr&gt;, adoption_date &lt;date&gt;, ## # location &lt;chr&gt;, country_name_es &lt;chr&gt;, action_type_string &lt;chr&gt;, ## # action_date &lt;date&gt;, ccode &lt;chr&gt; ¡Perfecto! Una vez que hemos creado la variable ccode en la base de los tratados internacionales, queremos unir la base de Schulz a Correlates of War. Nosotros bajamos los datos COW desde este link y la base ya está disponible en el paquete del libro: data(cinc_cow) Ahora podemos chequear que la base se ha cargado en nuestra sesión de R: ls() ## [1] &quot;cinc_cow&quot; &quot;tratados_int&quot; &quot;tratados_int_estandar&quot; El código que identifica a cada país es ccode, el mismo que creamos en la base de tratados. También tenemos year que denota el año, milex el gasto militar del país, milper el tamaño del ejército en hombres activos , irst producción de acero y hierro, pec consumo energético, tpop población total, upop población urbana, el cinc que es el índice compuesto de capacidades materiales que nos interesa, y por último version que es la versión de la base. Este índice refleja la fracción del poder global que cada país posee. Por ejemplo, si vemos su valor para los dos países más poderosos del mundo, Estados Unidos y China, veremos que hace algunos años China habría superado a los Estados Unidos. En la literatura de RRII existe un debate respecto a si este índice es un reflejo cabal del poder total de los estados, que podrás ver en Chan (2005) y Xuetong (2006). Para poder crear la figura usando ggplot hay que saber el código de cada país. Si te resulta útil continuar usando el estándar de COW en tus trabajos, podés bajar la lista completa de códigos en este link. ggplot(subset(cinc_cow, ccode %in% c(&quot;2&quot;, &quot;710&quot;))) + geom_line(aes(x = year, y = cinc, group = ccode, colour = stateabb)) Figura 9.1: Índice de capacidades materiales para Estados Unidos y China Para unir las dos bases vamos a usar la función left_join del paquete dplyr. De esta forma las estamos pegando de manera horizontal, como si a la base original de Schulz le agregáramos nuevas variables a la derecha de las que había. Sin embargo, hay un detalle a tener en consideración: la base de cinc_cow tiene los datos del índice CINC para más de cien años, pero nosotros aquí necesitamos los valores de cada país en el año específico en que firmaron cada tratado internacional. Por suerte, dentro del gran tidyverse existe un paquete llamado lubridate. Esta función nos permite tomar una fecha, como por ejemplo en la variable adoption_date y a partir de ella extraer el año en que el tratado internacional fue adoptado para que el valor del índice CINC sea el de este año. library(dplyr) tratados_int_estandar &lt;- tratados_int_estandar %&gt;% mutate(year = lubridate::year(adoption_date)) base_unida &lt;- left_join(tratados_int_estandar %&gt;% mutate(ccode = as.character(ccode)), cinc_cow %&gt;% mutate(ccode = as.character(ccode)), by = c(&quot;ccode&quot;, &quot;year&quot;)) Al haber unido las dos bases, queremos quedarnos únicamente con los países de América del Sur. El problema es que no tenemos una variable que registre la región de cada país. Aquí se vuele útil countrycode una vez más pues tiene incorporada una función por la cual reconoce a qué region pertenece cada país. Creamos la variable region y filtramos América del Sur. base_unida &lt;- base_unida %&gt;% mutate(region = countrycode(ccode, origin = &quot;cown&quot;, dest = &quot;region&quot;)) ## Warning in countrycode(ccode, origin = &quot;cown&quot;, dest = &quot;region&quot;): Some values were not matched unambiguously: VENEZUELA (REPÚBLICA BOLIVARIANA DE) Si quieres, puedes abrir la base para ver como se ve la variable región en este momento. Ahora, filtramos la que queremos mantener base_unida_samerica &lt;- base_unida %&gt;% filter(region == &quot;South America&quot;) Así, hemos logrado una base que tiene más variables que la original, y que nos permite filtrar por regiones geográficas del mundo. De esta forma te hemos ejemplificado la enorme utilidad de countrycode y lubridate cuando estés armando tus bases de datos. 9.3 ¿Cómo imputar datos faltantes? De manera suscita, existen cuatro tipos diferentes de datos faltantes para decidir si necesitas imputar datos recomendamos una profunda reflexión y también la lectura de los textos de Graham y Allison que están en las lecturas de referencia. Tus datos faltantes pueden ser “estructurales”, faltantes de manera absolutamente aleatoria (esto en la literatura se denomina MCAR), faltantes de manera no aleatoria y los faltantes de manera aleatoria. Solo querremos imputar cuando los datos sean faltantes de manera aleatoria. Para poner un breve ejemplo de cada tipo de dato faltante, ¿qué significa que haya datos faltantes de manera estructural? En el ejemplo debajo tenemos una variable que codifica si el país tiene vigente una ley de matrimonio entre personas del mismo sexo. La variable siguiente codifica el año en que dicha ley se ha sancionado. Si te fijas, Venezuela y Perú tienen valores faltantes en esta variable, pero la razón es que no tienen en principio una ley de este tipo. Entonces, los datos faltantes son estructurales cuando el dato está faltando porque el dato no existe. ID Ley de matrimonio igualitario Año de aprobación Argentina 1 2010 Brasil 1 2013 Venezuela 0 NA Peru 0 NA Tendremos datos faltantes de manera absolutamente aleatoria si cuando los datos fueron generados hubo algún criterio de aleatorización. Por ejemplo, si en un cuestionario de diez preguntas hemos preguntado ocho a cada persona de manera aleatoria. Las dos preguntas faltantes para cada persona no estarán explicadas por variables relacionadas al encuestado (su ideología, edad, género, religión, etc). Puede darse que nuestros datos sean faltantes no aleatoriamente. Este es posiblemente el escenario más frecuente en el que equivocadamente se imputan datos en ciencia política, cuando en verdad no se debería hacer. Los datos faltantes no aleatorios son aquellos cuya condición de faltantes se correlaciona otra variable de tal forma que hay un patrón en la falta de datos. Cuando nos encontramos ante una situación en que faltan datos en nuestra base debemos reflexionar cuidadosamente respecto a qué variables pueden estar explicando esta falta de datos, y si es que existe un sesgo de selección. Por ejemplo, si utilizamos datos del banco mundial respecto a crecimiento del PBI es muy probable que países muy pobres no reporten datos. Esto se debe a que en estos países la calidad de las mediciones no es óptima. También puede darse que las mediciones no se han hecho si el país atraviesa un momento difícil en su economía que repercute sobre las estadísticas nacionales. Por ejemplo, desde 2016 que el Banco Mundial no reporta índices de inflación para Venezuela. Idealmente uno debería corregir este sesgo mediante una covariable en el modelo, usando modelos de selección (no los vemos en el libro, lamentablemente), o simplemente hacer pairwise en las regresiones sabiendo que nuestra muestra puede no ser representativa de la población. Por último, si asumimos que nuestros datos son faltantes de manera aleatoria (los textos los describen como MAR) estos estarán generados sin un patrón, pero la otra información con la que contamos en la base de datos nos permitiría “adivinar” el valor faltante. Pensemos en el índice CINC, por ejemplo: imagina que tenemos veinte observaciones faltantes en el índice de los Estados Unidos. Como el índice se compone de las otras variables de capacidades materiales, podemos utilizar estas para predecir los valores que faltan. Tomando este caso vamos a hacer dos escenarios de imputación. El primero es de imputación con fines descriptivos (creación de gráficos). El segundo, quizás más útil, será una imputación para aumentar la cantidad de observaciones en nuestros modelos de regresión. Utilicemos como ejemplo el gráfico con el índice CINC de Estados Unidos y China solo que lo haremos con puntos para poder ver los valores año a año más facilmente. ggplot(subset(cinc_cow, ccode %in% c(&quot;2&quot;, &quot;710&quot;))) + geom_point(aes(x = year, y = cinc, group = ccode, colour = stateabb)) + scale_x_continuous(breaks = seq(1810, 2020, 10)) + theme(panel.grid.minor = element_blank(), axis.text.x = element_text(angle = 90, hjust = 1)) Figura 9.2: En vez de ver el índice de capacidades materiales como una línea vemos cada año como un punto para ver mejor datos faltantes ¿Cómo se vería el gráfico si Estados Unidos no tuviera datos entre 1950 y 1970? Primero eliminamos estos datos de la base cinc_cow2 &lt;- cinc_cow %&gt;% mutate(cinc_na = if_else(ccode == 2 &amp; between(year, 1950, 1970), NA_real_, cinc)) Luego vemos el gráfico ggplot(subset(cinc_cow2, ccode %in% c(&quot;2&quot;, &quot;710&quot;))) + geom_point(aes(x = year, y = cinc_na, group = ccode, colour = stateabb)) + scale_x_continuous(breaks = seq(1810, 2020, 10)) + theme(panel.grid.minor = element_blank(), axis.text.x = element_text(angle = 90, hjust = 1)) ## Warning: Removed 21 rows containing missing values (geom_point). Figura 8.5: A la figura le faltan los datos de Estados Unidos entre 1950 y 1970 Es importante en este punto hacer la reflexión del tipo de datos faltantes que tenemos. Vamos a suponer que no habría motivos para asumir que esos datos faltasen en la base, por lo tanto son faltantes aleatoriamente. Para hacer la imputación vamos a usar Amelia. 9.4 Imputar datos usando Amelia Amelia II es un programa creado por Gary King, James Honaker y Matthew Blackwell. En R lo consigues mediante install.packages(&quot;Amelia&quot;). Nos gusta mucho Amelia II, y lo preferimos sobre otros paquetes conocidos como mice por su facilidad, versatilidad y rapidez. El paquete está diseñado para poder indicar cuantas imputaciones queremos que se hagan para cada dato faltante (con la opción m=, que por defecto hace 5 imputaciones), y permite trabajar tanto con bases de datos transversales (como una encuesta) como con datos en panel, como por ejemplo en nuestro caso del índice CINC donde hay datos de muchos años para cada país. library(Amelia) ## Loading required package: Rcpp ## ## ## ## Amelia II: Multiple Imputation ## ## (Version 1.7.5, built: 2018-05-07) ## ## Copyright (C) 2005-2019 James Honaker, Gary King and Matthew Blackwell ## ## Refer to http://gking.harvard.edu/amelia/ for more information ## ## Lo primero que querrás confirmar es la cantidad de datos faltantes en tu variable de interés. Para esto recomendamos usar la función skimde skimr. En la variable cinc_na tenemos veinte datos faltantes más que en la variable cinc, que son las observaciones que acabamos de eliminar para hacer el ejemplo. skimr::skim(cinc_cow2) ## Skim summary statistics ## n obs: 14124 ## n variables: 12 ## ## ── Variable type:character ────────────────────────────── ## variable missing complete n min max empty n_unique ## stateabb 1937 12187 14124 3 3 0 159 ## ## ── Variable type:numeric ──────────────────────────────── ## variable missing complete n mean sd p0 ## p25 p50 p75 p100 hist ## [ reached getOption(&quot;max.print&quot;) -- omitted 11 rows ] El segundo paso en la imputación consiste en elegir las variables que utilizarás. Solo necesitas tu variable temporal (year), el ID de cada observación (en este caso tenemos dos: stateabb y ccode) y las variables con las que harás las predicciones de los valores. Dejaremos fuera del análisis a version que es una variable que siempre asume el valor 2011 porque indica el año de la versión de los datos y también eliminaremos cinc que es la variable sin datos faltantes. Llamaremos a este conjunto de variables cinc_cow2_amelia. cinc_cow2_amelia &lt;- cinc_cow2 %&gt;% select(ccode, stateabb, year, milex, milper, irst, pec, tpop, upop, cinc_na) Para asegurarnos que nuestra base esté en formato de data frame haremos el siguiente paso cinc_cow2_amelia &lt;- as.data.frame(cinc_cow2_amelia) Llamaremos a la imputación imputado_m1. Para hacer la imputación vamos a necesitar indicar a Amelia algunas opciones del comando: cs es la variable que identifica a las observaciones, en este caso los países. ts es la variable temporal, en este caso el año. m es la cantidad de imputaciones que queremos que simule amelia. Le pediremos que haga una sola, y ahora veremos un ejemplo con 4 imputaciones. Con la opción idvars estamos indicando al comando que la variable ccode identifica a los países, por lo que no debe ser tenida en cuenta en el cálculo de los valores faltantes. Por último, la opción polytime incorpora un término extra a la imputación para dar cuenta de los efectos temporales de los valores. En nuestro caso esta opción es fundamental pues todos los valores faltantes corresponden a veinte años consecutivos. Si definimos la opción 1 se incorpora un término que modela efectos temporales lineares, si definimos la opción 2 se modelan efectos cuadráticos, y si definimos la opción 3 se modelan efectos cúbicos. imputado_m1 &lt;- amelia(x = cinc_cow2_amelia, cs = &quot;stateabb&quot;, ts = &quot;year&quot;, m = 1, idvars = &quot;ccode&quot;, polytime=1) ## Warning: There are observations in the data that are completely missing. ## These observations will remain unimputed in the final datasets. ## -- Imputation 1 -- ## ## 1 2 summary(imputado_m1) ## ## Amelia output with 1 imputed datasets. ## Return code: 1 ## Message: Normal EM convergence. ## ## Chain Lengths: ## -------------- ## Imputation 1: 2 ## ## Rows after Listwise Deletion: 12166 ## Rows after Imputation: 12187 ## Patterns of missingness in the data: 3 ## ## Fraction Missing for original variables: ## ----------------------------------------- ## ## Fraction Missing ## ccode 0.14 ## stateabb 0.14 ## year 0.14 ## milex 0.14 ## milper 0.14 ## irst 0.14 ## pec 0.14 ## tpop 0.14 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 2 rows ] Una vez realizada la imputación, la graficaremos para ver cómo se ven nuestros datos. La imputación ha sido guardada como un objeto llamado imp1, así que le damos un nombre. datos_imputados&lt;-imputado_m1$imputations$imp1 ggplot(subset(datos_imputados, ccode %in% c(&quot;2&quot;, &quot;710&quot;))) + geom_point(aes(x = year, y = cinc_na, group = ccode, colour = stateabb)) + scale_x_continuous(breaks = seq(1810, 2020, 10)) + theme(panel.grid.minor = element_blank(), axis.text.x = element_text(angle = 90, hjust = 1)) Figura 9.3: Los valores entre 1950 y 1970 para EEUU han sido imputados con Amelia Ahora haremos la simulación definiendo cuatro imputaciones en vez de una. Para ello, repetiremos el proceso, con la única diferencia que en la opción m indicaremos 4 en vez de 1. cinc_cow2_amelia &lt;- cinc_cow2 %&gt;% select(ccode, stateabb, year, milex, milper, irst, pec, tpop, upop, cinc_na) cinc_cow2_amelia &lt;- as.data.frame(cinc_cow2_amelia) set.seed(9999) imputado_m4 &lt;- amelia(x = cinc_cow2_amelia, cs = &quot;stateabb&quot;, ts = &quot;year&quot;, m = 4, idvars = &quot;ccode&quot;, polytime=2) ## Warning: There are observations in the data that are completely missing. ## These observations will remain unimputed in the final datasets. ## -- Imputation 1 -- ## ## 1 2 ## ## -- Imputation 2 -- ## ## 1 2 ## ## -- Imputation 3 -- ## ## 1 2 ## ## -- Imputation 4 -- ## ## 1 2 summary(imputado_m4) ## ## Amelia output with 4 imputed datasets. ## Return code: 1 ## Message: Normal EM convergence. ## ## Chain Lengths: ## -------------- ## Imputation 1: 2 ## Imputation 2: 2 ## Imputation 3: 2 ## Imputation 4: 2 ## ## Rows after Listwise Deletion: 12166 ## Rows after Imputation: 12187 ## Patterns of missingness in the data: 3 ## ## Fraction Missing for original variables: ## ----------------------------------------- ## ## Fraction Missing ## ccode 0.14 ## stateabb 0.14 ## year 0.14 ## milex 0.14 ## milper 0.14 ## irst 0.14 ## pec 0.14 ## tpop 0.14 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 2 rows ] datos_imputados_1&lt;-imputado_m4$imputations$imp1 datos_imputados_2&lt;-imputado_m4$imputations$imp2 datos_imputados_3&lt;-imputado_m4$imputations$imp3 datos_imputados_4&lt;-imputado_m4$imputations$imp4 Hemos llamado a cada una de las imputaciones como datos_imputados y lo que haremos a continuación en unirlos a todos en un solo data frame. imputaciones_4 &lt;- bind_rows( datos_imputados_1 %&gt;% mutate(imp = &quot;1&quot;, obs_original = row_number()), datos_imputados_2 %&gt;% mutate(imp = &quot;2&quot;, obs_original = row_number()), datos_imputados_3 %&gt;% mutate(imp = &quot;3&quot;, obs_original = row_number()), datos_imputados_4 %&gt;% mutate(imp = &quot;4&quot;, obs_original = row_number()) ) %&gt;% select(obs_original, imp, everything()) %&gt;% filter(ccode == &quot;2&quot;, between(year, 1950, 1970)) %&gt;% mutate(fuente = &quot;imputaciones_4&quot;) Para poder analizar los datos imputados de manera gráfica haremos un promedio de las imputaciones. imputaciones_promedio &lt;- imputaciones_4 %&gt;% group_by(ccode, stateabb, year) %&gt;% summarize(cinc_na = mean(cinc_na)) %&gt;% ungroup() %&gt;% mutate(fuente = &quot;imputaciones_promedio&quot;) Unimos las cuatro imputaciones a su promedio. plot_df &lt;- cinc_cow2 %&gt;% select(ccode, stateabb, year, cinc_na) %&gt;% filter(ccode %in% c(&quot;2&quot;, &quot;710&quot;)) %&gt;% mutate(fuente = &quot;original&quot;) %&gt;% bind_rows(imputaciones_4, imputaciones_promedio) La figura muestra en valores claros las imputaciones individuales, y en un tono más oscuro el valor promedio de las cuatro imputaciones. ggplot(mapping = aes(x = year, y = cinc_na, group = ccode)) + geom_point(data = plot_df %&gt;% filter(fuente == &quot;original&quot;), aes(color = stateabb)) + geom_point(data = plot_df %&gt;% filter(fuente == &quot;imputaciones_4&quot;), aes(color = fuente, shape = imp)) + geom_point(data = plot_df %&gt;% filter(fuente == &quot;imputaciones_promedio&quot;), aes(color = fuente)) + scale_x_continuous(breaks = seq(1810, 2020, 10)) + scale_color_manual(breaks = c(&quot;CHN&quot;, &quot;USA&quot;, &quot;imputaciones_4&quot;, &quot;imputaciones_promedio&quot;), labels = c(&quot;China&quot;, &quot;USA&quot;, &quot;USA (4 imputaciones)&quot;, &quot;USA (prom. imputaciones)&quot;), values = c(&quot;CHN&quot; = &quot;#F8766D&quot;, &quot;USA&quot; = &quot;turquoise3&quot;, &quot;imputaciones_4&quot; = &quot;lightsteelblue3&quot;, &quot;imputaciones_promedio&quot; =&quot;navyblue&quot;)) + labs(color = &quot;País&quot;, shape = &quot;Número de imputación&quot;) + theme(panel.grid.minor = element_blank(), axis.text.x = element_text(angle = 90, hjust = 1)) + guides(color = guide_legend(order = 1), shape = guide_legend(order = 2)) ## Warning: Removed 21 rows containing missing values (geom_point). Figura 6.8: Esta figura refleja las cuatro imputaciones que hicimos con amelia junto a su promedio Es importante repetir aquí que el ejercicio de imputación cuando hagamos trabajos descriptivos es diferente al ejercicio cuando imputemos para rodar regresiones. Figura 9.4: El proceso de imputación para análisis visual se hace luego de promediar las imputaciones que hayamos pedido a Amelia De esta forma hemos visto como completar bases de datos con valores faltantes, ya sea mediante una o más imputaciones usando amelia. 9.4.1 Diagnosticos El paquete nos ofrece también una herramienta para comparar más de una imputación, si es que no estamos seguros respecto a cuál utilizar. Por ejemplo, supongamos que no sabemos si los efectos temporales serían mejor ajustados con efectos lineares, cuadráticos o cúbicos. Mediante la función tscsPlot veremos el promedio de las imputaciones con su respectivo intérvalo de confianza. #Efectos temporales lineales imputado_m4_p1 &lt;- amelia(x = cinc_cow2_amelia, cs = &quot;stateabb&quot;, ts = &quot;year&quot;, m = 4, idvars = &quot;ccode&quot;, polytime=1) ## Warning: There are observations in the data that are completely missing. ## These observations will remain unimputed in the final datasets. ## -- Imputation 1 -- ## ## 1 2 ## ## -- Imputation 2 -- ## ## 1 2 ## ## -- Imputation 3 -- ## ## 1 2 ## ## -- Imputation 4 -- ## ## 1 2 tscsPlot(imputado_m4_p1, cs = c(&quot;USA&quot;), var = &quot;cinc_na&quot;, main=&quot;Valores imputados para con polytime=1&quot;) ## Warning: There are observations in the data that are completely missing. ## These observations will remain unimputed in the final datasets. #Efectos temporales cuadráticos imputado_m4_p2 &lt;- amelia(x = cinc_cow2_amelia, cs = &quot;stateabb&quot;, ts = &quot;year&quot;, m = 4, idvars = &quot;ccode&quot;, polytime=2) ## Warning: There are observations in the data that are completely missing. ## These observations will remain unimputed in the final datasets. ## -- Imputation 1 -- ## ## 1 2 ## ## -- Imputation 2 -- ## ## 1 2 ## ## -- Imputation 3 -- ## ## 1 2 ## ## -- Imputation 4 -- ## ## 1 2 tscsPlot(imputado_m4_p2, cs = c(&quot;USA&quot;), var = &quot;cinc_na&quot;, main=&quot; Valores imputados con polytime=2&quot;) ## Warning: There are observations in the data that are completely missing. ## These observations will remain unimputed in the final datasets. # Efectos temporales cúbicos imputado_m4_p3 &lt;- amelia(x = cinc_cow2_amelia, cs = &quot;stateabb&quot;, ts = &quot;year&quot;, m = 4, idvars = &quot;ccode&quot;, polytime=3) ## Warning: There are observations in the data that are completely missing. ## These observations will remain unimputed in the final datasets. ## -- Imputation 1 -- ## ## 1 2 ## ## -- Imputation 2 -- ## ## 1 2 ## ## -- Imputation 3 -- ## ## 1 2 ## ## -- Imputation 4 -- ## ## 1 2 tscsPlot(imputado_m4_p3, cs = c(&quot;USA&quot;), var = &quot;cinc_na&quot;, main=&quot; Valores imputados con polytime=1&quot;) ## Warning: There are observations in the data that are completely missing. ## These observations will remain unimputed in the final datasets. En este caso no hay una diferencia notable entre las tres opciones. Cuando la cantidad de datos faltantes son más, es posible que sea necesario evaluar varias versiones de la imputación para decidir cuál utilizar. 9.5 Regresiones luego de imputar datos A diferencia del proceso para visualización de datos, cuando queremos imputar datos que utilizaremos en análisis de regresiones lo que recomienda la lectura es no promediar las imputaciones, sino promediar los coeficientes de las regresiones obtenidos con cada una de las imputaciones que hemos hecho. El proceso es el siguiente Figura 9.5: El proceso de imputación para análisis de regresión consiste en promediar los coeficientes de los datos imputados, no promediar las bases imputadas. Primero, veamos como se ve la regresión con los datos incompletos para Estados Unidos. Estaremos explicando el índice CINC de cada país por las variables pec y upop. modelo_incompleto &lt;- lm(I(cinc_na*100) ~ I(pec / 1000) + I(upop / 1000), data = cinc_cow2) texreg::screenreg(modelo_incompleto) ## ## ========================== ## Model 1 ## -------------------------- ## (Intercept) 1.11 *** ## (0.03) ## I(pec/1000) 0.01 *** ## (0.00) ## I(upop/1000) 0.02 *** ## (0.00) ## -------------------------- ## R^2 0.18 ## Adj. R^2 0.18 ## Num. obs. 12166 ## RMSE 3.60 ## ========================== ## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 Estamos usando el data frame que llamamos imputaciones_4 donde guardamos las cuatro imputaciones (imp1, imp2, imp3 e imp4) que hicimos en el ejercicio anterior. Haremos una regresión para cada una de las cuatro imputaciones. modelo_imputado_1 &lt;- lm(I(cinc_na*100) ~ I(pec / 1000) + I(upop / 1000), data = imputado_m4$imputations$&quot;imp1&quot;) modelo_imputado_2 &lt;- lm(I(cinc_na*100) ~ I(pec / 1000) + I(upop / 1000), data = imputado_m4$imputations$&quot;imp2&quot;) modelo_imputado_3 &lt;- lm(I(cinc_na*100) ~ I(pec / 1000) + I(upop / 1000), data = imputado_m4$imputations$&quot;imp3&quot;) modelo_imputado_4 &lt;- lm(I(cinc_na*100) ~ I(pec / 1000) + I(upop / 1000), data = imputado_m4$imputations$&quot;imp4&quot;) lista_modelos &lt;- list(modelo_imputado_1, modelo_imputado_2, modelo_imputado_3, modelo_imputado_4) Al correr las regresiones las guardamos todas en una lista que llamaremos lista_modelos y luego extraeremos de ella los coeficientes y los errores estándar de cada uno de ellos con la siguiente función df_coefs &lt;- map_dfr(lista_modelos, ~ coef(.x) %&gt;% bind_rows()) df_se &lt;- map_dfr(lista_modelos, ~ sqrt(diag(vcov(.x))) %&gt;% bind_rows()) La función mi.meld del paquete de amelia nos permite combinar todos los coeficientes de las diferentes imputaciones. Luego, para evitar errores en el comando, indicamos que los valores extraídos son numéricos. res_imputado &lt;- mi.meld(df_coefs, df_se) coefs_imputados &lt;- res_imputado[[&quot;q.mi&quot;]] %&gt;% as.numeric() se_imputados &lt;- res_imputado[[&quot;se.mi&quot;]] %&gt;% as.numeric() ¡Ya estamos cerca! Con los coeficientes y sus respectivos errores estándar podemos calcular la significancia estadística de cada uno de ellos p_values_imputados &lt;- 2 * pt(coefs_imputados / se_imputados, modelo_imputado_1$df.residual, lower.tail = FALSE) Una vez que tenemos todo lo que necesitamos, podemos crear un elemento, que llamamos e_modelo_imputado para poder crear la tabla de texreg e_modelo_imputado &lt;- texreg::extract(modelo_incompleto, include.rsquared = F, include.adjrs = F, include.rmse = F) e_modelo_imputado@gof &lt;- nobs(modelo_imputado_1) e_modelo_imputado@coef &lt;- coefs_imputados e_modelo_imputado@se &lt;- se_imputados e_modelo_imputado@pvalues &lt;- p_values_imputados Finalmente logramos lo que buscábamos. Una regresión donde tenemos más observaciones gracias al proceso de imputación que hemos hecho. En este caso la diferencia de observaciones es muy pequeña, pues pasamos de 12165 a 12187, con casi ninguna diferencia en los coeficientes. Cuando el porcentaje de valores faltantes sea mayor, notarás que el esfuerzo de la imputación hace una diferencia en los resultados. texreg::screenreg(list(modelo_incompleto, e_modelo_imputado)) ## ## ======================================== ## Model 1 Model 2 ## ---------------------------------------- ## (Intercept) 1.11 *** 1.11 *** ## (0.03) (0.03) ## I(pec/1000) 0.01 *** 0.01 *** ## (0.00) (0.00) ## I(upop/1000) 0.02 *** 0.01 *** ## (0.00) (0.00) ## ---------------------------------------- ## R^2 0.18 ## Adj. R^2 0.18 ## Num. obs. 12166 12187 ## RMSE 3.60 ## ======================================== ## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 ¡Esperamos que hayas encontrado útil el capítulo! Si te interesa profundizar sobre el trabajo con valores faltantes te recomendamos que leas el texto de James Honaker y Gary King “What to do About Missing Values in Time Series Cross-Section Data” American Journal of Political Science Vol. 54, No. 2 (April, 2010): Pp. 561-581.. Ejercicios antes de continuar al próximo capítulo A la base que hemos llamado base_unida agregale una variable de tu elección bajada de Worldwide Governance Indicators. Usa el paquete coutrycode para unir las variables. En vez de 4 imputaciones, repite el ejercicio de utilizar datos imputados para modelos de regresión pero esta vez con 6 imputaciones. "],
["pca.html", "Capítulo 10 Creación de índices con PCA 10.1 Aplicación en R", " Capítulo 10 Creación de índices con PCA Por Caterina Labrín y Francisco Urdinez Lecturas de referencia Box-Steffensmeier, J. M., Brady, H. E., &amp; Collier, D. (Eds.). (2008). The Oxford handbook of political methodology (Vol. 10).Oxford University Press. Caps.6 y 7: “Measurement” y “Typologies: Forming Concepts and Creating Categorical Variables”. Goertz, G. (2006). Social science concepts: A user’s guide. Princeton University Press. Capítulo 4 “Incrasing Concept-Measure Consistency”. Abeyasekera, S. (2005). “Multivariate Methods for Index Construction.” In Household Sample Surveys and Transition Countries.367-387. New York City: DESA/UNSD. Miller, J. E. (2013). The Chicago guide to writing about multivariate analysis. University of Chicago Press. Cap. 15: “Speaking about Multivariate Analyses”. Grimm, L. G., &amp; Yarnold, P. R. (2000). Reading and understanding multivariate statistics. American Psychological Association. Cap. 7: “Assessing the validity of measure”. A esta altura del libro has visto herramientas suficientes como para tener una caja de herramientas que te permita escribir un artículo académico, hacer consultoría o formular políticas públicas. En este capítulo veremos cómo crear índices compuestos a partir de un conjunto de variables que miden una misma variable latente. En toda ciencia social y sobre todo en la ciencia política, constantemente trabajamos con variables sumamente abstractas, que muchas veces son difíciles de medir con un solo indicador. “Democracia” o “capacidad estatal” son conceptos con los que debemos trabajar, pero muchas veces poder construirlos conceptualmente es un poco complejo. Cuando construimos una medición empírica de un concepto abstracto, además de dar cuenta de su complejidad, queremos que sea válido (ver Grimm y Yarnold 2000 para un excelente resumen sobre diferentes tipos de validez). La validez de constructo, que es la que nos importa aquí, refiere a que el concepto mida lo que dice medir y en disciplinas como la ciencia política siempre debe estar cuestionada, precisamente por lo que anteriormente decíamos de la complejidad de los conceptos con los que trabajamos. Por ejemplo, esto es muy frecuente con las mediciones que existen de democracia. Incluso cuando hemos avanzado enormemente como disciplina, y hoy disponemos de mediciones comparables entre países y a lo largo del tiempo, como Polity y Varieties of Democracy, ambos están sujetos a críticas sobre como miden la variable. Es natural que así sea, de eso se trata la ciencia. Hay dos formas, sin embargo, de tener confianza respecto a la validez de constructo de nuestra variable de interés. La primera es por validez convergente, es decir, la correlación que mi medida de interés tiene con medidas de otros. Por ejemplo, la democracia tal como la mide V-Dem tiene una correlación de 0.8 con la forma como la mide Polity. Si yo creo una tercera medición, cuya correlación es de 0.70 con Polity y 0.9 con V-Dem podré estar confiado de que las tres variables están aproximándose de manera similar a la variable latente. La segunda alternativa para tener confianza respecto a la validez de constructo de nuestra variable es por medio de lo que se llama “validez discriminativa”, es decir que nuestro constructo no debería estar altamente correlacionado a variables que nuestro indicador dice no medir. Este concepto se puede ejemplificar bien con la forma en que V-Dem ha elegido crear su índice de democracia (al que llaman de poliarquía): como dice el nombre, lo que hacen es medir variedades de democracia a partir de diferentes sub-dimensiones, a saber, las dimensiones liberal, participativa, deliberativa, electoral e igualitaria. Lo que esperaríamos es que cada una de ellas tenga baja correlación de manera que tengamos confianza en que verdaderamente capturan las diferentes aristas de este concepto multifacético. El poder crear índices compuestos a partir de varias variables que miden un mismo concepto puede sernos muy útil. El análisis de componentes principales (PCA por sus siglas en inglés) es una técnica muy útil a la hora de combinar distintas variables y veremos cómo hacerlo en R. Consiste literalmente en trabajar con la correlación entre las variables, y dar cuenta de lo que ellas miden en común y lo que cada una mide individualmente que las otras variables no capturan. La técnica de PCA nos entrega distintos “componentes”, que son variables conformadas a partir de una combinación lineal de las variables usadas para crear el índice, y estos componentes pueden ser unidos para generar una nueva variable. Vamos a pasar a un ejemplo práctico para evitar que la discusión se vuelva muy abstracta. Te recomendamos, si vas a usar índices en tus trabajos, que antes leas los capítulos que te hemos recomendado del Oxford handbook of political methodology. También te recomendamos esta visualización creada por el proyecto de visualización de datos Setosa donde podrás ver de qué manera PCA “reduce” muchas variables en la menor cantidad posible de “componentes” maximizando la cantidad de varianza común que ellos tienen. Por ejemplo, verás como a partir de 17 variables con las dietas de los cuatro países que conforman el Reino Unido, PCA logra reducir estas diferencias a dos variables. Como técnica de reducción de dimensiones, PCA logra resultados muy útiles que vamos a explotar a continuación. 10.1 Aplicación en R Existen hoy en día distintas mediciones de democracia a nivel internacional, tales como Polity o Freedom House, pero ¿qué pasa si queremos saber qué es lo que opinan los ciudadanos que votan en las democracias sobre las instituciones democráticas que respaldan este sistema político? En América Latina, dado el pasado de gobiernos dictatoriales y el reciente debilitamiento democrático de muchos países, puede ser importante en nuestra investigación considerar la opinión pública sobre las instituciones de sus propios países. Una buena pregunta para hacerse es ¿cómo poder saber cuál es la opinión que los ciudadanos latinoamericanos tienen respecto a las instituciones democráticas de sus países? El Proyecto de Opinión Pública de América Latina ( LAPOP en inglés) – es coordinado desde la Universidad de Vanderbilt y está especializado en conducir estudios de evaluación de impacto, y producir reportes acerca de las actitudes, evaluaciones y experiencias de los individuos de países latinoamericanos. Este proyecto pone a disposición de los investigadores distintas preguntas que, en su conjunto, podrían ayudarnos a aproximar cuanta confianza existe en la región con respecto a instituciones democráticas del país correspondiente a cada individuo. La aplicación de esta herramienta en R responde a una serie de pasos que nos permitirá generar un índice que caracterice a cada uno de los individuos desde aquellos que sean más propensos a tener confianza en las instituciones formales democráticas y aquellos que no. A partir de este índice individual, luego podremos hacer una gran variedad de estudios, desde comparaciones entre países, a comparaciones por edad, género, renta u otra variable de interés. Partiremos cargando una base con las preguntas. Es importante como primer paso seleccionar las variables que nos servirán para realizar el PCA, dejando de lado todo aquello que no queramos usar en el índice final. Para esto, generamos una nueva base que contenga sólo las variables que queramos ocupar, en este caso, para conocer cuál es la opinión sobre las instituciones democráticas que tienen los latinoamericanos. Para poder seleccionar las variables y utilizar los pipes cargamos tidyverse. Luego, cargamos la base desde el paquete del libro, paqueteadp. library(tidyverse) library(paqueteadp) data(lapop) Una vez cargada la base, filtramos aquellas preguntas que nos interesan + del: Variable dicotómica que mide el nivel de justificación de golpes de estado militares en el país del encuestado frente a mucha delincuencia. Basada en la pregunta “jc10” de Encuesta LAPOP. + pdte: Variable dicotómica que mide el nivel de justificación de cierre del congreso en situaciones difíciles por parte del presidente. Basada en la pregunta “jc15a” de Encuesta LAPOP. + trib: Mide en una escala del 1 al 7 el nivel de confianza en los tribunales del país del encuestado. Basada en la pregunta “b1” de Encuesta LAPOP. + inst: Mide en una escala del 1 al 7 el nivel de respeto por las instituciones políticas del país del encuestado. Basada en la pregunta “b2” de Encuesta LAPOP. + confcon: Mide en una escala del 1 al 7 el nivel de confianza en el Congreso Nacional (poder legislativo) del país del encuestado. Basada en la pregunta “b13” de Encuesta LAPOP. + confpdte: Mide en una escala del 1 al 7 el nivel de confianza en el presidente (poder ejecutivo) del país del encuestado. Basada en la pregunta “b21a” de Encuesta LAPOP. + confpp: Mide en una escala del 1 al 7 el nivel de confianza en los partidos políticos del país del encuestado. Basada en la pregunta “b21” de Encuesta LAPOP. + confmedia: Mide en una escala del 1 al 7 el nivel de confianza en los medios de comunicación del país del encuestado. Basada en la pregunta “b37” de Encuesta LAPOP. + confelec: Mide en una escala del 1 al 7 el nivel de confianza en las elecciones del país del encuestado. Basada en la pregunta “b47a” de Encuesta LAPOP. + satdem: Variable dicotómica que mide el nivel de satisfacción con la democracia tienen los encuestados. Basada en la pregunta “pn4” de Encuesta LAPOP. + dervoto: Mide en una escala del 1 al 7 la satisfacción con la idea de que aquellos que se encuentren en contra del gobierno de turno puedan ejercer el voto en las elecciones del país del encuestado. Basada en la pregunta “d1” de Encuesta LAPOP. + derman: Mide en una escala del 1 al 7 la satisfacción con la idea de que aquellos que se encuentren en contra del gobierno de turno puedan llevar a cabo manifestaciones pacíficas para expresar su punto de vista. Basada en la pregunta “d2” de Encuesta LAPOP. Estas variables pueden explorarse gráficamente antes de pasar a la creación del índice. Por ejemplo, podemos ver por país la confianza que sus ciudadanos tienen en las elecciones lapop &lt;- lapop %&gt;% group_by(paisnom) %&gt;% mutate(confelec_prom = mean(confelec)) %&gt;% ungroup() ggplot(lapop, aes(x = confelec)) + geom_histogram() + labs(title=&quot;Confianza en las elecciones&quot;,x=&quot;En azul el promedio de cada país&quot;, y = &quot;conteo&quot;)+ facet_wrap(~ paisnom) + geom_vline(aes(xintercept=(confelec_prom)), color=&quot;blue&quot;, linetype=&quot;dashed&quot;, size=1) Figura 10.1: En la figura vemos el valor promedio de confianza de las elecciones en azul y la distribución de la variable para cada país Vamos a filtrar nuestras variables de interés, esto quiere decir que dejamos fuera toda variable que no sea proxy de democracia, por ejemplo la variable paisnom que indica el país de cada individuo no debería ser incorporada. datos_pca_lapop &lt;- lapop %&gt;% select(del, pdte, trib, inst, confcon, confpdte, confpp, confmedia, confelec, satdem, dervoto, derman) %&gt;% mutate_all(as.numeric) Esta data contiene doce preguntas de la encuesta de LAPOP, realizadas a una selección de cerca de 7000 personas en 10 países latinoamericanos. El cuestionario entero puede accederse en este link. Este primer paso es fundamental. Elegir bien las variables que van a integrar nuestro índice merece mucha reflexión y debe haber justificación teórica para cada variable. Como se dice en la jerga: garbage in, garbage out. Por eso, reflexiona sobre cuál es el concepto o variable latente que estas intentando medir empíricamente (en este caso, la opinión individual sobre las instituciones democráticas), seleccionando las variables que realmente necesites a la hora de construir el índice. Si agregamos variables que no se condicen realmente con aquello que queremos medir, el PCA como herramienta nos arrojará un resultado que no servirá de mucho. Ya habiendo seleccionado las variables, el siguiente paso es observar la correlación entre ellas. Esto sirve también para saber cómo se relacionan las variables elegidas, y además para ver si hay correlaciones extremadamente altas entre dos o más variables ya que si existe el caso en que tengamos dos o más variables con alta correlación entre ellas, éstas tendrán enorme influencia sobre el resultado de nuestro índice (esto quedará más claro en un segundo cuando hablemos de los componentes). La mejor alternativa para observar correlación entre variables creemos que es con el paquete GGally. A mayor intensidad del color, más fuerte la correlación. En azul tendremos correlaciones negativas, y en rojo correlaciones positivas. library(GGally) ggcorr(datos_pca_lapop, label = T) Figura 10.2: Matriz de correlación de las variables elegidas para nuestro índice Lo que vemos es que las variables sobre confianza están correlacionadas de manera positiva. Es decir, quien confía en el presidente tiende a confiar también en medios, elecciones, etc. Una vez que observamos las correlaciones, pasamos al próximo paso que es el de generar análisis de componentes principales con el paquete FactoMineR de la siguiente forma: library(FactoMineR) pca_1 &lt;- PCA(datos_pca_lapop, graph = F) Lo que obtendremos es un componente (R las llama dimensiones) por cada variable que hemos agregado al análisis. Como utilizamos doce variables, esperamos doce componentes. Estos componentes son variables que capturan la correlación entre si que todas ellas tienen. Cada dimensión tiene su valor propio o eigenvalue y el porcentaje de varianza que esta dimensión representa. Ambos valores que podemos encontrar con el paquetefactoextra. library(factoextra) get_eig(pca_1) ## eigenvalue variance.percent cumulative.variance.percent ## Dim.1 3.44 28.7 29 ## Dim.2 1.48 12.3 41 ## [ reached getOption(&quot;max.print&quot;) -- omitted 10 rows ] Vea que la primera dimensión condensa el 29% de la varianza común entre las variables. Antes de continuar con estos, nos vamos a detener un minuto así explicamos bien la forma en que estas dimensiones son creadas y qué representan. Ahora detengámonos un segundo para hacer un ejercicio teórico para entender cómo funciona PCA: Imagine que en vez de las doce variables tenemos apenas dos, y que estas variables tienen una correlación baja entre ellas. De esta forma, ambas variables se verían de la siguiente forma en un scatterplot: n.casos &lt;- 240 n.vars &lt;- 1 set.seed(26) simulacion &lt;- rnorm(n.vars, 0, 1/4) x &lt;- matrix(rnorm(n.casos * (n.vars+1)), nrow=n.casos) beta &lt;- rbind(c(1,rep(0, n.vars)), c(0,rep(1, n.vars)), cbind(rep(0,n.vars), diag(simulacion))) cor(x) ## [,1] [,2] ## [1,] 1.00 -0.12 ## [2,] -0.12 1.00 plot(data.frame(x)) Figura 10.3: Scatterplot de dos variables cuya correlación es cercana a 0 Si realizaramos un PCA entre ambas, veríamos que los dos componentes creados a partir de estas dos variables se dividen, más o menos equitativamente, la varianza total explicada. peso.componentes &lt;- lapply(2:dim(beta)[1], function(k) prcomp(x[, 1:k], scale=T)) grafico &lt;- lapply(peso.componentes, summary) par(mfrow=c(1,1)) grafico &lt;- lapply(peso.componentes, plot) Figura 10.4: En esta figura vemos nuestros dos componentes, y que ambos tienen eigenvalores similares Sin embargo, a medida que agregamos variables que correlacionan fuertemente entre si, todas estas se condensarán en el primer componente que nos dará PCA. Es como si toda la correlación en común se condensara en una sola variable. Veamos cómo sería el ejemplo si tuviéramos una tercer variable, fuertemente correlacionada con \\(x2\\) pero poco correlacionada a \\(x1\\): n.casos &lt;- 240 n.vars &lt;- 2 set.seed(26) simulacion.2 &lt;- rnorm(n.vars, 0, 1/4) x &lt;- matrix(rnorm(n.casos * (n.vars+2)), nrow=n.casos) beta &lt;- rbind(c(1,rep(0, n.vars)), c(0,rep(1, n.vars)), cbind(rep(0,n.vars), diag(simulacion.2))) y &lt;- x%*%beta cor(y) ## [,1] [,2] [,3] ## [1,] 1.00 -0.10 -0.12 ## [2,] -0.10 1.00 0.82 ## [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ] plot(data.frame(y)) Figura 10.5: Ahora hemos incorporado una tercer variable fuertemente correlacionada con una de las variables que teníamos pero no con la otra En este caso, el primer componente del PCA representa casi \\(2/3\\) de la varianza explicada. peso.componentes &lt;- lapply(3:dim(beta)[2], function(k) prcomp(y[, 1:k], scale=TRUE)) grafico &lt;- lapply(peso.componentes, summary) par(mfrow=c(1,1)) grafico &lt;- lapply(peso.componentes, plot) Figura 10.6: En este caso el primer componente condensa aún más varianza, luego de agregar la tercera variable Veamos, entonces, que sucedería si agregáramos tres variables más que correlacionan fuertemente con \\(x2\\) pero poco con \\(x1\\). Aquí notaremos que el primer componente del PCA concentra aún más el porcentaje de varianza explicada. n.casos &lt;- 240 n.vars &lt;- 5 set.seed(26) simulacion.3 &lt;- rnorm(n.vars, 0, 1/4) x &lt;- matrix(rnorm(n.casos * (n.vars+2)), nrow=n.casos) beta &lt;- rbind(c(1,rep(0, n.vars)), c(0,rep(1, n.vars)), cbind(rep(0,n.vars), diag(simulacion.3))) y &lt;- x%*%beta cor(y) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1.00 -0.11 -0.12 -0.10 -0.11 -0.12 ## [ reached getOption(&quot;max.print&quot;) -- omitted 5 rows ] plot(data.frame(y)) Figura 9.1: Ahora agregamos más variables correlacionadas con x2 al análisis Al extraer con PCA los seis componentes, lo que veremos es que el primero de ellos concentra casi \\(5/6\\) positiva entre estas variables. A medida que agregamos una variable muy correlacionada a \\(x2\\), el componente 1 se va volviendo más y más relevante en el PCA. peso.componentes &lt;- lapply(3:dim(beta)[2], function(k) prcomp(y[,1:k], scale=TRUE)) # grafico &lt;- lapply(p, summary) par(mfrow=c(1,1)) grafico &lt;- lapply(peso.componentes, plot) Figura 10.7: En esta figura verás como a medida que agregamos una variable más a la correlación (primero x4, luego x5 y finalmente x6 el primer componente condensa más varianza común entre las variables Así, lo que te queremos mostrar es como PCA aglutina las correlaciones altas entre variables y las condenas en un único componente. Cuanto más parecidas sean las variables (correlaciones altas y en la misma dirección) se necesitarán menos componentes para capturar toda la complejidad de los datos. Volviendo a nuestro ejemplo de LAPOP, una vez que obtenemos los componentes ¿cuál es el próximo paso? Bueno, como queremos reducir las 12 variables de LAPOP en un solo índice, tenemos que decidir cuantos componentes vamos a retener para seguir trabajando. Si retenemos los 12 componentes no descartamos nada de la información original pero tampoco ganamos nada en haber hecho el ejercicio de reducción. Si retenemos apenas el primero, nos quedamos con 29% de la varianza. ¿Cuál es la regla que se usa para maximizar la cantidad de componentes que se retienen? fviz_eig(pca_1, choice = &quot;variance&quot;, addlabels = T) Figura 10.8: Screeplot de todas las dimensiones del PCA con la varianza explicada de cada uno de ellos Lo que se hace es expresar este mismo gráfico con el eigenvalor que PCA da a cada componente, y retener aquellos cuyo valor es mayor a 1. De esta forma, retendríamos 4 componentes, que sumados representan 59.25% de la varianza. fviz_eig(pca_1, choice = &quot;eigenvalue&quot;, addlabels = T) Figura 10.9: Screeplot de todas las dimensiones del PCA con la varianza explicada de cada uno de ellos Nada mal, cuatro variables expresan 60% de la varianza acumulada de las 12 originales. Ahora, el siguiente paso es lograr que estas 4 se transformen en una sola. Podemos ver que variables “alimentaron” cada uno de los cuatro componentes que hemos retenido. fviz_contrib(pca_1, choice = &quot;var&quot;, axes = 1) fviz_contrib(pca_1, choice = &quot;var&quot;, axes = 2) fviz_contrib(pca_1, choice = &quot;var&quot;, axes = 3) fviz_contrib(pca_1, choice = &quot;var&quot;, axes = 4) Figura 9.2: De esta manera podemos saber cuales variables contribuyen a cada componente Por ejemplo, el primer componente es el más diverso pero se alimenta en gran medida de las variables de confianza. Si recuerdas la matriz de correlación que hicimos con GGally, todas estas variables tenían correlaciones altas entre sí. El segundo componente se alimenta de la correlación fuerte entre derman y dervoto. La línea roja punteada nos expresa el valor que asumiría un escenario en el que todas las variables contribuyen en igual medida, es decir \\(1/12\\) (8.33%), y nos sirve apenas como referencia visual. Para poder condensar los componentes elegidos en una sola variable es necesario recordar cuanta varianza acumulada representan del total. Habíamos visto que los cuatro componentes representaban casi 60% de la varianza total: El primer componente un 28.7%, el segundo componente un 12.3%, el tercer componente un 10.3% y el cuarto componente un 7.9%. El siguiente paso consiste en sumar estos cuatro componentes, pero ponderando cada uno por el porcentaje de la varianza que representan. Lo hacemos de la siguiente forma: datos_pca_final &lt;- pca_1$ind$coord%&gt;% as_tibble()%&gt;% mutate(pca_01 = (Dim.1 * 28.7 + Dim.2 * 12.3 + Dim.3 * 10.3 + Dim.4 * 7.9) / 60) lapop &lt;- lapop %&gt;% bind_cols(datos_pca_final %&gt;% select(pca_01)) De esta forma, hemos creado una única variable, que llamamos pca_01. ¡Estamos muy cerca de que esta variable sea nuestro indicador de valoración de la democracia! Sucede que la variable pca_01 está en una escala poco amigable. Idealmente queremos que nuestro indicador oscile de 0 a 1, de 0 a 10, o 0 a 100 de tal forma que sea más fácil su interpretación. Lo haremos para que sea de 0 a 100, si quieres que sea de 0 a 10 o de 0 a 1 tienes que reemplazar el 100 de la formula a continuación por el número que te interese. rescale01 &lt;- function(x) { rng &lt;- range(x, na.rm = TRUE) (x - rng[1]) / (rng[2] - rng[1])} lapop &lt;- lapop %&gt;% mutate(indice_democracia = rescale01(pca_01)*100)%&gt;% select(indice_democracia, everything()) Veamos como se ve la densidad de nuestro nuevo índice: densidad_indice &lt;- ggplot(data = lapop, mapping = aes(x = indice_democracia)) + labs(x=&quot;índice de confianza en la democracia&quot;, y = &quot;densidad&quot;) + geom_density() densidad_indice Figura 8.6: Densidad el índice que acabamos de crear Ahora que tenemos el índice listo, podemos hacer todo tipo de análisis. Por ejemplo, podemos hacer comparación por países. Si tuviéramos variables individuales, podríamos proceder a modelos de regresión con controles por género, ideología, renta, nivel educativo. Para ello, puedes usar lo aprendido en los capítulos 7 y 8. lapop &lt;- lapop %&gt;% group_by(paisnom) %&gt;% mutate(democracia_prom = mean(indice_democracia)) ggplot(lapop, aes(x = indice_democracia)) + geom_density() + labs(title=&quot;Confianza en la democracia en América Latina (N = 7000)&quot;,x=&quot;En azul el promedio de cada país&quot;, y = &quot;densidad&quot;)+ facet_wrap(~ paisnom) + geom_vline(aes(xintercept=(democracia_prom)), color=&quot;blue&quot;, linetype=&quot;dashed&quot;, size=1) Figura 8.7: Media y distribución de confianza en la democracia para cada país de América del Sur Ejercicios antes de continuar al próximo capítulo - Utilizando el índice de confianza en la democracia en América Latina que acabamos de crear, analice con modelos de regresion lineales que variables tienen alto poder explicativo sobre esta variable ¿Son la ideología, renta o edad variables importantes? Utilizando la base de LAPOP (puedes consultar el codebook aquí) elige un set de variables para crear un índice de antiamericanismo siguiendo los lineamientos del capítulo. "],
["casos.html", "Capítulo 11 Selección de casos a partir de regresiones 11.1 ¿Qué casos de estudio debo seleccionar para testear mi hipótesis? 11.2 La relevancia de combinar métodos", " Capítulo 11 Selección de casos a partir de regresiones Por Inés Fynn y Lihuen Nocetto Lecturas de referencia Seawright, J., &amp; Gerring, J. (2008). Case selection techniques in case study research: A menu of qualitative and quantitative options. Political Research Quarterly, 61(2), 294-308. Gerring, J. (2008). Case selection for case‐study analysis: qualitative and quantitative techniques. In The Oxford handbook of political methodology. Oxford University Press. Lieberman, E. S. (2005). Nested analysis as a mixed-method strategy for comparative research. American Political Science Review, 99(3), 435-452. Seawright, J. (2016). Multi-method social science: Combining qualitative and quantitative tools. Cambridge University Press. Este capítulo te dará herramientas para que a partir de lo que vimos en el capítulo de modelos lineales, puedas usar las regresiones para seleccionar casos de estudio mediante un diseño de métodos mixtos. Cuando estamos trabajando con datos observacionales (y no experimentales), las regresiones MCO no pueden, por sí mismas, responder preguntas de inferencia causal. Es decir, si bien nos permiten dilucidar si es que existe algún tipo de relación entre nuestra variable independiente y dependiente, la investigación quedará incompleta si no logramos evidenciar, con otro tipo de métodos, cómo es que se conectan causalmente estas variables. Un libro excelente para consultar y aprender más sobre este tema es Multi-method social science: Combining qualitative and quantitative tools de Jason Seawright. La selección de métodos para hacer una investigación está guiada por la pregunta que queremos responder. Por ejemplo, si el interés está en entender cuáles son los determinantes de desigualdad en los países de América Latina y el Caribe, vamos a emplear un análisis estadístico de n-grande que nos permita analizar la mayor cantidad de países posible. De este modo, hemos encontrado en las secciones anteriores que, en promedio, el gasto en educación tiene un efecto positivo sobre los niveles de desigualdad. Sin embargo, el hallazgo de que mayor gasto en educación genere mayores niveles de desigualdad resulta un tanto inquietante y contraintuitivo. Además, un hallazgo como este podría tener implicancias importantes para la elaboración de políticas públicas y consecuencias para la vida real de las personas. Por tanto, para avanzar en nuestra investigación sería aconsejable intentar responder, por ejemplo, ¿Por qué la educación afecta positivamente los niveles de desigualdad? Es decir, cuál es el mecanismo causal que explica que en los países de América Latina y el Caribe mayor gasto en educación genere mayores niveles de desigualdad. Para responder preguntas de este tipo muchas veces recurrimos a los métodos cualitativos (como por ejemplo, el estudio de casos en profundidad y análisis de process tracing) que nos permitan comprender cuáles son los procesos que explican por qué y cómo se da una relación causal. De este modo, lo que pretendemos hacer es integrar (Seawright, 2016) dos métodos de investigación, donde un método plantea la pregunta de investigación (el análisis estadístico), mientras que otro la pretende responder (estudio de caso). Otra alternativa para fortalecer nuestra investigación podría ser la triangulación: abordar la misma pregunta de investigación pero a partir de distintos métodos que, en su conjunto, nos permitirán una explicación más compleja y completa del fenómeno que pretendemos explicar. Más allá del camino que se tome (integración o triangulación), el objetivo es combinar métodos distintos para ofrecer una explicación más compleja al fenómeno que nos interesa estudiar. A la combinación de métodos la conocemos como “métodos mixtos” donde justamente el objetivo es abordar un mismo fenómeno a partir de distinas metodologías que permitan capturar distintos ángulos o dimensiones. Si bien existen infinitas formas de combinar métodos, algunos métodos son más compatibles entre sí que otros y, de hecho, algunas combianciones pueden llevarnos a mayor confusión que claridad (Lieberman, 2005). En esta sección veremos una combinación de métodos que Lieberman (2005) ha denominado nested analysis y no es otra cosa que la combinación de análisis estadístico de una muestra grande con el estudio en profundidad de uno o más casos contenido en dicha muestra. En definitiva, lo que haremos será seleccionar casos (en este caso países) a partir de la estimación de nuestro modelo. Luego de haber estimado el modelo, el primer paso para seleccionar casos de estudio es calcular los residuos y los valores predichos por el modelo para cada una de nuestras observaciones. Esto porque, para seleccionar nuestros casos de estudio, vamos a estar comparando lo que nuestro modelo predijo contra los valores reales (de la muestra) de cada uno de esos casos: la diferencia entre estos dos valores (los predichos y los reales) son los residuos. Para extraer del modelo los residuos y los valores predichos utilizamos el comando augment del paquete broom. Lo que hace este comando es crear una nueva base de datos que le agrega variables sobre el modelo a la base original (para cada caso): valores predichos, errores estándar, el residuo y el residuo estandarizado, entre otros estadísticos. Usaremos para el ejemplo el modelo 2 que estimamos en el Capítulo 6 con los datos de Huber et al (2006). Residuos y valores predichos: library(tidyverse) ## ── Attaching packages ──────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0 ✔ purrr 0.3.1 ## ✔ tibble 2.0.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ─────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(paqueteadp) data(&quot;bienestar_la&quot;) bienestar_la_sinna &lt;- bienestar_la %&gt;% drop_na(gini_slc, cseduc, fdiingdp, cshlth, csssw, pop014wdi, s_dualism, ethnicdicot, rgdpch, demrss, legbal, repressauthor) modelo_2 &lt;- lm(gini_slc ~ 1 + cseduc + fdiingdp + cshlth + csssw + pop014wdi + s_dualism + ethnicdicot + rgdpch + demrss + legbal + repressauthor, data = bienestar_la_sinna) modelo_aug &lt;- broom::augment(modelo_2, data = bienestar_la_sinna) modelo_aug ## # A tibble: 167 x 21 ## country id year pop014wdi gini_slc s_dualism rgdpch fdiingdp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Argent… ARG 1982 30.8 40.2 9.50 7711. 0.269 ## 2 Argent… ARG 1983 30.9 40.4 8.36 7907. 0.178 ## 3 Argent… ARG 1990 30.7 43.1 7.72 6823. 1.30 ## 4 Argent… ARG 1991 30.4 44 6.42 7392. 1.29 ## 5 Argent… ARG 1992 30.1 43 5.59 7986. 1.94 ## 6 Argent… ARG 1993 29.8 42 4.99 8411. 1.18 ## 7 Argent… ARG 1994 29.4 43 5.01 8764. 1.41 ## 8 Argent… ARG 1995 29.1 46 5.19 8578. 2.17 ## 9 Argent… ARG 1996 28.9 46 5.28 8905. 2.55 ## 10 Argent… ARG 1997 28.6 46 4.87 9425. 3.13 ## # … with 157 more rows, and 13 more variables: ethnicdicot &lt;dbl&gt;, ## # demrss &lt;dbl&gt;, cseduc &lt;dbl&gt;, cshlth &lt;dbl&gt;, csssw &lt;dbl&gt;, legbal &lt;dbl&gt;, ## # repressauthor &lt;dbl&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;, .std.resid &lt;dbl&gt;, ## # .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt; 11.1 ¿Qué casos de estudio debo seleccionar para testear mi hipótesis? Los casos seleccionados para un estudio en profundidad se eligen de una población, y las razones de esta selección dependen de la forma en que están situados dentro de esa población. En este sentido, de acuerdo a Gerring (2006), un estudio de caso no puede existir aislado del análisis de un n relativamente grande de casos cruzados. El mejor caso de estudio dependerá de cuál es el objetivo para el cual se está seleccionando el caso. De este modo, la selección de casos debe ser intencional y no aleatoria (Gerring, 2006). A continuación se detallan distintos objetivos para los que se seleccionan casos y su implementación en R a partir del modelo estadístico sobre los determinantes de la desigualdad en América Latina y el Caribe. 11.1.1 Casos Típicos Uno de los objetivos de la selección de casos radica en ilustrar la relación encontrada y profundizar sobre los mecanismos que vinculan la variable independiente con la dependiente. Si este es nuestro objetivo, entonces querremos seleccionar casos que sean ejemplos típicos de la relación que encontramos con el análisis estadístico. Por tanto, lo que buscamos es encontrar aquellos casos con residual más pequeño. Es decir, los casos que nuestro modelo predijo mejor: on the line cases (casos que están sobre la línea de regresión). Para esto, graficaremos a partir de la base de datos creada con el comando augment los valores predichos sobre los residuos (los cuales transformaremos a valor absoluto porque, por construcción, siempre hay residuos negativos). Además, para identificar los casos, le pediremos al ggplotque agregue las etiquetas de los dos (top_n(-2, .resid_abs)) países (mapping=aes(label=country)) con menores residuos. La línea horizontal (geom_hline(aes(yintercept = 0))) la incorporamos al gráfico para visualizar donde el residuo es nulo (allí se encontrarán los casos que el modelo predijo perfectamente: los más típicos). ggplot2::qplot(data = modelo_aug, x = .fitted, y = .resid, geom = &quot;point&quot;, main = &quot;Casos típicos&quot;) + geom_hline(aes(yintercept = 0)) + geom_text(data = . %&gt;% mutate(.resid_abs = abs(.resid)) %&gt;% top_n(-4, .resid_abs), mapping = aes(label = country)) De acuerdo a lo graficado, Brasil, Honduras y Uruguay son dos casos típicos del modelo estimado sobre los determinantes de la desigualdad en América Latina y el Caribe. Es decir, son casos que a partir de las variables del Modelo 2 podríamos explicar muy bien sus niveles de desigualdad, sean estos altos (Brasil, Honduras) o bajos (Uruguay). 11.1.2 Casos desviados Los casos desviados son aquellos que, dado nuestro modelo, presentan un comportamiento no esperado; son desviados pues no pueden ser bien explicados por el modelo general. En definitiva son “anomalías teóricas” (Gerring, 2006: 106). Por lo general seleccionamos este tipo de casos para explorar nuevas hipótesis y que, eventualmente pueden arrojar luz sobre variables omitidas del modelo estadístico. La selección de casos desviados funciona de manera opuesta a la selección de casos típicos: en lugar de seleccionar aquellos con menor residual, se seleccionan los casos cuyo valor predicho difiere más del valor real (mayor residual). ggplot2::qplot(data = modelo_aug, x = .fitted, y = .resid, geom = &quot;point&quot;, main = &quot;Casos desviados&quot;) + geom_hline(aes(yintercept = 0)) + geom_text(data = . %&gt;% mutate(.resid_abs = abs(.resid)) %&gt;% top_n(4, .resid_abs), mapping = aes(label = country)) Jamaica (varios años) aparece como un país muy mal explicado por nuestro modelo. Es un país que presenta valores relativamente bajos de desigualdad, y las variables del modelo no dan cuenta de estos valores. Hay un año particularmente interesante, que es el de 1993, en el que el score de gini es de 35.7, un valor que la ubica entre las más equitativas de la muestra. En un barrio de países de los más desiguales del mundo estaríamos necesitando incororar alguna variable a nuestro modelo para lograr explicar el caso jamaiquino. 11.1.3 Casos Influyentes Los casos influyentes son aquellos casos que muestran valores extremos pero que tienen mucho peso sobre la relación encontrada por el modelo. Es decir, son casos que influyen en la pendiente de regresión que observamos (recuerda que la pendiente está dada por el coeficiente de regresión \\(\\beta_i\\)). Se trata de casos que, al igual que los casos desviados, también son inusuales aunque de un modo distinto. Cuando selecciono un caso influyente es para confirmar el modelo, mientras que la selección de casos desviados se utiliza para explorar hipótesis alternativas (Gerring, 2006). Para identificar los casos infulyentes podemos tomar dos caminos: Por un lado, se pueden utilizar los dfbetas que son estadísticos que indican cuánto el coeficiente de regresión \\(\\beta_i\\) cambia en unidades de desviación estándar si la i-ésima observación fuera eliminada. Por tanto, tendremos un dfbeta para cada observación que indica cuánto cambiaría el \\(\\beta_i\\) de la variable cseduc (gasto en educación), si ese caso no estuviera presente. Por tanto, cuánto más varíe la pendiente (\\(\\beta_i\\)) con la ausencia del caso, más influyente será dicho caso. De este modo, lo que queremos es seleccionar los casos que generan mayores cambios en desviación estándar del \\(\\beta_i\\) si fuesen eliminados. Así, los casos influyentes pueden ser utilizados para confirmar la teoría, aunque si su eliminación anula la relación encontrada (si al quitar el caso \\(\\beta_i\\) deja de ser significativo), es también útil para explorar nuevas hipótesis o identificar variables que hayan sido omitidas en el modelo. modelo_aug %&gt;% mutate(dfb_cseduc = as.tibble(dfbetas(modelo_2))$cseduc) %&gt;% arrange(-dfb_cseduc) %&gt;% slice(1:3) %&gt;% dplyr::select(country, dfb_cseduc) ## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics). ## This warning is displayed once per session. ## # A tibble: 3 x 2 ## country dfb_cseduc ## &lt;chr&gt; &lt;dbl&gt; ## 1 Barbados 0.483 ## 2 Jamaica 0.298 ## 3 Venezuela 0.241 Utilizando la distancia de Cook que se basa en una lógica muy similar a los dfbetas. La distancia de Cook considera los valores que asume cada observación en la variable independiente y dependiente para calcular cuánto varían los coeficientes cuando en ausencia de cada caso. En definitiva, esta distancia lo que nos indica es qué tanto influye cada caso en la regresión en su conjunto: a mayor distancia de Cook, mayor es la contribuión del caso a las inferencias del modelo. Es decir, los casos con gran distancia de Cook son centrales para mantener las conclusiones analíticas (esto sobre todo con muestras relativamente pequeñas, con muestras muy grandes es menos probable que existan casos con tal poder de influencia). Es por esto que seleccionar estos casos para un estudio en profundidad puede ser relevante: si en el estudio cualitativo de un caso influyente no podemos confirmar nuestra teoría, es poco probable que lo podamos confirmar en otros casos. ggplot2::qplot(data = modelo_aug, x = .fitted, y = .cooksd, geom = &quot;point&quot;, main = &quot;Casos influyentes&quot;) + geom_text(data = . %&gt;% top_n(3, .cooksd), mapping = aes(label = country)) Nuevamente Jamaica se destaca como un país que debemos observar. 11.1.4 Casos extremos La selección de casos extremos supone identificar observaciones que se ubiquen lejos de la media de la distribución de la variable independiente o dependiente (que tenga valores extremos). El interés está en la “rareza” del valor que asume ese caso en la variable. Es importante destacar que un caso extremos puede coincidir tanto con un caso típico como con uno desviado (Gerring, 2006). El estudio en profundidad de casos extremos es más bien exploratorio: es una forma de evaluar y buscar causas posibles de \\(y\\) o efectos posibles de \\(x\\). Esta técnica se recomienda para cuando no hay demasiada teoría elaborada y, por tanto, la investigación está concentrada en la construcción teórica. Un trabajo clásico de selección de casos extremos en la variable dependiente es el de Theda Skocpol (1979) sobre revoluciones sociales, donde la teoría se desarrolla en base a tres casos que presentan el valor más extremo de revolución (de hecho son los únicos casos que presentan dicho valor de acuerdo a Skocpol). 11.1.4.1 Casos extremos en la variable independiente: \\(x\\) Veamos como se comporta nuestra variable independiente: ggplot(bienestar_la_sinna, aes(x = cseduc)) + geom_histogram(binwidth = 1,color=&quot;white&quot;, fill=&quot;black&quot;) + labs(title = paste( &quot;Distribución de la Variable Independiente: Gasto en Educación&quot;),subtitle = paste(&quot;% de PBI destinado a la Educación&quot;), caption = paste (&quot;Fuente: Huber et al (2012))&quot; ), x = &quot;Gasto en Educación&quot;, y = &quot;Frecuencia&quot; ) Para seleccionar casos extremos en la variable independiente, a partir del modelo estadístico estimado, simplemente calculamos las diferencias - en valor absoluto - entre el valor de cada caso y la media muestral del gasto en educación. Luego, se seleccionan los tres casos que muestran mayor diferencia entre la media muestral y su valor de la variable independiente. Primero, calculamos la media para tener la referencia. mean(modelo_aug$cseduc, na.rm = T) ## [1] 4 modelo_aug %&gt;% mutate(dif_cseduc = abs(cseduc - mean(cseduc, na.rm = T))) %&gt;% top_n(3, dif_cseduc) %&gt;% arrange(-dif_cseduc) %&gt;% dplyr::select(country, year, cseduc, dif_cseduc) ## # A tibble: 3 x 4 ## country year cseduc dif_cseduc ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Barbados 1981 0.8 3.16 ## 2 Honduras 2001 6.8 2.84 ## 3 Uruguay 1984 1.4 2.56 Graficamos los resultados para una mejor visualización: modelo_aug$dif_cseduc = abs(modelo_aug$cseduc - mean(modelo_aug$cseduc, na.rm = T)) ggplot2::qplot(data = modelo_aug, x = .fitted, y = dif_cseduc, geom = &quot;point&quot;, main = &quot;Casos extremos en Gasto en Educación&quot;) + geom_text(data = . %&gt;% top_n(3, dif_cseduc), mapping = aes(label = country)) Barbados se destaca por ser extremo pues está muy por debajo de la media amostral. Honduras, por el contrario, esta muy por encima. Sería interesante comparar ambos. Al ver que el tercer país es Uruguay, que tiene valores bajos comparados a la media, surge una duda que seguramente nos hará mejorar el modelo: ¿acaso no deberíamos controlar por el tamaño de la economía, medido por su PBI? Esta duda podría llevaros a un nuevo modelo, donde las significancias estadísticas podrían cambiar. 11.1.4.2 Casos extremos en la variable dependiente: \\(y\\) La selección de casos extremos en la variable dependiente se realiza del mismo modo que con los casos extremos en \\(x\\). Solo que ahora calculamos las diferencias - en valor absoluto - entre el valor de cada caso en la variable dependiente y la media muestral (Índice de Gini en el ejemplo). Luego, se seleccionan los tres casos que muestran mayor diferencia entre la media muestral y su valor de la variable dependiente. Primero, calculamos la media para tener la referencia. mean(modelo_aug$gini_slc, na.rm = T) ## [1] 50 modelo_aug %&gt;% mutate(dif_gini = abs(gini_slc - mean(gini_slc, na.rm = T))) %&gt;% top_n(2, dif_gini) %&gt;% arrange(-dif_gini) %&gt;% dplyr::select(country, gini_slc, dif_gini) ## # A tibble: 2 x 3 ## country gini_slc dif_gini ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Barbados 28.9 21.4 ## 2 Jamaica 66 15.7 Podemos también graficarlo para una mejor visualización: modelo_aug$dif_gini = abs(modelo_aug$gini_slc - mean(modelo_aug$gini_slc, na.rm = T)) ggplot2::qplot(data = modelo_aug, x = .fitted, y = dif_gini, geom = &quot;point&quot;, main = &quot;Casos extremos en Índice de Gini&quot;) + geom_text(data = . %&gt;% top_n(2, dif_gini), mapping = aes(label = country)) Nuevamente Barbados y Jamaica aparecen como casos atípicos en la variable dependiente. Ambas tienen en común que fueron colonias caribeñas del Imperio Británico, quizás podríamos incluir ese control para todos los países con este legado y ver como se ajusta el nuevo modelo. De haber grandes cambios en los valores predichos, podríamos explorar mediante evidencia cualitativa el rol que tuvieron las instituciones coloniales del Imperio Británico sobre la desigualdad de estas colonias. 11.1.5 Casos más similares La selección de casos similares supone identificar dos casos que son similares en todas las variables salvo en las variables de interés. Cuando estamos en una etapa exploratoria de nuestra investigación y no tenemos una teoría (no tenemos identificada una variable independiente en particular), se buscan un par de casos que sean iguales en sus variables independientes pero que difieran en la variable de resultado (dependiente). De este modo, el objetivo será identificar uno o más factores que difieran entre los casos y que puedan explicar la divergencia en el resultado. Esta estrategia es la del método de semejanza (o concordancia) de Stuart Mill. Sin embargo, cuando ya tenemos una teoría sobre cómo se vincula una determinada variable independiente con la variable dependiente, la seleción de casos similares se enfoca en identificar dos casos que sean similares en todos los controles pero diferentes en la variable independiente de interés. Aquí, el interés estará en confirmar el argumento y profundizar en los mecanismos causales que conectan la variable independiente con la dependiente. Para seleccionar casos similares, se recomienda utilizar alguna técnica de matching (Gerring, 2006: 134). En simples palabras, esta técnica supone justamente juntar pares (en su versión más básica) de observaciones que sean lo más similares posible en todas las variables de control pero que difieran en la variable independiente de interés. Para simplificar el análisis, la variable independiente suele ser dicotómica (0 y 1) emulando una situación experimental donde hay una tratamiento (1) y un placebo o control (0). De este modo, el objetivo es “matchear” (la menor distancia posible entre los valores de las variables de control) pares donde una observación pertenece al grupo del tratamiento y la otra al grupo de control. Cómo encontrar pares que coincidan en todas las variables de control es, por lo general, bastante exigente, se suele utilizar un procedimiento denominado como matching en base al puntaje de propensión (propensity score). Este procedimiento supone encontrar pares de observaciones que tengan probabilidades estimadas similares de estar en el grupo de tratamiento (tener valor 1 en la variable independiente de interés), condicionadas en las variables de control. Para implementar esta selección de casos en nuestra investigación vamos a crear una variable dummy de tratamiento (para la variable de gasto en educación), donde 0 es gasto menor a la media muestral de gasto y 1 gasto mayor a la media. media &lt;- mean(bienestar_la_sinna$cseduc) bienestar_la_sinna$tratamiento &lt;- ifelse(bienestar_la_sinna$cseduc &gt; media, 1, 0) Ahora que tenemos la variable de tratamiento, podemos calcular los puntajes de propensión. Es decir, la probabilidad de estar en el grupo de tratamiento (gasto en educación mayor a la media muestral), condicionado en las variables de control del modelo. Este cálculo se hace a partir de un modelo logit (ver Capítulo ##), ya que nuestra variable dependiente es una variable dicotómica. propensityscore &lt;- glm(tratamiento ~ s_dualism + fdiingdp + rgdpch + pop014wdi + ethnicdicot + demrss + demrss * csssw + cshlth + csssw + legbal + repressauthor, data = bienestar_la_sinna, family = binomial(link = logit), na.action = na.exclude) Al igual que como hicimos con el modelo general de los determinantes de la desigualdad, crearemos una base de datos con el comando augment para guardar algunos estadísticos que nos serán útiles para seleccionar los casos. propensity_scores&lt;- broom::augment(propensityscore, data = bienestar_la_sinna, type.predict = &quot;response&quot;) %&gt;% dplyr::select(propensity_scores = .fitted, country, tratamiento, year, gini_slc) Ahora, identificaremos los casos con menores puntajes de propensión tanto para el grupo de tratamiento (alto gasto en educación) como para el grupo de control (bajo gasto en educación), para decidir la selección de casos. Cabe destacar que esto también puede hacerse para altos puntajes de propensión, o para cualquier puntaje de propensión, lo importante es que tengan puntajes similares o “cercanos” (igual probabilidad de recibir el tratamiento). Casos con bajo puntaje de propensión, en el grupo de países con gasto en educación mayor a la media muestral: propensity_scores %&gt;% filter(tratamiento == 1) %&gt;% arrange(propensity_scores) %&gt;% dplyr::select(country, year, propensity_scores) %&gt;% slice(1:2) ## # A tibble: 2 x 3 ## country year propensity_scores ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Brazil 1984 0.0815 ## 2 Mexico 2000 0.159 Por otra parte, veamos cuáles son los casos con bajo puntaje de propensión pero entre aquellos países con gasto en educación menor a la media muestral: propensity_scores %&gt;% filter(tratamiento == 0) %&gt;% arrange(propensity_scores) %&gt;% dplyr::select(country, year, propensity_scores) %&gt;% slice(1:2) ## # A tibble: 2 x 3 ## country year propensity_scores ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Paraguay 1994 0.00309 ## 2 Argentina 1982 0.00673 De acuerdo a los resultados obtenidos, tanto Brasil como México podrían ser seleccionados para ser comparados con Paraguay o Argentina para realizar estudios de casos más similares en profundidad. Por proximidad geográfica, podríamos elegir Brazil y Argentina, e intentar así dilucidad de qué manera el gasto sobre educación ha repercutido sobre la equidad de renta en ambos países. 11.1.6 Casos más diferentes El procedimiento de selección de casos más diferentes supone una lógica opuesta a la de casos más similares. Aquí se buscan casos que sean en realidad distintos en las variables de control, pero que sean similares en el valor asumido por la variable independiente de interés y la variable dependiente. En definitiva, lo que buscamos son distintos puntajes de propensión pero coincidencia en la variable independiente y dependiente. Cabe destacar que este tipo de selección de casos es útil cuando se asume “causalidad única” (Gerring, 2006: 143). Es decir, cuando la variable dependiente es causada por una única variable (o cuando nos interesa explicar el efecto de un sólo factor). Si el interés es indagar sobre la combinación de distintos factores causales, este procedimiento de selección de casos no resulta el más indicado. Para seleccionar casos “más diferentes” también utilizaremos los puntajes de propensión, pero ahora nos interesa seleccionar en base a iguales resultados en la variable dependiente, así como en la variable independiente y con puntajes de propensión muy distintos. Veamos, entonces, cuáles son los casos donde hay coincidencia en la variable independiente de interés y en la de resultado pero con diferente puntaje de propensión.Primero creamos una variable dummy para el gini mayor y menor a la media. Luego, identificamos los casos tratados con menor puntaje de propensión (baja probabilidad de tener gasto mayor a la media) para valores de gini mayores a la media muestral y valores de gasto en educación también mayor a la media muestral: propensity_scores$gini &lt;- ifelse(propensity_scores$gini_slc &gt; (mean(propensity_scores$gini_slc, na.rm = TRUE)), 1, 0) propensity_scores %&gt;% filter(gini == 1 &amp; tratamiento==0) %&gt;% arrange(propensity_scores) %&gt;% dplyr::select(country, year, propensity_scores) %&gt;% slice(1:2) ## # A tibble: 2 x 3 ## country year propensity_scores ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Paraguay 1999 0.00953 ## 2 Paraguay 1997 0.0221 A continuación, hacemos lo mismo pero para los puntajes de propensión más altos (es decir, donde la probabilidad de recibir el tratamiento - tener gasto en educación mayor a la media - es muy elevada). Es decir, identificamos los casos con mayor puntaje de propensión para valores de gini mayores a la media muestral y gasto en educación mayor a la media muestral: propensity_scores %&gt;% filter(gini == 1 &amp; tratamiento==0) %&gt;% arrange(-propensity_scores) %&gt;% dplyr::select(country, year, propensity_scores) %&gt;% slice(1:2) ## # A tibble: 2 x 3 ## country year propensity_scores ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Honduras 1994 0.983 ## 2 Honduras 1996 0.969 Nuestros resultados indican que, Paraguay podría ser seleccionados para ser comparado con Honduras para realizar estudios de casos “más diferentes” en profundidad. Ambos tienen bajos niveles de gasto en educación como porcentaje del PBI, y ambos son altamente desiguales. 11.2 La relevancia de combinar métodos Para finalizar, consideramos importante insistir sobre la relevancia de combinar métodos al momento de responder una pregunta de investigación. Si bien cuáles serán los métodos apropiados dependerán de cuál es la pregunta, una respuesta a un fenómeno requiere tanto de la identificación de una relación entre dos (o más) variables, y una explicación que detalle cómo estas dos variables se vinculan y por qué se genera el efecto identificado. Para abordar estas dos dimensiones resulta necesario combinar distintas estrategias empíricas, para explotar las respectivas virtudes y complementar sus debilidades. En el caso concreto que nos ocupa, la estimación por MCO permite identificar relaciones promedio entre dos variables en un gran número de casos y para varios años, algo que la investigación cualitativa no puede realizar. Sin embargo, MCO no puede responder sobre el por qué o el cómo de estas relaciones y, para eso, es necesaria una investigación cualitativa que profundice en los procesos y actores que “producen” estas relaciones. Claro que el proceso también puede ser el inverso: identificar primero una relación entre dos variables a partir de estudios en profundidad de casos y, luego, testear la relación encontrada en otros casos en un estudio cuantitativo de n-grande para evaluar la generalización del hallazgo. En todo caso, la combinación de métodos - ya sea por triangulación o por integración (Seawright, 2016) -, es aconsejable para ofrecer explicaciones más complejas y acabadas de los fenómenos que nos interesa estudiar. "],
["web-mining.html", "Capítulo 12 Mínería de datos", " Capítulo 12 Mínería de datos Por Gonzalo Barría PENDIENTE. "],
["redes.html", "Capítulo 13 Análisis de redes 13.1 Introducción 13.2 Conceptos iniciales 13.3 Bases de datos de redes 13.4 Disposición gráfica 13.5 Análisis básico de redes", " Capítulo 13 Análisis de redes Por Andrés Cruz Lecturas de referencia Newman, M. (2018). Networks: An Introduction (2a ed). New York, NY: Oxford University Press. Scott, J. (2013). Social Network Analysis (3a ed). London: Sage Publications. 13.1 Introducción No es exagerado decir que en política todo está conectado con todo. Por ejemplo, fíjate en el caso de las y los legisladores. Las conexiones son evidentes: partidos, coaliciones, comisiones, familias, colegios… Para comprender mejor estas enmarañadas relaciones, una de las herramientas que está en el cinturón de herramientas de los politólogos es la del análisis de redes. Las redes son capaces de reconocer que todo está conectado con todo, y dar pistas sobre la lógica detrás de esas conexiones. No solo permiten visualizar de forma atractiva dichas conexiones, sino que también calcular distintas mediciones interesantes sobre los actores en cuestión y los vínculos que los unen. En este capítulo aprenderás las bases del análisis de redes con R, principalmente utilizando los paquetes tidygraph y ggraph, que se aproximan al análisis de redes desde los preceptos del tidyverse. 13.2 Conceptos iniciales 13.2.1 Nodos y enlaces Dos conceptos son básicos para comenzar a expresar una situación como una red. En primer lugar, los nodos (a veces llamados actores) son las unidades de análisis principales: queremos entender cómo se relacionan entre sí. En el ejemplo anterior, las y los legisladores serían los nodos de la red. Segundo, los enlaces (a veces llamadas conexiones o vínculos) muestran cómo los nodos están conectados unos con otros. Entre las y los legisladores, una forma de enlace posible es “haber propuesto una ley juntos”, también llamado co-presentación (co-sponsorship). Una red no es más que una serie de nodos conectados a través de enlaces, como se puede apreciar en la Figura 13.1. En términos del ejemplo anterior, podemos imaginar que dicha red grafica conecta a las legisladoras A, B, C, D (los nodos) según su co-presentación de proyectos w, x, y, z (los enlaces). Así, la red muestra que la legisladora B ha presentado al menos un proyecto de ley con todas las demás legisladores de la red, mientras que la legisladora D solo lo ha hecho con B. Por su lado, las legisladoras A y C tienen dos enlaces de co-presentación cada una, formando una tríada A-B-C. Figura 13.1: Esquema de red de co-sponsorship entre cuatro legisladoras 13.2.2 Matriz de adyacencia Aparte de una descripción visual, como la de la Figura 13.1, es también posible representar las redes como matrices de adyacencia. La Tabla 13.1 muestra la misma red que hemos visualizado anteriormente, esta vez en formato matriz. Los 1 indican que existe un enlace entre ambos nodos (de co-presentación, en nuestro ejemplo), mientras que los 0 indican lo contrario. Nota cómo la diagonal de la matriz está llenada solo por unos: esta es una convención útil para distintos cálculos matemáticos. Adicionalmente, este tipo de matriz para una red básica es simétrica: si sabemos que el nodo A está enlazado con el nodo B, automáticamente sabemos que el nodo B está enlazado con el A. Tabla 13.1: Matriz de adyacencia A B C D A 1 1 1 0 B 1 1 1 1 C 1 1 1 0 D 0 1 0 1 13.2.3 Pesos y dirección Una red tan básica como la vista hasta ahora puede complejizarse bastante más, de acuerdo a la naturaleza de los datos. Dos complejizaciones típicas refieren a los enlaces: añadir pesos y dirección. Para comenzar con los pesos, en nuestro ejemplo los legisladores están conectados si es que alguna vez han presentado un proyecto de ley en conjunto. Sin embargo, a menudo es de interés no solo conocer la existencia de una conexión entre dos actores, sino que también la fuerza de esta: no es lo mismo que dos legisladores hayan aceptado con reticencia presentar un proyecto juntos en una ocasión, a que hayan presentado múltiples proyectos de ley en conjunto. Volvamos a la matriz de adyacencia, esta vez incluyendo pesos. En este nuevo ejemplo, de la Tabla 13.2, las legisladoras A y B han presentado 9 proyectos de ley juntas. Nota cómo, por convención, la diagonal de la matriz sigue llena de unos. Tabla 13.2: Matriz de adyacencia, red con pesos. A B C D A 1 9 1 0 B 9 1 1 6 C 1 1 1 0 D 0 6 0 1 La segunda forma de añadir información adicional a los enlaces es registrando su dirección. En algunas legislaturas los proyectos de ley tienen un autor o autora principal (sponsor), a quien el resto de los legisladores se suman (co-sponsors). En estos casos, la red de co-sponsorship naturalmente tendrá dirección: un legislador “auspiciará” a otro firmando en su proyecto de ley, sin que esta relación sea necesariamente recíproca. Es posible incluir esta información en la red, tal como realiza Fowler (2006) para el Congreso de Estados Unidos. Una matriz de adyacencia con direcciones (y pesos) podría verse como la de la Tabla 13.3. Nota que ahora la matriz no es simétrica, pues existe más información sobre la relación de co-sponsorship entre las legisladoras A y B: mientras que la legisladora A auspició siete proyectos de la legisladora B, esta solo reciprocó en dos proyectos de ley8. Tabla 13.3: Matriz de adyacencia, red con pesos y direcciones. A B C D A 1 7 1 0 B 2 1 1 6 C 1 1 1 0 D 0 6 0 1 13.3 Bases de datos de redes Siguiendo con el espíritu del ejemplo anterior, en este capítulo trabajaremos con datos de co-sponsorship de leyes en el Senado argentino. Utilizaremos los datos de Alemán et al. (2009), en específico para el año 1983, justo después del regreso de la democracia. Comencemos cargando la base de datos con la ayuda de nuestro paquete paqueteadp: library(tidyverse) library(paqueteadp) data(leyes_argentina) Podemos revisar que nuestra base se cargó adecuadamente con ls(): ls() ## [1] &quot;leyes_argentina&quot; Para pasar esta base de datos a un formato de redes utilizaremos el paquete tidygraph. Nuestro objetivo será el siguiente: crear dos bases de datos, una con información sobre los nodos de la red (legisladores/as) y otra con información sobre los vínculos entre esos nodos (firmas en común). Nota que el procedimiento exacto para crear estas bases dependerá de la estructura original de tus datos. Comencemos explorando nuestra base inicial: leyes_argentina ## # A tibble: 665 x 7 ## id_proyecto n_firmantes id_leg nombre_leg provincia_leg partido_leg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0001-S-84 2 25 JOSE H MA… JUJUY JUSTICIALI… ## 2 0001-S-84 2 4 ALFREDO L… JUJUY JUSTICIALI… ## 3 0002-S-84 2 34 OLIJELA D… TUCUMAN JUSTICIALI… ## 4 0002-S-84 2 3 RAMON A A… TUCUMAN JUSTICIALI… ## 5 0003-S-84 2 3 RAMON A A… TUCUMAN JUSTICIALI… ## 6 0003-S-84 2 34 OLIJELA D… TUCUMAN JUSTICIALI… ## 7 0004-S-84 1 37 LUIS SALIM S DEL ESTERO JUSTICIALI… ## 8 0006-S-84 2 38 LIBARDO N… LA RIOJA JUSTICIALI… ## 9 0006-S-84 2 29 EDUARDO M… LA RIOJA JUSTICIALI… ## 10 0007-S-84 2 38 LIBARDO N… LA RIOJA JUSTICIALI… ## # … with 655 more rows, and 1 more variable: bloque_leg &lt;chr&gt; Como puedes notar, esta es una base de formato “long”, donde cada firma es una fila. De esta forma, podemos inmediatamente saber que hubo 665 firmas en proyectos durante el año 1983. ¿En cuántos proyectos se distribuyeron esas firmas? Hagamos una tabla de frecuencia para la variable id_proyecto, utilizando la función count(): leyes_argentina %&gt;% count(id_proyecto) ## # A tibble: 248 x 2 ## id_proyecto n ## &lt;chr&gt; &lt;int&gt; ## 1 0001-S-84 2 ## 2 0002-S-84 2 ## 3 0003-S-84 2 ## 4 0004-S-84 1 ## 5 0006-S-84 2 ## 6 0007-S-84 2 ## 7 0009-S-84 6 ## 8 0010-S-84 3 ## 9 0011-S-84 5 ## 10 0012-S-84 9 ## # … with 238 more rows Sabemos entonces que hubo 248 proyectos de ley. ¿Cuántos legisladores/as firmaron en proyectos? leyes_argentina %&gt;% count(id_leg) ## # A tibble: 46 x 2 ## id_leg n ## &lt;dbl&gt; &lt;int&gt; ## 1 1 13 ## 2 2 21 ## 3 3 24 ## 4 4 18 ## 5 5 23 ## 6 6 19 ## 7 7 26 ## 8 8 21 ## 9 9 15 ## 10 10 10 ## # … with 36 more rows Nota que la variable id_leg asigna un número correlativo, entre 1 y 46, para cada senador/a. Entonces, hasta ahora sabemos que 46 senadores/as firmaron en proyectos, sumando 665 firmas en 248 proyectos de ley distintos. Pero volvamos a nuestro objetivo inicial para comenzar con el análisis de redes: necesitamos crear una base de datos de nodos y otra de enlaces. Nuestra base de datos de nodos tendrá, entonces, información de los legisladores/as, totalizando 46 observaciones. Otras variables en la base que nos entregan información sobre los legisladores/as son nombre_leg, provincia_leg, partido_leg y bloque_leg, por lo que estas deberían estar presentes en nuestra base de datos de nodos. Construyámosla: df_nodos &lt;- leyes_argentina %&gt;% group_by(id_leg) %&gt;% summarize(nombre_leg = unique(nombre_leg), provincia_leg = unique(provincia_leg), partido_leg = unique(partido_leg), bloque_leg = unique(bloque_leg)) df_nodos ## # A tibble: 46 x 5 ## id_leg nombre_leg provincia_leg partido_leg bloque_leg ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 RAMON A ALMENDRA SANTA CRUZ JUSTICIALISTA JUSTICIALIS… ## 2 2 JULIO AMOEDO CATAMARCA JUSTICIALISTA JUSTICIALIS… ## 3 3 RAMON A ARAUJO TUCUMAN JUSTICIALISTA JUSTICIALIS… ## 4 4 ALFREDO L BENITEZ JUJUY JUSTICIALISTA JUSTICIALIS… ## 5 5 ANTONIO TOMAS BERHONGAR… LA PAMPA UCR UCR ## 6 6 DEOLINDO FELIPE BITTEL CHACO JUSTICIALISTA JUSTICIALIS… ## 7 7 LUIS BRASESCO ENTRE RIOS UCR UCR ## 8 8 HORACIO F BRAVO HERRERA SALTA JUSTICIALISTA JUSTICIALIS… ## 9 9 ORALDO N BRITOS SAN LUIS JUSTICIALISTA JUSTICIALIS… ## 10 10 JORGE A CASTRO S DEL ESTERO JUSTICIALISTA JUSTICIALIS… ## # … with 36 more rows ¡Nuestra base de datos de nodos está lista! Ahora necesitamos crear la base de datos de enlaces, que una a los legisladores/as de acuerdo a su co-sponsorship. Para los propósitos de este ejercicio realizaremos una base binaria, como la del primer ejemplo del capítulo: dos legisladores/as estarán conectados (1) si es que alguna vez en 1983 presentaron un proyecto en conjunto. Siguiendo el espíritu de las pipes, que revisamos en el Capítulo 3, realizaremos una operación compleja con una sola concatenación de comandos –¡revisa muy bien los comentarios del código!–. Como vimos, nuestro objetivo será crear los pares de diputados que están conectados por, a lo menos, un proyecto en común. df_enlaces &lt;- leyes_argentina %&gt;% # Seleccionemos solo las variables de proyecto y legislador select(id_proyecto, id_leg) %&gt;% # Ahora unamos la base consigo misma, para crear todas las combinaciones ## de pares de legisladores/as por proyecto left_join(., ., by = &quot;id_proyecto&quot;) %&gt;% # Eliminemos los pares de un legislador/a consigo mismo filter(id_leg.x != id_leg.y) %&gt;% # Quedémonos con lo pares únicos (aquí el orden no importa). ## En la variable id_leg_1 quedará el legislador/a con id menor. mutate(id_leg_1 = pmin(id_leg.x, id_leg.y), id_leg_2 = pmax(id_leg.x, id_leg.y)) %&gt;% select(id_leg_1, id_leg_2) %&gt;% distinct() %&gt;% # Ordenemos la base según las ids arrange(id_leg_1, id_leg_2) %&gt;% # Creemos una variable solo con 1, que explicitan la conexión entre ## el par de legisladores/as mutate(d_firma_conjunta = 1L) df_enlaces ## # A tibble: 509 x 3 ## id_leg_1 id_leg_2 d_firma_conjunta ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 2 1 ## 2 1 3 1 ## 3 1 4 1 ## 4 1 5 1 ## 5 1 6 1 ## 6 1 8 1 ## 7 1 9 1 ## 8 1 10 1 ## 9 1 12 1 ## 10 1 16 1 ## # … with 499 more rows Existen 509 vínculos entre legisladores/as en nuestra red. Nota que aquí solo hemos obtenido los pares que están conectados: nuestra base no contiene información sobre los pares sin conexión. Luego, tidygraph asumirá, correctamente, que los pares no explicitados en la base de enlaces se encuentran desconectados. Ahora solo nos queda crear el objeto tbl_graph, propio del paquete tidygraph. Este simplemente contiene nuestras dos bases de datos: library(tidygraph) red_cosp_arg &lt;- tbl_graph(nodes = df_nodos, edges = df_enlaces, directed = F # ¡nuestra base no tiene dirección! ) Nota que, para que tbl_graph funcione correctamente en este caso, las id de identifación de cada nodo deben ser correlativas, abarcando desde 1 hasta el total de nodos. Como es el caso en nuestro ejemplo, podemos seguir adelante. 13.4 Disposición gráfica Ahora tenemos una red de legisladores que se dispone de la siguiente forma: library(ggraph) set.seed(100); layout_red &lt;- create_layout(red_cosp_arg, &quot;fr&quot;) ggraph(layout_red) + # Añadir los enlaces: geom_edge_link(color = &quot;lightgrey&quot;) + # Añadir los nodos (con color según bloque): geom_node_point(size = 5) + # Utilizar fondo blanco y tema vacío: theme_void() + # Añadir una etiqueta más legible labs(color = &quot;Bloque legislativo&quot;) 13.4.1 Color y forma en la representación visual 13.5 Análisis básico de redes 13.5.1 Medidas de centralidad PENDIENTE 13.5.2 Comunidades en las redes PENDIENTE Referencias Alemán, E., Calvo, E., Jones, M. P., &amp; Kaplan, N. (2009). Comparing Cosponsorship and Roll‐Call Ideal Points. Legislative Studies Quarterly, 34(1), 87-116. Fowler, J. H. (2006). Connecting the Congress: A study of cosponsorship networks. Political Analysis, 14(4), 456-487. Nota que esta tercera red, como está descrita en nuestro ejemplo, no registra cuando dos legisladores apoyan en conjunto un proyecto de otro legislador distinto. En este sentido, solo mide apoyos directos entre pares de legisladores.↩ "],
["qta.html", "Capítulo 14 Análisis cuantitativo de texto 14.1 Introducción 14.2 Exploración de la bases de datos 14.3 Cálculo de frecuencias 14.4 Wordcloud de hashtags 14.5 Graficar los hashtags más utilizados por grupos de interés 14.6 Hashtags de género por características del diputado 14.7 Variación temporal del uso de estos hashtags 14.8 Hashtags de género por semana a nivel de diputados (ponderados por semana) 14.9 Comentarios finales 14.10 Anexo: Problemas que se presentan cuando analizas discursos, debates o leyes 14.11 Tidy text", " Capítulo 14 Análisis cuantitativo de texto Por Sebastián Huneeus Lecturas de referencia Calvo, E., &amp; Aruguete, N. (2018). #Tarifazo. Medios tradicionales y fusión de agenda en redes sociales. En Inmediaciones de la Comunicación, 13(1), 189–213. Calvo, E., Dunford, E., &amp; Lund, N. (2016). Hashtags that Matter: Measuring the propagation of Tweets in the Dilma Crisis. Working Paper. Jelani Ince, Fabio Rojas &amp; Clayton A. Davis (2017). The social media response to Black Lives Matter: how Twitter users interact with Black Lives Matter through hashtag use.* Ethnic and Racial Studies, 40*:11, 1814-1830. Salganik, Matthew J. (2017). Bit by Bit: Social Research in the Digital Age. Princeton, NJ: Princeton University Press. Steinert-Threlkeld, Z. (2018). Twitter as Data (Elements in Quantitative and Computational Methods for the Social Sciences). Cambridge: Cambridge University Press. Trott, Verity. (2018) Connected feminists: foregrounding the interpersonal in connective action, Australian Journal of Political Science, 53:1, 116-129. 14.1 Introducción Twitter, Facebook y otras redes sociales se han convertido en una fuente de datos políticos de enorme valor. Por ejemplo, #olafeminista, #metoo, #blacklivesmatter o #niunasmenos son hashtags de temáticas políticas que han inundado las comunicaciones en Twitter en el último tiempo. Un hashtag (expresado con el símbolo ‘#’) es un texto que conecta a varias cuentas de Twitter a través de un tema. Son conexiones entre nodos y aglutina cuentas individuales, lo que es sumamente útil para entender la dinámica de la propagación de la protestas online y examinar cómo se desarrollan en el tiempo y en el espacio. Recientemente, ha emergido una literatura que aborda las protestas sociales y la defensa de derechos a través de la propagación de hashtags. En este contexto hay trabajos recientes que abordan reclamos feministas o formas de activismo racial en Twitter. En este capítulo vamos a aplicar estadística descriptiva para abordar el uso de hashtags en la #olafeminista como ejemplo de cómo podemos usar redes sociales para tratar nuestros temas de investigación. La pregunta que dio origen a este análisis fue ¿qué patrones se observan en la manera con que los diputados chilenos usan los hashtags en la protesta social #olafeminista? Se trata de una pregunta exploratoria y no explicativa. El tema de fondo que nos interesa abordar es que en redes sociales, usar un hashtag en general equivale a estar interesado sobre un tema - independientemente de si se está a favor o en contra de cierto tema. En cierta forma, podríamos argumentar, los hashtags son proxis de la visibilidad de ciertos temas políticos. Este ejercicio que haremos es un excelente paso previo al análisis cualitativo en profundidad, por ejemplo mediante entrevistas, ya que los hashtags pueden servirnos para identificar a qué políticos están favor, en contra o simplemente no les interesa cierto tema. Así mismo, sirve para identificar a diputados que tienen como prioridad de su agenda política las temáticas de género y comprender de qué forma se vinculan con su electorado a partir de este tema. En términos metodológicos, con este ejercicio vamos a aproximarnos a cómo las coaliciones legislativas y diputados individuales usan hashtags de género y/o feminismo. Los paquetes de R utilizados serán tidyverse, wordcloud2, skimr y lubridate. De estos cuatro, al menos tidyverse y skimr deberían serte familiares a esta altura del libro. La unidad de análisis para este ejercicio serán los 155 diputados chilenos, y el período de recolección de datos abarcó entre el 1 de mayo y el 10 de junio de 2018. En total, 131 de 155 diputados escribieron al menos un tweet durante ese período, que coincidió con el auge de la ola feminista. La búsqueda en Google Trends de los términos “acoso” y “feminismo” en Chile muestra que durante el rango de fechas que hemos seleccionado corresponde al de mayor interés entre 2014 y 2019, como se ve entre las líneas punteadas que marcan el inicio y fin de nuestra recolección de datos. Figura 1.2: Interés en Chile por la #olafeminista Existen varios mecanismos para recolectar datos desde Twitter y transformarlos en variables. El primer método, que quisiéramos evitar es el que podríamos llamar de “fuerza bruta”, es decir recolectar los tweets uno a uno, de manera manual. La segunda alternativa es a través de una API (del inglés application programming interface). Por medio de una API gratuita podemos bajar tweets de manera automatizada. Para acceder a la API de Twitter recomendamos usar los paquetes twitteR o rtweet. Crendo una cuenta de developer podemos ingresar a Twitter a través de esta plataforma. A grosso modo hay dos alternativas de usar la API para bajar tweets. (a) Para un hashtag en particular (por ejemplo ‘#Trump’) se pueden bajar hasta 18 mil tweets de hasta ocho días de antigüedad, que serán obtenidos del conjunto de tweets que hayan usado este hashtag de manera aleatoria. Si nuestro tema de interés supera el número de 18 mil tweets, podemos iterar la función para ampliar la muestra. En una hora se pueden bajar hasta 54 mil tweets. Si quieres hacer un seguimiento de un fenómeno de más de ocho días de antigüedad vas a tener que buscar alternativas pagas, que son muy costosas ¡Es importante estar alerta para empezar a recolectar datos antes de que sea tarde! (b) Cuando quieres hacer un seguimiento de un usuario en particular, la extracción te permite seguir una cuenta y de esa cuenta bajar una determinada fija de los tweets (hasta 3200) que haya publicado esta cuenta. Para más información de cómo utilizar esta herramienta puedes ir al capítulo 12 sobre web scrapping. Para el análisis usaremos una base de datos lista, con variables de identificación de diputados, tweets y hashtags. Las variables de identificación de los diputados se extrajeron fácilmente de la Página oficial de la Cámara de Diputados. Para hacer la extracción de datos de Twitter, utilizamos el paquete Rtweet, y así accedimos de manera gratuita al API de Twitter para descargar información por usuarios, fechas y hashtags. Para extraer datos de Twitter con R se recomienda leer Twitter as Data, que contiene algunas rutinas estandarizadas para descargar datos de esta plataforma. La base final tiene 15 variables y 9758 observaciones: tiene tantas observaciones como veces cada diputado usó cualquier hashtag en cualquiera de sus tweets y retweets. En esta base, por tanto, están incorporados solamente los tweets que usaron algún hashtag y se excluyeron todos los mensajes que no ocuparon algún hashtag durante ese período. 14.2 Exploración de la bases de datos Cargamos la base de datos y la exploramos con el paquete skimr para tener una imagen rápida del tamaño de la base, su número de observaciones y variables, así como el tipo de variables (carácter, integer, etc.). library(tidyverse) library(skimr) library(paqueteadp) data(tweets_dips_chile) Ahora la base se ha cargado en nuestra sesión de R: ls() ## [1] &quot;estudio&quot; &quot;tweets_dips_chile&quot; skimr también nos entrega información sobre el número de datos perdidos (missings), la cantidad de categorías o valores que toma la variable de factor (n_unique), así como estadísticos de dispersión para las variables cuantitativas (valores mínimos y máximos, cuartiles, media y desviación estándar). Esto nos permite hacer un diagnóstico rápido sobre los datos con los que vamos a trabajar: vemos que hay once variables de tipo “character” (variables con texto), una de tipo “date” (variables con fechas), dos de tipo “factor” (variables categóricas) y una de tipo “POSIXct” que permite trabajar con fechas. La variable que contiene los hashtags de interés es la variable del mismo nombre, con skimr vemos que no hay valores perdidos, que tiene 9758 observaciones, siendo el más corto de ellos uno con 2 caracteres y el más largo con 54. Por último, vemos que en total se usaron 2309 hashtags únicos. Construir esta variable fue bastante sencillo, en la medida que el API separa, en una variable distinta, los mensajes de los hashtags. A través del paquete stringr para operar con expresiones regulares, separamos los hashtags cuando en un mensaje había más de uno y además hicimos una limpieza, quitando espacios, acentos y mayúsculas. skim(tweets_dips_chile) ## Skim summary statistics ## n obs: 9758 ## n variables: 11 ## ## ── Variable type:character ────────────────────────────── ## variable missing complete n min max empty n_unique ## coalicion 0 9758 9758 2 4 0 7 ## [ reached getOption(&quot;max.print&quot;) -- omitted 7 rows ] ## ## ── Variable type:numeric ──────────────────────────────── ## variable missing complete n mean sd p0 p25 p50 p75 p100 hist ## [ reached getOption(&quot;max.print&quot;) -- omitted 2 rows ] ## ## ── Variable type:POSIXct ──────────────────────────────── ## variable missing complete n min max median ## created_at 0 9758 9758 2018-05-01 2018-06-10 2018-05-23 ## n_unique ## 6197 Podemos observar los primeros valores de la variable hashtags simplemente seleccionando dichas variables en la base: tweets_dips_chile %&gt;% select(hashtags) ## # A tibble: 9,758 x 1 ## hashtags ## &lt;chr&gt; ## 1 lugaresquehablan ## 2 puertomontt ## 3 salvemoslagochapo ## 4 lagochapo ## 5 ancud ## 6 chiloe ## 7 interpelacionlarrain ## 8 interpelacionlarrain ## 9 sqm ## 10 poncelerou ## # … with 9,748 more rows ¡Ahora vamos a empezar el análisis de frecuencias de nuestra información! 14.3 Cálculo de frecuencias 14.3.1 ¿Cuáles son los diez hashtags más usados de las cuentas de los diputados? Si nos interesa saber es cuáles son los hashtags más usados usaremos count() para ver la frecuencia y luego arrange(-n) para ordenar el conteo de mayor a menor (nota que si quitamos el “–“ delante de la n el ordenamiento es de menor a mayor). Agregando los comandos as.tibble()y dplyr::slice(), generamos una tabla en la que solamente se muestra una cantidad acotada de valores, en este caso, los veinte mayores. Verás que el hashtag más usado es #cuentapublica, asociado a la rendición de cuentas presidencial del 21 de mayo en el que el presidente Sebastián Piñera se dirigió al Congreso Nacional. El hashtag #aborto3causales se ubica en quinto lugar, con 122 menciones en total. Éste hace referencia al debate que se dio en Chile respecto a la legalización del aborto en caso de riesgo de la madre, inviabilidad fetal o violación a partir de la sanción de la ley 21030. En la doceava posición se ubica #interpelacionaborto3causales que remite a la interpelación al ministro de Salud, Emilio Santelices sobre las modificaciones al protocolo de objeción de conciencia en el marco de la ley de aborto en tres causales. Además, en la posición decimosexta se ubica el hashtag #olafeminista. tweets_dips_chile %&gt;% count(hashtags) %&gt;% arrange(-n) %&gt;% dplyr::slice(1:20) ## # A tibble: 20 x 2 ## hashtags n ## &lt;chr&gt; &lt;int&gt; ## 1 cuentapublica 602 ## 2 chile 178 ## 3 valdiviacl 153 ## 4 iquique 132 ## 5 aborto3causales 122 ## 6 losrios 120 ## 7 chilelohacemostodos 109 ## 8 diputadamarisela 109 ## 9 chilecuentacontigo 100 ## 10 interpelacionlarrain 100 ## 11 elranco 92 ## 12 interpelacionaborto3causales 79 ## 13 coquimbo 76 ## 14 diadelpatrimonio 73 ## 15 antofagasta 69 ## 16 olafeminista 67 ## 17 laserena 66 ## 18 baltolutudiputado 65 ## 19 tiemposmejores 65 ## 20 semanadistrital 62 También creamos una tabla en que las frecuencias hashtags están separadas por la variable de género dummy_mujer. De esta forma, podemos ver de manera rápida si hay diferencias claras en la frecuencia de tweets por género del diputado. tweets_dips_chile %&gt;% group_by(dummy_mujer) %&gt;% count(hashtags) %&gt;% arrange(hashtags) ## # A tibble: 2,760 x 3 ## # Groups: dummy_mujer [2] ## dummy_mujer hashtags n ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 0 106añospc 18 ## 2 1 106añospc 2 ## 3 1 11mayo1935 1 ## 4 0 1demayo 18 ## 5 1 1demayo 9 ## 6 0 1demayo2018 1 ## 7 1 1erodemayo 1 ## 8 1 1m 4 ## 9 0 1mayo 1 ## 10 1 1mayo 2 ## # … with 2,750 more rows Analicemos más en profundidad las diferencias por género. 14.3.2 Frecuencias por género: cantidad de hashtags usados por los diputados en total Las funciones group_by() y summarise() nos permiten contar el número de observaciones asociados a una variable de base. Al hacer un conteo de los valores 0 (hombre) y 1 (mujer) de dummy_mujer, sabremos cuantas veces cada género ocupó algunhashtag al escribir un tweet. Vemos que los hombres ocuparon un hashtag 6169 veces, mientras que las mujeres 3589 veces. tweets_dips_chile %&gt;% group_by(dummy_mujer) %&gt;% summarise(total = n()) ## # A tibble: 2 x 2 ## dummy_mujer total ## &lt;dbl&gt; &lt;int&gt; ## 1 0 6169 ## 2 1 3589 Ahora, podríamos ver cuáles son los principales 20 hashtags entre mujeres: tweets_dips_chile %&gt;% filter(dummy_mujer==1) %&gt;% count(hashtags) %&gt;% arrange(-n) %&gt;% slice(1:20) ## # A tibble: 20 x 2 ## hashtags n ## &lt;chr&gt; &lt;int&gt; ## 1 cuentapublica 205 ## 2 diputadamarisela 109 ## 3 aborto3causales 77 ## 4 chile 69 ## 5 atacama 57 ## 6 interpelacionlarrain 57 ## 7 antofagasta 53 ## 8 olafeminista 53 ## 9 interpelacionaborto3causales 44 ## 10 agendamujer 39 ## 11 semanadistrital 33 ## 12 diadelpatrimonio 29 ## 13 leydeidentidaddegenero 29 ## 14 domatuvida 28 ## 15 leycultivoseguro 27 ## 16 educacionnosexista 25 ## 17 reformaintegral 24 ## 18 diadeltrabajador 23 ## 19 niunamenos 23 ## 20 las5posverdadesdebachelet 22 Y entre hombres: tweets_dips_chile %&gt;% filter(dummy_mujer==0) %&gt;% count(hashtags) %&gt;% arrange(-n) %&gt;% slice(1:20) ## # A tibble: 20 x 2 ## hashtags n ## &lt;chr&gt; &lt;int&gt; ## 1 cuentapublica 397 ## 2 valdiviacl 151 ## 3 iquique 131 ## 4 losrios 120 ## 5 chile 109 ## 6 chilelohacemostodos 99 ## 7 elranco 92 ## 8 chilecuentacontigo 89 ## 9 coquimbo 75 ## 10 baltolutudiputado 65 ## 11 laserena 65 ## 12 puertomontt 59 ## 13 tiemposmejores 58 ## 14 aricayparinacota 57 ## 15 aborto3causales 45 ## 16 diadelpatrimonio 44 ## 17 interpelacionlarrain 43 ## 18 chiloe 40 ## 19 arica 39 ## 20 diputadosrn 39 Entre las mujeres contamos 45 referencias a #aborto3causales y 122 entre los hombres, una relación de 1 a 3. Sin embargo, son muchas menos las legisladoras mujeres, lo que puede explicar esta diferencia. Si controlamos por esta variable, vemos que la probabilidad de usar el hashtag #aborto3causales es virtualmente la misma entre hombres y mujeres. library(sjPlot) tweets_dips_chile &lt;- mutate(tweets_dips_chile, a3c=ifelse(hashtags == &quot;aborto3causales&quot;,1,0)) probabilidad &lt;- glm(a3c~dummy_mujer + as.factor(diputado), data = tweets_dips_chile, family = binomial(&quot;logit&quot;)) # plot_model(probabilidad, # type = &quot;pred&quot;, # show.ci = TRUE, # terms = &quot;dummy_mujer&quot;) 14.4 Wordcloud de hashtags La nube de palabras permite generar una representación visualmente amena de las frecuencias, ubicando al centro y más grandes los casos que tiene una mayor frecuencia. Para ello cargamos wordcloud2 y lo aplicamos sobre el conteo de cada hashtag. library(wordcloud2) wordcloud2(tweets_dips_chile %&gt;% count(hashtags) %&gt;% arrange(-n)) Figura 10.6: Nube de palabras con wordcloud2 Si vas a usar esta nube para un poster académico podés darle la forma, por ejemplo, del pajarito de Twitter para hacerlo más atractivo a la audiencia. library(wordcloud2) wordcloud2(tweets_dips_chile %&gt;% count(hashtags) %&gt;% arrange(-n), figPath = &quot;00-images/twitter_logo.jpg&quot;) Figura 9.1: Nube de palabras con forma personalizada Agregando un filtro sobre la variable dummy_mujer podemos generar nubes diferencindo entre diputadas y diputados. Con esto se aprecia inmediatamente que los hashtags como #olafeminista, #agendamujer y #educacionnosexista aparecen con mucha mayor frecuencia entre diputadas que entre los diputados. wordcloud2(tweets_dips_chile %&gt;% filter(dummy_mujer == 1) %&gt;% count(hashtags) %&gt;% arrange(-n)) Figura 10.7: Nube de palabras con wordcloud2 divididas por género wordcloud2(tweets_dips_chile %&gt;% filter(dummy_mujer == 0) %&gt;% count(hashtags) %&gt;% arrange(-n)) Figura 10.7: Nube de palabras con wordcloud2 divididas por género 14.5 Graficar los hashtags más utilizados por grupos de interés 14.5.1 Hashtags más usados entre las diputadas Usaremos ggplot2 para graficar estas frecuencias ordenándolas de mayor a menor a través de fct_reorder(). Vamos a generar este gráfico en dos pasos, primero creando una nueva tabla con los diez hashtags más utilizados por las mujeres; en esta tabla solamente habrán dos variables, una con el nombre del hashtag y la segunda con su frecuencia n. Luego, haremos un gráfico de columnas agregando el argumento geom_col() a la función ggplot. Al filtrar por género y considerar solamente a las diputadas mujeres, los hashtags #aborto3causales se ubica en tercer lugar, así como #olafeminista que se ubica en el octavo lugar y #agendamujer en el décimo lugar. plot_10_mujeres &lt;- tweets_dips_chile %&gt;% filter(dummy_mujer == 1) %&gt;% count(hashtags) %&gt;% arrange(-n) %&gt;% dplyr::slice(1:10) ggplot(data = plot_10_mujeres, mapping = aes(x = fct_reorder(hashtags, n), y = n)) + geom_col (width =0.1) + coord_flip() + labs(x = &quot;&quot;, y = &quot;Menciones&quot;, subtitle = &quot;1 mayo - 10 de junio&quot;, caption = &quot;Fuente: Elaboración propia&quot;) Figura 10.8: Hashtags más comunes en las cuentas de Twitter de las diputadas 14.5.2 Hashtags más usados entre los diputados Al filtrar por género y considerar solamente a los diputados hombres, los hashtags #aborto3causales, #olafeminista y #agendamujer desaparecen del ranking. Esto entrega señales para posteriores preguntas empíricas, por ejemplo, sobre las motivaciones que tienen los diputados para posicionarse a temas de género. plot_10_hombres &lt;- tweets_dips_chile %&gt;% filter(dummy_mujer == 0) %&gt;% count(hashtags) %&gt;% arrange(-n) %&gt;% dplyr::slice(1:10) ggplot(data = plot_10_hombres, mapping = aes(x = fct_reorder(hashtags, n), y = n)) + geom_col(width = 0.1) + coord_flip() + labs(x = &quot;&quot;, y = &quot;Menciones&quot;, subtitle = &quot;1 mayo - 10 de junio&quot;, caption = &quot;Fuente: Elaboración propia&quot;) Figura 10.9: Hashtags más comunes en las cuentas de Twitter de los diputados 14.6 Hashtags de género por características del diputado 14.6.1 Por el género del diputado Contamos la frecuencia con que las diputadas usan algunos de los hashtags más emblemáticos de la ola feminista. Para ello aplicamos dos filtros, uno sobre la variable hashtags y otro sobre dummy_mujer. A esto se le agrega la función count() que permite contar frecuencias. tweets_dips_chile %&gt;% filter(hashtags %in% c(&quot;olafeminista&quot;, &quot;agendamujer&quot;, &quot;interpelacionaborto3causales&quot;, &quot;educacionnosexista&quot;) &amp; dummy_mujer == 1) %&gt;% count(hashtags) %&gt;% arrange(-n) ## # A tibble: 4 x 2 ## hashtags n ## &lt;chr&gt; &lt;int&gt; ## 1 olafeminista 53 ## 2 interpelacionaborto3causales 44 ## 3 agendamujer 39 ## 4 educacionnosexista 25 Luego, contamos la frecuencia con que los diputados hombres usan algunos de los hashtags más emblemáticos de la ola feminista: tweets_dips_chile %&gt;% filter(hashtags %in% c(&quot;olafeminista&quot;, &quot;agendamujer&quot;, &quot;interpelacionaborto3causales&quot;, &quot;educacionnosexista&quot;) &amp; dummy_mujer == 0) %&gt;% count(hashtags) %&gt;% arrange(-n) ## # A tibble: 4 x 2 ## hashtags n ## &lt;chr&gt; &lt;int&gt; ## 1 interpelacionaborto3causales 35 ## 2 agendamujer 16 ## 3 olafeminista 14 ## 4 educacionnosexista 8 Solamente contando frecuencias y comparando a hombres y mujeres, vemos que existen diferencias notorias. Por ejemplo, las diputadas usaron #olafeminista 53 veces, mientras que los diputados solamente 14 veces. Posteriormente, se podría ver qué tan significativa es esta diferencia a través de un test de comparación de proporciones con prop.test o a partir de una regresión como ya ejemplificamos anteriormente para #aborto3causales. 14.6.2 Por la coalición de los diputados Ahora vamos a examinar la frecuencia con que cada coalición emplea el hashtag #olafeminista, uno de los más emblemáticos de la movilización.A nivel de coaliciones, vemos que el hashtag #olafeminista es mencionado por La Fuerza de la Mayoría (LFM) y Frente Amplio (FA). La coalición de derecha, Chile Vamos (ChV) y Convergencia Democrática (CODE) no ocuparon este hashtag durante período analizado. Este dato puede servir para explorar el posicionamiento de los políticos de centro y de derecha en temas como el avance en la igualación de los derechos de las mujeres. Primero vemos la cantidad de veces que una coalición ocupó algún hashtag. 14.6.2.1 Chile Vamos tweets_dips_chile %&gt;% filter(hashtags== &quot;olafeminista&quot; &amp; coalicion == &quot;ChV&quot;) %&gt;% count(hashtags) ## # A tibble: 0 x 2 ## # … with 2 variables: hashtags &lt;chr&gt;, n &lt;int&gt; 14.6.2.2 La Fuerza de la Mayoría tweets_dips_chile %&gt;% filter(hashtags== &quot;olafeminista&quot; &amp; coalicion == &quot;LFM&quot;) %&gt;% count(hashtags) ## # A tibble: 1 x 2 ## hashtags n ## &lt;chr&gt; &lt;int&gt; ## 1 olafeminista 8 14.6.2.3 Frente Amplio ## # A tibble: 1 x 2 ## hashtags n ## &lt;chr&gt; &lt;int&gt; ## 1 olafeminista 48 14.6.2.4 Convergencia Democrática ## # A tibble: 0 x 2 ## # … with 2 variables: hashtags &lt;chr&gt;, n &lt;int&gt; ## # A tibble: 5 x 2 ## semana agendamujer_semana ## &lt;dttm&gt; &lt;int&gt; ## 1 2018-05-06 00:00:00 5 ## 2 2018-05-13 00:00:00 13 ## 3 2018-05-20 00:00:00 28 ## 4 2018-05-27 00:00:00 10 ## 5 2018-06-03 00:00:00 11 floor_date Ahora tomando una tríada de hashtags feministas y/o que hacen referencia a temas de género, constatamos que la coalición Convergencia Democrática (CODE) desaparece totalmente de la conversación. Es decir, que la coalición de centro, integrada por la Democracia Cristiana, no ocupó los tres hashtags durante el período analizado. 14.6.2.5 Chile Vamos ## # A tibble: 2 x 2 ## hashtags n ## &lt;chr&gt; &lt;int&gt; ## 1 agendamujer 32 ## 2 interpelacionaborto3causales 36 ## # A tibble: 13 x 3 ## # Groups: semana [5] ## semana hashtags hashtags_por_semana ## &lt;dttm&gt; &lt;chr&gt; &lt;int&gt; ## 1 2018-05-06 00:00:00 educacionnosexista 1 ## 2 2018-05-06 00:00:00 olafeminista 5 ## 3 2018-05-13 00:00:00 educacionnosexista 6 ## 4 2018-05-13 00:00:00 olafeminista 13 ## 5 2018-05-20 00:00:00 agendamujer 40 ## 6 2018-05-20 00:00:00 educacionnosexista 2 ## 7 2018-05-20 00:00:00 olafeminista 28 ## 8 2018-05-27 00:00:00 agendamujer 10 ## 9 2018-05-27 00:00:00 educacionnosexista 13 ## 10 2018-05-27 00:00:00 olafeminista 10 ## 11 2018-06-03 00:00:00 agendamujer 5 ## 12 2018-06-03 00:00:00 educacionnosexista 11 ## 13 2018-06-03 00:00:00 olafeminista 11 ## Joining, by = &quot;semana&quot; 14.6.2.6 La Fuerza de la Mayoría ## # A tibble: 3 x 2 ## hashtags n ## &lt;chr&gt; &lt;int&gt; ## 1 agendamujer 8 ## 2 educacionnosexista 2 ## 3 interpelacionaborto3causales 12 14.6.2.7 Frente Amplio ## # A tibble: 3 x 2 ## hashtags n ## &lt;chr&gt; &lt;int&gt; ## 1 agendamujer 15 ## 2 educacionnosexista 29 ## 3 interpelacionaborto3causales 27 14.6.2.8 Convergencia Democrática ## # A tibble: 0 x 2 ## # … with 2 variables: hashtags &lt;chr&gt;, n &lt;int&gt; 14.7 Variación temporal del uso de estos hashtags Para examinar la frecuencia semanal de cada hashtag por cada una de las coaliciones utilizamos el paquete lubridate que opera con datos en formato de fechas, por ejemplo, variables tipo “date” o “POSIXct”. En nuestra base tenemos dos variables de ese tipo: “fechanacimiento” y “created_at”. La segunda nos va a servir para generar una nueva base, con una variable que es la cantidad de hashtags por semana, a partir de los comandos mutate() y floor_date().Vemos que en la base hay un total de 7 semanas consecutivas, siendo la semana 5 de nuestro análisis, entre el 21 y 27 de mayo, la que concentra la mayor cantidad de tweets con hashtags. 14.7.1 Frecuencia semanal de #olafeminista Para examinar la frecuencia semanal con que se utiliza un determinado hashtag, simplemente agregamos un filter(). ## # A tibble: 5 x 2 ## week agendamujer_week ## &lt;dttm&gt; &lt;int&gt; ## 1 2018-05-06 00:00:00 5 ## 2 2018-05-13 00:00:00 13 ## 3 2018-05-20 00:00:00 28 ## 4 2018-05-27 00:00:00 10 ## 5 2018-06-03 00:00:00 11 14.7.2 Frecuencia semanal de tres hashtags de género Lo mismo si queremos calcular cuantas veces se dijeron tres hashtags en una misma semana. ## # A tibble: 13 x 3 ## # Groups: week [5] ## week hashtags hashtags_por_semana ## &lt;dttm&gt; &lt;chr&gt; &lt;int&gt; ## 1 2018-05-06 00:00:00 educacionnosexista 1 ## 2 2018-05-06 00:00:00 olafeminista 5 ## 3 2018-05-13 00:00:00 educacionnosexista 6 ## 4 2018-05-13 00:00:00 olafeminista 13 ## 5 2018-05-20 00:00:00 agendamujer 40 ## 6 2018-05-20 00:00:00 educacionnosexista 2 ## 7 2018-05-20 00:00:00 olafeminista 28 ## 8 2018-05-27 00:00:00 agendamujer 10 ## 9 2018-05-27 00:00:00 educacionnosexista 13 ## 10 2018-05-27 00:00:00 olafeminista 10 ## 11 2018-06-03 00:00:00 agendamujer 5 ## 12 2018-06-03 00:00:00 educacionnosexista 11 ## 13 2018-06-03 00:00:00 olafeminista 11 14.7.3 Grafiquemos los hashtags de género por semana A continuación graficaremos el peak del uso de hashtags de género, que se dio en la semana del 14-20 de mayo. Esto coincide con la antesala de la cuenta pública del 21 de mayo y con la marcha feminista del día 16 de mayo. Este gráfico se nutre de una tabla como la generada justo más arriba. Figura 14.1: Hashtags de género en las cuentas de Twitter de los diputados 14.7.4 Ponderamos en relación al total de hashtags por semana Si queremos ponderar por el total de hashtags usados semanalmente, generamos una nueva variable con mutate()es es la razón entre hashtags de interés dividido por el total semana. ## Joining, by = &quot;week&quot; 14.7.5 Grafiquemos los hashtags interesantes ponderados por el total semanal Observamos que #agendamujer y #olafeminista tienen su peak durante la semana del 21 de mayo, período que coincidió con una marcha y con la cuenta pública presidencial. ## Warning: Removed 2 rows containing missing values (geom_point). ## Warning: Removed 2 rows containing missing values (geom_path). Figura 6.12: Hashtags de género en las cuentas de Twitter de los diputados (proporciones semanales) 14.7.6 Diputadas Para generar comparaciones, ahora hacemos el mismo ejercicio con las diputadas mujeres. Para ello agregamos un filtro a la variable “dummy_mujer”, seleccionando el valor 1. 14.7.6.1 Menciones de género por semana 14.7.6.2 Hashtags de interés por semana Observamos que #agendamujer y #olafeminista tienen su peak durante la semana del 21 de mayo, período que coincidió con una marcha Figura 6.14: Hashtags de género en las cuentas de Twitter de las diputadas (total semanal) 14.7.6.3 Hashtags de género por semana a nivel de diputadas (ponderados por el total semanal) Ahora el grafico está representando el total de hashtags de género, ponderados por el total de hashtags usados durante esa semana. Vemos que se mantiene la relación, ya que #agendamujer y #olafeminista siguen preponderando en la semana del 21 de mayo, pero el eje Y ahora representa porcentajes semanales, de modo que en la semana peak, de todos los hashtags usados por la diputadas, cerca del 5% fueron #agendamujer. ## Warning: Removed 2 rows containing missing values (geom_point). ## Warning: Removed 2 rows containing missing values (geom_path). Figura 14.2: Hashtags de género en las cuentas de Twitter de las diputadas (ponderación) 14.7.6.4 Diputados Repetimos elejercicio con los diputados hombres, Para ello agregamos un filtro a la variable “dummy_mujer”, seleccionando el valor 0. 14.7.6.5 Hashtags de género por semana Al graficar la frecuencia de uso de hashtags, en la semana del 21 de mayo los diputados usaron #agendamujer 16 veces, pero no lo ocuparon en ninguna otra semana. En ese mismo período se da el peak de #olafeminista. Figura 6.16: Hashtags de género en las cuentas de Twitter de los diputados 14.8 Hashtags de género por semana a nivel de diputados (ponderados por semana) Al graficar la ponderación, los valores del eje Y representan porcentajes semanales. Vemos que #agendamujer es un poco más del 1% y #olafeminista cerca del 0,5%, en la semana del 21 de mayo. ## Warning: Removed 2 rows containing missing values (geom_point). ## Warning: Removed 2 rows containing missing values (geom_path). Figura 14.3: Hashtags de género en las cuentas de Twitter de los diputados (ponderación) 14.9 Comentarios finales Este ejercicio permite constatar varias diferencias en el uso de hashtags tanto a nivel de género de los diputados como de las coaliciones estudiadas. Respecto de la variación de género, las mujeres ocupan los hashtags relacionados con temas de género con mayor frecuencia, lo que fue particularmente evidente con #olafeminista y #agendamujer. Respecto de las coaliciones, el Frente Amplio y La Fuerza de la Mayoría (ex Nueva Mayoría) se posicionaron con mayor intensidad, ya que usaron hashtags de interés con mayor frecuencia, lo que queda patente nuevamente con #olafeminista. Al contrario, coaliciones como Convergencia Democrática desaparecen totalmente de la conversación si se filtra, por ejemplo, por #agendamujer. Respecto de la variación temporal, la mayor intensidad de las menciones a temas de género se dío en la semana del 14-20 de mayo, semana previa al discurso presidencial del 21 de mayo, que además coincidió con una marcha en varias ciudades del país. Vemos que en términos relativos, las diputadas mujeres están hasta cinco veces más interesadas en el movimiento feminista, esto ya que en la semana del 14-20 de mayo, las mujeres ocuparon #agendamujer 5 veces más que los hombres. ¿Para qué hacer este ejercicio? Principalmente, puede servir como paso previo para un diseño de investigación descriptivo, que aborde los tipos de apoyo e intensidades de opiniones entorno al movimiento feminista y los temas de género. Por otra parte, también puede servir de punto de arranque para diseños explicativos que indaguen en las causas del posicionamiento de los políticos en temas de derechos de la mujer. La estrategia empleada puede servir como una forma de medir, indirectamente, el apoyo o interés que tienen los diputados sobre derechos de la mujer y temáticas de género. Para profundizar sobre el tipo de estrategias de investigación más comunes en ciencias sociales usando bigdata, se puede leer online este capítulo del libro Bit by Bit de Matthew Salganik. En términos de destrezas necesarias, este ejercicio no requiere más que manejo básico de tidyverse. Posiblemente la parte más compleja sea la obtención y la estructuración de la base de datos. Afortunadamente, el API de Twitter y paquetes como TwwiteRo Rtweet permiten gestionar las descargas de información de manera sencilla y ordenada. Para una referencia completa sobre como como se han abordado y de cómo enfrentar preguntas de investigación de ciencias sociales en Twitter, más sugerencias técnicas sobre como descargar y procesar datos de esta plataforma, se recomienda ver el libro Twitter as Data de Zachary Steinert-Threlkeld. Por último, para profundizar sobre el movimiento feminista chileno durante el año 2018, se recomienda el libro editado por la periodista Faride Zerán, Mayo feminista. La rebelión contra el patriarcado y para una referencia obligada sobre cómo una crisis política latinoamericana puede ser estudiada desde Twitter, el libro de Ernesto Calvo, Anatomía Política de Twitter en Argentina es un fluido y claro trabajo sobre la polarización desatada a raíz de la muerte del Fiscal Nisman. 14.10 Anexo: Problemas que se presentan cuando analizas discursos, debates o leyes Es probable que quieras utilizar este aprendizaje para analizar otro tipo de textos, como pueden ser discursos, debates parlamentarios o cualquier otro texto mucho más extenso que un tweet. Si es así, entonces necesitarás hacer dos pasos que no cubrimos en el ejemplo anterior antes de proceder a analizar tus palabras. Primero necesitarás transformar los discursos en un data frame, y luego necesitarás eliminar “stop words”, es decir, palabras que quieres ignorar porque refieren a conectores (y, o, entonces), preposiciones (a, ante, bajo, etc) y artículos (nosotros, ellos). Usarás para esto el paquete tidytext. library(tidytext) Para mostrarte cómo funciona vamos a usar datos del archivo de Salvador Allende en marxists.org que contiene discursos públicos de Salvador Allende entre finales de 1969 y el 11 de septiembre de 1973. Los cargaremos, como antes, con la ayuda de paqueteadp, que debe estar cargado: data(discursos_allende) La base tiene cuatro variables: el título del discurso, su fecha, el link del cual fue extraído, y finalmente el discurso entero. Cada observación es un discurso diferente. glimpse(discursos_allende) ## Observations: 249 ## Variables: 4 ## $ titulo &lt;chr&gt; &quot;Programa Básico de la Unidad Popular&quot;, &quot;Discurso e… ## $ fecha &lt;date&gt; 1969-12-17, 1970-01-06, 1970-04-07, 1970-04-14, 19… ## $ full_link &lt;chr&gt; &quot;https://www.marxists.org/espanol/allende/1969/dici… ## $ text_allende &lt;chr&gt; &quot;Programa básico de la Unidad Popular Pronunciado:… Vamos a eliminar discursos previos a 1970 y también vamos a unir algunas palabras. Por ejemplo, “Estados Unidos” pasará a ser “EstadosUnidos” ya que es un único concepto. Si no las unimos, “Unidos” será contada junto a expresiones como “el pueblo está unido en esta lucha” y perderemos calidad de nuestro análisis. Cuando trabajes con tus propios textos te recomendamos que sigas esta indicación. discursos_allende &lt;- discursos_allende %&gt;% # colapsar nombres de países mutate(text_allende = str_replace_all(text_allende, &quot;Estados Unidos&quot;, &quot;EstadosUnidos&quot;), text_allende = str_replace_all(text_allende, &quot;Unión Soviética&quot;, &quot;UniónSoviética&quot;), text_allende = str_replace_all(text_allende, &quot;Naciones Unidas&quot;, &quot;NacionesUnidas&quot;)) %&gt;% # solo del 70 en adelante. Para el 69 los archivos contienen solo el programa de la UP filter(fecha &gt;= &quot;1970-01-01&quot;) Ahora que la base está lista, vamos a utilizar tidytext para dejarla lista para el análisis cuantitativo. 14.11 Tidy text En nuestra variable text_allende tenemos cada uno de los discursos del ex presidente chileno. Para poder analizarlos cuantitativamente debemos subdividirlos en unidades menores. Como el ejercicio consiste en hacer frecuencia de palabras más comunes, querremos dividir los discursos en palabras. Para ello usaremos la función unnest_tokens con la opción words. Si quisieras dividir los discursos por oraciones puedes usar la opción sentences y si lo quieres por párrafos paragraphs. discursos_allende_tidy &lt;- discursos_allende %&gt;% unnest_tokens(word, text_allende) ¡Ahora en vez de tener 248 discursos, tenemos casi 1 millón de palabras bajo la variable words! discursos_allende_tidy ## # A tibble: 959,525 x 4 ## titulo fecha full_link word ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Discurso en el Senado sobr… 1970-01-06 https://www.marxists.org/… discu… ## 2 Discurso en el Senado sobr… 1970-01-06 https://www.marxists.org/… en ## 3 Discurso en el Senado sobr… 1970-01-06 https://www.marxists.org/… el ## 4 Discurso en el Senado sobr… 1970-01-06 https://www.marxists.org/… senado ## 5 Discurso en el Senado sobr… 1970-01-06 https://www.marxists.org/… sobre ## 6 Discurso en el Senado sobr… 1970-01-06 https://www.marxists.org/… la ## 7 Discurso en el Senado sobr… 1970-01-06 https://www.marxists.org/… forma… ## 8 Discurso en el Senado sobr… 1970-01-06 https://www.marxists.org/… de ## 9 Discurso en el Senado sobr… 1970-01-06 https://www.marxists.org/… la ## 10 Discurso en el Senado sobr… 1970-01-06 https://www.marxists.org/… unidad ## # … with 959,515 more rows glimpse(discursos_allende_tidy) ## Observations: 959,525 ## Variables: 4 ## $ titulo &lt;chr&gt; &quot;Discurso en el Senado sobre la formación de la Unidad… ## $ fecha &lt;date&gt; 1970-01-06, 1970-01-06, 1970-01-06, 1970-01-06, 1970-… ## $ full_link &lt;chr&gt; &quot;https://www.marxists.org/espanol/allende/1970/enero06… ## $ word &lt;chr&gt; &quot;discurso&quot;, &quot;en&quot;, &quot;el&quot;, &quot;senado&quot;, &quot;sobre&quot;, &quot;la&quot;, &quot;form… Para eliminar los “stop words” necesitamos cargar el paquete tm e indicar el idioma de nuestro texto. ¡La lista es enorme! library(tm) ## Loading required package: NLP ## ## Attaching package: &#39;NLP&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## annotate stopwords(kind = &quot;es&quot;) ## [1] &quot;de&quot; &quot;la&quot; &quot;que&quot; &quot;el&quot; &quot;en&quot; &quot;y&quot; &quot;a&quot; &quot;los&quot; ## [ reached getOption(&quot;max.print&quot;) -- omitted 300 entries ] Saquémoslas de nuestra base. La nueva base discursos_allende_tidy_limpia tiene casi la mitad de observaciones que la base con “stop words”. Aunque no parezca, este paso es fundamental para que el análisis cuantitativo de texto salga bien. discursos_allende_tidy_limpia &lt;- discursos_allende_tidy %&gt;% filter(!(word %in% stopwords(kind = &quot;es&quot;))) %&gt;% # voy a aprovechar de sacar los dígitos / números: filter(!str_detect(word, &quot;\\\\d&quot;)) Ahora que tienes una base limpia, puedes hacer la rutina de análisis cuantitativo de texto de la igual manera que lo hicimos con los tweets de la #olafeminista. Ejercicios antes de continuar al próximo capítulo - Trabajaremos con el debate presidencial argentino de 2015 entre Mauricio Macri y Daniel Scioli, por la segunda vuelta, que tuvo como ganador a Macri. La versión taquigráfica del debate fue publicada por el diario La Nación y puedes abrirla acá.. Importe dos archivos separados, uno para Daniel Scioli, y otro para Mauricio Macri. Analice las palabras más frecuentes para cada candidato, eliminando “stop words”. Ahora importe el debate de la primera ronda, donde había más candidatos (http://www.lanacion.com.ar/1833848-transcripcion-completa-del-debate-presidencial). ¿Cuáles fueron los temas que estructuraron el debate? "],
["mapas.html", "Capítulo 15 Generación de mapas 15.1 Datos Espaciales 15.2 Datos Espaciales en R 15.3 Special Features en R 15.4 Manejo de Datos Espaciales 15.5 Mapear en R 15.6 Mapeando variables 15.7 Inferencia a partir de Datos Espaciales", " Capítulo 15 Generación de mapas Por Andrea Escobar y Gabriel Ortiz Lecturas de referencia Brunsdon, C., &amp; Comber, L. (2015). An introduction to R for spatial analysis and mapping. Sage Press. Pebesma, E. (2018). “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal, 10(1),439-446. R-Spatial D. Kahle and H. Wickham. (2013). ggmap: Spatial Visualization with ggplot2. The R Journal, 5(1), 144-161. Anselin, L. (1995). Local Indicators of Spatial Association-LISA. Geographical Analysis 27: 93-115 Weidmann, N. B., Kuse, D., &amp; Gleditsch, K. S. (2010). The geography of the international system: The CShapes dataset. International Interactions, 36(1), 86-106. En este capítulo veremos cómo podemos trabajar con datos espaciales spatial data utilizando R dentro del tidyverse. Aquí nos concentraremos en la riqueza que poseen los datos espaciales tanto como herramienta para el análisis exploratorio de datos sobre diversos fenómenos y también para una visualización y comunicación más atractiva y efectiva de nuestros hallazgos. Es decir, este capítulo tiene como misión principal: Caracterizar qué son los datos espaciales. Explicar por qué estos son interesantes de utilizar para el análisis en ciencia política Su manejo y manipulación en R, teniendo como foco principal su visualización mediante la creación de mapas estáticos y animados mediante ggplot a partir de su relación con otras variables descriptivas. Por lo que no nos adentraremos en la realización de modelos de inferencia estadística a partir de datos espaciales (Spatial Autoreggresion) y tampoco como generar datos geográficos ya que, en general, siempre estaremos trabajando a partir de bases de datos geográficos ya realizadas con anterioridad. Los datos a utilizar en este capítulo serán un shapefile (formato del cual hablaremos más adelante) de los estados de Brasil, obtenido del Spatial Data Repository de la New York University, el cual complementamos con datos de Danilo Freire. Por un lado, el repositorio de datos espaciales de NYU provee una multitud de datos espaciales para distintos países y áreas geográficas tanto inter como intra nacionales. En este caso, utilizamos una base de límites estaduales de Brasil al año 1991, provista por el Instituto Brasileiro de Geografia e Estatística, la cual sigue vigente dado a que el último cambio administrativo en Brasil a nivel de estados ocurrió el año 1988. Por otro lado, el artículo de Freire (2018) intenta explicar la causa de la disminución en las tasas de homicidios del estado de Sao Paulo a partir de la implementación de medidas para la reducción del crimen impulsadas por el gobierno estatal. Lo útil de este trabajo es que recopila datos a nivel estadual de diversos indicadores socioeconómicos como PIB, Índice GINI, años de escolaridad promedio, etc., a lo largo de diez años (1990 - 2010), los cuales podremos unir a nuestro shapefile y utilizarlos para graficar estos factores visualmente. 15.1 Datos Espaciales 15.1.1 ¿Qué son? Los datos espaciales, o más bien geoespaciales, se refieren a los datos que obtenemos sobre lugares geográficos, es decir, lugares en la superficie de la tierra. El uso de este tipo de datos reconoce “el rol clave que conceptos espaciales como la distancia, la ubicación, proximidad, vecindario y región juegan en la sociedad humana”, permitiendo enfrentarnos a los fenómenos desde una perspectiva multivariada y multidimensional, al entregarnos un nuevo tipo de información adicional a nuestras unidades de estudio (CSISS, 2-3). En la ciencia política la perspectiva espacial para el análisis se ha empleado en trabajos como el de King (1997) sobre cómo realizar inferencias ecológicas en la disciplina y en Gimple y Shuknecht (2003) sobre la accesibilidad de los lugares de votación. Este tipo de datos permite responder preguntas tales como si existe influencia en términos de proximidad de los países vecinos sobre la adopción de ciertas políticas, pero también resultan ser una herramienta clave en la exploración de nuestros datos para observar la existencia de patrones territoriales en los fenómenos estudiados. Durante el último tiempo, la accesibilidad a datos de tipo espacial se ha vuelto cada vez más ampliamente disponible, con iniciativas por parte de los gobiernos para hacer disponible datos de diverso tipo que se relacionan a los procesos sociales y económicos de los países (Brunsdon y Comber, 2015: 18) y con plataformas que permiten obtener coordenadas geográficos e información adicional relacionada a la ubicación geográfica en todo el mundo como Google Maps, Open Street Map, o desde redes sociales como Twitter o Facebook. Mientras que, para investigadores y estudiantes, bases como la mencionada Spatial Data Repository de la NYU proveen archivos de shapefiles de fácil acceso y manipulación para generar visualizaciones y agregar datos georreferenciados de otras bases. Quienes estudian relaciones internacionales encontrarán un buen complemento entre este capítulo y el capítulo 9 donde se explota al máximo la función countrycode. Si eres uno de ellos, te recomendamos ver la página web del proyecto Cshapes de Niels Weidmann y leer su artículo en la lectura de referencia. 15.1.2 Estructura de los Datos Espaciales Los archivos en los cuales se contienen datos espaciales usualmente se les conoce como shapefiles, los cuales generalmente se contienen en una carpeta que contiene al menos tres archivos con las extensiones .shp, .dbf y.shx. Figura 15.1: Ejemplo de carpeta que contiene un shapefile en Windows. Nota que el archivo .shp contiene la mayor cantidad de información y es el más pesado. Además, esta carpeta contiene un archivo adicional .prj Figura 15.2: Ejemplo de carpeta que contiene un shapefile en Mac Este tipo de objetos son usualmente representados por vectores (si no recuerdas lo que es un vector podés refrescarlo en el capítulo 3), que consisten en la descripción de la geometría o forma (shape) de los objetos, conteniendo usualmente variables adicionales, llamadas “atributos”, sobre estos mismos datos. Tomando como ejemplo los datos que usaremos durante este capítulo, nuestra base de datos describe los bordes de los estados de Brasil (geometría) y además contiene información sobre la tasa de homicidios de cada uno de estos y otro tipo de datos socioeconómicos de ellos (atributos). Los datos espaciales son diversos en sus características y son usualmente divididos en tipos de vectores que en todos los casos consisten en un set de par de coordenadas (x,y) (Rspatial): Puntos: Una sola ubicación descrita por un punto como una dirección de una casa geocodificada. Líneas: Segmento compuesto por una serie de puntos unidos el uno con el otro que no conforman un objeto cerrado. Polígonos: Superficie plana compuesta por una serie de líneas conectadas la una con la otra que forman un objeto cerrado. Multi-polígonos: Multi-superficie compuesta por polígonos que no necesariamente deben intersectar. Figura 15.3: Tipos de formas de datos espaciales. (1) Punto, (2) Línea, (3) Polígono. Tip: Como hemos mencionado, el Repositorio de Datos Espaciales de la NYU puede ser una buena alternativa si es que deseas empezar a trabajar con datos espaciales de tu país, estado, o comuna. Si bien está en inglés, puedes buscar shapefiles fácilmente entrando a la página del Repositorio y seleccionas “Polygon” dentro de la categoría “Data Type”. Una vez es hecho esto, es solo un caso de introducir el nombre del área que te interesa en el buscador, seleccionar un resultado y apretar “Download Shapefile” en el costado superior derecho de la página para obtener tus archivos. Otra ventaja de estos datos es que suelen ser de bajo tamaño sin dejar de tener una calidad suficiente, lo cual es conveniente en el caso de que tu hardware posea limitaciones para trabajar con archivos de mayor tamaño. 15.2 Datos Espaciales en R Durante los últimos años R ha avanzado considerablemente en sus herramientas para manipular y analizar datos geográficos de tal manera que esta sea compatible con el formato y las funcionalidades de otros paquetes del programa. En este capítulo nos concentraremos en el paquete sf, lanzado en el 2016, el cual se creó en base a las funcionalidades de tres paquetes anteriores: sp, rgeos y rgdal y que implementa el modelo estándar de código abierto simple features (cualidades simples) para la representación de objetos del mundo real en el computador, el cual posee atributos tanto espaciales como no espaciales, basados en geometría 2D con interpolación linear entre vértices (Pebesma, 2018). 15.3 Special Features en R La principal ventaja que nos entrega el paquete sf es que este permite trabajar con datos espaciales dentro del tidyverse, es decir, poder manejar datos espaciales como si fueran cualquier otro tipo de dataset, utilizando así las funciones de los paquetes de R contenidos dentro del metapaquete tidyverse como ggplot2 y dplyr que has visto en los capítulos 4 y 5. Es importante tener la última versión de tidyverse para correr los geomas de ggplot que usaremos en este capítulo. Puedes actualizar a la última versión con el comando install.packages(&quot;tidyverse&quot;): library(tidyverse) Ahora necesitamos instalar el paquete sf. Para que este funcione en Mac y Linux se deben tener las versiones recientes de GDAL, GEOS, Proj.4 y UDUNTIS instaladas (Para más información ir a https://github.com/r-spatial/sf#installling). Una vez hecho esto podemos instalar el paquete sf: # devtools::install_github(&quot;robinlovelace/geocompr&quot;) # install.packages(&quot;sf&quot;) library(sf) 15.3.1 Estructura Para comenzar a trabajar, crea un proyecto de RStudio y descarga la carpeta comprimida con los shapefiles para este capítulo. El procedimiento para obtener los archivos es idéntico al que fue utilizado en el Capítulo 5, de carga de bases de datos. Para ver la estructura de los objetos de tipo sf cargamos nuestra base de datos en formato shapefile, utilizando el comando st_read() (Nota que estamos cargando el archivo “shp_brasil.shp” dentro de la carpeta “datos”) y llamando el objeto a la consola: ## Reading layer `shp_brasil&#39; from data source `/home/andres/Dropbox/Proyecto de libro/libroadp/00-datos/shp_brasil&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 28 features and 1 field ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -2200000 ymin: -3700000 xmax: 2400000 ymax: 590000 ## epsg (SRID): NA ## proj4string: +proj=poly +lat_0=0 +lon_0=-54 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs shp_brasil &lt;- st_read(&quot;datos/&quot;) shp_brasil ## Simple feature collection with 28 features and 1 field ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -2200000 ymin: -3700000 xmax: 2400000 ymax: 590000 ## epsg (SRID): NA ## proj4string: +proj=poly +lat_0=0 +lon_0=-54 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs ## First 10 features: ## estado geometry ## 1 Rondônia MULTIPOLYGON (((-977421 -89... ## 2 Acre MULTIPOLYGON (((-2179954 -8... ## 3 Amazonas MULTIPOLYGON (((-1482456 23... ## 4 Roraima MULTIPOLYGON (((-686729 582... ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 6 rows ] En la siguiente imagen detallamos las partes de nuestro shapefile con sus nombres: Figura 15.4: Estructura de un archivo de tipo sf Como se puede observar en la figura anterior, las simple features son guardados por sf como formato data.frame y con el formato adicional de clase sf, por lo que como se mencionaba anteriormente, podemos tratar a este set de datos como cualquier otro dentro del tidyverse, utilizando sus funciones, pipes (%&gt;%), etc. Aquí cada fila consiste en un simple feature y cada columna consiste en un atributo, la diferencia con el data normal es que encontramos una nueva columna de geometrías simple feature (simple feature geometry column - sfc) que contiene las geometrías simple feature (simple feature geometry - sfg), resultando en un data.frame con una columna extra para la información espacial. Podemos comprobar esto al revisar el tipo de archivo que hemos cargado con el comando class. Nota que “shp_brasil” es al mismo un archivo de tipo “sf” y de tipo “data.frame”: class(shp_brasil) ## [1] &quot;sf&quot; &quot;data.frame&quot; Ahora, si vemos más en detalle nuestros datos podemos encontrarnos con que este estándar puede implementar distintos tipos de geometría en nuestras bases de datos: Un vector numérico para un solo punto (POINT) Una matriz numérica (cada fila es un punto) para una serie de puntos (MULTIPOINT o LINESTRING) Una lista de matrices para un set de set de puntos (MULTIINESTRING, POLYGON) Una lista de listas de matrices (MULTIPOLYGON), la cual se vuelve la más utilizada cuando se representan datos geográficos como la forma y ubicación de países u otras unidades administrativas de estos. Una lista de cualquiera de los elementos mencionados anteriormente (GEOMETRYCOLLECTION) Utilizando el comando st_geometry podemos ver las geometrías que incluye nuestro shapefile: st_geometry(shp_brasil) ## Geometry set for 28 features ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -2200000 ymin: -3700000 xmax: 2400000 ymax: 590000 ## epsg (SRID): NA ## proj4string: +proj=poly +lat_0=0 +lon_0=-54 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs ## First 5 geometries: ## MULTIPOLYGON (((-977421 -892385, -975932 -89366... ## MULTIPOLYGON (((-2179954 -836549, -2081720 -877... ## MULTIPOLYGON (((-1482456 230568, -1478207 21450... ## MULTIPOLYGON (((-686729 582625, -684216 581302,... ## MULTIPOLYGON (((883666 -122169, 883026 -123338,... Una vez cargado nuestro shapefile podemos empezar a generar mapas. Nota que la forma para hacer esto está basada en ggplot; solo tenemos que seleccionar nuestra base de datos y agregar el geoma geom_sf: ggplot(data = shp_brasil)+ geom_sf() Figura 10.5: Mapa de los estados de Brasil 15.4 Manejo de Datos Espaciales Ya vimos como cargar shapefiles con el st_read. Ahora que ya tenemos el archivo cargado, procederemos a ver que tipo de modificaciones podemos hacerle para generar nuevos datos a partir de nuestra información, así como incorporar datos de otras bases a nuestro shapefile. 15.4.1 Modificaciones Existen dos principales maneras en que podemos realizar modificaciones a nuestra base georreferenciada. La primera consiste en aplicar las técnicas que hemos aprendidos ocupando el tidyverse, mientras que la segunda contempla ocupar funciones incorporadas en el paguete sf. En este paquete todas las funciones comienzan con st_ para que se pueda captar fácilmente por la herramienta de completition de Rstudio. Estas funciones se utilizan principalmente para transformar y realizar operaciones geográficas. En esta sección combinaremos ambas técnicas para generar nuevas variables y datos asociados a nuestra base. 15.4.1.1 Filtrar y seleccionar por unidades geográficas Una de las primeras funciones que podemos utilizar las funciones de dplyr para seleccionar datos dentro de nuestra base. Así, por ejemplo, podemos ocupar filter para seleccionar ciertos estados de Brasil, como por ejemplo el estado de São Paulo: shp_brasil %&gt;% filter(estado == &quot;São Paulo&quot;) ## Simple feature collection with 1 feature and 1 field ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 92000 ymin: -2800000 xmax: 1e+06 ymax: -2200000 ## epsg (SRID): NA ## proj4string: +proj=poly +lat_0=0 +lon_0=-54 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs ## estado geometry ## 1 São Paulo MULTIPOLYGON (((719692 -271... Esto es más útil cuando lo ocupamos de manera combinada con ggplot, para elegir graficar solo algunos polígonos. Así, podemos graficar el estado de São Paulo: ggplot(data = shp_brasil %&gt;% filter(estado == &quot;São Paulo&quot;)) + geom_sf() Figura 9.1: Mapa del estado de São Paulo Un desafío común que solemos encontrar cuando trabajamos con shapefiles de países completos es la existencia de regiones o zonas insulares que forman parte del país administrativamente pero se encuentras aisladas geográficamente, como por ejemplo la Isla de Pascua en Chile. Por diversos motivos, el acceso a datos sobre estas regiones puede ser más limítado, y suelen dejarse fuera de los análisis. Por esto, nos interesa sacar estas regiones de nuestros shapefile. En Brasil, el Distrito Estadual de Fernando de Noronha, un archipielago de 21 islas ubicado en el Océano Atlántica, corresponde a este tipo de casos (posiblemente lo puedes identificar como un pequeño punto en la parte superior derecha de los mapas anteriores). Podemos eliminar este tipo de datos de nuestro shapefile fácilmente utilizando, de nuevo, la función filter. shp_brasil &lt;- shp_brasil %&gt;% filter(estado != &quot;Distrito estadual de Fernando de Noronha (PE)&quot;) Esto se ve reflejado en los mapas que haremos a continuación: ggplot(data = shp_brasil)+ geom_sf() Figura 10.8: Mapa de los Estados de Brasil sin el distrito estadual de Fernando de Noronha 15.4.1.2 Generar centroides Otra opción es generar una nuevas variables asociadas a nuestras unidades, utilizando el comando mutate. Una acción común es la de generar lo que se denomina “centroides”, es decir, puntos ubicados en el centro de nuestras unidades. Para generar los centroides debemos crear las variables centroid, coords, coords_xy coords_yasociadas a nuestras geometrías, utilizando los comandos mapy map_dbl del paquete purr y los comandos st_centroid y st_coordinates del paquete sf: shp_brasil &lt;- shp_brasil %&gt;% mutate(centroid = map(geometry, st_centroid), coords = map(centroid, st_coordinates), coords_x = map_dbl(coords, 1), coords_y = map_dbl(coords, 2)) head(shp_brasil) ## Simple feature collection with 6 features and 5 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -2200000 ymin: -1500000 xmax: 880000 ymax: 590000 ## epsg (SRID): NA ## proj4string: +proj=poly +lat_0=0 +lon_0=-54 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs ## estado centroid coords coords_x coords_y ## 1 Rondônia -967595, -1219996 -967595, -1219996 -967595 -1219996 ## geometry ## 1 MULTIPOLYGON (((-977421 -89... ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 5 rows ] Una vez generadas estas variables podemos graficarlas con ggplot y el paquete ggrepel para generar los textos: library(ggrepel) ggplot(data = shp_brasil) + geom_sf()+ geom_text_repel(mapping = aes(coords_x, coords_y, label = estado), size = 4, min.segment.length = 0) Figura 8.4: Mapa de Brasil con nombres de los estados 15.4.1.3 Generar nuevas unidades con st_union Aún otra opción interesante es la de generar nuevas variables que agrupen a varias de nuestras unidades geográficas. Al hacer esto, estaremos efectivamente generando nuevas unidades geográficas que van más allá de la información originalmente contenida en nuestro shapefile. Por ejemplo, en 1969 el Instituto Brasileiro de Geografia e Estatística dividió el país en 5 regiones que agrupan a los 27 estados del país. Dado que esta es una división con fines académicos y no es reconocida en términos político-administrativos, no es posible encontrar archivos de tipo shapefile que muestren estas regiones. Sin embargo, utilizando mutate y case_when, podemos fácilmente generar nosotros mismos esta categoría: shp_brasil &lt;- shp_brasil %&gt;% mutate(region = case_when(estado %in% c(&quot;Goiás&quot;,&quot;Mato Grosso&quot;,&quot;Mato Grosso do Sul&quot;, &quot;Distrito Federal&quot;) ~ &quot;Centro-Oeste&quot;, estado %in% c(&quot;Acre&quot;,&quot;Amapá&quot;,&quot;Amazonas&quot;,&quot;Pará&quot;,&quot;Rondônia&quot;, &quot;Roraima&quot;,&quot;Tocantins&quot;) ~ &quot;Norte&quot;, estado %in% c(&quot;Alagoas&quot;,&quot;Bahia&quot;,&quot;Ceará&quot;,&quot;Maranhão&quot;,&quot;Paraíba&quot;, &quot;Pernambuco&quot;,&quot;Piauí&quot;,&quot;Rio Grande do Norte&quot;, &quot;Sergipe&quot;) ~ &quot;Nordeste&quot;, estado %in% c(&quot;Espírito Santo&quot;,&quot;Minas Gerais&quot;,&quot;Rio de Janeiro&quot;, &quot;São Paulo&quot;) ~ &quot;Sudeste&quot;, estado %in% c(&quot;Paraná&quot;,&quot;Rio Grande do Sul&quot;,&quot;Santa Catarina&quot;) ~ &quot;Sul&quot;)) Una vez generada esta variable, la podemos incorporar a nuestro mapa: ggplot(data = shp_brasil)+ geom_sf(aes(fill = region)) Figura 8.6: Mapa de los estados de Brasil según región Mejor aún, podemos ocupar esta categoría para generar nuevas geometrías con group_by, summarise y st_union: shp_brasil_regiones &lt;- shp_brasil %&gt;% group_by(region)%&gt;% summarise(geometry = st_union(geometry)) %&gt;% ungroup() Podemos graficar este resultado con ggplot: ggplot(shp_brasil_regiones) + geom_sf(aes(fill = region)) Figura 15.5: Mapa de las regiones de Brasil Nota que esto no genera objetos completamente “lisos”, es decir, todavía tienen se observan algunas líneas en su interior, probablemente ocasionadas debido los polígonos de nuestro shapefile no se superponen de manera perfecta. Esta es una dificultad común al hacer este tipo de operaciones y puede ocurrir hasta al trabajar con shapefiles más sofísticados. Pese a estas dificultades, este tipo de operaciones puede ser muy útil al, por ejemplo, elaborar distritos electorales compuestos de varias provincias o municipalidades, por ejemplo. 15.4.2 Crear nuevos shapefiles con st_write Podemos guardar este nuevo shapefile con la función st_write, en la que solo tenemos que seleccionar el objeto que deseamos guardar y la ruta en que queremos que se guarde. En este caso guardaremos el archivo “shp_brasil_regiones.shp” en una carpeta del mismo nombre. Esto genera automáticamente no solo el .shp sino que también todos los otros archivos que componen el shapefile: st_write(shp_brasil_regiones,&quot;datos/shp_brasil_regiones/shp_brasil_regiones.shp&quot;) Advertencia: El comando st_write no puede sobrescribir archivos ya existentes, e intentar hacer esto reportará automáticamente un error. Si deseas modificar un shapefile que ya has generado previamente, deberás borrarlo manualmente de su carpeta antes de generar los nuevos archivos. 15.4.3 Incorporar datos de otras bases con merge Primero vamos a ver como incorporar datos de otras bases de datos a nuestro shapefile, ya que usualmente vamos a querer conocer atributos de nuestros lugares geográficos para generar análisis exploratorios o de inferencia estadística. En primer lugar, vamos a cargar la base de datos de Freire (previamente hemos eliminados algunos datos para simplificar la base). Podemos hacer esto con facilidad, utilizando el paquete del libro: library(paqueteadp) data(crimen_brasil) Ahora se ha cargado la base adecuadamente, como podemos ver utilizando head() para obtener sus seis primeras observaciones: head(crimen_brasil) ## # A tibble: 6 x 6 ## codigo estado anio tasa_homicidios gini poblacion_extrema_pobreza ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12 Acre 1990 15.8 0.544 48965 ## 2 12 Acre 1991 25.1 NA NA ## 3 12 Acre 1992 24.7 0.560 60910 ## 4 12 Acre 1993 26.1 0.553 57492 ## 5 12 Acre 1994 19.7 NA NA ## 6 12 Acre 1995 22.6 0.582 38659 Notarás que nuestra base contiene la variable estado. Esta misma variable está presente en nuestro shapefile: shp_brasil$estado ## [1] Rondônia Acre Amazonas Roraima Pará Amapá Tocantins ## [8] Maranhão ## [ reached getOption(&quot;max.print&quot;) -- omitted 19 entries ] ## 28 Levels: Acre Alagoas Amapá Amazonas Bahia ... Tocantins Previamente nos hemos asegurado de que las observaciones estén codificadas de la misma manera tanto en la base como en el shapefile. De esta forma podemos juntar las bases utilizando el comando left_join: shp_brasil_datos &lt;- shp_brasil %&gt;% left_join(crimen_brasil) ## Joining, by = &quot;estado&quot; ## Warning: Column `estado` joining factor and character vector, coercing into ## character vector Utilizando head podemos ver el resultado de esta operación: head(shp_brasil_datos) ## Simple feature collection with 6 features and 11 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -1400000 ymin: -1500000 xmax: -630000 ymax: -890000 ## epsg (SRID): NA ## proj4string: +proj=poly +lat_0=0 +lon_0=-54 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs ## estado centroid coords coords_x coords_y region codigo anio ## tasa_homicidios gini poblacion_extrema_pobreza geometry ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 6 rows ] Una vez que tenemos esta información incorporada a nuestro shapefile, estos datos nos servirán cuando empecemos a mapear variables en la sección 4. 15.5 Mapear en R Los mapas han sido históricamente la manera principal para almacenar y comunicar datos espaciales, y los objetos y sus atributos pueden ser plasmados fácilmente de manera tal que el ojo humano pueda reconocer rápidamente patrones y anomalías en un mapa bien diseñado (Spatial Analysis Online). En esta sección aprenderás a realizar mapas de distinto tipo (tanto estáticos como animados) a partir de datos geográficos y de sus atributos teniendo como base el formato de ggplot2: En primer lugar, podemos generar visualizaciones más complejas de elementos que ya hemos visto en secciones anteriores. Por ejemplo, podemos mapear cada una de nuestras unidades con un color distinto utilizando el argumento fill de ggplot: ggplot(shp_brasil)+ geom_sf(aes(fill = estado))+ guides(fill = &quot;none&quot;) Figura 15.6: Mapa de Brasil con colores por estado También podemos cambiar el color base de nuestra imagen, así como el color de las líneas divisorias entre unidades geográficas: ggplot(shp_brasil)+ geom_sf(fill = &quot;white&quot;, color = &quot;blue&quot;)+ guides(fill = FALSE, color = FALSE) Figura 6.8: Mapa de los estados de Brasil con fondo blanco y fronteras azules También podemos generar una visualización más simple si eliminamos los indicadores de coordenadas que aparecen al borde de nuestro gráfico. Podemos hacer esto con el comando coord_sf y el comando theme_void: ggplot(data = shp_brasil) + geom_sf() + coord_sf(crs = st_crs(shp_brasil), datum = NA)+ theme_void() Figura 6.9: Mapa de los estados Brasil sin coordenadas ni líneas cartesianas O incluso podemos combinar varias de estas opciones en una sola imagen. Por ejemplo, en la siguiente imagen generamos un mapa con bordes negros y colores de relleno distintos para cada estado, donde además se señaliza el punto centroide de cada unidad y se eliminan los indicadores de coordenadas: ggplot(data = shp_brasil, aes(fill = estado)) + geom_sf(color = &quot;black&quot;) + geom_text_repel(mapping = aes(coords_x, coords_y, label = estado), size = 4, min.segment.length = 0) + coord_sf(crs = st_crs(shp_brasil), datum = NA)+ theme_void()+ guides(fill = &quot;none&quot;) Figura 15.7: Mapa de Brasil con colores y nombres por estado y sin coordenadas ni líneas cartesianas 15.6 Mapeando variables Como ya hemos agregado los datos de Freire (2018) a nuestro shapefile, podemos ocupar estas variables para ejemplificar diferencias entre nuestras unidades geográficas. Así, por ejemplo, podemos mostrar el índice Gini para cada estado con el argumento fill. Los datos corresponden al promedio de los años observados: ggplot(shp_brasil_datos %&gt;% filter(gini != &quot;NA&quot;)) + geom_sf(aes(fill = gini)) Figura 6.10: Mapa de los estados de Brasil según índice Gini También podemos combinar esto con filter para seleccionar solo ciertos años de nuestra base. Por ejemplo, podemos mostrar la cantidad de personas en extrema pobreza para el año 2009 en los diferentes estados: ggplot(shp_brasil_datos %&gt;% filter(anio == 2009) ) + geom_sf(aes(fill = poblacion_extrema_pobreza)) Figura 15.8: Mapa de los estados de Brasil según población en extrema pobreza 15.6.1 Mapas animados con gganimate Cuando estamos trabajando con datos por ciertos periodos de tiempo una visualización que puede ser de utilidad es el mostrar la evolución de una variable a través de ese periodo. Para hacer esto generalmente lo que haríamos es generar varios gráficos de distintos puntos temporales y después unirlos en una sola figura. Por ejemplo, podríamos generar distintos mapas con las tasas de homicidios para cada estado desde el año 2006 hasta el año 2009 e incorporarlos todos a una misma figura con el paquete gridExtra: a &lt;- ggplot(shp_brasil_datos %&gt;% filter(anio == 2006)) + geom_sf(aes(fill = tasa_homicidios))+ labs(subtitle = &quot;Año: 2006&quot;) b &lt;- ggplot(shp_brasil_datos %&gt;% filter(anio == 2007)) + geom_sf(aes(fill = tasa_homicidios)) + labs(subtitle = &quot;Año: 2007&quot;) c &lt;- ggplot(shp_brasil_datos %&gt;% filter(anio == 2008)) + geom_sf(aes(fill = tasa_homicidios)) + labs(subtitle = &quot;Año: 2008&quot;) d &lt;- ggplot(shp_brasil_datos %&gt;% filter(anio == 2009)) + geom_sf(aes(fill = tasa_homicidios)) + labs(subtitle = &quot;Año: 2009&quot;) library(gridExtra) grid.arrange(a,b,c,d) Figura 6.12: Mapas de los estados de Brasil según tasas de homicidios Sin embargo, una opción más dinámica para mostrar estos cambios es la de mostrar los mismos datos en un mismo mapa que cambie a medida que cambie la unidad temporal específica. Al usar tan solo un mapa, esto también nos permite incorporar más unidades temporales; así, por ejemplo, podemos ver las tasas de homicidios en cada estado desde 1990 a la fecha. Para hacer esto primero debemos instalar el paquete gganimate. Como este aún no está disponible desde CRAN, debemos utilizar devtoolspara instalarlo: devtools::install_github(&#39;thomasp85/gganimate&#39;) library(gganimate) Una vez hecho esto, podemos generar el objeto utilizando ggploty el comando transition_manual. En este ultimo seleccionamos como argumento nuestra variable temporal, que en este caso corresponde a “anio”. También al seleccionar la función “{current_frame}” como parte de nuestro subtitulo hace que se muestre automáticamente el año al cambiar la imagen: e &lt;- ggplot(shp_brasil_datos)+ geom_sf(aes(fill=tasa_homicidios))+ transition_manual(anio)+ labs(subtitle = &quot;Año: {current_frame}&quot;) Una vez generado el objeto, simplemente utilizamos el comando animate para generar la animación: animate(e) Figura 14.2: Mapa animado de los estados de Brasil según tasas de homicidios También podemos guardar nuestra animación en diferentes formatos. Por ejemplo, podemos guardar nuestra animación en forma de gif con el argumento renderer y el comando gifski_renderer. Solo tenemos que seleccionar una ruta de exportación: animate(e, renderer = gifski_renderer(&quot;00-images/tasas_homicidio_brasil.gif&quot;)) Para generar otros tipos de archivos animados, podemos revisar la documentación de renderers: ?renderers 15.7 Inferencia a partir de Datos Espaciales Más allá del análisis exploratorio y descriptivo que podemos realizar con los datos espaciales, estos también son de gran utilidad para realizar inferencia sobre la relación entre distintos fenómenos. La inferencia en base a datos espaciales parte de la base de reconocer que las observaciones en el espacio no pueden ser asumidas todo el tiempo como mutuamente independientes, debido a que las observaciones que están cerca la una de la otra son la mayoría del tiempo similares, por lo que debemos prestar atención a los diferentes patrones de asociación espacial que existen en los fenómenos que estudiamos. Estos patrones espaciales (que llamamos autocorrelación espacial), miden cómo la distancia influencia una variable en particular, pudiendo ser utilizados como información relevante sobre tipos de influencia que no han sido observadas o consideradas. (Bivand, Pebesma &amp; Gomez-Rubio, 2008: 11). 15.7.1 Indicadores locales de asociación espacial (LISA) En esta sección encontrarás los mecanismos básicos para adentrarse en la correlación espacial, basándonos en los LISA (Local indicator of spatial asociation), indicadores locales de asociación espacial introducidos por Luc Anselin (1995). Estos permiten indicar la existencia de similitud (correlación) entre observaciones próximas las unas a las otras, es decir, si están agrupadas (clustered) espacialmente. Para esto Anselin indica que: El LISA para cada observación da un indicador sobre el grado de clustering espacial significativo de valores similares alrededor de esa observación La suma de los LISAs de todas las observaciones es proporcional a un indicador global de correlación espacial. Y mediante el testeo de significancia estadística para estos indicadores, se pueden identificar localidades en donde hay un grado significativo de clustering (o repulsión espacial) (Brunsdon &amp; Comber, 2015, 249). Se dice que las variables tienen una correlación espacial positiva cuando valores similares tienden a estar más cercanos entre ellos que los valores distintos (Lansley &amp; Cheschire, 2016, 77) 15.7.2 Matriz de Pesos Espaciales El primer paso para realizar este tipo de análisis es determinar el set de vecindarios para cada observación, es decir, identificar los polígonos que comparten fronteras entre ellos. Luego, debemos asignar pesos a cada relación vecina, que permiten definir la fuerza de esta relación en base a cercanía. En las matrices de peso, los vecinos son definidos por un método binario (0,1) en cada fila indicando si existe o no relación. Para hacer este tipo de análisis primero necesitamos cargar el paquete spdep y guardar las coordenadas de las unidades de nuestro mapa: library(spdep) coords &lt;- coordinates(as((shp_brasil), &#39;Spatial&#39;)) También en esta sección trabajaremos con el shapefile al cual hemos agregado datos de la base de Freire (2018), shp_brasil_datos,pero solo ocuparemos datos del último año disponible, correspondiente a 2009: shp_brasil_datos &lt;- shp_brasil_datos %&gt;% filter(anio == 2009) Existen tres criterios diferentes para calcular los vecindarios: Rook: Considera como vecinos a cualquier par de celdas que compartan alguna arista (borde). Figura 15.9: Criterio Rook Para generar este critero se utiliza el comando poly2nb del paquete spdep. Debemos fijarnos que el argumento queen este fijado en ‘FALSE’. rook_brasil &lt;- poly2nb(as(shp_brasil_datos, &#39;Spatial&#39;), row.names = shp_brasil$estado, queen = FALSE) Podemos plotear con el comando plot.nb: plot.nb(rook_brasil,coords) Y podemos graficar esto por encima de nuestro mapa con ggplot. Primero necesitamos pasar nuestras coordenadas a un dataframe con esta función de Maxwell B. Joseph. nb_to_df &lt;- function(nb, coords){ x &lt;- coords[, 1] y &lt;- coords[, 2] n &lt;- length(nb) cardnb &lt;- card(nb) i &lt;- rep(1:n, cardnb) j &lt;- unlist(nb) return(data.frame(x=x[i], xend=x[j], y=y[i], yend=y[j])) } Generamos el dataframe: rook_brasil_df &lt;- nb_to_df(rook_brasil, coords) Y ahora podemos generar el gráfico con los los geomas geom_pointy geom_segment: ggplot(shp_brasil_datos) + geom_sf()+ geom_point(data = rook_brasil_df, mapping = aes(x = x, y = y))+ geom_segment(data = rook_brasil_df, mapping = aes(x = x, xend = xend, y = y, yend = yend)) Figura 15.10: Mapa de vecinos, criterio Rook Queen: Considera como vecinos a cualquier par de celdas que compartan alguna arista o un punto. Figura 15.11: Criterio Queen Para generar este criterio igualmente utilizamos el comando poly2nb y generamos el dataframe: queen_brasil &lt;- poly2nb(as(shp_brasil_datos, &#39;Spatial&#39;), row.names = shp_brasil$estado) queen_brasil_df &lt;- nb_to_df(queen_brasil, coords) Y ahora podemos generar el gráfico, lo cual hacemos directamente sobre nuestro mapa de Brasil: ggplot(shp_brasil_datos) + geom_sf()+ geom_point(data = queen_brasil_df, mapping = aes(x = x, y = y))+ geom_segment(data = queen_brasil_df, mapping = aes(x = x, xend = xend, y = y, yend = yend)) Figura 15.12: Mapa de vecinos, criterio Queen K-nearest: Los vecindarios se generan en base la distancia entre vecinos, donde “k” se refiere al número de vecinos de una determinada locación, calculada como la distancia entre los puntos centrales de los polígonos. Se aplica usualmente cuando las áreas tienen diferentes tamaños para asegurarse que cada lugar tenga el mismo número de vecinos, independiente de cuán grande sean las áreas vecinas (Fuente: https://geodacenter.github.io/glossary.html ). En este caso utilizaremos a los seis vecinos más cercanos. Esto lo podemos hacer con el comando knn2nb. También generamos inmediatamente el dataframe: kn_brasil &lt;- knn2nb(knearneigh(coords, k = 6), row.names = shp_brasil_datos$estado) kn_brasil_df &lt;- nb_to_df(kn_brasil,coords) Y generamos el mapa con ggplot: ggplot(shp_brasil_datos) + geom_sf()+ geom_point(data = kn_brasil_df, mapping = aes(x = x, y = y))+ geom_segment(data = kn_brasil_df, mapping = aes(x = x, xend = xend, y = y, yend = yend)) Figura 15.13: Mapa de vecinos, criterio k-nearest Nota que si bien Rook y Queen generan, para nuestro caso, resultados similares en cuanto a la vecindad en nuestro mapa, el modelo k-nearest añade muchas más relaciones 15.7.3 Moran’s I El Moran’s I es la estadística más utilizada para identificar autocorrelación espacial: \\[ I = \\frac{n}{\\sum_{i=1}^{n}(yi - \\bar{y})^2} \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}wij(yi - \\bar{y})(yj - \\bar{y})}{\\sum_{i=1}^{n}\\sum_{j=1}^{n}wij}\\] Esta fórmula si bien parece compleja, no es más que una versión expandida de la fórmula para computar el coeficiente de correlación, al cual se le agrega una matriz de pesos espaciales RSpatial mediante la cual se puede testear y visualizar la presencia de autocorrelación espacial. Esta estadística es utilizada para realizar un test que permite evaluar la significancia de la correlación espacial. Primero, realizaremos un test a nivel global que crea una sola medida de correlación espacial. Este test de Moran creará una medida de correlación entre -1 y 1 en donde: • 1 determina una correlación espacial positiva perfecta (que indica que nuestros datos están agrupados en clusters) • 0 significa que nuestros datos están distribuidos aleatoriamente • -1 representa autocorrelación espacial negativa (valores disímiles están cercanos entre sí). Figura 15.14: Ejemplos de Moran’s I Utilizando la simulación de Monte Carlo en donde los valores son asignados aleatoriamente a los polígionos para computar el Moran’s I, lo cual es repetido varias veces para establecer una distribución de los valores esperados. Luego de esto, el valor observado del Moran’s I es comparado con la distribución simulada para ver cuán probable es que los valores observadores puedan ser considerados como aleatorios y así determinar si existe autocorrelación espacial significativa (Rspatial) Para hacer el Moran’s I test utilizaremos el comando moran.test. La variable para la cual identificaremos su nivel de correlacion espacial es la variable de índice gini, y ocuparemos el criterio Queen. Para esto debemos generar un objeto de tipo listw(matriz de pesos) basado en el criterio Queen: queen_brasil_lw &lt;- nb2listw(queen_brasil) Con esto podemos generar el test de Moran’s I, donde seleccionamos matriz de pesos el objeto que acabamos de crear: moran.test(shp_brasil_datos$tasa_homicidios, listw = queen_brasil_lw) ## ## Moran I test under randomisation ## ## data: shp_brasil_datos$tasa_homicidios ## weights: queen_brasil_lw ## ## Moran I statistic standard deviate = 1, p-value = 0.1 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.113 -0.038 0.019 El resultado del test de Moran’s I nos muestra que existe una leve relación de correlación positiva, versus una expectativa de una leve relación negativa. Pero el test resulta no ser significativo estadísticamente al tener un valor-p menor a una significancia del 0.05. Por ende, podemos señalar que la tasa de homicidios no presenta altos grados de autocorrelación espacial cuando estamos analizando estos datos a nivel estadual en Brasil. También realizamos un scatter plot para visualizar el tipo y fuerza de la correlación espacial en la distribución de nuestros datos. Para generar el scatter plot utilizamos el comando moran.test: moran.plot(shp_brasil_datos$tasa_homicidios, listw = queen_brasil_lw) En este mapa la curva del gráfico indica el valor del Moran’s I, es decir, la medida global de autocorrelación espacial de nuestros datos. Como ya vimos en el test anterior, esta es ligeramente positiva. El eje horizontal del gráfico muestra los datos de las tasas de homicidios a nivel estatal en Brasil y el eje vertical muestra estos mismos datos, pero laggeados espacialmente. Los cuatro cuadrantes del gráfico describen el valor de la observación en relación a sus vecinos: alta-alta, baja-baja (autocorrelación espacial positiva), baja-alta o alta-baja (correlación espacial negativa), y el gráfico también puntualiza en los valores considerados como outliers en esta relación, como el Distrito Federal o Acre. (Fuente: https://geodacenter.github.io/glossary.html) Si bien el test de Moran nos permite identificar si es que existe clustering a nivel global, no nos permite identificar si existen clusters locales significantes en la variable que estamos analizando (Lansley &amp; Cheschire, 2016, 78). Es por esto que realizamos un test de Moran’s I a nivel local en donde se calculan los indicadores locales de asociación espacial para cada unidad de nuestros datos y se testea si es que esta relación es estadísticamente significativa, lo que nos entrega datos sobre los patrones geográficos de la relación de los datos espaciales y si existen desviaciones locales de los patrones globales de autocorrelación espacial. Para hacer este test de Moran’s I utilizamos localmoran. Sin embargo, en esta ocasión generamos el objeto listw inmediatamente al interior del comando, al cual le agregamos un argumento nuevo style. Nota también que no realizamos el test de inmediato, sino que generamos un objeto con él, el cual después utilizaremos para mapear la correlación espacial: lmoran_brasil &lt;- localmoran(x = shp_brasil_datos$gini, listw = nb2listw(queen_brasil, style = &quot;B&quot;)) Una vez obtenidos estos datos para cada polígono, podemos mapearlos para indicar cómo varía la correlación espacial en las zonas que estamos estudiando. Primero unimos los resultados a nuestro shapefile: mapa_moran &lt;- cbind(shp_brasil_datos, lmoran_brasil) A continuación podemos mapear los resultados con ggplot: ggplot(mapa_moran)+ geom_sf(aes(fill = Ii))+ labs(fill = &quot;Estadísitico Moran Local&quot;) Figura 15.15: Moran’s I a nivel local para distintos valores de Gini Este mapa nos permite observar la variación en la autocorrelación a lo largo del espacio, pero no es posible identificar si los patrones geográficos de autocorrelación son clusters con valores altos o bajos, lo que nos permitirá analizar el tipo de autocorrelación espacial que existe y su nivel de significancia. Para esto debemos crear un mapa de clusters LISA, que creará una etiqueta basada en los tipos de relación que se comparte con sus vecinos (alta-alta, baja-alta, insignificante, etc) en relación a los valores de la variable que estamos analizando (Gini). Para poder realizar esto, primero se deben realizar una serie de transformaciones que detallamos a continuación: #Genera el mapa de cluster LISA cuadrante &lt;- vector(mode=&quot;numeric&quot;,length=nrow(lmoran_brasil)) #Centra la variable de interés en su media m_gini&lt;- shp_brasil_datos$gini - mean(shp_brasil_datos$gini) #Centra el Moran&#39;s local en la media m_lmoran &lt;- lmoran_brasil[,1] - mean(lmoran_brasil[,1]) #Umbral de significancia signif &lt;- 0.1 #Crea el cuadrante de datos cuadrante[m_gini &gt;0 &amp; m_lmoran&gt;0] &lt;- &quot;Alto-Alto&quot; cuadrante[m_gini &lt;0 &amp; m_lmoran&lt;0] &lt;- &quot;Bajo-Bajo&quot; cuadrante[m_gini &lt;0 &amp; m_lmoran&gt;0] &lt;- &quot;Bajo-Alto&quot; cuadrante[m_gini &gt;0 &amp; m_lmoran&lt;0] &lt;- &quot;Alto-Bajo&quot; cuadrante[lmoran_brasil[,5]&gt;signif] &lt;- &quot;Insignificante&quot; A continuación podemos generar el gráfico con ggplot. Nota que utilizamos `scale_fill_manual&quot; para cambiar los colores de modo que reflejen la intensidad de la autocorrelación: ggplot(shp_brasil_datos, aes(fill = cuadrante))+ geom_sf()+ labs(fill = &quot;Cuadrante&quot;)+ scale_fill_manual(values=c(&quot;red&quot;, &quot;lightblue&quot;, &quot;white&quot;)) Figura 15.16: Patrones Geográficos de Clusterización para distintos valores de Gini Este mapa nos entrega una mayor información sobre los patrones geográficos de autocorrelación espacial. Este mapa nos muestra si es que existen clusters, es decir, regiones en donde en su núcleo existe autocorrelación espacial positiva, por ende, nos muestra regiones clusterizadas más que lugares individuales. Es importante señalar que estos mapas no son significativos, sino que nos permiten encontrar lugares o relaciones que pueden ser interesantes para un análisis posterior. Podemos interpretar en este caso que en la mayor parte de Brasil, existe una correlación espacial insignificante para los valores de Gini, es decir, su valor no está influenciado por la proximidad de otros valores similares. Sin embargo, en algunos estados del noreste de Brasil podemos observar que existe un clusters en donde se concentran altos valores de Gini que tienen vecinos también con altos valores en esa misma variable (hot spots), indicando que esas regiones contribuyen significativamente hacia una autocorrelación espacial positiva a nivel global. Por otro lado, en los estados que están coloreados con celeste encontramos clusters que concentran valores bajos de Gini en donde sus vecinos poseen altos valores de la misma variable y, por ende, contribuyen significativamente a una autocorrelación espacial negativa a nivel global (ya que esta está ocurre cuando valores disímiles son próximos entre sí). Ejercicios antes de continuar al próximo capítulo - Desde el Repositorio de Datos Espaciales de la NYU baja un shapefile de América Latina y cárgalo a R. Utilizando left_join() agrégale datos por país de variables como el gini e el Indice de Desarrollo Humano a partir de la base datos extendida de V-Dem. Crea un mapa con el valor de 2015 para una de estas variables y representalo en forma de mapa con geom_sf. ¿Te animas a crear el mismo mapa para varios años usando gganimate? "]
]
