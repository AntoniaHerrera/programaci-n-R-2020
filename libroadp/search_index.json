[
["index.html", "AnalizaR Datos Políticos (v1.0) Introducción Inicio ¿Cómo contribuir? Prefacio Qué esperar del libro Estructura del libro ¿Cómo utilizar el libro en un curso de métodos? Prerrequisitos", " AnalizaR Datos Políticos (v1.0) Francisco Urdinez y Andrés Cruz Labrín (editores) 2020-08-17 Introducción Inicio AnalizaR Datos Políticos es un libro que podrás usar como manual de referencia cuando estés quebrandote la cabeza usando R. Por eso, decidimos que sea más aplicado que teórico, y hemos considerado tanto temas de ciencia política como de relaciones internacionales. De ahí el subtítulo del libro: “Manual aplicado para politólogos y relacionistas internacionales en R”. Una gran ventaja del libro es que utiliza para cada tarea la opción más actualizada y sencilla disponible en la red. ¡Además, ocupa cada vez que es posible el tidyverse, el grupo de paquetes que ha revolucionado el uso de R recientemente por su sencillez! Construido gracias a la tecnología de libro digital llamada bookdown e inspirado por la filosofía de software libre y código abierto, este libro es de uso libre y gratuito. Esto garantiza que sus contenidos sean reproducibles y accesibles públicamente para personas de todo el mundo. La versión del libro que estás leyendo ahora (1.0) se construyó en abril de 2020 luego de dos años de trabajo. A medida que el libro reciba retroalimentación de los usuarios iremos actualizando los contenidos. Este libro digital se encuentra bajo una licencia Creative Commons Atribución-NoComercial-SinDerivadas 4.0 Internacional. ¿Cómo contribuir? bookdown hace que la edición de un libro sea tan fácil como editar un wiki, siempre que tengas una cuenta de GitHub. Puedes proponer cambios en el libro desde el foro “Issues” en el repositorio del libro en GitHub, que contiene todo el código utilizado para generarlo. Adicionalmente, si tienes más experiencia con GitHub y git, también puedes proponer pull requests. Haciendo click en el ícono de edición, como se aprecia en la siguiente figura, puedes rápidamente crear un fork del repositorio y comenzar a editar. ¡Creemos que las contribuciones –a través de issues o pull requests– nos ayudarán muchísimo a mejorar el libro! Figura 0.1: Botón de ‘Editar’ en cada capítulo Adicionalmente, si encuentras el libro útil, por favor apóyalo de alguna de estas formas: Leyéndolo. Recomendándolo o compartiéndolo en redes sociales. Citando la versión del libro que hemos publicado en inglés: Urdinez, F. and Cruz, A. (2020). Political Data Science Using R: A Practical Guide. CRC Press. Si deseas, puedes recomendar a la biblioteca de tu universidad que compre la versión en papel del libro. Figura 0.2: Tapa del libro editado en inglés por CRC Press Sobre los autores y autoras Este libro está editado por Francisco Urdinez, Profesor Asistente del Instituto de Ciencia Política de la Pontificia Universidad Católica de Chile, y Andrés Cruz, Instructor Adjunto de Análisis de Datos en la Escuela de Ciencia Política de la Universidad Diego Portales, Chile. La mayoría de los autores que contribuyeron con capítulos a este volumen son politólogos afiliados al Instituto de Ciencia Política de la Pontificia Universidad Católica de Chile, y muchos son investigadores y colaboradores del Instituto de la Fundación Millennium Data, institución que tiene como objetivo la recolección, depuración y análisis de datos públicos para apoyar políticas públicas. Andrew Heiss está afiliado al Departamento de Gestión y Política Pública de Georgia State University y se unió a este proyecto contribuyendo con un capítulo sobre la inferencia causal. Andrew suele compartir mucho de su trabajo en R en su sitio web. Sobre todo, todos los autores son usuarios entusiastas de R. Prefacio Este libro nació haciendo análisis de datos políticos. Es decir, es hijo de la praxis. Por ello su naturaleza es aplicada, y tiene su foco puesto en ser una caja de herramientas para el lector. AnalizaR Datos Políticos está pensado para ser un manual de referencia que podrá ser consultado tanto por un estudiante universitario viviendo en Bogotá, como por un consultor político viviendo en México D.F. o un o funcionario público en Brasilia, todos con la necesidad de transformar sus bases de datos en conclusiones sustantivas y fácilmente interpretables. Trabajando juntos en la cátedra de Análisis Cuantitativo de Datos II del Instituto de Ciencia Política de la Universidad Católica de Chile encontramos que ni aquí, ni en otras universidades de la región, había material didáctico y aplicado hecho en casa para enseñar a nuestros alumnos de ciencia política cómo extraer conclusiones a partir de datos duros. Todo el material utilizado en nuestra cátedra era publicado en inglés, por politólogos anglosajones trabajando en universidades anglosajonas. Por ello AnalizaR Datos Políticos tiene como público imaginario al politólogo latinoamericano, ya sea alumno de pregrado o posgrado, o ya en el mercado. Hemos querido que nuestro libro esté disponible en español y portugués, y esto lo hace extensible a otras universidades de realidades similares fuera de América Latina, como en los países lusófonos de África y en la región ibérica. Los autores son todos politólogos que tuvieron que enfrentarse a problemas aplicados y tuvieron curvas de aprendizaje, más o menos empinadas, en el uso de R. Algunos han migrado de otros software de análisis de datos, otros han comenzado su experiencia directamente en R. Algunos son hábiles usuarios, otros, usuarios funcionales, pero todos tienen en común que tienen conocimiento aplicado que será de utilidad a quien quiera tener material de apoyo. Las universidades latinoamericanas han hecho grandes esfuerzos en que sus alumnos de politología se alfabeticen en herramientas estadísticas y de análisis de datos, algo que hasta hace diez años era algo poco frecuente. Hoy las cinco mejores universidades de la región, según el ranking de Times Higher Education, tienen cursos de análisis cuantitativo de datos en sus programas de ciencia política. Algunos departamentos, como el Departamento de Ciencia Política de la Universidad de São Paulo, que co-organiza la escuela de verano de IPSA en métodos, el Instituto de Ciencia Política de la Universidad Católica de Chile, que organiza su escuela de verano en Métodos Mixtos, o la División de Estudios Políticos del CIDE, han hecho esfuerzos por exponer a sus alumnos a profesores norteamericanos y europeos que cuentan con muchas décadas de tradición cuantitativa en sus programas. Lo bueno es que poco a poco comienzan a aparecer metodólogos nacidos y formados en América Latina. Entendemos que, hoy por hoy, ningún politólogo puede salir al mercado laboral sin saber utilizar con holgura software de análisis cuantitativo, y es a esa demanda a la que apuntamos aquí. Ahora mismo, R es probablemente la mejor opción que el mercado provee para análisis estadístico de datos. Esto puede ser sorpresivo para un lector recién salido de una máquina del tiempo: hace diez años, o tal vez menos, R era simplemente mirado como la alternativa gratis a los programas comerciales de verdad, que sí podían realizar análisis cuantitativo serio. Sin embargo, esto ha cambiado drásticamente en los últimos años. La Figura 0.3 muestra las tendencias de búsqueda en Google en América Latina para los programas más comúnmente utilizados en ciencia. R ha pasado a ocupar un lugar en el mercado que hace 15 años le correspondía a SPSS, y los programas de nicho -como Stata y Minitab- son cada vez menos buscados. La tendencia sugiere que R será cada vez más popular en la ciencia latinoamericana, siguiendo una tendencia global. Figura 0.3: Elaborada por los autores usando el paquete ggplot2 de R, y datos extraídos de Google Trends. Los datos corresponden a promedios anuales para países latinoamericanos en el sector ‘ciencia’ El modelo de software libre en el que se basa R —con licencias de derechos de autor permisivas, que ponen prácticamente todas las herramientas en forma gratuita a disposición del público, tanto para su uso como para su reformulación— finalmente rindió frutos. Una activa comunidad de desarrolladores se ha anclado en R, añadiéndole nuevas funcionalidades que lo han dotado de elegancia, simplicidad y flexibilidad. R ya no solo brilla en la generación de modelos estadísticos, sino que hoy es hogar de un vasto universo de herramientas que permite al usuario importar, ordenar, transformar, visualizar, modelar y comunicar los datos que le interesen, sin tener que cambiar de herramienta. Es esta la novedad tecnológica que queremos acercar al lector interesado en el análisis político, con la esperanza de que contribuya a optimizar el proceso entre la pregunta que le quita el sueño (y/o le promete el pan) y su solución. Esto sin desconocer, claro está, que el beneficio inicial de R, el que estaba incluso cuando nadie quisiera usarlo si no era a regañadientes, permanece. Sabemos que el lector —y tal vez su casa de estudios— agradecerá la amable coincidencia de que el mejor software disponible en términos de calidad es también el mejor para su bolsillo. Francisco Urdinez y Andrés Cruz. Santiago de Chile, 2020. Agradecimientos Agradecemos a todos los prestigiosos académicos que permitieron usar sus bases de datos para ejemplificar nuestros capítulos. Queremos agradecer a Matías Piña, Eliana Jung, Vicente Quintero y Beatriz Urrutia por ayudar con correcciones y la edición del libro y a Daniel Alcatruz, Javiera Venegas, Santiago Olivella, Laura Levick y Carsten Schulz por sus comentarios sobre varios capítulos. Queremos agradecerle al Instituto Milenio Fundamentos de los Datos por apoyar a los alumnos de doctorado que participaron en este proyecto, así como a la Vicerrectoría de Investigación de la Pontificia Universidad Católica de Chile por apoyar su edición en la versión en inglés, publicada por CRC press como parte de R Series. Qué esperar del libro El análisis cuantitativo de datos es una de las tantas herramientas que los investigadores tenemos para abordar las preguntas que nos interesan, ya sea en el mundo profesional o en la academia (o “por amor al arte” en muy encendidas noches de viernes, por qué no). Es por esto que AnalizaR Datos Políticos tiene un fuerte énfasis en ejemplos politológicos aplicados. Utilizar ejemplos de texto trillados e idealizados sobre autitos o islas imaginarias sería una falta de respeto para el lector, a quien sabemos ávido por ocupar las herramientas de este libro en las preguntas de investigación política que le parecen importantes. Por el contrario, queremos mostrar el potencial de dichas herramientas metiendo las manos en la masa, con datos de verdad, investigaciones que colegas ya han realizado y dificultades particulares de llevar el análisis de datos a preguntas políticas. Será bueno que nos acompañe a lo largo del libro con RStudio abierto en su computador, nada mejor que aprender juntos. El libro se alimenta del avance enorme que se ha hecho en los últimos años a partir de la creación del lenguaje de tidyverse y de la publicación de R for Data Science, el cual puede ser accedido de manera gratuita en español gracias al trabajo colaborativo de sus usuarios en el siguiente link. Todo el trabajo hecho por Garrett Grolemund y Hadley Wickham por facilitar la sintaxis de R es una excelente invitación para quienes nunca han utilizado software estadístico o para quienes quieren mudarse de otras alternativas como Stata o SPSS. Estructura del libro # Capítulo Autores I. Introducción a R 1 R básico Andrés Cruz 2 Manejo de datos Andrés Cruz 3 Visualización de datos Soledad Araya 4 Carga de bases Soledad Araya and Andrés Cruz II. Modelos 5 Modelos lineales Inés Fynn and Lihuen Nocetto 6 Selección de casos basada en regresiones Inés Fynn and Lihuen Nocetto 7 Modelos de panel Francisco Urdinez 8 Modelos logísticos Francisco Urdinez 9 Modelos de supervivencia Francisco Urdinez 10 Inferencia causal Andrew Heiss III. Aplicaciones 11 Manejo avanzado de datos políticos Andrés Cruz and Francisco Urdinez 12 Minería de datos web Gonzalo Barría 13 Análisis cuantitativo de textos políticos Sebastián Huneeus 14 Análisis de redes Andrés Cruz 15 Análisis de componentes principales Caterina Labrin and Francisco Urdinez 16 Mapas y datos espaciales Andrea Escobar and Gabriel Ortiz El libro está organizado en tres secciones temáticas. Dentro de las secciones, cada capítulo se esfuerza por resolver problemas puntuales, balanceando teoría y práctica en R. Al iniciar el capítulo recomendamos literatura que te ayudará a profundizar en el tema si es que te interesó. También te invitamos a que reproduzcas los capítulos con tus propios datos a modo de ejercicio práctico. La sección I está dedicada al manejo de datos. Lo ideal es que el lector consiga algo más que mirar una base de datos con cara de “no entiendo nada”. Introduciremos R desde su instalación y aprenderemos a sacarle el jugo para obtener datos, conocerlos en profundidad, transformarlos de acuerdo a las preguntas que nos interesan y representarlos gráficamente en formas tanto funcionales como atractivas. En la sección II está el corazón del libro. Veremos cómo responder a preguntas políticas desde una perspectiva estadística —siempre podemos contestar desde nuestra intuición aunque esto suela ser menos serio y poco científico—. En general, la sección trata modelos estadísticos, que intentan explicar y predecir la variación de ciertas variables (dependientes) de acuerdo a cómo varía otras variables (independientes). Exploraremos distintos tipos de modelos de acuerdo a las distintas formas de variables dependientes que se encuentran comúnmente en la arena de lo político. Revisaremos cómo interpretar resultados y presentarlos en forma clara y atractiva, cómo elegir entre modelos competidores y cómo comprobar simplemente algunos de los supuestos estadísticos necesarios para que los modelos funcionen. Debemos notar que este no es un libro de econometría, claro está, por lo que para cada modelo haremos referencia a trabajos más avanzados en términos teóricos, con el fin de que el lector pueda profundizar por su cuenta si cree que debe utilizar algún modelo en específico para responder a sus preguntas de interés. Por último, en la sección III dejaremos el mundo ideal y nos adentraremos en la resolución de problemas. Ya sea porque un colega nos prestó su base de datos y se ve más bien como una obra de arte surrealista, o simplemente porque la dificultad de los problemas a los que nos enfrentamos deja corto lo que aprendimos al principio del libro, aquí presentaremos un popurrí de herramientas para que el lector integre en su flujo de trabajo cotidiano. Estas han sido seleccionadas desde nuestra experiencia y son cuáles creemos las más requeridas en la práctica del análisis de datos políticos. ¿Cómo utilizar el libro en un curso de métodos? Si este libro te ha interesado como recurso pedagógico, y deseas utilizarlo para estructurar tu clase de métodos cuantitativos o de análisis de datos en ciencia política, te ofrecemos dos alternativas, para diagramar un curso semestral de 10 clases, según donde se quiera poner el foco. Los capítulos de la sección II y III sugieren lecturas que son fuertemente recomendadas para dar robustez a los contenidos del libro. Recuerda que éste no es un libro de metodología, sino que es un libro aplicado de R, por lo tanto, es importante que las lecturas cubran bien la parte teórica de cada tema. Primera opción: Curso semestral de análisis de datos políticos Ésta es una diagramación para un curso donde el énfasis está puesto en que el alumno aprenda a utilizar R, y rápidamente sacar conclusiones descriptivas de los datos que se usen. Nota: antes de comenzar el curso los alumnos deben haber leído el capítulo 1 “R Básico” y tener instalado RStudio y el paquete del libro. Es importante siempre llevar scripts listos a clases para que los alumnos rápidamente se pongan a trabajar sin sufrir la codificación, que se aprenderá de a poco en casa. Otra opción a considerar es RStudio Cloud, que permite a los estudiantes trabajar directamente en el navegador. Clase 1. Utilice el Capítulo 2 para trabajar con la base de datos de aprobación presidencial hasta cubrir mutate(), pasando por arrange(), filter(), rename(), select() y skim(). Es fundamental que en esta primera clase la alumna salga de ella sintiendo que ha aprendido algo sustantivo de la base de datos. Es recomendable ofrecer el script ya redactado para evitar frustraciones con la sintaxis. Clase 2. Continúe con el Capítulo 2. Utilizando summarize() por grupos, el estudiante debe comenzar a interpretar medias, valores máximos y mínimos y compararlos entre grupos de interés. Puede pedirles crear una variable dicotómica para comparar dos grupos de interés. Haga énfasis en el potencial de combinar mutate() con if_else() y no se detenga en la idea de pivotear. Es recomendable ofrecer el script ya redactado para evitar frustraciones con la sintaxis, el alumno podrá ponerlos en práctica si se le pide hacer los ejercicios del capítulo en casa. Clase 3. Comience el Capítulo 3, introduciendo el mundo de capas de ggplot2. Ofrezca un script con la base de datos de las dos clases previas, a saber, la base de datos de aprobación presidencial, y deje la base de datos que utiliza el capítulo para que el alumno la analice en casa. Ejemplifique la idea de los mapeos estéticos con geom_boxplot(), geom_bar() y geom_line(), y combine las tres con patchwork. Con lo aprendido previamente cree variables categóricas para agrupar países, lo que a continuación puede servir para introducir la idea de las facetas. Termine la clase demostrando opciones de temas de ggplot2. Clase 4. Continúe con el Capítulo 3 a partir de geom_histogram(). Combine esta herramienta con skim() y filter(). Nuevamente, ofrezca un script redactado, pero permita que de a poco los alumnos tengan que participar activamente en la sintaxis de los comandos. Incorpore las opciones complementarias como labs() y scale_x_continuous(). Combine geom_point() y geom_smooth() para analizar la correlación entre varias variables. Invite a los alumnos a sacar conclusiones de los datos y, si desea, aproveche a profundizar en el concepto de correlación y su cálculo. Clase 5. Cubra el Capítulo 4 en una sola clase. La carga de datos puede ser frustrante, es importante preparar un ejercicio práctico para llevar a la clase y enfatizar sobre la importancia de trabajar con RStudio Projects. Cree versiones en Excel y .csv de la base de aprobación presidencial y ejemplifique cómo se importan a su proyecto. Luego, repita el proceso con versiones de su base en Stata y SPSS. Para finalizar, haga a los alumnos cargar una base y responder a preguntas sobre los datos utilizando lo aprendido en las cuatro clases previas. Ahora sí, los alumnos están listos para redactar su propio código. Clase 6. Usando el contenido del capítulo 12 enseñe a los alumnos a extraer datos de Twitter utilizando APIs con rtweet. Ejemplifique el paso a paso del uso de las funciones search_tweets(), get_followers() y lookup_users() y luego deje un espacio de tiempo para que cada alumno replique este ejemplo con el tema que sea de su interés. Con lo aprendido en la clase 5 pida a los alumnos que guarden sus bases para ser usadas en la próxima clase. Clase 7. Usando el contenido del Capítulo 12 analice la base creada en la clase anterior con el paquete tidytext, introduciendo análisis de frecuencias de palabras y tf-idf. Retome lo cubierto en clase dos sobre combinar mutate() con if_else() para agrupar tweets por variables de interés. A esta altura del curso es importante que el alumno intente escribir código por sí solo, y es bueno exigir el uso de opciones estéticas en las figuras creadas con ggplot2. Clase 8. Profundice en análisis de texto, centrándose en modelamientos de tópicos sencillos. Demuestre cómo utilizar en conjunto los paquetes tidytext, lubridate y stm con un ejemplo creado previamente, y luego invite a los alumnos a aplicar estas funciones en su base de datos. Clase 9. En esta clase retomaremos lo visto en clases 3 y 4 para avanzar sobre el contenido del capítulo 16. Parta explicando la lógica de un shapefile, y ejemplifique como visualizarla con geom_sf(). Para una primera experiencia con el tema recomendamos un shapefile de América Latina con sus fronteras políticas. Usando la base de aprobación presidencial demuestre cómo agregar variables a su shapefile utilizando left_join(). En este curso debe asegurarse que la variable de unión coincida lo mejor posible en ambas bases. No querremos cubrir temas de fuzzy join, por ejemplo. Clase 10. En esta clase cubra ideas de LISAs y clusters espaciales utilizando spdep. Recomendamos también cubrir la utilidad de geom_point(), que puede ejemplificarse utilizando el paquete pinochet y rnaturalearthhires. A modo de trabajo final del curso sería bueno pedir un R Markdown que sintetice este ejercicio y que incorpore la mayor cantidad de herramientas aprendidas en las clases previas. Segunda opción: Curso semestral de introducción a métodos cuantitativos Ésta es una diagramación para un curso donde el énfasis está puesto en Mínimos Cuadrados Ordinarios. *antes de comenzar el curso los alumnos deben haber leído el capítulo 1 “R Básico” y tener instalado RStudio y el paquete del libro. Es importante siempre llevar scripts listos a clases para que los alumnos rápidamente se pongan a trabajar sin sufrir la codificación, que se aprenderá de a poco en casa. Clase 1. Utilice el Capítulo 2 para trabajar con la base de datos de aprobación presidencial hasta cubrir mutate(), pasando por arrange(), filter(), rename(), select() y skim(). Utilizando summarize() por grupos, el alumno debe comenzar a interpretar medias, valores máximos y mínimos y compararlos entre grupos de interés. Puede pedirles crear una variable dicotómica para comparar dos grupos de interés. Haga énfasis en el potencial de combinar mutate() con if_else() y no se detenga en pivot. Es fundamental que en esta primera clase la alumna salga de ella sintiendo que ha aprendido algo sustantivo de la base de datos. Es recomendable ofrecer el script ya redactado para evitar frustraciones con la sintaxis e introducir a los alumnos a R Markdown. Clase 2. Comience el Capítulo 3, introduciendo el mundo de capas de tidyverse. Parta por geom_histogram() y combine esta herramienta con skim y filter. Nuevamente, ofrezca un script redactado, pero permita que de a poco los alumnos tengan que participar activamente en la sintaxis de los comandos. Incorpore las opciones complementarias como labs() y scale_x_continuous(). Combine geom_point() y geom_smooth() para analizar la correlación entre varias variables. Presente el concepto de mínimos cuadrados ordinarios con un ejemplo aplicado de geom_smooth(). Termine la clase demostrando opciones de themes. Clase 3. Rápidamente cubra el capítulo 4 con un ejercicio práctico para llevar a la clase y enfatizar sobre la importancia de trabajar con RStudio Projects. Cree versiones en Excel, csv de la base de aprobación presidencial y ejemplifique como se importan a su proyecto. Si el curso tiene experiencia en Stata, explique haven(). De en .csv la base de municipios del capítulo 3 a los alumnos, muestre ggcorrplot() y avance al capítulo 5 cubriendo rápidamente lm() con una regresión univariada y texreg(). Los alumnos tendrán dos semanas para trabajar en casa con la base de bienestar de Huber et al. y crear un Markdown. Clase 4. En esta clase se profundizará en el capítulo 5 partiendo por regresiones multivariadas. Explique factor(), list() y deténgase en la interpretación de coeficientes. Profundice este tema con prediction() y ejemplifique su utilidad con ggplot(). A partir de esta clase deje tiempo de clases para que los alumnos trabajen solos o en pares y redacten su propio código. Clase 5. Dedique esta clase a cubrir ajuste de modelos y presupuestos de MCO, así como el uso de transformaciones logarítmicas en variables dependientes e independientes y la implementación de errores estándar robustos. Clase 6. Con la base de datos que se haya usado para cubrir los contenidos de las clases 4 y 5 cubra los contenidos del capítulo 6 de manera aplicada, pidiendo a los alumnos que apliquen broom() para seleccionar casos a partir de regresiones. Es recomendable que los alumnos hagan en clase el ejercicio 7.C. y entreguen un Markdown con sus respuestas. Clase 7. Desarrolle el capítulo 10 sobre DAGs en una clase, introduciendo ggdag(), dagitty(), MatchIt() y utilizando los ya conocidos broom() y texreg(). Para abordar la clase recomendamos que se desarrolle un ejemplo similar al que el autor utiliza en el capítulo, y que luego se trabaje sobre la base de la clase anterior para que los alumnos diseñen su propio DAG. + Clase 8. Cubra los contenidos del capítulo 7 sobre datos en panel, donde se presente lubridate() y plm(). Dedique una parte de la clase a presentar devtools, cuyas funciones sirven para visualizar resultados y se complementa muy bien con prediction(). Cubra modelos de efectos fijos y aleatorios, phtest(), tests de raíz unitaria y correlación temporal y la incorporación de errores estándar robustos corregidos para panel. Refuerce el uso de texreg y la exportación de tablas para su uso en artículos académicos. Clase 9. Las últimas dos clases pueden ser dedicadas a ofrecer al alumno herramientas para complementar lo que ha aprendido hasta este punto. Utilice el capítulo 15 para introducir a los alumnos a la creación de índices que puedan ser incorporados en modelos tanto como variables dependientes como independientes. Clase 10. Finalice el curso usando el capítulo 11 con una discusión sobre estandarización de códigos y unión de bases de datos mediante left_join() y demostrando el uso de countrycode, stringdist, inexact. Pida a los alumnos hacer un ejercicio en clases. Luego discuta datos faltantes e imputación presentando naniar y mice y pida a los alumnos hacer un ejercicio en clases el paquete. Prerrequisitos Este libro está pensado para alumnos que más que brillantes son motivados: el análisis cuantitativo de datos exige sobre todo tenacidad y curiosidad. Es altamente deseable que el lector tenga nociones básicas de matemática, probabilidad y/o estadística universitaria antes de leer este libro, aun cuando nos esforzamos por mantenerlo lo más simple que pudimos en dichas materias. En términos de hardware, prácticamente cualquier computador moderno con acceso a internet será suficiente, pues las herramientas que utilizaremos son más bien livianas. Todo el software que utilizaremos es gratuito. En el Capítulo 1, de R Básico, se explica cómo instalar todo el software que se utilizará en el libro. Si ya sabes un poco sobre el funcionamiento de R, debes saber que en la subsección 1.4.3.3 instalaremos dos paquetes que luego asumiremos instalados en el resto del libro: tidyverse y paqueteadp (un paquete especialmente creado para apoyar al libro). "],
["basic-r.html", "Capítulo 1 R Básico 1.1 Instalación 1.2 Consola 1.3 Script 1.4 Objetos (y funciones)", " Capítulo 1 R Básico Andrés Cruz1 1.1 Instalación R R es un lenguaje de programación desarrollado especialmente para el análisis estadístico. Una de sus principales características, como se sugirió en el prefacio, es que es de fuente abierta: además de ser libre, significa que las licencias que protegen legalmente a R son permisivas. Bajo estas licencias, miles de desarrolladores de todo el mundo han contribuido con sus dos céntimos a la usabilidad y el atractivo de R. En AnalizaR Datos Políticos utilizaremos esta diversidad para nuestro beneficio. Instalar R es fácil, tanto si usas Windows, Mac o Linux. Sólo tienes que acceder a https://cran.r-project.org/ y seguir las instrucciones de descarga e instalación. RStudio Como dijimos, el R es un lenguaje de programación. Para decirlo de forma coloquial, es una forma ordenada de pedir al ordenador que realice ciertas operaciones. Esto significa que podemos usar R exclusivamente desde una consola o un terminal -las pantallas negras que los hackers usan en las películas-. Aunque esto tiene cierto atractivo (entre ellos, asemejarse a un hacker), en general queremos interfaces más amigables. Aquí es donde aparece RStudio en nuestra vida, el programa más popular para usar R. Una vez instalado, todo nuestro análisis ocurrirá dentro de RStudio, que también es de código abierto y es actualizado regularmente por un equipo de programadores. Para instalar RStudio, es necesario tener R. La descarga e instalación es accesible para Windows, Mac y Linux. El enlace es https://www.rstudio.com/products/rstudio/download/#download. Ejercicio 1A. ¡Esperaremos mientras instalas R y RStudio! Adelante, lo necesitarás para trabajar con nosotros durante todo el libro. Si pudiste descargar e instalar R y RStudio, sólo tienes que abrir este último para empezar a trabajar. Encontrarás una pantalla como la siguiente figura 1.1. Figura 1.1: Los cuatro paneles de la interfaz básica de RStudio. RStudio está dividido en cuatro paneles (consola, guión, objetos y miscelánea), que abordaremos a continuación. La idea de esta sección es familiarizar al lector principiante con la base de R. Además, le recomendamos que edite algunas configuraciones de RSutdio antes de comenzar, lo que en nuestra experiencia mejorará su experiencia de aprendizaje2: General &gt; Desmarcar “Restaurar .RData en el espacio de trabajo al inicio” General &gt; Guardar el espacio de trabajo en .RData al cerrar &gt; Nunca 1.2 Consola El panel inferior izquierdo de RSutdio es nuestro espacio de comunicación directa con el ordenador, donde le exigimos que realice tareas específicas, utilizando el “lenguaje R”. Llamaremos a estas demandas comandos. Intentemos usar o “ejecutar” un comando que realice una operación aritmética básica: 2 + 2 ## [1] 4 Un consejo para la consola es que con las teclas de flecha arriba y abajo puedes navegar por el registro de comandos utilizados. 1.3 Script El panel superior izquierdo de RStudio puede describirse como un “registro de comandos”. Aunque la consola puede ser útil para algunos comandos, los análisis complejos requerirán que llevemos un registro de nuestro código. Para escribir un nuevo script es tan fácil como presionar Ctrl + Shift + N o ir a Archivo &gt; Nuevo archivo &gt; R Script (usar atajos de teclado es una gran idea, y no sólo por el “factor hacker”). La pantalla blanca de un nuevo script es similar a un bloc de notas en blanco, con la característica de que cada línea debe ser pensada como un comando. Note que escribir un comando en el script y presionar “Enter” no hace nada más que un salto de párrafo. Para ejecutar el comando en una línea, tienes que presionar Ctrl + Enter (si tienes Mac, Cmd + Enter). Es posible seleccionar varias líneas/comandos a la vez y ejecutarlos todos con Ctrl + Enter. Escribir sólo código en nuestros scripts no es suficiente, ya que normalmente también queremos escribir comentarios explicativos. Esto no sólo es relevante para el trabajo en grupo (el código extranjero puede ser ininteligible sin una guía clara), sino que también denota atención para tu futuro “tú”. En varias ocasiones, tenemos que comprobar el código que escribimos hace un par de meses, sin entender nada, y pensar cosas desagradables sobre nosotros mismos. Al escribir los comandos, R reconoce que todo lo que sigue a un signo numérico (#) es un comentario. Por lo tanto, hay dos maneras de escribir comentarios, como “comandos estériles” o como un apéndice de comandos funcionales: # Este es un comando estéril. ¡R sabe que es sólo un comentario! 2 + 2 # Este es un apéndice-comando, no modifica el código ## [1] 4 Para guardar un Script, todo lo que tienes que hacer es presionar “Ctrl + S” o hacer clic en “Archivo &gt; Guardar”. 1.4 Objetos (y funciones) Este es el panel superior derecho de RStudio. Aunque tiene tres pestañas (“Ambiente”, “Historia” y “Conexiones”), la gran estrella es “Ambiente”, que funciona como un registro de los objetos que creamos mientras trabajamos. Una de las principales características de R es que permite almacenar objetos y luego ejecutar comandos con ellos. La forma de crear un objeto es usando una flecha &lt;-, para que el nombre_del_objeto &lt;- contenido. Llamaremos a esto una asignación. Por ejemplo: objeto_1 &lt;- 2 + 2 Después de ejecutar esto, un nuevo objeto aparecerá en el panel de “Ambiente”, objeto_1. Este contiene el resultado de 2+2. Es posible preguntarle a R cuál es el contenido de un objeto con sólo ejecutar su nombre como si fuera un comando: objeto_1 ## [1] 4 Crucialmente, los objetos pueden ser insertados en otros comandos refiriéndose a su contenido. Por ejemplo: objeto_1 + 10 ## [1] 14 También es posible reasignar los objetos: si nos aburrimos de objeto_1 como un 4, podemos asignarle el valor que queramos. Los valores de texto (caracteres) también son válidos, tienen que ser escritos entre comillas: objeto_1 &lt;- &quot;democracia&quot; objeto_1 ## [1] &quot;democracia&quot; Borrar un objeto también es una tarea sencilla. Aunque puede sonar como si perdiéramos nuestro trabajo duro, tener una ficha de “Ambiente” limpia y fácil de leer a menudo vale la pena. Para ello, tenemos que usar la función rm(nombre_del_objeto). También puedes usar rm(list=ls()) para borrar todos los objetos. 1.4.1 Vectores Hasta ahora hemos aprendido los objetos más básicos de R, que contienen un único valor. Los vectores son objetos más complejos3. Crear un vector es simple, sólo tenemos que insertar sus componentes entre una c(), separados por comas: vector_1 &lt;- c(15, 10, 20) vector_1 ## [1] 15 10 20 Una necesidad básica al crear vectores es la inserción de secuencias de números. Una forma sencilla de hacerlo es con colones (:). Por ejemplo, examinemos el siguiente vector: vector_2 &lt;- c(9, 7:10, 2, 14) vector_2 ## [1] 9 7 8 9 10 2 14 Podemos seleccionar elementos específicos de un vector utilizando sus posiciones: vector_2[2] # nos da el segundo elemento ## [1] 7 vector_2[4:6] # nos da el cuarto, quinto y sexto elemento. ## [1] 9 10 2 1.4.2 Functions Mira el siguiente ejemplo de comando: 2 + sqrt(25) - log(1) # equivalente a 2 + 5 + 0 ## [1] 7 R interpreta que sqrt(25) es la raíz cuadrada de 25, mientras que log(1) es el logaritmo natural de 1. Tanto sqrt() como log() son funciones de R. En términos sencillos, una función es un procedimiento que puede ser delineado como sigue: Figura 1.2: Esquema de la función. Adaptado de Wikimedia Commons: https://commons.wikimedia.org/wiki/File:Function_machine2.svg. sqrt() asume un valor numérico como entrada y entrega su raíz cuadrada como salida. log() asume esa misma entrada, pero entrega su logaritmo natural. c(), una función que usamos previamente, asume diferentes valores únicos como entradas y entrega un vector que los contiene. Es debido a los vectores que las funciones en R comienzan a brillar y se alejan de las cualidades básicas de una calculadora (eso, a grandes rasgos, es lo que hemos visto hasta ahora en R, nada impresionante). Examinemos otras funciones que extraen información útil de un vector. ¿Qué hace cada una de ellas? mean(vector_1) # media ## [1] 15 median(vector_1) # median ## [1] 15 sd(vector_1) # desviación estándar ## [1] 5 sum(vector_1) # sum ## [1] 45 min(vector_1) # valor mínimo ## [1] 10 max(vector_1) # valor máximo ## [1] 20 log(vector_1) # logaritmo natural ## [1] 2.7 2.3 3.0 exp(vector_1) # exponencial ## [1] 3.3e+06 2.2e+04 4.9e+08 length(vector_1) # longitud (cantidad de valores) ## [1] 3 sort(vector_1) # ... ## [1] 10 15 20 Se puede deducir que sort(), la última función de la lista anterior, ordena el vector de menor a mayor: sort(vector_1) ## [1] 10 15 20 ¿Qué pasaría si quisiéramos ordenarlo desde el más grande al más pequeño? Esto nos permite introducir los argumentos, partes de las funciones que modifican su comportamiento. Ahora, añadiremos el argumento decreasing = TRUE en el comando anterior, lo que logra nuestro objetivo: sort(vector_1, decreasing = TRUE) ## [1] 20 15 10 Un concepto que tiene que ver tanto con objetos como con funciones es el de valores perdidos, un tema que exploraremos con más detalle en el capítulo 11. Las bases de datos con las que trabajamos pueden tener valores perdidos por varias razones: errores de codificación, gobiernos que ocultan información, datos aún no recogidos, entre otras. En cualquier caso, debemos tener esto en cuenta al hacer nuestro análisis R registra los valores perdidos como “NA” (no disponible). Nótese que no se trata de un valor de carácter con las letras “N” y “A”, sino de un tipo de valor distinto. Agreguemos un valor faltante a nuestro vector anterior: vector_1_con_na &lt;- c(vector_1, NA) vector_1_con_na ## [1] 15 10 20 NA ¿Cómo deberían reaccionar las funciones a este nuevo vector? Por defecto, la mayoría de las operaciones R realizadas con valores perdidos fallan (devolviendo NA), alertando que no pueden computar lo que necesitas. Por ejemplo: mean(vector_1_con_na) ## [1] NA Tal vez en algunos casos quiera hacer saber a R que debe ignorar los valores faltantes en el vector y simplemente continuar con la operación. En la mayoría de las funciones, puedes especificar esto con el argumento na.rm = TRUE: mean(vector_1_con_na, na.rm = TRUE) ## [1] 15 Otra posibilidad es hacer esta omisión de la NA ex ante, modificando el vector. Una función útil para este caso es na.omit(), que devuelve el vector sin ningún valor perdido: na.omit(vector_1_con_na) ## [1] 15 10 20 ## attr(,&quot;na.action&quot;) ## [1] 4 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; Por último, una función común para tratar con NAs es is.na(), que permite comprobar qué valores de un vector faltan, algo que será útil más adelante (por ejemplo, para filtrar bases de datos): is.na(vector_1_con_na) ## [1] FALSE FALSE FALSE TRUE Ejercicio 1B. Mira el siguiente gráfico y crea un vector con los salarios mínimos de los países de la región. ¿Qué significa? ¿Y su mediana? Figura 1.3: Salario mínimo en América Latina, enero de 2019. Adaptado de Statista: https://es.statista.com/grafico/16576/ajuste-de-los-salarios-minimos-en-latinoamerica). 1.4.3 Archivos / gráficos / paquetes / ayuda El cuadrante inferior derecho de la pantalla de RStudio contiene una mezcla de diferentes usos, que examinaremos a continuación. 1.4.3.1 Archivos y proyectos de RStudio Esta pestaña es una ventana a los archivos que tenemos en nuestro directorio de trabajo. Funciona como un pequeño gestor, y nos permite moverlos, renombrarlos y copiarlos. En cuanto a los archivos, una de las grandes innovaciones recientes de R son los proyectos de RStudio. Los desarrolladores de RStudio se dieron cuenta de que sus usuarios tenían scripts y otros archivos R esparcidos por todo el disco duro, sin ningún orden. Por eso implementaron la filosofía de “un proyecto, una carpeta”. “Un proyecto, una carpeta” es tan importante como suena: la idea es que cada proyecto en el que trabajamos sea autosuficiente, que incluya todo lo que necesitamos para trabajar (scripts, bases de datos, etcétera). Los proyectos se pueden gestionar desde la esquina superior derecha en R. ¿Viste las tres pestañas, “Ambiente”, “Historia” y “Conexiones”? Bueno, mira un poco más arriba y verás el logo de RStudio Projects. Aquí tienes que tener cuidado y tener en cuenta que al crear o abrir un nuevo proyecto se reiniciará tu sesión de R, borrando todo el trabajo que no hayas guardado. Como aún no ha creado un proyecto, su sesión debería decir “Proyecto”: (Ninguno)“. Al hacer clic en”Nuevo proyecto\", aparecen tres opciones: Figura 1.4: Crear un nuevo proyecto en RStudio. Una vez que creas un proyecto y empiezas a trabajar en él, todos los enlaces a los archivos serán locales. Por ejemplo, si la carpeta del proyecto contiene una subcarpeta data con un archivo ejemplo.csv, la referencia al archivo será simplemente data/ejemplo.csv. Recuerde el lema: “un proyecto, una carpeta”. Le recomendamos que cree un proyecto de RStudio para cada capítulo del libro que quiera seguir con el código! 1.4.3.2 Gráficos Aquí están los gráficos que creamos con R. En el capítulo 3, visualización de datos, aprenderás a dominarlos. 1.4.3.3 Instalando paquetes Una de las principales características que hemos destacado de R es su versatilidad. Su filosofía de código abierto motiva a los desarrolladores a traer nuevas características a la comunidad R. En general, lo hacen a través de paquetes, que los usuarios pueden instalar como un apéndice adicional a R. Estos paquetes contienen nuevas funciones, bases de datos y características. Las pestañas de RStudio que hemos revisado nos permiten acceder a los paquetes instalados. La instalación de un paquete es bastante sencilla a través de la función install.packages(). Ahora, instalaremos el paquete tidyverse, que será central para nuestros próximos análisis. El tidyverse es una compilación que incluye algunos de los mejores paquetes modernos para el análisis de datos en R. install.packages(&quot;tidyverse&quot;) Cada vez que un usuario abre una nueva sesión en R, ésta se abre como “recién salido de fábrica”. Es decir, no sólo se abre sin objetos, sino también con sólo los paquetes básicos que permiten que R funcione. Entonces, tenemos que cargar los paquetes extra que queremos usar. Es como cuando compras un smartphone y descargas las aplicaciones que usarás de acuerdo a tus necesidades diarias. La forma más común de hacerlo es a través de la función library, como se ve a continuación. Tenga en cuenta que esta vez el “tiempo de espera” no está entre comillas4. library(tidyverse) Además, para sacar el máximo provecho de este libro, debe instalar nuestro paquete complementario, paqueteadp. Esto le dará acceso a las bases de datos que se utilizarán en los diferentes capítulos de este libro, además de algunas funciones de soporte. La instalación es ligeramente diferente, porque es un trabajo en progreso. Para obtenerlo, primero debes tener instalado el paquete remotes, que te permitirá utilizar los paquetes almacenados en GitHub, una plataforma de desarrollo de software. install.packages(&quot;remotes&quot;) Una vez cargado el paquete remotes, su función install.github() le permitirá instalar el paquete de este libro: library(remotes) install_github(&quot;arcruz0/paqueteadp&quot;) Fíjate que uno de nosotros tiene el nombre de usuario “arcruz0” en GitHub, donde se almacena el paquete “paqueteadp”. ¡Ahora está instalado en tu sistema! Cada vez que lo necesites en una sesión R, tienes que cargarlo con library(): library(paqueteadp) 1.4.3.4 Ayuda Buscar ayuda es esencial cuando se programa en R. Mira la figura 1.5: esta pestaña de RStudio abre los archivos de ayuda que necesitamos y que podemos buscar. Las funciones tienen un archivo de ayuda para cada una de ellas. Por ejemplo, podemos acceder al archivo de ayuda de sqrt() a través del comando help(sqrt) (?sqrt también funciona). Los paquetes en su conjunto también contienen archivos de ayuda, que son más completos. Por ejemplo, para ver el fichero de ayuda de tidyverse sólo tenemos que invocar el argumento “paquete”: help(package=tidyverse). Además, los archivos de ayuda de los paquetes y las funciones de los paquetes sólo están disponibles cuando se han cargado los paquetes correspondientes. Figura 1.5: Archivo de ayuda de la función sum(). Dentro del archivo de ayuda de un paquete podemos buscar funciones o dudas sobre comandos específicos, en el cuadrante de la imagen que hemos señalado en rojo. Ejercicio 1C. Instale y busque ayuda para el paquete de ggparliament. Se recomienda que juegues con todas las funciones que vimos del ecosistema R. Mencionaremos brevemente \" ggparliament más adelante, en el capítulo de visualización de datos. E-mail: arcruz@uc.cl↩︎ La idea de estos cambios es que cada sesión de RStudio comience de cero para evitar malentendidos. Esto es coherente con los proyectos de RStudio, que revisaremos en un momento↩︎ Técnicamente hablando, los objetos anteriores son vectores de longitud 1 para R↩︎ Esta es la forma más común de usar library por convención. El comando funcionará con comillas, aunque no es muy común verlo así: library(\"tidyverse\"). Seguir las convenciones es una gran idea, y por eso le recomendamos que omita las comillas↩︎ "],
["data.html", "Capítulo 2 Manejo de datos 2.1 Introducción al manejo de datos 2.2 Describiendo una base de datos 2.3 Operaciones básicas 2.4 Comandos en cadena 2.5 Recodificar valores", " Capítulo 2 Manejo de datos Andrés Cruz5 Los paquetes que necesitas instalar tidyverse (Wickham 2019b), paqueteadp (Urdinez and Cruz 2020), skimr (Waring et al. 2020). 2.1 Introducción al manejo de datos Las bases de datos tabulares son la forma por excelencia de guardar información en las ciencias sociales. Su poder se basa en la capacidad de registrar múltiples dimensiones de información para cada observación que nos interesa. Por ejemplo, para cada representante en el Congreso podemos saber su sexo, edad, porcentaje de asistencia a la sala, número de proyectos de ley presentados, etcétera: Tabla 2.1: Base de datos de ejemplo con representantes. representante género edad asistencia n_proyectos_presentados Estuardo, Carlos M 48 68 8 Cerna, Marta F 57 74 3 Probablemente estés familiarizado con este tipo de datos gracias a Microsoft Excel o Google Sheets. La primera fila es la cabecera, que indica qué datos están registrados en las celdas de esa columna. La primera columna en este caso es nuestra columna de observaciones: al consultar representante, podemos saber a qué observación se refiere cada fila. Así, sobre esta pequeña base de datos podemos decir que la unidad de observación es el legislador o representante, para los cuales tenemos información en cuatro variables: género, edad, asistencia a la sala y número de proyectos presentados. En este capítulo aprenderás a modificar bases de datos tabulares como el del ejemplo. Aprenderás a ordenar las bases de datos, filtrar sus observaciones, crear nuevas variables, generar resúmenes, cambiar los nombres, recodificar los valores y modificar la estructura de la base de datos. Todas estas operaciones son un paso inicial para cualquier análisis o visualización: se estima que el 80% del tiempo de análisis de datos se invierte en modificar y limpiar nuestros datos para su uso óptimo (Dasu &amp; Johnson, 2003 en Wickham 2014). 2.1.1 Nuestra base de datos En el presente capítulo utilizaremos datos de Reyes-Housholder (2019), con algunas adiciones de los Indicadores de Desarrollo Mundial del Banco Mundial, recogidos por Quality of Government. La autora argumenta que, ante escándalos de corrupción similares, las presidenteas de América Latina sufren caídas más pronunciadas en la aprobación en comparación con sus homólogos masculinos. Comencemos por cargar el paquete tidyverse, uno de los principales paquetes del libro, que nos dará herramientas útiles para trabajar con nuestra base de datos. library(tidyverse) Ahora carguemos nuestra base de datos en la sesión de R. Podemos hacerlo con facilidad con el paquete de este libro usando la función data(). La base de datos se llama “aprobacion”: library(paqueteadp) data(&quot;aprobacion&quot;) Ahora, podemos empezar a trabajar con nuestra base. Puedes comprobar que se cargó correctamente utilizando el comando ls() (o mirando la pestaña de Ambiente en RStudio): ls() ## [1] &quot;aprobacion&quot; Las siguientes son las variables de la base de este ejercicio: Tabla 2.2: Variables en la base Variable Descripción pais País anio Año trimestre Cuatrimestre presidente Presidente presidente_genero Sexo del presidentee aprobacion_neta Aprobación presidencial neta (% de aprobación - % de desaprobación) pib Producto Interno Bruto del país, dólares constantes de 2011 y ajustados por la Paridad de Poder Adquisitivo (PPA) poblacion Población corrupcion Corrupción del Poder Ejecutivo, según V-Dem. De 0 a 100 (un número mayor significa más corrupción) desempleo Tasa de desempleo crecimiento_pib Crecimiento del PIB 2.2 Describiendo una base de datos Para aproximarnos a los datos recién cargados tenemos varias opciones. Podemos, igual que antes, escribir su nombre como si fuera un comando para tener un resumen rápido: aprobacion ## # A tibble: 1,020 x 11 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arge… 2000 1 Fernando … Masculino 40.1 14.0 ## 2 Arge… 2000 2 Fernando … Masculino 16.4 14.0 ## 3 Arge… 2000 3 Fernando … Masculino 24.0 14.0 ## # … with 1,017 more rows, and 4 more variables También podemos usar la función glimpse() para obtener un resumen desde otra perspectiva, mirando las primeras observaciones en cada variable: glimpse(aprobacion) ## Rows: 1,020 ## Columns: 11 ## $ pais &lt;chr&gt; &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Arge… ## $ anio &lt;dbl&gt; 2000, 2000, 2000, 2000, 2001, 2001, 2001, 20… ## $ trimestre &lt;dbl&gt; 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3,… ## $ presidente &lt;chr&gt; &quot;Fernando de la Rúa&quot;, &quot;Fernando de la Rúa&quot;, … ## $ presidente_genero &lt;chr&gt; &quot;Masculino&quot;, &quot;Masculino&quot;, &quot;Masculino&quot;, &quot;Masc… ## $ aprobacion_neta &lt;dbl&gt; 40.1, 16.4, 24.0, -18.3, -7.0, -20.1, -19.4,… ## $ pib &lt;dbl&gt; 14, 14, 14, 14, 14, 14, 14, 14, 25, 25, 25, … ## $ corrupcion &lt;dbl&gt; 5.5e+11, 5.5e+11, 5.5e+11, 5.5e+11, 5.3e+11,… ## $ poblacion &lt;dbl&gt; 3.7e+07, 3.7e+07, 3.7e+07, 3.7e+07, 3.7e+07,… ## $ desempleo &lt;dbl&gt; 15, 15, 15, 15, 18, 18, 18, 18, 18, 18, 18, … ## $ crecimiento_pib &lt;dbl&gt; -0.8, -0.8, -0.8, -0.8, -4.4, -4.4, -4.4, -4… Una alternativa que nos permite la base completa es la función View(), similar a hacer clic en nuestro objeto en la pestaña “Ambiente” en RStudio: View(aprobacion) Podemos obtener un rápido resumen de las variables de nuestro conjunto de datos utilizando la función skimr::skim(), como se muestra en la Figura 2.1. Figura 2.1: Skim de nuestra base de datos. Nota: Cada vez que ves que después de un paquete usamos :: significa que dentro de ese paquete estamos pidiendo que se use una función específica. Por ejemplo, en el ejemplo anterior llamamos a la función skim() del paquete skimr. Obtener la tabulación de una de las columnas de nuestra base, una función común para las variables categóricas, es una tarea fácil gracias a la función count(). Por ejemplo, podemos comprobar que los países-cuatrimestre con mujeres como presidentees son una minoría en la región, 98 de 1020: count(aprobacion, presidente_genero, sort = T) # orden de mayor a menor por n ## # A tibble: 2 x 2 ## presidente_genero n ## &lt;chr&gt; &lt;int&gt; ## 1 Masculino 922 ## 2 Femenino 98 2.3 Operaciones básicas A continuación, veremos algunas operaciones básicas para nuestra base de datos, que en su conjunto nos permitirán hacer una importante edición en la estructura y los contenidos (Wickham and Grolemund 2016, cap. 5). Esta subsección utiliza las herramientas del paquete dplyr, que está disponible al cargar tidyverse. 2.3.1 Seleccionar columnas A veces queremos trabajar sólo con un extracto de las variables de nuestros datos. Para ello, existe la función select(). Seleccionemos sólo la columna de países: select(aprobacion, pais) ## # A tibble: 1,020 x 1 ## pais ## &lt;chr&gt; ## 1 Argentina ## 2 Argentina ## 3 Argentina ## # … with 1,017 more rows El primer argumento de la función anterior (aprobacion) es la base de datos en la que queremos ejecutar la operación. El siguiente argumento indica qué columnas seleccionar (pais). Todas las funciones para operaciones básicas que veremos en esta subsección siguen la misma lógica: el primer argumento es siempre la base de datos en la que operaremos, mientras que el resto designa cómo queremos ejecutar la operación. Recordemos que el código anterior no creó ningún objeto nuevo, es sólo un comando que estamos ejecutando en la consola. Si quisiéramos crear un nuevo objeto, tendríamos que asignarlo, usando el operador &lt;-: aprobacion_reducida &lt;- select(aprobacion, pais) aprobacion_reducida ## # A tibble: 1,020 x 1 ## pais ## &lt;chr&gt; ## 1 Argentina ## 2 Argentina ## 3 Argentina ## # … with 1,017 more rows Podemos seleccionar varias columnas a la vez, separadas por comas: select(aprobacion, pais, anio, desempleo) ## # A tibble: 1,020 x 3 ## pais anio desempleo ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Argentina 2000 15 ## 2 Argentina 2000 15 ## 3 Argentina 2000 15 ## # … with 1,017 more rows Supongamos que queremos las primeras cinco variables de nuestra base. A continuación se presentan tres formas de obtener el mismo resultado, aunque recomendamos la segunda, ya que es breve y clara: select(aprobacion, pais, anio, trimestre, presidente, aprobacion_neta) select(aprobacion, pais:aprobacion_neta) # forma recomendada select(aprobacion, 1:5) ## # A tibble: 1,020 x 6 ## pais anio trimestre presidente presidente_gene… aprobacion_neta ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Argent… 2000 1 Fernando de l… Masculino 40.1 ## 2 Argent… 2000 2 Fernando de l… Masculino 16.4 ## 3 Argent… 2000 3 Fernando de l… Masculino 24.0 ## # … with 1,017 more rows El comando select() también puede ayudarnos a reordenar las columnas. Supongamos que quisiéramos que la variable presidente fuera la primera. Podemos reordenar las variables, obteniendo la misma base con un nuevo orden para las columnas: select(aprobacion, presidente, pais:anio, aprobacion_neta:desempleo) ## # A tibble: 1,020 x 8 ## presidente pais anio aprobacion_neta pib corrupcion poblacion ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Fernando … Arge… 2000 40.1 14.0 5.52e11 37057452 ## 2 Fernando … Arge… 2000 16.4 14.0 5.52e11 37057452 ## 3 Fernando … Arge… 2000 24.0 14.0 5.52e11 37057452 ## # … with 1,017 more rows, and 1 more variable Este método es tedioso, especialmente para las bases de datos con múltiples variables. Hay una función que puede ser útil para estos escenarios, llamada everything(). En este caso, seleccionará la columna presidente y “todo lo demás”: select(aprobacion, presidente, everything()) ## # A tibble: 1,020 x 11 ## presidente pais anio trimestre presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Fernando … Arge… 2000 1 Masculino 40.1 14.0 ## 2 Fernando … Arge… 2000 2 Masculino 16.4 14.0 ## 3 Fernando … Arge… 2000 3 Masculino 24.0 14.0 ## # … with 1,017 more rows, and 4 more variables Otra función útil para select() es starts_with(), que nos permite seleccionar las columnas según los patrones en sus nombres. Por ejemplo, a continuación se seleccionarán todas las columnas que empiecen con el prefijo “pib”. select(aprobacion, starts_with(&quot;pib&quot;)) ## # A tibble: 1,020 x 1 ## pib ## &lt;dbl&gt; ## 1 14.0 ## 2 14.0 ## 3 14.0 ## # … with 1,017 more rows 2.3.2 Renombra las columnas Podemos cambiar los nombres de las columnas de una base con el comando rename(). Por ejemplo, hagamos más explícito el nombre de la variable con el PIB: rename(aprobacion, pib_ppp_c2011 = pib) ## # A tibble: 1,020 x 11 ## pais anio trimestre presidente presidente_gene… aprobacion_neta ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Arge… 2000 1 Fernando … Masculino 40.1 ## 2 Arge… 2000 2 Fernando … Masculino 16.4 ## 3 Arge… 2000 3 Fernando … Masculino 24.0 ## # … with 1,017 more rows, and 5 more variables También es posible cambiar varios nombres a la vez. Observa cómo modificamos tres nombres con un solo comando: rename(aprobacion, pib_ppp_c2011 = pib, desempleo_porcentaje = desempleo, crecimiento_pib_porcentaje = crecimiento_pib) ## # A tibble: 1,020 x 11 ## pais anio trimestre presidente presidente_gene… aprobacion_neta ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Arge… 2000 1 Fernando … Masculino 40.1 ## 2 Arge… 2000 2 Fernando … Masculino 16.4 ## 3 Arge… 2000 3 Fernando … Masculino 24.0 ## # … with 1,017 more rows, and 5 more variables 2.3.3 Filtrar las observaciones A menudo queremos mantener sólo algunas observaciones de nuestra base de datos, filtrando según características específicas. Podemos hacerlo gracias a la función filter() y a los operadores lógicos. Para empezar, mantengamos sólo las observaciones para Chile: filter(aprobacion, pais == &quot;Chile&quot;) ## # A tibble: 60 x 11 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Chile 2000 1 Eduardo F… Masculino 6.22 3.63 ## 2 Chile 2000 2 Ricardo L… Masculino 19.8 3.63 ## 3 Chile 2000 3 Ricardo L… Masculino 19.5 3.63 ## # … with 57 more rows, and 4 more variables Le decimos filter(), a través del segundo argumento, sólo para retener las observaciones en las que la variable país es igual a “Chile”. Esta es igual a es un operador lógico, que se escribe como “==” en R6 Aquí hay una lista de operadores lógicos comunes: operador descripción == es igual a != es diferente a &gt; es más grande que &lt; es menor que &gt;= es mayor o igual a &lt;= es menor o igual que &amp; intersección | Unión %in% …está contenida en… Por ejemplo, podemos obtener todas las observaciones (país-año-cuatrimestre) en las que la aprobación presidencial neta es positiva: filter(aprobacion, aprobacion_neta &gt; 0) ## # A tibble: 709 x 11 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arge… 2000 1 Fernando … Masculino 40.1 14.0 ## 2 Arge… 2000 2 Fernando … Masculino 16.4 14.0 ## 3 Arge… 2000 3 Fernando … Masculino 24.0 14.0 ## # … with 706 more rows, and 4 more variables También es posible ejecutar filtros más complejos. Filtremos sólo las observaciones para el Cono Sur: filter(aprobacion, pais == &quot;Argentina&quot; | pais == &quot;Chile&quot; | pais == &quot;Uruguay&quot;) ## # A tibble: 180 x 11 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arge… 2000 1 Fernando … Masculino 40.1 14.0 ## 2 Arge… 2000 2 Fernando … Masculino 16.4 14.0 ## 3 Arge… 2000 3 Fernando … Masculino 24.0 14.0 ## # … with 177 more rows, and 4 more variables # Lo mismo, pero con otro operador lógico: filter(aprobacion, pais %in% c(&quot;Argentina&quot;, &quot;Chile&quot;, &quot;Uruguay&quot;)) ## # A tibble: 180 x 11 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arge… 2000 1 Fernando … Masculino 40.1 14.0 ## 2 Arge… 2000 2 Fernando … Masculino 16.4 14.0 ## 3 Arge… 2000 3 Fernando … Masculino 24.0 14.0 ## # … with 177 more rows, and 4 more variables También podemos incluir pequeñas operaciones en nuestros filtros. Obtengamos todas las observaciones en las que la corrupción del ejecutivo sea mayor que la media de la muestra: filter(aprobacion, corrupcion &gt; mean(corrupcion)) ## # A tibble: 252 x 11 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arge… 2000 1 Fernando … Masculino 40.1 14.0 ## 2 Arge… 2000 2 Fernando … Masculino 16.4 14.0 ## 3 Arge… 2000 3 Fernando … Masculino 24.0 14.0 ## # … with 249 more rows, and 4 more variables Tip: Una advertencia práctica: no puedes buscar valores perdidos (NAs) con el intuitivo == NA. Necesitas usar la función is.na() que revisamos brevemente en la sección 1.4.3.3. Nuestra base de datos no tiene valores perdidos, y por lo tanto un filtro como el siguiente no devolverá ninguna fila: filter(aprobacion, is.na(corrupcion)) ## # A tibble: 0 x 11 ## # … with 11 variables Ejercicio 2A. Seleccione sólo las dos columnas que registran el género del presidentee en la base de datos. Ejercicio 2B. Filtrar los datos para que contengan sólo observaciones del año 2000. 2.3.4 Cambiar el orden de una base de datos Una de las operaciones más comunes con los bases de datos es clasificarlas según una variable de interés. Esto puede darnos pistas claras sobre nuestras observaciones. Podemos hacerlo gracias a la función arrange(). Por ejemplo, clasifiquemos las observaciones desde el país menos corrupto - hasta el más corrupto, por trimestre del año: arrange(aprobacion, corrupcion) ## # A tibble: 1,020 x 11 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Nica… 2000 1 Arnoldo A… Masculino 7.60 85.7 ## 2 Nica… 2000 2 Arnoldo A… Masculino 7.57 85.7 ## 3 Nica… 2000 3 Arnoldo A… Masculino 3.87 85.7 ## # … with 1,017 more rows, and 4 more variables Si quisiéramos ordenarlas a la inversa, tendríamos que añadir un - (signo menos) antes de la variable: arrange(aprobacion, -corrupcion) ## # A tibble: 1,020 x 11 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bras… 2014 1 Dilma Van… Femenino 22.6 27.3 ## 2 Bras… 2014 2 Dilma Van… Femenino 12.6 27.3 ## 3 Bras… 2014 3 Dilma Van… Femenino 16.2 27.3 ## # … with 1,017 more rows, and 4 more variables Para utilizar un orden alfabético inverso (de la Z a la A), tenemos que utilizar la función desc(). arrange(aprobacion, desc(presidente)) ## # A tibble: 1,020 x 11 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Méxi… 2001 1 Vicente F… Masculino 53.8 37.6 ## 2 Méxi… 2001 2 Vicente F… Masculino 40.7 37.6 ## 3 Méxi… 2001 3 Vicente F… Masculino 40.1 37.6 ## # … with 1,017 more rows, and 4 more variables Por último, podemos clasificar la base de datos por más de una variable. Esto es, ordenar los datos según la primera variable, y luego ordenarla según una segunda variable. Examinemos el siguiente ejemplo: arrange(aprobacion, presidente_genero, -aprobacion_neta) ## # A tibble: 1,020 x 11 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bras… 2013 1 Dilma Van… Femenino 62.5 27.3 ## 2 Bras… 2012 4 Dilma Van… Femenino 60.9 33.4 ## 3 Bras… 2012 2 Dilma Van… Femenino 60.5 33.4 ## # … with 1,017 more rows, and 4 more variables 2.3.5 Transformar y crear variables La mayoría de las veces queremos crear nuevas variables a partir de las que ya tenemos. Supongamos que quisiéramos transformar la escala de población (poblacion) en millones: mutate(aprobacion, poblacion_mill = poblacion / 1000000) ## # A tibble: 1,020 x 12 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arge… 2000 1 Fernando … Masculino 40.1 14.0 ## 2 Arge… 2000 2 Fernando … Masculino 16.4 14.0 ## 3 Arge… 2000 3 Fernando … Masculino 24.0 14.0 ## # … with 1,017 more rows, and 5 more variables El comando anterior genera una nueva variable en la base de datos, pop_mill, que es poblacion pero en la escala de millones. Podemos ejecutar todo tipo de operaciones en nuestras columnas, como crear una variable PIB en una escala logarítmica: mutate(aprobacion, log_gdp = log(pib)) ## # A tibble: 1,020 x 12 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arge… 2000 1 Fernando … Masculino 40.1 14.0 ## 2 Arge… 2000 2 Fernando … Masculino 16.4 14.0 ## 3 Arge… 2000 3 Fernando … Masculino 24.0 14.0 ## # … with 1,017 more rows, and 5 more variables También podemos crear nuevas variables a partir de operaciones entre variables. Por ejemplo, calculamos el PIB per cápita, lo que nos permite comparar mejor los países con diferentes poblaciones: mutate(aprobacion, pib_pc = pib / poblacion) ## # A tibble: 1,020 x 12 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arge… 2000 1 Fernando … Masculino 40.1 14.0 ## 2 Arge… 2000 2 Fernando … Masculino 16.4 14.0 ## 3 Arge… 2000 3 Fernando … Masculino 24.0 14.0 ## # … with 1,017 more rows, and 5 more variables Por último, también podemos generar más de una transformación a la vez con mutate(), utilizando múltiples argumentos: mutate(aprobacion, pop_mill = poblacion / 1000000, pib_pc = pib / poblacion) ## # A tibble: 1,020 x 13 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arge… 2000 1 Fernando … Masculino 40.1 14.0 ## 2 Arge… 2000 2 Fernando … Masculino 16.4 14.0 ## 3 Arge… 2000 3 Fernando … Masculino 24.0 14.0 ## # … with 1,017 more rows, and 6 more variables Ejercicio 2C. Crear una nueva base, que está ordenada por país-trimestre del año con menos aprobación presidencial al de más alto nivel (recuerde crear un nuevo objeto y darle un nombre descriptivo). En tu nuevo objeto, conserva sólo las observaciones con mujeres como presidenteas. Ejercicio 2D. Crear una nueva variable, que registre el desempleo como proporción en lugar de como porcentaje. 2.3.6 Resúmenes Podemos hacer resúmenes para nuestros bases de datos con summarize: summarize(aprobacion, desempleo_medio = mean(desempleo)) ## # A tibble: 1 x 1 ## desempleo_medio ## &lt;dbl&gt; ## 1 7.04 Este proceso a menudo se llama colapsar la base de datos: estamos comprimiendo la información de las filas para generar una sola fila de resumen. En este caso, la función de colapso mean() opera en el vector desempleo para obtener su media. Como en las otras operaciones, podemos hacer varios resúmenes a la vez: summarize(aprobacion, desempleo_medio = mean(desempleo), crecimiento_medio = mean(crecimiento_pib), aprobacion_media = mean(aprobacion_neta)) ## # A tibble: 1 x 3 ## desempleo_medio crecimiento_medio aprobacion_media ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.04 3.77 15.3 2.3.7 Resúmenes por grupo ¡Esta es una función muy divertida! Esta tarea consiste en colapsar filas hasta obtener una fila por observación que resuma la información de los diferentes grupos de la base. Para ello, primero necesitamos tener variables que agrupen nuestras observaciones (partido, país, región, etc.). Le haremos saber a R cuál es la variable que estamos agrupando nuestras observaciones, y este nuevo conjunto de datos será el mismo que el original, pero R sabrá que las próximas operaciones que hagamos necesitan ser agrupadas. aprobacion_por_pais &lt;- group_by(aprobacion, pais) Hagamos una operación de resumen en este nuevo objeto: summarize(aprobacion, desempleo_medio = mean(desempleo), crecimiento_medio = mean(crecimiento_pib), aprobacion_media = mean(aprobacion_neta)) ## # A tibble: 1 x 3 ## desempleo_medio crecimiento_medio aprobacion_media ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.04 3.77 15.3 Los grupos también pueden ser combinaciones de variables. Por ejemplo, nuestra base de datos a nivel de país-año-cuarimestre puede ser agrupado por país-año, y entonces podemos obtener el mismo cálculo previo: aprobacion_por_pais_anio &lt;- group_by(aprobacion, pais, anio) summarize(aprobacion_por_pais, desempleo_mean = mean(desempleo), crecimiento_pib_mean = mean(crecimiento_pib), approv_mean = mean(aprobacion_neta)) ## # A tibble: 17 x 4 ## pais desempleo_mean crecimiento_pib_mean approv_mean ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Argentina 11.0 2.72 16.7 ## 2 Bolivia 3.70 4.24 11.3 ## 3 Brasil 8.35 3.4 34.2 ## # … with 14 more rows Por cierto, podemos desagrupar un conjunto de datos con ungroup(). Es una gran idea si no queremos seguir ejecutando operaciones agrupadas, evitando errores: aprobacion_por_pais_anio %&gt;% ungroup() # nota que ya no hay &quot;grupos&quot; en el resumen de datos ## # A tibble: 1,020 x 11 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arge… 2000 1 Fernando … Masculino 40.1 14.0 ## 2 Arge… 2000 2 Fernando … Masculino 16.4 14.0 ## 3 Arge… 2000 3 Fernando … Masculino 24.0 14.0 ## # … with 1,017 more rows, and 4 more variables 2.4 Comandos en cadena La mayoría de las veces queremos hacer más de una operación sobre una base. Por ejemplo, podríamos querer (a) crear una nueva variable del PIB per cápita, y luego (b) filtrar las observaciones con valores iguales o mayores que el PIB medio per cápita de la muestra: aprobacion_con_pib_pc &lt;- mutate(aprobacion, pib_pc = pib / poblacion) filter(aprobacion_con_pib_pc, pib_pc &gt; mean(pib_pc)) ## # A tibble: 344 x 12 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Boli… 2000 1 Hugo Banz… Masculino -19.0 60.5 ## 2 Boli… 2000 2 Hugo Banz… Masculino -19.0 60.5 ## 3 Boli… 2000 3 Hugo Banz… Masculino -23.8 60.5 ## # … with 341 more rows, and 5 more variables La misma cadena de operaciones puede escribirse de la siguiente manera en lenguaje “tidy”: aprobacion %&gt;% mutate(pib_pc = pib / poblacion) %&gt;% filter(pib_pc &gt; mean(pib_pc)) ## # A tibble: 344 x 12 ## pais anio trimestre presidente presidente_gene… aprobacion_neta pib ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Boli… 2000 1 Hugo Banz… Masculino -19.0 60.5 ## 2 Boli… 2000 2 Hugo Banz… Masculino -19.0 60.5 ## 3 Boli… 2000 3 Hugo Banz… Masculino -23.8 60.5 ## # … with 341 more rows, and 5 more variables ¡Este código es sorprendentemente legible! Las pipas (%&gt;%) se leen como “entonces” (o “pero entonces”), y se pueden insertar con Ctrl o Cmd + Shift + M en RStudio7. A continuación se reproduce nuestro código anterior en español: Tomar el conjunto de datos “aprobacion”, entonces generar una nueva variable llamada “pib_pc” (la división entre “pib” y “poblacion”) y entonces filtrar las observaciones para que sólo queden aquellas en las que “pib_pc” sea mayor que la media. Uno de los usos más comunes de las pipes es el combo group_by()+ summarize(). Repitamos nuestras operaciones anteriores para hacer un resumen agrupado: aprobacion %&gt;% group_by(pais) %&gt;% summarize(desempleo_medio = mean(desempleo), crecimiento_medio = mean(crecimiento_pib), aprobacion_media = mean(aprobacion_neta)) ## # A tibble: 17 x 4 ## pais desempleo_medio crecimiento_medio aprobacion_media ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Argentina 11.0 2.72 16.7 ## 2 Bolivia 3.70 4.24 11.3 ## 3 Brasil 8.35 3.4 34.2 ## # … with 14 more rows Ejercicio 3E. Calcular, con la ayuda de las pipas, la corrupción del ejecutivo medio y el PIB por país. Recuerde que puede insertar tuberías con Ctrl o Cmd + Shift + M. Ejercicio 3F. Una vez más, utilizando pipas, clasifique los países de la base de datos desde el que obtuvo el mayor promedio de PIB per cápita en el período 2010-2014 hasta el más bajo. Ejercicio 3G. ¿Qué cuatrimestre del año, entre los gobernados por mujeres presidenteas, tuvo la corrupción más alta? ¿Y la mayor aprobación neta? 2.5 Recodificar valores Un ejercicio común en la gestión de bases de datos es la generación de variables (o la edición de las ya existentes) basadas en ciertas condiciones lógicas. Ya construimos condiciones lógicas antes de usar filter(), así que la sintaxis general debería ser familiar. Por ejemplo, podríamos querer registrar los valores de una variable categórica binaria como ceros y unos, creando una variable dummy. Esto es fácil gracias al comando if_else(). Podemos especificar la condición lógica (presidente_genero==female), y luego los valores que se asignarán cuando se cumpla esta condición (1) o no (0): aprobacion %&gt;% mutate(d_pres_mujer = if_else(condition = presidente_genero == &quot;female&quot;, true = 1, false = 0)) %&gt;% select(pais:presidente, presidente_genero, d_pres_mujer) # para legibilidad ## # A tibble: 1,020 x 6 ## pais anio trimestre presidente presidente_gene… d_pres_mujer ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Argenti… 2000 1 Fernando de la … Masculino 0 ## 2 Argenti… 2000 2 Fernando de la … Masculino 0 ## 3 Argenti… 2000 3 Fernando de la … Masculino 0 ## # … with 1,017 more rows Es posible especificar condiciones lógicas más complejas, como en filter(). Por ejemplo, generemos una variable dummy para los países-años-cuatrimestre en crisis económica, definida como: el crecimiento del PIB es negativo y/o la tasa de desempleo es superior al 20%. Bajo esta simple clasificación, Argentina estaría en crisis en 2001 y en 2010: aprobacion %&gt;% # No explicamos los argumentos para hacer el código conciso: mutate(d_crisis_ec = if_else(crecimiento_pib &lt; 0 | desempleo &gt; 20, 1, 0)) %&gt;% # Lo siguiente es sólo para mostrar los resultados más claramente: select(pais:trimestre, crecimiento_pib, desempleo, d_crisis_ec) %&gt;% filter(pais == &quot;Argentina&quot; &amp; anio %in% c(2001, 2013)) ## # A tibble: 8 x 6 ## pais anio trimestre crecimiento_pib desempleo d_crisis_ec ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Argentina 2001 1 -4.4 18.3 1 ## 2 Argentina 2001 2 -4.4 18.3 1 ## 3 Argentina 2001 3 -4.4 18.3 1 ## # … with 5 more rows Sin embargo, if_else() a menudo no es lo suficientemente flexible, ya que sólo permite asignar dos valores basados en una condición lógica. ¿Qué pasa si la variable que queremos crear puede asumir más de dos valores? Por ejemplo, podríamos querer una variable que divida nuestras observaciones en tres categorías, según el país: “Cono Sur” (Argentina, Chile, Uruguay), “Centroamérica” y “Resto de AL”. Para empezar, examinemos los valores que la variable “pais” puede asumir: unique(aprobacion$pais) ## [1] &quot;Argentina&quot; &quot;Bolivia&quot; &quot;Brasil&quot; &quot;Chile&quot; ## [5] &quot;Colombia&quot; &quot;Costa Rica&quot; &quot;Ecuador&quot; &quot;El Salvador&quot; ## [9] &quot;Guatemala&quot; &quot;Honduras&quot; &quot;México&quot; &quot;Nicaragua&quot; ## [13] &quot;Panamá&quot; &quot;Paraguay&quot; &quot;Perú&quot; &quot;Uruguay&quot; ## [17] &quot;Venezuela&quot; if_else() no nos permitiría generar esta nueva variable, pero su función hermana case_when() sí lo permite. aprobacion %&gt;% mutate(grupo_de_paises = case_when( pais %in% c(&quot;Argentina&quot;, &quot;Chile&quot;, &quot;Uruguay&quot;) ~ &quot;Cono Sur&quot;, pais %in% c(&quot;Costa Rica&quot;, &quot;El Salvador&quot;, &quot;Guatemala&quot;, &quot;Honduras&quot;, &quot;Nicaragua&quot;, &quot;Panama&quot;) ~ &quot;Centroamérica&quot;, TRUE ~ &quot;Resto de AL&quot; )) %&gt;% # reduciremos la base de datos para ver mejor los resultados: filter(anio == 2000 &amp; trimestre == 1) %&gt;% select(pais, grupo_de_paises) ## # A tibble: 17 x 2 ## pais grupo_de_paises ## &lt;chr&gt; &lt;chr&gt; ## 1 Argentina Cono Sur ## 2 Bolivia Resto de AL ## 3 Brasil Resto de AL ## # … with 14 more rows La nueva variable (grupo_de_paises) se construye sobre la base de múltiples condiciones lógicas, que se evalúan en orden. Si se cumple la primera condición (pais %in% c(\"Argentina\", \"Chile\", \"Uruguay\")), se asigna el valor “Cono Sur” a la nueva variable. La condición lógica y el valor asignado se separan por un “~”8, que se puede leer como “por lo tanto”. Lo mismo ocurrirá con la siguiente condición, que asignará “Centroamérica” si se cumple. Nuestro último argumento para case_when() tiene una condición lógica de gran alcance: en todos los demás casos, se aplicará el valor “Resto de AL”. Ejercicio 2H. if_else() puede ser pensado como una versión reducida de case_when(): todo lo que hacemos con la primera función podría ser convertido en la sintaxis de la segunda. Traduzca uno de los ejemplos anteriores con if_else() a la sintaxis case_when(). Ejercicio 2I. Crea una nueva variable que separa los países en tres grupos: “América del Norte”, “América Central” y “América del Sur”. 2.5.1 Pivoteo de datos {data-pivot} La estructura de la base anterior, donde las filas son las observaciones, las variables son las columnas, y la base de datos tiene sólo una unidad de observación, es la estructura tidy de la presentación de datos (Wickham 2014). En general, R y el tidyverse funcionan muy bien bajo este formato, así que querremos usarlo cuando sea posible. Sin embargo, los datos con los que trabajamos en el mundo real no siempre están disponibles en este formato. A menudo, otros formatos son más adecuados en contextos diferentes al análisis de datos, por ejemplo, las codificaciones manuales de la administración pública. Para empezar este ejemplo, creemos una base de datos a nivel de país-año con los niveles medios de aprobación presidencial: aprobacion_anual &lt;- aprobacion %&gt;% group_by(pais, anio) %&gt;% summarize(aprobacion_neta = mean(aprobacion_neta)) %&gt;% ungroup() aprobacion_anual ## # A tibble: 255 x 3 ## pais anio aprobacion_neta ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Argentina 2000 15.6 ## 2 Argentina 2001 -17.4 ## 3 Argentina 2002 -16.0 ## # … with 252 more rows Este conjunto de datos tidy puede ser presentado en diferentes formatos. El más común de ellos es el formato ancho o wide9, en el que una de las variables de identificación se distribuye entre las columnas (en este caso, “anio”). Ahora cargaremos el conjunto de datos en formato ancho, desde paquete del libro: library(paqueteadp) data(aprobacion_wide1) aprobacion_wide1 ## # A tibble: 17 x 16 ## pais `2000` `2001` `2002` `2003` `2004` `2005` `2006` `2007` `2008` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arge… 15.6 -17.4 -16.0 32.6 48.5 43.8 45.9 34.3 9.52 ## 2 Boli… -18.8 -14.1 -5.77 -16.8 -0.301 24.5 34.7 28.1 16.9 ## 3 Bras… -8.72 -2.87 3.51 45.8 26.3 21.3 30.8 40.1 58.3 ## # … with 14 more rows, and 6 more variables Esta base de datos contiene la misma información que la que creamos manualmente, sólo cambia su forma de presentación. Esta estructura ancha tiene algunos beneficios, el más destacado es su brevedad: los años no se repiten en múltiples celdas, como sucede en un conjunto de datos tidy. Para un codificador manual, este ahorro de espacio (y tiempo) resulta atractivo. Sin embargo, el formato wide tiene una mayor desventaja en comparación con el formato tidy: en su forma tradicional, sólo es posible registrar información para una variable por base de datos. En el caso del ejemplo, no hay una forma posible de añadir, por ejemplo, información sobre la corrupción de los años-país. Como hemos visto, este ejercicio es trivial en un conjunto de datos tidy, donde las variables pueden ser añadidas como columnas. Tener múltiples variables en nuestra base de datos es exactamente lo que necesitamos para generar análisis de datos sociales, donde exploramos las diferentes dimensiones de nuestros fenómenos de estudio. Afortunadamente, el paquete tidyr, que se carga automáticamente con tidyverse, proporciona funciones que convierten rápidamente los datos de un formato ancho en una versión más amigable para el análisis. Este tipo de transformación de la estructura se llama a menudo “pivote”. La función clave aquí es pivot_longer(), que permite a un conjunto de datos pivotar en un formato vertical. El conjunto de datos que obtendremos es igual al conjunto de datos tidy que creamos antes: aprobacion_wide1 %&gt;% pivot_longer(cols = -pais, names_to = &quot;anio&quot;, values_to = &quot;aprobacion_neta&quot;) ## # A tibble: 255 x 3 ## pais anio aprobacion_neta ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Argentina 2000 15.6 ## 2 Argentina 2001 -17.4 ## 3 Argentina 2002 -16.0 ## # … with 252 more rows El primer argumento en pivot_longer(), cols =, nos pide que seleccionemos las columnas para transformarlas en una variable de identificación, utilizando la sintaxis select() que aprendimos antes en este capítulo. En este caso, estamos indicando pivot_longer() para transformar todas las variables, excepto país, en una variable de identificación. Entonces, el argumento names_to() nos pregunta cómo queremos llamar a la nueva variable de identificación, que se crea cuando se transforma el conjunto de datos. Por último, values_to = requiere nombrar la nueva variable que se crea, basándose en los valores de las celdas de los datos originales. En algunos casos también es útil hacer la operación inversa, para transformar un conjunto de datos tidy en un formato wide (ancho). Para ello, podemos usar otra función importante en tidyr, llamada pivot_wider(). Veamos un ejemplo, partiendo de un formato de conjunto de datos tidy, que creamos antes: aprobacion_anual %&gt;% pivot_wider(names_from = &quot;anio&quot;, values_from = &quot;aprobacion_neta&quot;) ## # A tibble: 17 x 16 ## pais `2000` `2001` `2002` `2003` `2004` `2005` `2006` `2007` `2008` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arge… 15.6 -17.4 -16.0 32.6 48.5 43.8 45.9 34.3 9.52 ## 2 Boli… -18.8 -14.1 -5.77 -16.8 -0.301 24.5 34.7 28.1 16.9 ## 3 Bras… -8.72 -2.87 3.51 45.8 26.3 21.3 30.8 40.1 58.3 ## # … with 14 more rows, and 6 more variables Los argumentos, en este caso, son prácticamente espejos de los anteriores. Aquí lo que queremos es que la base de datos tome de “año” sus nombres de columna a lo ancho (names_from=\"anio\"), mientras que los valores se toman de nuestra variable de interés “aprobacion_neta” (values_from = \"aprobacion_neta \"). Por lo tanto, estos comandos son perfectamente simétricos. Por ejemplo, la siguiente cadena de comandos es inofensiva, ya que pivot_wider() revertirá la transformación aplicada por pivot_longer(): aprobacion_wide1 %&gt;% pivot_longer(cols = -pais, names_to = &quot;anio&quot;, values_to = &quot;aprobacion_neta&quot;) %&gt;% pivot_wider(names_from = &quot;anio&quot;, values_from = &quot;aprobacion_neta&quot;) ## # A tibble: 17 x 16 ## pais `2000` `2001` `2002` `2003` `2004` `2005` `2006` `2007` `2008` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arge… 15.6 -17.4 -16.0 32.6 48.5 43.8 45.9 34.3 9.52 ## 2 Boli… -18.8 -14.1 -5.77 -16.8 -0.301 24.5 34.7 28.1 16.9 ## 3 Bras… -8.72 -2.87 3.51 45.8 26.3 21.3 30.8 40.1 58.3 ## # … with 14 more rows, and 6 more variables Ejercicio 2J. Genera una base en formato tidy con el crecimiento medio del PIB por país-año. Convierte estos datos a un formato ancho/wide, moviendo los años a las columnas. 2.5.2 Bases de datos anchas con más de una variable de interés Anteriormente, mencionamos que no es posible registrar, de manera simple, información para más de una variable de interés en una estructura ancha. Sin embargo, nuestras fuentes de datos a menudo contendrán sorpresas que no son agradables para nosotros, como el siguiente ejemplo: library(paqueteadp) data(aprobacion_wide2) Observa que en esta base las columnas registran información a lo largo del tiempo para dos variables, “pib” y “poblacion”. Lo que queremos es extender esta información en las filas, reconstruyendo nuestro par país-años y las dos variables de interés. Primero, podemos pivotear el conjunto de datos para dejarlos en el nivel de la variable país-año. En pivot_longer(), podemos indicar que los nombres de las columnas contienen información de más de una variable. Primero, el argumento names_to = c(\"variable\", \"año\") toma dos valores en esta ocasión, los nombres de las nuevas variables después del pivote. En segundo lugar, names_sep= \"_\" indica que en las columnas del conjunto de datos original la información de las dos variables está separada por un guión bajo (puede ser otro carácter, como un guión alto o una barra vertical10). aprobacion_wide2 %&gt;% pivot_longer(cols = -pais, names_to = c(&quot;variable&quot;, &quot;anio&quot;), names_sep = &quot;_&quot;) ## # A tibble: 510 x 4 ## pais variable anio value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Argentina pib 2000 552151219031. ## 2 Argentina pib 2001 527807756979. ## 3 Argentina pib 2002 470305820970. ## # … with 507 more rows Entonces, podemos pivotar las variables a través del ancho para obtener nuestra base datos de destino, como lo hicimos antes, con pivot_wider(). Hagamos todo en una cadena: aprobacion_wide2 %&gt;% pivot_longer(cols = -pais, names_to = c(&quot;variable&quot;, &quot;anio&quot;), names_sep = &quot;_&quot;) %&gt;% pivot_wider(names_from = &quot;variable&quot;, values_from = &quot;value&quot;) ## # A tibble: 255 x 4 ## pais anio pib poblacion ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Argentina 2000 552151219031. 37057452 ## 2 Argentina 2001 527807756979. 37471509 ## 3 Argentina 2002 470305820970. 37889370 ## # … with 252 more rows E-mail: arcruz@uc.cl↩︎ Los usuarios de Stata encontrarán esto familiar.↩︎ Puedes ver todos los atajos de teclado en RStudio en Ayuda &gt; Ayuda de los atajos de teclado↩︎ Aprendí que algunos usuarios de R nombran este símbolo una “colita de chancho”!↩︎ A veces se utiliza el término largo (opuesto a ancho) en lugar de lo que llamamos tidy.↩︎ Si la separación entre sus variables es menos clara, podemos usar el argumento names_pattern= en lugar de names_sep=. Para esto necesitarás usar expresiones regulares, un tema tratado en el capítulo 13 de análisis cuantitativo de textos. Por ejemplo, podríamos escribir la misma operación aquí con el siguiente argumento: name_pattern = \"(\\\\D+)_(\\\\d+)\"↩︎ "],
["dataviz.html", "Capítulo 3 Visualización de datos 3.1 ¿Por qué visualizar mis datos? 3.2 Primeros pasos 3.3 Ejemplo aplicado: Elecciones locales y visualización de datos 3.4 Para continuar aprendiendo", " Capítulo 3 Visualización de datos Soledad Araya11 Lecturas sugeridas Henshaw, A. L., &amp; Meinke, S. R. (2018). Data analysis and data visualization as active learning in political science. Journal of Political Science Education, 14(4), 423-439. Kastellec, J. P., &amp; Leoni, E. L. (2007). Using graphs instead of tables in political science. Perspectives on Politics, 5(4), 755-771. Tufte, E. R. (2006). Beautiful evidence. Graphics Press. Los paquetes que necesitas instalar tidyverse (Wickham 2019b), paqueteadp (Urdinez and Cruz 2020), ggrepel (Slowikowski 2020). 3.1 ¿Por qué visualizar mis datos? Ya has aprendido a usar los comandos de Tidyverse, y probablemente quieras sumergirte en el mundo de los gráficos, y aplicar todo lo que has aprendido a tu propios datos. Con tidyverse y ggplot2, la visualización de datos se convierte en una tarea fácil, pero hay algunos pasos que debes seguir antes de escribir tu código. Por ejemplo, conocer tus variables. ¿Son variables continuas o categóricas? Cuando son categóricas, ¿tienen dos o más niveles? Además, esos niveles, ¿están en orden o no? Estas no son las únicas preguntas que tienes que considerar. Parece una tarea fácil, pero si no consideras este paso en tu trabajo con ggplot2 las cosas pueden ponerse feas bastante rápido. Se pueden encontrar ejemplos divertidos de esto en accidental aRt. Una pregunta rápida: ¿Por qué representar nuestros datos gráficamente? En primer lugar, sé que muchos de nosotros estamos interesados en representar nuestros datos gráficamente porque es una forma atractiva de hacerlo. Sin embargo, tener un buen o mal sentido de la estética no significa mucho si nuestros datos no son claros. Por lo tanto, es necesario comprender lo que queremos expresar, lo cual puede ser una tarea difícil si no reflexionamos sobre por qué estamos haciendo este tipo de representación. A veces, podemos utilizar tablas para resumir cantidades y/o patrones, pero la gran gestión de datos de hoy en día hace que esto sea una tarea compleja e ineficiente. Por lo tanto, volvamos a la pregunta principal: ¿por qué visualizar? ¿Por qué no hacer simplemente tablas que expresen lo que queremos decir? A través de la visualización de datos podemos entender otros tipos de problemas que los números por sí solos no pueden mostrar. Mediante la visualización, queremos explorar y comprender nuestros datos. Además, la representación gráfica puede ayudarnos a interpretar patrones, tendencias, distribuciones y a comunicarlos mejor a nuestros lectores. Figura 3.1: Estadística Florence Nightingale (1820-1910). Florence Nightgale (1820-1910) fue una enfermera y estadística que ayudó a reorganizar la administración de los hospitales cívicos y militares de Gran Bretaña. Ella, con la ayuda de un equipo, logró hacer un registro de las muertes y enfermedades en los hospitales militares durante la Guerra de Crimea. Para su sorpresa, la gran mayoría de las muertes eran evitables, y la razón principal de ellas eran las malas condiciones del sistema de salud. Uno de sus informes para el gobierno británico fue el siguiente diagrama: Figura 3.2: Diagrama de las causas de muerte en el ejército británico En rojo se destacan las muertes por heridas de guerra, en azul las muertes debidas a enfermedades evitables, y en negro, las muertes causadas por otro tipo de causas. Este gráfico no sólo proporciona información cuantitativa sobre las muertes, sino que también señala un problema sustancial en el sistema de salud de los militares en ese momento. El diagrama de Nightingale reveló el problema, que fue el paso inicial para una serie de reformas. Así, la visualización se convierte en una herramienta que puede ser aplicada en todas las etapas de la investigación. En una etapa inicial, es importante para la exploración de los datos, y para entender cómo se relacionan las variables entre sí, sus distribuciones y frecuencias. Al interpretar los datos, la visualización es útil para mostrar posibles tendencias o patrones en los datos. Por último, la visualización es una gran herramienta para la difusión del conocimiento. Pero recuerde, con un gran poder viene una gran responsabilidad, y las relaciones espurias dejan de ser graciosas cuando la gente se las toma demasiado en serio12. Monogan (2015 cap. 3) ya había explicado de forma sencilla para los científicos sociales, por qué la visualización de datos es importante cuando se trabaja con datos cuantitativos. En la introducción del capítulo, Monogan afirma la importancia y las ventajas de trabajar con cifras, desde la simple distribución de variables, valores atípicos o sesgos, hasta las tendencias a lo largo del tiempo. Por esta razón, la visualización de datos es una herramienta crucial para cualquiera que trabaje con datos. No es, de ninguna manera, un “movimiento estético”, la gráfica es extremadamente útil. Sin embargo, para algunas personas, la visualización de datos es tanto un elemento funcional para el análisis como un elemento estético por excelencia. Para Edward Tufte (2006), visualizar los datos de manera efectiva tiene un componente artístico inevitable. Con formación de estadístico y un doctorado en Ciencias Políticas de la Universidad de Yale, Edward Tufte se dedicó a entender y explicar cómo la ciencia y el arte tienen en común una observación a ojos abierto que genera información empírica. Su libro Beautiful Evidence (Tufte 2006) describe el proceso de cómo observar se transforma en mostrar, y cómo la observación empírica se convierte en explicaciones y pruebas. Necesitamos entender que la visualización de datos es un lenguaje como cualquier otro. Como emisores, necesitamos conocer nuestra audiencia: quiénes son los receptores de nuestro mensaje, si es una audiencia experta o sólo el público en general. En cualquier circunstancia, ajustaríamos nuestro mensaje al tipo de audiencia. Lo mismo ocurre cuando visualizamos los datos. Los gráficos que hacemos deben adaptarse a nuestro público. Sin embargo, incluso con las personas más conocedoras no debemos entusiasmarnos demasiado. No se trata de aplicar todo lo que sabemos inmediatamente, sino de entender lo que estamos tratando de comunicar. Comprender las funciones de este lenguaje es esencial. En la siguiente subsección hablaremos de cómo funciona ggplot2. A partir de ahora, comenzaremos con ejemplos aplicados. Los tipos de representación visual más comunes son el histograma, el gráfico de barras, el gráfico de densidad y el gráfico de líneas. Además, introduciremos otros paquetes de utilidades para hacer gráficos más sofisticados. Finalmente, aprenderemos sobre otros paquetes que pueden ser útiles dentro de las ciencias sociales, y en particular, las ciencias políticas, como son sf y ggparliament. Consejo: Después de este capítulo, si quieres aprender más sobre la visualización de datos, consulta Data Visualization: A Practical introduction de Kieran Healy, un libro disponible de forma gratuita que es divertido y útil para aprender ggplot2 paso a paso. En este libro no sólo encontrará una parte teórica, sino también una práctica. Por otro lado, la página web From Data to Viz puede ayudarte a aprender a presentar tus datos, pero no sólo eso: tanto si trabajas con R como con Python, puedes encontrar los paquetes y códigos para su aplicación. 3.2 Primeros pasos Ahora que entendemos el proceso antes de construir un gráfico, tenemos que familiarizarnos con ggplot2, el paquete para crear gráficos que es parte del tidyverse. A Layered Grammar of Graphics, de Hadley Wickham, explica en detalle cómo funciona esta nueva “gramática” para hacer gráficos. Recomendamos que se lea de la fuente original cómo se creó este paquete para entender más tarde el uso de las capas en la construcción de los gráficos. Aunque el uso de ggplot2 se expandió rápidamente, dentro de la comunidad R hay constantes discusiones sobre la enseñanza de ggplot2 como primera opción sobre los gráficos base de R. Por ejemplo, David Robinson tiene en su blog diferentes entradas sobre este tema, donde explica en detalle las ventajas de ggplot2 sobre otras opciones. Si eres un principiante en R, empezar con ggplot2 te dará una herramienta poderosa, y su curva de aprendizaje no es tan empinada como las R básico. Algunas ventajas que David Robinson menciona en “Por qué uso ggplot2”13 son: Subtítulos. R básico requiere más conocimiento de los usuarios para poder añadir subtítulos en los gráficos. Nuestro amigo ggplot2 lo hace automáticamente. ¡Facetas! Básicamente, podemos crear subgráficos con una tercera o cuarta variable y superponerlos, lo que nos permitirá una mejor comprensión del comportamiento de nuestros datos. Funciona junto con tidyverse. Esto significa que podemos hacer más con menos. Al final de este capítulo entenderán lo que quiero decir. Hay atajos para todo. Estéticamente, es mejor. Hay miles de opciones de paletas cromáticas, temas y fuentes. Si no te gusta, hay una forma de cambiarlo. Con esto en consideración, empecemos con lo práctico. 3.2.1 Las capas del “universo ggplotiano” Empecemos con nuestro tema de interés: ¿Cómo funciona ggplot2? Este paquete está incluido en el tidyverse, por lo que no es necesario cargarlo por separado. Además, utilizaremos las herramientas de ambos paquetes a lo largo de todo el capítulo. Así, el primer paso es cargar el paquete: library(tidyverse) La intuición detrás de ggplot2 es directa. La construcción de los datos se basa en capas que contienen un cierto tipo de información. 3.2.1.1 Datos La primera capa corresponde a los datos que usaremos. Para hacerlo más demostrativo, cargaremos la base datos que se utilizarán a lo largo del capítulo. library(paqueteadp) data(&quot;datos_municipales&quot;) El conjunto de datos debería estar ahora en nuestro ambiente. Estos datos corresponden a la información de los municipios chilenos. Algunos son del Servicio Electoral y otros del Sistema Nacional de Información Municipal de Chile. En la primera base de datos, encontramos los resultados electorales de las elecciones locales, regionales y nacionales del país; mientras que en la segunda encontramos las características económicas, sociales y demográficas de los municipios chilenos. En este caso, tenemos los datos electorales comunales de 1992 a 2012, con datos descriptivos como la población, el ingreso total del municipio, el gasto en asistencia social y el porcentaje de personas en situación de pobreza en base al total comunal de la Encuesta de Caracterización Socioeconómica Nacional (CASEN). glimpse(datos_municipales) ## Rows: 1,011 ## Columns: 6 ## $ anio &lt;chr&gt; &quot;2004&quot;, &quot;2004&quot;, &quot;2004&quot;, &quot;2004&quot;, &quot;2004&quot;, &quot;2004&quot;, … ## $ zona &lt;chr&gt; &quot;Norte Grande&quot;, &quot;Norte Grande&quot;, &quot;Norte Grande&quot;, … ## $ municipalidad &lt;chr&gt; &quot;Alto Hospicio&quot;, &quot;Arica&quot;, &quot;Camarones&quot;, &quot;Camina&quot;,… ## $ genero &lt;chr&gt; &quot;0&quot;, &quot;0&quot;, &quot;1&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;… ## $ ingreso &lt;int&gt; 1908611, 12041351, 723407, 981023, 768355, 55806… ## $ pobreza &lt;dbl&gt; NA, 23.5, 10.6, 37.3, 58.3, 38.8, 31.3, 7.7, 23.… Al mirar a la base de datos, encontramos que hay variables continuas (numéricas) y categóricas (de texto). Saber con qué tipo de variable estamos trabajando es esencial para el siguiente paso. 3.2.1.2 Mapeos estéticos La segunda capa corresponde al mapeo de las variables dentro del espacio. En este paso, usamos mapping=aes(), que contendrá la variable que tendremos en nuestros ejes x e y. Para aes(), hay muchas opciones que veremos a lo largo del capítulo: algunas de ellas son, por ejemplo, fill, color, shape, y alpha. Todas estas opciones son un conjunto de señales que nos permitirán traducir mejor lo que queremos decir a través de nuestro gráfico. Normalmente, estas opciones se llaman estéticas o aes(). ggplot(data = datos_municipales, mapping = aes(x = anio, y = pobreza)) Figura 3.3: Marco vacío El resultado muestra un marco vacío. Esto se debe a que no le hemos dicho a R qué objeto geométrico usar. 3.2.1.3 Objeto geométrico Suena extraño, pero cuando hablamos del objeto geométrico o geom, nos referimos al tipo de gráfico que queremos hacer, ya sea un gráfico lineal, un gráfico de barras, un histograma, un gráfico de densidad, o un gráfico de puntos, o si queremos hacer un gráfico de cajas. Esto corresponde a la tercera capa. En este caso, ya que tenemos datos de la encuesta CASEN, haremos un gráfico de caja para ver cómo se distribuyen los municipios en nuestra muestra. ggplot(data = datos_municipales, mapping = aes(x = anio, y = pobreza)) + geom_boxplot() Figura 3.4: Añadiendo un objeto geométrico a su gráfica Lo primero que notamos es la ausencia de datos durante tres períodos. Desafortunadamente, no hay datos anteriores a 2002, por lo que no se encuentran entradas para esos años. Por ello, es una gran idea filtrar los datos y dejar sólo los años que contienen datos sobre la encuesta CASEN. Además de eso, nuestro gráfico no nos dice mucho sobre el porcentaje de pobreza y su distribución. Considerando la geografía de Chile, es una gran idea ver la distribución de la pobreza por zona de la región geográfica. 3.2.1.4 Facetas Ahora, usaremos nuestras nuevas habilidades para hacer dos cosas: primero, usaremos filter() para conservar sólo los años que nos interesan. Segundo, dividiremos los resultados por zonas usando facet_wrap(), que corresponde a la cuarta capa que podemos usar para construir un gráfico con ggplot2. Cuando usamos esta capa, lo que queremos es organizar las “geoms” que estamos usando en función de una variable categórica. En este caso, la zona. Sin embargo, las facetas, como acción, son mucho más que eso. facet_wrap() y facet_grid() pueden adoptar una serie de argumentos, siendo el primero el más importante. En este caso, la sintaxis que utilizamos es la misma que se usa para las fórmulas en R, y denotamos el primer argumento con un signo “~”. Con los argumentos nrow = y ncol = podemos especificar cómo queremos ordenar nuestro gráfico. Finalmente, añadimos dos líneas de código, una para filtrar y otra para subdividir nuestra información. Esto es lo que logramos: ggplot(data = datos_municipales %&gt;% filter(anio == c(2004, 2008, 2012)), mapping = aes(x = anio, y = pobreza)) + geom_boxplot() + facet_wrap(~ zona, nrow = 1) Figura 3.5: Añadiendo una faceta a su gráfico Tanto con facet_wrap() como con facet_grid() podemos usar más de un argumento, pero los resultados son diferentes. facet_wrap() no sólo ordena los geoms, sino que es capaz de cruzarlos, creando gráficos con dos o más dimensiones usando variables categóricas. Mira los siguientes ejemplos: ggplot(data = datos_municipales%&gt;% filter(anio == c(2004, 2008, 2012)), mapping = aes(x = anio, y = pobreza)) + geom_boxplot() + facet_wrap(zona ~ genero) Figura 3.6: Comparando wraps y grillas, ejemplo A ggplot(data = datos_municipales %&gt;% filter(anio == c(2004, 2008, 2012)), mapping = aes(x = anio, y = pobreza)) + geom_boxplot() + facet_grid(zona ~ genero) Figura 3.7: Comparando wraps y grillas, ejemplo B Este gráfico muestra que, por zonas, el porcentaje de pobreza ha variado considerablemente de 2004 a 2012, y que existe una gran variabilidad interregional. Además, nos muestra cómo ggplot2 ofrece resultados de alta calidad sin mucha complejidad. La función facet_wrap() es una capa opcional dentro de las múltiples capas de “Una Gramática de Gráficos en Capas”, pero es importante recordar que las otras tres deben estar presentes para cualquier tipo de resultados. 3.2.1.5 Transformaciones Otra capa que se puede usar es la que nos permite hacer transformaciones de escala en las variables. Normalmente, aparecerá con el nombre scale_x_discrete(), que variará dependiendo de la estética utilizada dentro de nuestro mapeo. Así, podemos encontrarnos con scale_fill_continous() o scale_y_log10(). Por ejemplo, podemos ver cómo se distribuyen los ingresos de los municipios según la tasa de pobreza de nuestra muestra. Normalmente, haríamos esto de la siguiente manera: ggplot(data = datos_municipales %&gt;% filter(anio == c(2004, 2008, 2012)), mapping = aes(x = pobreza, y = ingreso)) + geom_point() Figura 3.8: Ejemplo de una gráfica en la que no utilizamos la escala Lo más frecuente es que cuando usamos una variable relacionada con el dinero, aplicamos una transformación logarítmica. Sin embargo, ¿cómo se traduce esto en nuestra figura? ggplot(data = datos_municipales %&gt;% filter(anio == c(2004, 2008, 2012)), mapping = aes(x = pobreza, y = ingreso)) + geom_point() + scale_y_log10() Figura 3.9: Ejemplo de una trama en la que reescalamos el eje y 3.2.1.6 Sistema de coordenadas Por lo general, trabajaremos con un eje X y un eje Y. Hay funciones en ggplot2, como coord_flip, que nos permiten cambiar la dirección de nuestra gráfica. Sin embargo, también podemos utilizar este tipo de capa cuando trabajamos con datos geográficos, o cuando, por ejemplo, queremos hacer un gráfico de torta. Sin embargo, normalmente, no queremos hacer gráficos de pastel. Cuanto más se utilice ggplot2, más se aprenderá sobre cada opción. 3.2.1.7 Temas Cuando hacemos un mapeo de datos, usamos opciones estéticas. Cuando queremos cambiar el aspecto de un gráfico, cambiamos el tema. Esto se puede hacer a través de theme(), que permite modificar cosas que no están relacionadas con el contenido del gráfico. Por ejemplo, los colores de fondo o la fuente de las letras en los ejes. También puedes cambiar el lugar donde se ubicará la leyenda o el título. Por último, también puedes cambiar el título, el nombre de los ejes, añadir anotaciones, etcétera. Sólo tienes que saber labs() y annotate(). Ahora es el momento de aplicar todo lo que “aparentemente” ya entendemos. 3.3 Ejemplo aplicado: Elecciones locales y visualización de datos Como hemos mencionado anteriormente, la cuestión principal es entender que la visualización nos permite explorar nuestros datos y responder a preguntas sustantivas de nuestra investigación. Normalmente, los medios, las desviaciones estándar u otro tipo de parámetros no nos dicen mucho. Podemos expresar los mismos datos visualizándolos. Por ejemplo, un diagrama de caja puede ser útil para representar la distribución de los datos y ver sus posibles valores atípicos, mientras que un gráfico de barras puede ayudarnos a observar la frecuencia de nuestros datos categóricos, y un gráfico lineal es práctico para comprender el cambio a lo largo del tiempo. Estos son sólo algunos ejemplos dentro de una variedad de posibilidades. En esta tercera sección, aprenderemos a visualizar diferentes tipos de gráficos con datos de la reelección municipal en Chile. Para contextualizar, la división político-administrativa más pequeña de Chile es la comuna o municipio, que cada cuatro años elige a sus autoridades locales: un alcalde y un consejo municipal. Desde 1992 a 2000, los alcaldes fueron elegidos indirectamente, y desde 2004 comenzaron a ser elegidos directamente por los ciudadanos. Como ya conocemos nuestros datos, podemos empezar con los más simples. Una buena idea, por ejemplo, es ver el número de mujeres elegidas como alcaldes en comparación con el número de hombres elegidos. Para ello, podemos utilizar un gráfico de barras. Como aprendimos en la sección anterior, para construir cualquier tipo de gráfico necesitamos saber la(s) variable(s) que queremos usar y qué geometría o geom nos permite representarla. En este caso, usaremos geom_bar() para ver cuántos hombres y mujeres han sido elegidos desde 1992. 3.3.1 Gráfico de barras plot_a &lt;- ggplot(datos_municipales, mapping = aes(x = genero)) plot_a + geom_bar() Figura 3.10: Gráfica simple de barras Como podemos ver, construir un gráfico de barras es una tarea fácil. Vemos que, a partir de 2004, más de 800 hombres fueron elegidos como alcaldes, un número que supera con creces el número de mujeres elegidas para el mismo cargo en el mismo período. Tal vez, este número ha cambiado con el tiempo, y no podemos verlo en este tipo de gráfico? Esto parece ser una buena razón para usar facet_wrap. plot_a + geom_bar() + facet_wrap(~anio, nrow = 1) Figura 3.11: Gráfico de barras con una faceta por año Como vemos, el número de mujeres alcaldesas parece aumentar, aunque es un aumento mucho menor del que se esperaría. Esto podría ser un problema sustantivo para hacer un análisis del gobierno local en Chile. Geometrías como geom_bar, geom_col, geom_density y geom_histogram tienden a no llevar un eje Y explícito en su estética, ya que son un recuento en el eje horizontal. Sin embargo, uno puede modificar el eje vertical en estas geometrías aplicando algún tipo de transformación. Por ejemplo, al especificar y=..prop.. como una estética dentro del objeto geométrico, estamos ordenando el cálculo de la proporción, no el conteo. Normalmente, usaremos aes() además de los datos en ggplot(), pero dependiendo de tus preferencias, también es posible usarlo con geom. Esta última es más común cuando ocupamos más de una base de datos o cuando queremos hacer una transformación. Por ejemplo, podríamos estar interesados en el número de autoridades locales por zona geográfica. Para ello, sería útil utilizar una proporción, ya que cada zona geográfica está formada por un número diferente de municipios. De esta manera, será más fácil comparar la situación entre las zonas. plot_a + geom_bar(mapping = aes(y = ..prop.., group = 1)) + facet_wrap(~zona, nrow = 1) Figura 3.12: Gráfico de barras con una faceta por zona ¿Pero por qué usamos grupo=1? Cuando queremos calcular una proporción con y=..prop.., tenemos que tomar algunas precauciones si estamos usando facet_wrap. Esta función no calcula la proporción basada en la suma de ambos géneros por zona. Por ejemplo, esta función registra que hay 89 hombres y 13 mujeres elegidos en la zona del Gran Norte. Concluye que “en el Gran Norte, los 89 hombres corresponden al 100% de los hombres elegidos y las 13 mujeres al 100% de las mujeres elegidas”. Claramente, esto no es lo que intentamos representar en el gráfico. Por eso usamos group=1. Intenta ver el resultado sin group=1 para comprobar lo que sucede. Ya lo hemos hecho! Vemos que no hay grandes diferencias, donde la zona del “Norte Pequeño” es la que tiene más mujeres en la alcaldía que hombres. Sin embargo, no hay grandes diferencias entre las zonas, y los resultados del primer gráfico de barras se replican en este. Ahora, podemos cambiar la presentación del gráfico. Todo buen gráfico debe contener, por ejemplo, un título claro, la fuente de los datos y el detalle de los ejes. Sugerencia. El Chicago Guide to Writing about Multivariate Analysis (Miller 2013) tiene muchos buenos consejos sobre cómo crear gráficos efectivos. plot_a + geom_bar(mapping = aes(y = ..prop.., group = 1)) + facet_wrap(~zona, nrow = 1) + labs(title = &quot; Proporción de hombres y mujeres elegidos como alcaldes (2004-2012)\\n Por zonas económicas de Chile&quot;, x = &quot;Género&quot;, y = &quot;Proporción&quot;, caption = &quot; Fuente: Basado en datos de SERVEL y SINIM (2018)&quot;) Figura 3.13: Gráfico de barras con título y fuentes Ahora, sólo tenemos que añadir etiquetas para el eje X. Podemos hacerlo fácilmente con scale_x_discrete(). Tienes que considerar qué estética de aes() modificarás, ya que esto cambiará la scale = que necesitas. Si examináramos las etiquetas desde fill =, por ejemplo, tendríamos que usar scale_fill_discrete(). También hay que tener en cuenta el tipo de variable que se utiliza. scale_x_discrete() no tiene “discrete” al final sin motivo. Como comprenderás, depende totalmente del tipo de variable que estamos usando. plot_a + geom_bar(mapping = aes(y = ..prop.., group = 1)) + facet_wrap(~zona, nrow = 1) + scale_x_discrete(labels = c(&quot;Hombres&quot;, &quot;Mujeres&quot;)) + labs(title = &quot; Proporción de hombres y mujeres elegidos como alcaldes (2004-2012)\\n Por zonas económicas de Chile &quot;, x = &quot;Género&quot;, y = &quot;Proporción&quot;, caption = &quot; Fuente: Basado en datos de SERVEL y SINIM (2018)&quot;) Figura 3.14: Gráfica con etiquetas de grupo Consejo. Con labels = podemos cambiar las etiquetas. Considera el número de valores de tu variable categórica para que coincidan con la variable, y no pierdas ninguna categoría. ### Gráfico de líneas En el último gráfico de la sección anterior vimos que, aunque la elección de mujeres como alcaldesas en Chile ha aumentado, este aumento no parece ser significativo: en 2012, sólo el 13% de los alcaldes elegidos eran mujeres. Tal vez esto se deba a que los cambios socioeconómicos no han afectado las percepciones de los roles de género en la sociedad. El examen de los datos económicos de los ingresos municipales o del porcentaje de pobreza según la CASEN podría ayudarnos a comprender por qué la elección de mujeres en las instancias municipales no ha aumentado sustancialmente. Para ello, podemos utilizar geom_line, el objeto geométrico que permite observar la evolución en el tiempo de nuestro sujeto de interés. La intuición sería hacer la figura de esta manera: plot_b &lt;- ggplot(data = datos_municipales, mapping = aes(x = anio, y = ingreso)) plot_b + geom_line() Figura 3.15: Una especificación errónea para un gráfico de líneas El problema es que no da el resultado esperado. La intuición es correcta, pero tenemos que ayudar a geom_line() con algunas especificaciones. En este caso, se agrupa por lo que tiene más sentido: por año. Por eso tenemos que especificar cuál es la variable que agrupa toda la información y, como sabemos, la información que poseemos está agrupada por municipio. Cuando añadimos esta información, el resultado cambia y se parece a lo que buscamos: plot_b + geom_line(mapping = aes(group = municipalidad)) Figura 3.16: Evolución anual de los ingresos por municipio Una de las cuestiones que surge a primera vista es que, teniendo en cuenta que Chile tiene 345 municipios, parece imposible mostrarlos todos en un solo gráfico. Ahora, podemos separar el gráfico como lo hemos hecho antes. Se puede hacer por zonas o regiones, considerando sus intereses. Ya hemos visto resultados diferentes por zonas, por lo que valdría la pena ver los ingresos de la misma manera: plot_b + geom_line(aes(group = municipalidad)) + facet_wrap(~zona, nrow = 1) Figura 3.17: Evolución anual de los ingresos por municipio enfrentado por zona Como nuestra muestra se compone de un pequeño número de años, no podemos ver mucha variabilidad y, a primera vista, los ingresos de todos los municipios han aumentado considerablemente. Tal vez, todavía podemos hacer algunos ajustes a nuestro gráfico. Lo más probable es que no estés familiarizado con la notación científica y te resulte mejor leer números grandes. Tal vez sepas que es mejor trabajar con una variable monetaria en su transformación logarítmica, como nos han enseñado en diferentes cursos de metodología. Además, puede que quieras añadir otro tipo de información en este gráfico, por ejemplo, los promedios. ¿Qué piensas de este gráfico? medias &lt;- datos_municipales %&gt;% group_by(zona) %&gt;% summarize(media_ingreso = mean(ingreso, na.rm = T)) plot_b + geom_line(color = &quot;gray70&quot;, aes(group = municipalidad)) + geom_hline(aes(yintercept = media_ingreso), data = medias, color = &quot;dodgerblue3&quot;) + scale_x_discrete(expand = c(0,0)) + scale_y_log10(labels = scales::dollar) + facet_wrap(~ zona, nrow = 1) + labs(title = &quot; Ingresos municipales en los años electorales (2004-2012)&quot;, y = &quot; Ingresos&quot;, x = &quot;Año&quot;) + theme(panel.spacing = unit(2, &quot;lines&quot;)) Figura 3.18: Versión completa de nuestro gráfico de líneas para los ingresos de los municipios en los años electorales ¿Qué hemos especificado? Primero, creamos un conjunto de datos (“promedio”) que contiene los ingresos medios de cada zona. Lo hicimos usando group_by() y summarize() del tidyverse. datos_municipales %&gt;% group_by(zona) %&gt;% summarize(media_ingreso = mean(ingreso, na.rm = T)) ## # A tibble: 5 x 2 ## zona media_ingreso ## &lt;chr&gt; &lt;dbl&gt; ## 1 Austral 2609648. ## 2 Central 7302625. ## 3 Norte Chico 4816249. ## # … with 2 more rows Luego, especificamos el color de la geom_line(). Después de eso, añadimos a nuestro código geom_hline(). Este objeto geométrico, como geom_vline() o geom_abline(), nos permite añadir líneas con información. En este caso, lo usé para agregar el ingreso promedio de cada zona. Especificamos la variable que contiene la media yintercept = mean, la base de datos means, y el color con color = \"dodgerblue3\". A continuación, usamos scale_x_discrete() para especificar la expansión de los paneles. Si antes veíamos un espacio gris sin información, lo eliminamos. Esto es estético. Luego, usamos scale_x_discrete() para escalar nuestros datos. Esta es una transformación logarítmica que se hace normalmente cuando trabajamos con modelos lineales que contienen datos monetarios. Además, cambiamos las etiquetas del eje y: ya no aparece con notación científica. Esto se hizo con un paquete llamado scales. Aquí llamamos a la función directamente con scales::dollar. Añadimos el título y los nombres de los ejes x e y con labs(). Finalmente, especificamos la información sobre el tema. Sin ella, los años entre un panel y otro se colapsarían. Para eso, lo especificamos con panel.spacing = unit(2, \"lines\") en la capa de theme(). 3.3.2 Gráfico de caja Ya vimos que los ingresos de los municipios en Chile aumentaron entre 2004 y 2012. Si bien miramos el gráfico sin transformaciones funcionales, observamos que algunos municipios tenían ingresos muy superiores al promedio y se destacaban dentro de sus zonas. La intuición es que probablemente son extravagantes. Pudimos verlo claramente con un gráfico de caja, que nos permite graficar diversos datos descriptivos en nuestras variables como la mediana, el mínimo y el máximo. En este caso, lo utilizaremos para observar si nuestra intuición es correcta o no14. Comencemos filtrando los datos como lo hicimos en el gráfico anterior. En nuestro eje x colocaremos las zonas de Chile y en el eje y los ingresos: plot_c &lt;- ggplot(data = datos_municipales %&gt;% filter(anio %in% c(2004, 2008, 2012)), mapping = aes(x = zona, y = ingreso, color = zona)) + geom_boxplot() + facet_wrap(~anio, ncol = 1) plot_c Figura 3.19: Recuadro de ingresos del municipio por zona, facetado por año Podemos ver valores atípicos muy claros. Tal vez, luego de mirar estos resultados, nos gustaría identificar qué municipalidades tienen mayores ingreso. Para esto podemos usar el mapeo estético label =, parte de geom_text(). Para etiquetas solo para los valores atípicos, debemos hacer un filtro en nuestra base: plot_c + geom_text(data = municipal_data %&gt;% filter(income &gt; 50000000), mapping = aes(label = municipality)) Desafortunadamente, las etiquetas están sobre los puntos y, en algunos casos, estos se superponen cuando están cerca uno del otro. Podemos resolver esto con el paquete ggrepel, que tiene un elemento geométrico geom_text() “mejorado” que evita la coalición de las etiquetas: library(ggrepel) plot_c + geom_text_repel(data = datos_municipales %&gt;% filter(ingreso &gt; 50000000), mapping = aes(label = municipalidad)) Figura 3.20: Podemos arreglar las etiquetas que se superponen usando el ggrepel El límite puede estar en 50.000.000 de dólares o en números mayores o menores. Depende enteramente de lo que queramos observar. Además, con geom_text o geom_text_repel no sólo podemos cambiar el color, sino también el tipo de fuente del texto, o si debe estar en negrita, cursiva o subrayado. Para ver más opciones, puedes escribir ?geom_text o llamar a un help(\"geom_text\"). También podríamos añadir otra información o cambiar la forma en que se presenta actualmente el gráfico. plot_c + coord_flip() + geom_text_repel(data = datos_municipales %&gt;% filter(ingreso &gt; 50000000), mapping = aes(label = municipalidad), color = &quot;black&quot;, size = 3) + scale_y_continuous(labels = scales::dollar) + labs(title = &quot;Ingresos municipales por zona (2004-2012)&quot;, x = &quot;Ingresos&quot;, y = &quot;Zona&quot;, caption = &quot; Fuente: Basado en datos del SINIM (2018)&quot;) Figura 3.21: La versión pulida de nuestra gráfica de caja Algunas otras especificaciones: Hemos añadido la información descriptiva en el gráfico. Cambiamos el tamaño de la fuente. Esto era importante debido a la cantidad de municipios que están por encima de 50.000.000 dólares de ingresos. De nuevo, cambiamos las etiquetas del eje y con scales::dollar. Por último, con guides, y especificando las aes() que queríamos dirigir, escribimos el código color=F para eliminar la etiqueta, ya que era información repetida dentro del gráfico. Ejercicio 3A. Te invitamos a jugar con geom_text: cambiar los colores, el tamaño, las fuentes, etcétera. También te animamos a instalar paquetes que te permitan personalizar aún más tus gráficos: ggthemes de jrnorl tiene temas para gráficos de programas y revistas conocidas como Excel o The Economist. Por otro lado, hrbrthemes de hrbrmstr ha elaborado algunos temas minimalistas y elegantes que harán que todos tus gráficos se vean mejor. Si te gustan los colores, puedes consultar el paquete wespalette de karthik, una paleta cromática basada en las películas de Wes Anderson, o crear tus propias paletas basadas en imágenes con colorfindr. Puedes encontrar más sobre esto último en el siguiente link. 3.3.3 Histograma Como observamos en nuestro boxplot, muchos municipios, especialmente los de la zona central, están muy por encima de la media de ingresos por zona. Podemos ver la distribución de estos datos a través de un histograma. Construir un histograma es una tarea fácil, y como se mencionó anteriormente, geom_histogram no tiene un eje y explícito, ya que cuenta la frecuencia de un evento dentro de un intervalo. Al crear el histograma según nuestra intuición, el resultado es el siguiente: ggplot(data = datos_municipales, mapping = aes(x = ingreso)) + geom_histogram() ## Warning: Removed 7 rows containing non-finite values (stat_bin). Figura 3.22: La versión más simple de un histograma de los ingresos fiscales del municipio Como podemos ver, el gráfico da una “Advertencia” que indica la existencia de “738 filas que contienen valores no finitos”. Esta advertencia ha estado presente a lo largo de todo este capítulo, y no significa nada más que “Hay valores desconocidos dentro de esta variable” y se debe a que no hay datos de los primeros años. Así que no te preocupes, si filtramos los datos con filter(!is.na(ingreso)), esta advertencia seguramente desaparecerá. Además, la consola da el siguiente mensaje: stat_bin() usando bins = 30. Elija mejores valores con binwidth. Simplemente, dice que es posible modificar los intervalos para el histograma. El siguiente paso es modificar el eje x. Personalmente, nunca he sido buena leyendo números con notación científica. Por otro lado, intentaremos cambiar el número de intervalos con bins. ggplot(data = datos_municipales, mapping = aes(x = ingreso)) + geom_histogram(bins = 50) + scale_x_continuous(labels = scales::dollar) ## Warning: Removed 7 rows containing non-finite values (stat_bin). Figura 3.23: Histograma de los ingresos fiscales del municipio con una escala corregida en x Ejercicio 3B. ¿Qué pasa si ponemos bins = 15de intervalos? A continuación haremos un subconjunto de los datos. Considerando el número de valores atípicos que encontramos, eliminaremos los municipios con ingresos superiores a 50.000.000 dólares. También podemos examinar la frecuencia por zona. Como cuando usamos color con geom_boxplot, usaremos fill con geom_histogram. ggplot(data = datos_municipales %&gt;% filter(ingreso &lt; 50000000), mapping = aes(x = ingreso, fill = zona)) + geom_histogram(alpha = 0.5, bins = 50) + scale_x_continuous(labels = scales::dollar) + labs(title = &quot;Número de municipios según sus ingresos anuales (2004-2012)&quot;, x = &quot;Ingreso&quot;, y = &quot; Número de municipios&quot;, caption = &quot; Fuente: Basado en los datos del SINIM (2018)&quot;) Figura 3.24: Versión pulida de nuestro histograma en el que haremos ‘fill’ por zona 3.3.4 Relación entre las variables Es probable que una de tus mayores preocupaciones sea si las dos variables que estás estudiando están relacionadas de alguna manera. Con ggplot2 esto es fácil de verificar. En este caso, tenemos dos variables continuas: la tasa de pobreza, del conjunto de datos de CASEN, y los ingresos municipales. Siguiendo la teoría, debería haber algún tipo de correlación: cuanto mayor sea el ingreso municipal, menor será la tasa de pobreza en el municipio. Creamos nuestros datos: plot_f &lt;- ggplot(data = datos_municipales, mapping = aes(x = pobreza, y = log(ingreso))) Para este tipo de gráfico, usaremosgeom_smooth. Con este objeto, puedes modificar la forma en que las variables se relacionan con method. También puedes introducir tus propias fórmulas. Por defecto, se especifica una relación lineal entre las variables, por lo que no es necesario escribirla. plot_f + geom_smooth(method = &quot;lm&quot;, color = &quot;dodgerblue3&quot;) Figura 3.25: Ajuste lineal de la pobreza en el log de ingresos Parece vacía, ¿no? Normalmente, usamos geom_smooth con otras figuras geométricas, como geom_point, para indicar la posición de las columnas en el espacio. Usamos alpha para ver la superposición de los puntos. Como no son demasiados, no hay problemas para ver cómo se distribuyen. plot_f + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, color = &quot;dodgerblue3&quot;) Figura 3.26: El ajuste lineal más las observaciones dispersas Ahora podemos hacer dos mejoras. Primero, insertaremos el título y el nombre de los ejes. Segundo, en geom_x_continuous especificaremos donde empieza y acaba nuestra gráfica. Ya habíamos usado esto con geom_line. plot_f + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, color = &quot;dodgerblue3&quot;) + scale_x_continuous(expand = c(0,0)) + labs(title = &quot; Relación entre los ingresos del municipio y la tasa de pobreza de CASEN, Chile (2004-2012)&quot;, x = &quot;Tasa de pobreza de CASEN&quot;, y = &quot; Ingresos&quot;, caption = &quot; Fuente: Basado en los datos del SINIM (2018)&quot;) Figura 3.27: La versión pulida de nuestra gráfica de ajuste lineal Claramente, hay una correlación negativa entre ambas variables. ¡Esto es lo que esperábamos! Ahora, podemos calcular la correlación entre ambas variables, para estar más seguros de los resultados obtenidos visualmente: cor(datos_municipales $pobreza, datos_municipales$ingreso, use = &quot;pairwise.complete.obs&quot;) ## [1] -0.27 La correlación entre ambas variables es de -0,27. Sería interesante añadir esta información en el gráfico. Podemos hacer esto con annotate(). Sólo necesitamos especificar el tipo de objeto geométrico que queremos generar. En este caso, lo que queremos crear es el texto geom = \"text\", pero podría ser una caja que resalte un punto específico en el gráfico geom = \"rect\" o una línea geom = \"segment\". Especificamos dónde queremos ubicarlo y, finalmente, anotamos lo que queremos anotar. plot_f + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, color = &quot;dodgerblue3&quot;) + scale_x_continuous(expand = c(0, 0)) + labs( title = &quot; Relación entre los ingresos del municipio y la tasa de pobreza de CASEN, Chile (2004-2012)&quot;, x = &quot;Tasa de pobreza de CASEN &quot;, y = &quot; Ingresos&quot;, caption = &quot; Fuente: Basado en los datos del SINIM (2018)&quot;) + annotate(&quot;text&quot;, x = 50, y = 15, label = &quot;Correlación:\\n-0.27&quot;) Figura 3.28: Añadimos la coeficiente de correlación usando annotate 3.4 Para continuar aprendiendo Hay muchos caminos para visualizar sus datos. En este capítulo, aprendiste las principales funciones de ggplot2, un paquete dentro de tidyverse, pero hay muchos más paquetes que pueden ser de gran ayuda para otros tipos de visualizaciones. Aunque ggplot2 puede no tener todos los objetos geométricos que necesitas, hay paquetes para visualizar otros tipos de datos que funcionan bajo ggplot2 y las capas que constituyen su estructura “gramatical”. ### Otros paquetes: 3.4.0.1 sf Permite visualizar elementos espaciales. Para ggplot2 funciona con geom_sf. Permite la creación de figuras geométricas con diferentes tipos de datos espaciales. En el capítulo 16 de datos espaciales, Andrea y Gabriel entregan las herramientas para trabajar con sf, sus principales funciones y directrices. Aquí puede encontrar más detalles sobre cómo instalarlo y su rendimiento dependiendo de su ordenador. 3.4.0.2 ggparliament Todos los politólogos deberían conocer este paquete. Permite visualizar la composición del poder legislativo. Es un sueño para aquellos que trabajan con este tipo de información. Te permite especificar el número de escaños, el color de cada partido, y añadir diferentes características a tu gráfico. Aquí puedes encontrar más detalles sobre las herramientas de ggparliament. Figura 3.29: Ejemplo de una gráfica construida con ggparliament con datos del Parlamento Mexicano.^[Source: (“@leonugo”, Twitter)[https://twitter.com/leonugo/status/1014298553500479489?lang=es] 3.4.0.3 ggraph Si estudias redes y sabes cómo funciona ggplot2, este paquete puede convertirse en tu nuevo mejor amigo. Está hecho para todo tipo de datos relacionales, y aunque funciona bajo la lógica de ggplot2, tiene sus propios objetos geométricos, facetas, entre otros. Aquí puedes encontrar más información. El Capítulo 14 te mostrará cómo usar este paquete en profundidad. 3.4.0.4 patchwork Esta es una gran herramienta para combinar diferentes ggplots en el mismo gráfico. Usarás +, | y / para organizarlos. library(patchwork) (plot_f | plot_b) / plot_c Figura 3.30: Ejemplo de patchwork Ejercicio 3C. Ya hemos aprendido a hacer un histograma, sin embargo, los gráficos de densidad tienden a ser más utilizados para mirar la distribución de una variable. Usando las mismas variables, haz una gráfica de densidad con geom_density. Ejercicio 3D. Normalmente, los gráficos de barras se presentan con la frecuencia o proporción dentro de la barra. También podemos hacer esto con el ggplot2. Usando geom_bar y geom_text, apunta el número de alcaldes por área geográfica. Un consejo: tienes que hacer algunos cálculos con tidyverse antes de añadir esa información en la gráfica. Ejercicio 3E. Escogiendo sólo un año, haz un gráfico de líneas con geom_smooth que indique la relación entre los ingresos y la tasa de pobreza. Ahora, con annotate, haz ungráfico de caja que contenga los municipios con mayor índice de pobreza y, encima de él, escribe el nombre del municipio correspondiente. E-mail: snaraya@uc.cl↩︎ Aun así, siempre es entretenido observar cómo existe una correlación entre el consumo de queso per cápita y el número de personas estranguladas hasta la muerte por sus sábanas en los Estados Unidos!↩︎ http://varianceexplained.org/r/why-I-use-ggplot2/↩︎ El capítulo 16 será muy útil si desea detectar los valores atípicos a través de los mapas↩︎ "],
["load.html", "Capítulo 4 Carga de bases 4.1 Introducción 4.2 Diferentes formatos de bases de datos 4.3 Los archivos separados por delimitadores (.csv y .tsv) 4.4 Grandes bases de datos tabulares", " Capítulo 4 Carga de bases Soledad Araya15 y Andrés Cruz16 Los paquetes que necesitas instalar tidyverse (Wickham 2019b), paqueteadp (Urdinez and Cruz 2020), haven (Wickham and Miller 2020), readxl (Wickham and Bryan 2019), data.table (Dowle and Srinivasan 2019), ff (Adler et al. 2020). 4.1 Introducción Cargar bases de datos no siempre es una tarea fácil. Las personas que crean base de datos utilizan diferentes formatos de archivo, tratando de optimizar diversos criterios (facilidad para los usuarios , software disponible, tamaño, etcétera). A veces, la información que se necesita está distribuida en múltiples bases de datos pequeñas. En otros casos, el problema depende de la magnitud de los datos, con bases de datos que, empleando los métodos habituales de carga de datos, afectarán al rendimiento de la computadora. Las siguientes tres subsecciones ayudarán a sortear estos desafíos. A diferencia de los otros capítulos de este libro, aquí necesitarás descargar bases de datos directamente a tu computadora sin usar el paquete del libro. La idea es que aprendas a cargar datos en el mundo real! Comienza creando una nueva carpeta en tu ordenador, que será la carpeta de tu proyecto. Llamaremos a nuestra nueva carpeta de proyecto, muy originalmente, load-dataset. Luego, descarga desde este enlace el archivo .zip con las bases de datos para este capítulo, y guárdalo en la carpeta que creaste. En Windows, por ejemplo, esto debería ser similar a esta imagen: Figura 4.1: Carpeta con archivo .zip Después, necesitas descomprimir el contenido del archivo .zip. La forma exacta de hacerlo dependerá de tu sistema operativo y sus programas, pero tiende a ser algo similar a Haz clic derecho &gt; Descomprimir aquí. Ahora deberías tener una subcarpeta llamada “files”: Figura 4.2: Carpeta con subcarpeta ya descomprimida Revisa la nueva subcarpeta. Aquí están las bases de datos que se van a utilizar en este capítulo. Como verás, aprenderemos a cargar y utilizar múltiples formatos comunes: Figura 4.3: Subcarpeta con archivos de datos ¡Ya casi hemos terminado! Como se ha explicado con más detalle en el capítulo 2, vamos a utilizar Proyectos de RStudio con el fin de ordenar nuestro trabajo. Vamos a crear un proyecto de RStudio haciendo clic en la esquina superior derecha de RStudio, “Proyecto…”, y luego “Nuevo Proyecto”. Seleccionando “Directorio Existente” en la siguiente ventana puedes vincular el proyecto a la carpeta que ya has creado (en este caso, load-dataset): Figura 4.4: Creando un nuevo proyecto de RStudio en una carpeta ya existente ¡Perfecto! Ahora deberías tener un nuevo proyecto para empezar a trabajar de forma ordenada con los datos de este capítulo. 4.2 Diferentes formatos de bases de datos En esta subsección, aprenderás a cargar diferentes formatos de bases de datos en R: separados por delimitadores, creados en R, con etiquetas en Stata o SPSS, o desde Excel. 4.3 Los archivos separados por delimitadores (.csv y .tsv) Un archivo de texto plano (flat-file dataset) es un archivo constituido únicamente por texto o caracteres. Es un formato soportado por diferentes programas, y se puede trabajar con él sin esfuerzo. La única diferencia radica en los delimitadores que utiliza: (1) en el caso del .csv, el delimitador es una coma (,) y, (2) en el caso del .tsv, está delimitado por una tabulación (o parachoques) que se expresan con &lt;TAB&gt; o \\t. 4.3.1 R básico: utils R básico tiene funciones que leen este tipo de archivos. Probablemente has encontrado read.table or read.csv, que son funciones específicas que leen archivos csv con el paquete utils. La primera debería ser suficiente para abrir cualquiera de estos archivos. Sólo tenemos que centrarnos en las características específicas de cada uno. Como dijimos antes, las separaciones de estos archivos suelen ser diferentes, y esta función nos permite especificar el tipo de separación (“,” , “;” , “|” o \") con el argumento sep =. Sin embargo, hay mejores opciones para cargar estos tipos de bases de datos. Aunque read.table() puede ser amigable cuando se aprende R, no será suficiente para trabajar con archivos cada vez más grandes. La segunda desventaja de read_table() es que, normalmente, no funcionará correctamente con archivos que contengan caracteres especiales, ¡una gran desventaja para los no angloparlantes! Para esos casos, tenemos diferentes alternativas que presentamos a continuación. 4.3.1.1 readr El paquete readr se encarga de introducir una serie de funciones que pueden leer diferentes tipos de archivos. A primera vista, la diferencia entre las funciones utils y readr es un guión bajo (pasamos de read.table a read_table). Sin embargo, las funciones de readr son mejores cuando se trabaja con bases de datos grandes, y es mucho más rápido cuando se cargan. readr, como muchos de los paquetes ya presentados, es parte de los paquetes principales de tidyverse. Puedes cargarlo individualmente o a través de tidyverse. library(tidyverse) library(readr) # alternativa individual Comencemos presentando una de as bases de datos que se utilizarán en este capítulo, que proviene del proyecto Desiguales del PNUD - Programa de las Naciones Unidas para el Desarrollo. Este proyecto buscó mapear la complejidad de las desigualdades en Chile con la ayuda de una encuesta aplicada en 2016. Utilizaremos una pequeña subsección de esta base de datos, que contiene información para 300 encuestados a través de 20 variables. Como habrás notado, contamos con la base de datos en seis formatos diferentes para el desarrollo de este capítulo: Figura 4.5: Subcarpeta con bases de datos ¿Cómo podemos cargar la base de datos en el formato .csv? Como puedes sospechar, empezaremos con la base de datos más simple de cargar. Gracias a la función read_csv() en readr, sólo tendremos que escribir la ruta del archivo dentro de nuestro proyecto: df_desiguales_csv &lt;- read_csv(&quot;data/desiguales.csv&quot;) Empezamos el nombre del objeto con “df” como abreviatura de dataframe. Para confirmar que nuestro archivo está en el sistema en forma como objeto, usaremos ls(). Con esta función se puede ver la lista de los objetos creados o cargados. También puedes comprobarlo en el panel “Environrment” de RStudio. Figura 4.6: Panel de Environment, arriba a la derecha ls() ## [1] &quot;df_desiguales_csv&quot; Ahora, confirmamos con class() que este archivo está en el formato dataframe y tibble. Debido a que seguiremos usando las herramientas de tidyverse, es útil trabajar con este formato. class(df_desiguales_csv) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Es importante saber más sobre la función para poder sortear los obstáculos que surgen al cargar bases de datos en este formato. Para empezar, puede ser que tu archivo esté separado por un tipo diferente de separador. Puedes definirlo con delim=, que cumple la misma función que sep= en read.table. Además, cuando se manejan archivos con un cierto tipo de digitación, es útil usar locale para especificar separadores decimales y de miles. Los países angloparlantes usan un punto para separar los decimales, pero Europa continental y los países latinoamericanos usan una coma. Una segunda nota: Probablemente viste la función read_csv2() sugerida por R Studio cuando escribiste read_csv en R. La única diferencia entre las dos es el carácter que usamos para delimitar nuestros datos: read_csv2() usa “;” como delimitador por defecto, mientras que read_csv() usa “,”. Para los archivos .tsv se puede usar la función read_tsv(), que no está incluida por defecto en el paquete utils. Una vez cargada la base, exploremos brevemente los datos en los que estamos trabajando con glimpse(), como hicimos en el capítulo anterior. Las herramientas del paquete dplyr deberían estar ya disponibles, ya que previamente has cargado tidyverse. Usaremos glimpse() para ver un breve panorama de las primeras 10 columnas de nuestros datos. df_desiguales_csv %&gt;% select(1:10) %&gt;% glimpse() ## Rows: 300 ## Columns: 10 ## $ id &lt;dbl&gt; 34, 36, 70, 75, 99, 121, 122, 128, 160, 163, 166, 17… ## $ sexo &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2… ## $ zona &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2… ## $ macrozona &lt;dbl&gt; 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4… ## $ region &lt;dbl&gt; 13, 13, 7, 7, 13, 7, 7, 13, 7, 7, 7, 5, 7, 13, 13, 1… ## $ edad &lt;dbl&gt; 63, 52, 73, 78, 22, 51, 18, 21, 57, 41, 55, 64, 26, … ## $ p1_anyo &lt;dbl&gt; 1952, 1963, 1943, 1938, 1993, 1964, 1997, 1995, 1958… ## $ p1_mes &lt;dbl&gt; 8, 7, 2, 2, 12, 11, 10, 1, 12, 4, 10, 12, 12, 10, 10… ## $ p2 &lt;dbl&gt; 1, 1, 4, 7, 8, 1, 5, 5, 3, 3, 7, 4, 5, 1, 5, 1, 1, 5… ## $ p3 &lt;dbl&gt; 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 1, 1, 1, 3, 1, 2, 3… ¡Todo se ve muy bien! Sigamos explorando otras formas de cargar datos en R. 4.3.2 Archivos creados con R (.Rdata y .rds) Ahora nos centraremos en los archivos creados con R, que tienen una extensión .Rdata (.rda también es válido) o .rds. Estos formatos de archivo nos permiten almacenar cualquier objeto en R, ya sea un vector, un dataframe, una matriz, una lista, etc. Después de leer el archivo, R cargará el objeto tal y como fue guardado. Así pues, los archivos en este formato destacan por (a) su flexibilidad a la hora de almacenarlos, sin limitarse a su base de datos, y (b) por su perfecta compatibilidad con R: por ejemplo, puedes estar seguro de que cada variable de una base de datos .rds se cargará en el formato correcto (vectores numéricos como numéricos, factores como factores, etc.17) La diferencia entre un archivo .Rdata (o .rda) y un archivo .rds es simple pero importante: mientras que el primero puede contener cualquier número de objetos, el segundo se limita a un solo objeto. Ahora aprenderás a cargar los objetos creados en R, en cualquiera de los dos formatos. 4.3.2.1 Archivos .Rdata (uno o más objetos) Los archivos .Rdata pueden contener más de un objeto. Aunque esto suena conveniente, incluye una limitación: al cargar los objetos, estos adoptarán automáticamente el nombre con el que fueron creados. Por ejemplo, en el archivo .Rdata para la base de datos de Desiguales hemos guardado dos objetos: el dataframe que vimos anteriormente (ahora llamado df_desiguales_rdata) y un vector numérico para las edades de los encuestados (llamado age_vector). Para cargar el archivo sólo hay que usar la función load(). load(&quot;data/desiguales.Rdata&quot;) Como se ha dicho anteriormente, puedes usar el comando ls() para comprobar si los objetos se cargaron correctamente en la sesión R. Si los dos objetos fueron cargados correctamente, entonces ambos deben ser nombrados, además del marco de datos anterior originado de un archivo .csv. ls() ## [1] &quot;df_desiguales_csv&quot; &quot;df_desiguales_rdata&quot; &quot;vector_edades&quot; 4.3.2.2 Archivos .rds (sólo un objeto) Los archivos .rds se limitan a almacenar un solo objeto. Aunque suena menos atractivo que la flexibilidad del .Rdta, este formato se destaca por su modularidad, que ayuda a mantener sus archivos en orden. Otro rasgo positivo del .rds es que la sintaxis utilizada para cargar estos archivos es familiar, muy similar a la utilizada para cargar los archivos .csv. Ahora puedes ponerle un nombre al objeto cuando lo creas. El comando es el siguiente, usando la función read_rds() (que hemos cargado antes con el tidyverse): df_desiguales_rds &lt;- read_rds(&quot;data/desiguales.rds&quot;) De nuevo, puedes usar el comando ls() o RStudio para asegurarte de que el objeto haya sido creado sin problemas en la sesión: ls() ## [1] &quot;df_desiguales_csv&quot; &quot;df_desiguales_rdata&quot; &quot;df_desiguales_rds&quot; ## [4] &quot;vector_edades&quot; 4.3.3 Bases de datos con etiquetas (de Stata or SPSS) Las bases de datos con etiquetas son de uso común en las ciencias sociales, especialmente en los archivos SPSS (.sav) y Stata (.dta). La idea principal es relativamente simple: la información explicativa se guarda en el archivo además de los valores de la base de datos. Por ejemplo, la variable p2 de la base de datos de Desiguales, que corresponde a la segunda pregunta de la encuesta, podría tener una etiqueta que describa la pregunta en sí (“Estado civil”) o la pregunta explícita (“Independientemente de si tiene o no una pareja en este momento, ¿cuál es su estado civil o marital actual?”). Llamaremos a este tipo de etiqueta etiqueta variable. Además, las etiquetas pueden registrar información sobre los valores de las variables. Por ejemplo, la pregunta p2 tiene las siguientes respuestas posibles: Valor Etiqueta 1 Casado por primera vez 2 Casado por segunda vez o más 3 Legalmente casado, pero soltero de hecho 4 Divorciado 5 Soltero, nunca se ha casado 6 Soltero, pero con un matrimonio legalmente anulado 7 Viudo 8 En pareja 88 No sabe 99 No contesta Una base de datos puede almacenar los valores numéricos registrados por el encuestador, mientras que la información explicativa se puede guardar en etiquetas. Aunque una base de datos sólo puede registrar un “4” en la variable p2 para un caso específico, su etiqueta nos ayuda a identificar el estado civil contestado por el encuestado (“Divorciado”). Llamaremos a este tipo de etiqueta etiqueta de valor. En la siguiente sección, aprenderá a cargar en R las bases de datos de SPSS (.sav) y Stata (.dta) con etiquetas. Esto te permitirá reportar su análisis por etiquetas, tanto para las variables como para los valores. Necesitarás el paquete haven para usar esta función. Si instalaste tidyverse en el pasado ya tienes acceso a haven. Sin embargo, library(tidyverse) no es suficiente para instalarlo, así que tendrás que cargarlo por separado:18 library(haven) Los comandos utilizados para cargar las bases de datos con etiquetas son similares a los utilizados anteriormente para cargar los archivos .csv y .rds. La función usada para leer los archivos SPSS es read_spss(), mientras que la de Stata es read_stata(). Vamos a cargar ambas bases de datos: df_desiguales_spss &lt;- read_spss(&quot;data/desiguales.sav&quot;) df_desiguales_stata &lt;- read_stata(&quot;data/desiguales.dta&quot;) Puede comprobar si los objetos fueron creados usando el comando ls() o en el panel “Entorno” de RStudio. ls() ## [1] &quot;df_desiguales_csv&quot; &quot;df_desiguales_rdata&quot; &quot;df_desiguales_rds&quot; ## [4] &quot;df_desiguales_spss&quot; &quot;df_desiguales_stata&quot; &quot;vector_edades&quot; class(df_desiguales_spss) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; class(df_desiguales_stata) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Las bases de datos con etiquetas se diferencian de los demás no por la clase del objeto, sino por sus variables. Utilizando las herramientas aprendidas en el capítulo 2, puedes explorar las primeras diez variables de cualquiera de las bases de datos (ambos darán los mismos resultados, por lo que a partir de ahora utilizaremos la base de datos de SPSS). df_desiguales_spss %&gt;% select(1:10) %&gt;% glimpse() ## Rows: 300 ## Columns: 10 ## $ id &lt;dbl&gt; 34, 36, 70, 75, 99, 121, 122, 128, 160, 163, 166, 17… ## $ sexo &lt;dbl+lbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, … ## $ zona &lt;dbl+lbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, … ## $ macrozona &lt;dbl+lbl&gt; 4, 4, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 2, 4, 4, 4, … ## $ region &lt;dbl+lbl&gt; 13, 13, 7, 7, 13, 7, 7, 13, 7, 7, 7, 5, … ## $ edad &lt;dbl&gt; 63, 52, 73, 78, 22, 51, 18, 21, 57, 41, 55, 64, 26, … ## $ p1_anyo &lt;dbl+lbl&gt; 1952, 1963, 1943, 1938, 1993, 1964, 1997, 1995, … ## $ p1_mes &lt;dbl+lbl&gt; 8, 7, 2, 2, 12, 11, 10, 1, 12, 4, 10, 12, … ## $ p2 &lt;dbl+lbl&gt; 1, 1, 4, 7, 8, 1, 5, 5, 3, 3, 7, 4, 5, 1, 5, 1, … ## $ p3 &lt;dbl+lbl&gt; 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 1, 1, 1, 3, 1, … Como podrás notar, la mayoría de las variables no son sólo vectores numéricos (“dbl” o “doble”19), sino que también incluyen etiquetas (+ “lbl”). Obtengamos los primeros seis valores y la información de la variable etiquetada “p2” utilizando el comando head(): head(df_desiguales_stata$p2) ## &lt;labelled&lt;double&gt;[6]&gt;: P2 - Â¿podria decirme cual es su estado conyugal o civil actual? ## [1] 1 1 4 7 8 1 ## ## Labels: ## value label ## 1 Casado(a) por primera vez ## 2 Casado(a) por segunda vez o mÃ¡s ## 3 Casado(a) legalmente, pero separado de hecho ## 4 Divorciado ## 5 Soltero(a), nunca se ha casado ## 6 Soltero(a), pero con un matrimonio legalmente anulado ## 7 Viudo(a) ## 8 Conviviente o pareja ## 88 NS (no leer) ## 99 NR (no leer) ## &lt;labelled&lt;double&gt;[6]&gt;: P2 - ¿podria decirme cual es su estado conyugal...? ## [1] 1 1 4 7 8 1 ## ## Labels: ## value label ## 1 Casado(a) por primera vez ## 2 Casado(a) por segunda vez o mÃ¡s ## 3 Casado(a) legalmente, pero separado de hecho ## 4 Divorciado ## 5 Soltero(a), nunca se ha casado ## 6 Soltero(a), pero con un matrimonio legalmente anulado ## 7 Viudo(a) ## 8 Conviviente o pareja ## 88 NS (no leer) ## 99 NR (no leer) Analicemos brevemente el resultado mostrado en la consola. En primer lugar, podemos ver que la variable en cuestión es un vector numérico con una etiqueta (“”). Luego, después de los dos puntos, registra la etiqueta de variable: “P2 - ¿cuál es su estado civil o marital actual?”. Después, se muestran los seis primeros valores de la variable, tal y como pedimos a través del comando head(). Por último, se encuentran las etiquetas de valor *, que nos dan información sobre el significado de cada número en el contexto de una variable específica. De esta manera, obtenemos toda la información registrada por las etiquetas en esta base de datos. Con los comandos aprendidos se puede estar seguro del significado de cada valor en la base de datos etiquetada. Por último, note que R, con la ayuda de haven, también mostrará las etiquetas en otros casos cuando sea conveniente. Por ejemplo, veamos un simple resumen de las dos primeras variables de la base de datos. ¡Mira qué fácil es leerlo! df_desiguales_stata %&gt;% select(region, p2) ## # A tibble: 300 x 2 ## region p2 ## &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; ## 1 13 [Metropolitana de Santiago] 1 [Casado(a) por primera vez] ## 2 13 [Metropolitana de Santiago] 1 [Casado(a) por primera vez] ## 3 7 [Del Maule] 4 [Divorciado] ## # … with 297 more rows 4.3.4 Archivos de Excel Aunque la mayoría de las veces utilizaremos bases de datos que vienen en los formatos anteriores, es importante reconocer que no todas las instituciones presentan su información de esta manera. A menudo te enfrentarás a formatos que traen dolores de cabeza. Por ejemplo, Excel. En Chile, una cantidad considerable de organizaciones gubernamentales todavía trabajan con Excel, y el problema no es el formato sino la estructura de las bases de datos. La mayoría de las veces nos enfrentaremos a algo como esto: Figura 4.7: Base de datos del SINIM (Sistema Nacional de Información Municipal) O como esto: Figura 4.8: Base de datos del CEAD Aquí es donde surgen los problemas. Para empezar, carguemos uno de los paquetes más usados para leer archivos de Excel (sea .xls o .xlsx), readxl, un paquete de tidyverse20: library(readxl) Para el próximo ejemplo, utilizaremos la base de datos del CEAD (Centro de Estudios y Análisis del Delito). El CEAD es una institución chilena que se centra en la vigilancia y el estudio de la delincuencia en el país. Su objetivo es crear y analizar información sobre el crimen para ayudar a la formación de políticas públicas. Esta base de datos es la que se puede descargar por defecto en su página web. Para cargar el archivo, usaremos read_excel()de readxl. Sólo es necesario poner la ruta del archivo .xls o .xlsx: df_cead_excel &lt;- read_excel(&quot;data/cead.xls&quot;) Primero, comprobemos que los datos fueron cargados correctamente en el Environment con ls(). Luego, usemos glimpse para examinar las primeras observaciones en nuestra base de datos: ls() ## [1] &quot;df_cead_excel&quot; &quot;df_desiguales_csv&quot; &quot;df_desiguales_rdata&quot; ## [4] &quot;df_desiguales_rds&quot; &quot;df_desiguales_spss&quot; &quot;df_desiguales_stata&quot; ## [7] &quot;vector_edades&quot; df_cead_excel %&gt;% glimpse() ## Rows: 79 ## Columns: 7 ## $ Medida &lt;chr&gt; &quot;Tipo de Datos&quot;, NA, &quot;Unidad Territorial&quot;, &quot;Regione… ## $ Frecuencia &lt;chr&gt; &quot;Casos Policiales&quot;, NA, NA, &quot;Región Metropolitana&quot;,… ## $ ...3 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ ...4 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ ...5 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ ...6 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ ...7 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… Algo se ve mal. Si miramos de nuevo la imagen de la base de datos de la CEAD, nos daremos cuenta de lo que está pasando. Primero, el archivo de Excel tiene en sus filas iniciales información de la base de datos, pero estas no son observaciones. Figura 4.9: Base de datos del CEAD (Centro de Estudios y Análisis del Delito). Para resolver esto, es necesario usar skip =. Este argumento nos ayudará a omitir las filas que no nos interesan, en este caso, las primeras 18: df_cead_excel_v2 &lt;- read_excel(&quot;data/cead.xls&quot;, skip = 18) Otra forma de hacer lo mismo es delimitar la información que queremos cargar a través de un rango. Así, podemos delimitar el rango con range, obteniendo sólo la información que necesitamos dentro del rectángulo especificado. El rango puede ser delimitado con la letra de la columna y el número de la fila en el archivo .xls o .xlsx. df_cead_excel_v3 &lt;- read_excel(&quot;data/cead.xls&quot;, range = &quot;A20:G81&quot;) Comprobamos que los datos fueron cargados correctamente con ls(), y luego podemos ver el tipo de archivo con class(). Para los propósitos de este capítulo, sólo usaremos el último archivo .xls cargado (df_cead_excel_v3). ls() ## [1] &quot;df_cead_excel&quot; &quot;df_cead_excel_v2&quot; &quot;df_cead_excel_v3&quot; ## [4] &quot;df_desiguales_csv&quot; &quot;df_desiguales_rdata&quot; &quot;df_desiguales_rds&quot; ## [7] &quot;df_desiguales_spss&quot; &quot;df_desiguales_stata&quot; &quot;vector_edades&quot; class(df_cead_excel_v3) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Como pueden observar, es muy fácil cargar este tipo de archivos después de practicar durante un tiempo. Por ahora, esto es todo lo que necesitas saber para cargar archivos .xls o .xlsx. Es posible que tengas que lidiar con problemas de digitación, con nombres de variables imposibles o con errores de tecleo que pueden causar más de un problema al trabajar con skip. Pero ya sabes cómo usar algunas herramientas de tidyverse que pueden ayudarte con eso. Intentemos hacer algunos cambios básicos en la base de datos del CEAD: names(df_cead_excel_v3) ## [1] &quot;...1&quot; &quot;2013&quot; &quot;2014&quot; &quot;2015&quot; &quot;2016&quot; &quot;2017&quot; &quot;2018&quot; El primer problema es el nombre de la columna que indica la comuna. El segundo problema es que queremos una base de datos ancha, ¡así que no nos ayuda tener los años como columnas! Para el primer problema, usamos rename, y para el segundo pivot_longer. df_cead_excel_v4 &lt;- df_cead_excel_v3 %&gt;% rename(county = `...1`) %&gt;% pivot_longer(cols = -county, names_to = &quot;year&quot;, values_to = &quot;n_crime&quot;) %&gt;% filter(county != &quot;Unidad Territorial&quot;) Otro problema que puede surgir al utilizar los archivos de Excel es que la información puede estar dividida en diferentes hojas. Como verás, read_excel() resuelve esos problemas de una manera muy simple. Para demostrarlo, utilizaremos la base de datos del SINIM (Sistema Nacional de Información Municipal). El SINIM es un sistema de información chileno a través del cual los ciudadanos pueden acceder a una variedad de datos relativos a la administración del condado: df_sinim_excel &lt;- read_excel(&quot;data/sinim.xls&quot;, sheet = 2, skip = 2, na = &quot;Not received&quot;) Con el argumento sheet= podemos seleccionar la hoja que queremos cargar en R. Se puede cargar tanto con la posición como con el nombre de la hoja. Como ya hemos aprendido, con skip seleccionamos el número de filas que queremos saltar al cargar la base de datos, y con na podemos hacerle saber al programa qué otra frase, palabra o situación tiene que ser listada como NA además de las celdas en blanco. Una de las principales tareas cuando se trabaja en Excel es la limpieza de la base de datos. Desafortunadamente, estos dos ejemplos están muy extendidos, y para tratar algunos de estos problemas usamos janitor. Por lo general, los nombres de las variables en Excel tienden a venir de una manera muy detallada y/o descriptiva: pueden tener espacios, caracteres especiales y mayúsculas que dificultan el trabajo con nuestras variables de interés. Por ejemplo, los nombres en las columnas de esta base de datos tienen todo lo anterior: names(df_sinim_excel) ## [1] &quot;CODIGO&quot; &quot;MUNICIPIO&quot; ## [3] &quot;INGRESOS MUNICIPALES ($) 2017&quot; &quot;INGRESOS MUNICIPALES ($) 2016&quot; ¿Te imaginas tener que escribir esos nombres cada vez que queramos hacer algún tipo de análisis? Para eso tenemos la función clean_names() del paquete janitor. Este paquete fue creado para facilitar la limpieza de los datos, y, sin ser un paquete tidyverse, puede ser usado sin problemas con las tuberías. Y, para nuestra suerte, está optimizado para trabajar con readr y readxl. La función clean_names() funciona de una manera simple después de instalar janitor (con install.packages(“janitor\")): df_sinim_excel_v2 &lt;- df_sinim_excel %&gt;% janitor::clean_names() Ahora, veamos de nuevo los nombres de las variables: names(df_sinim_excel_v2) ## [1] &quot;codigo&quot; &quot;municipio&quot; ## [3] &quot;ingresos_municipales_2017&quot; &quot;ingresos_municipales_2016&quot; ¡Se ve mucho mejor! Estas funciones hacen que trabajar en R sea una experiencia mucho más simple y amigable, sin importar tu nivel de experiencia y lo que estés buscando hacer. Trabajar con nombres de variables puede ser un dolor de cabeza, y esta herramienta te ayudará a evitar el exceso de trabajo, permitiéndote centrarte en las partes importantes del análisis (que, por supuesto, no deberían ser nombres de columnas). Con estas herramientas, deberías estar más que preparado para enfrentarse al horrible mundo de trabajar con Excel. 4.4 Grandes bases de datos tabulares Junto con los avances tecnológicos y las conexiones más rápidas a Internet, las grandes bases de datos son muy atractivas para los científicos sociales. Recientemente ha habido un boom en lo que se llama “big data”. Sin embargo, las herramientas habituales de gestión de bases de datos tabulares tienden a no funcionar correctamente en las grandes bases de datos, y es necesario encontrar alternativas. La complejidad de enfrentarse al tamaño de las bases de datos depende de múltiples factores (por ejemplo, la naturaleza de los datos y las características de la computadora), pero, en general, una base de datos tabulares de más de 1 GB de tamaño generará problemas en R en una computadora personal media. ¿Qué podemos hacer para manejar este tipo de base de datos? En la siguiente sección, trabajarás con la base de datos Desiguales en formato .csv, tal y como hiciste en la subsección 4.2 de este capítulo. Pretenderemos que se trata de un caso de una base de datos “gigante”, a pesar de que sólo tiene 15KB de tamaño. Por cierto, si quieres probar hacer el siguiente análisis con bases de datos del mundo real que son de interés para la ciencia política, puedes encontrar algunos para descargar en el Observatorio de Complejidad Económica, que registra el comercio bilateral de los países por diferentes categorías productivas21. Entonces, ¿cómo podemos manejar una base de datos muy grande? Una primera alternativa es evaluar la necesidad de utilizar toda la base de datos para el análisis, o, si se puede acortar antes de utilizarla. Para iniciar un análisis exploratorio que pueda aclarar este punto, recomendamos usar el argumento n_max= en read_csv() y sus funciones hermanas (por ejemplo, read_tsv()). De esta manera podemos examinar las primeras cien observaciones de la base de datos, haciendo que el proceso de cálculo sea menos exigente: df_desiguales_large_100 &lt;- read_csv(&quot;data/desiguales.csv&quot;, n_max = 100) Seguramente tu computador manejará con facilidad esta nueva base de datos reducida. Ahora, ¿qué pasa si, después de comprobar los datos, descubres que sólo necesitas un par de variables para el análisis? Recortar la base de datos permitirá un uso eficiente de los recursos computacionales. Observe que el argumento n_max ya no es útil, ya que recorta las filas en lugar de las columnas. Supongamos que de la base de datos Desiguales (la “gigante”) sólo necesitas las variables age y p2. Con una función de asistencia de nuestro paquete paqueteadp (cols_only_chr()), puedes hacer que read_rds()cargue la base de datos con sólo esas dos variables, omitiendo todas las demás, y además evitando el costo computacional que implican: library(paqueteadp) ## Warning: The following named parsers don&#39;t match the column names: age df_desiguales_laod_2vars &lt;- read_csv(&quot;data/desiguales.csv&quot;, col_types = cols_only_chr(c(&quot;age&quot;, &quot;p2&quot;))) Sin embargo, es posible que algunos análisis no permitan la omisión de variables, o que incluso después de todos los procedimientos para reducir su tamaño, la base de datos siga siendo demasiado grande. En este caso, el ecosistema de R también proporciona alternativas. Una primera opción son las funciones fread() del paquete data.table. Esta función, optimizada para la velocidad, suele ser más rápida que read_csv(), aunque no tiene la misma variedad de opciones y no es tan amigable en su uso. Una vez que data.table se instala en nuestro sistema –es decir, después de install.packages(\"data.table\")– el siguiente comando nos permitirá cargar la base de datos: library(data.table) df_desiguales_large_fread &lt;- fread(&quot;data/desiguales.csv&quot;) Nota que, aunque el objeto creado es un marco de datos, también es un tipo especial llamado data.table: class(df_desiguales_large_fread) ## [1] &quot;data.table&quot; &quot;data.frame&quot; El paquete data.table tiene varias funciones para tratar con grandes tipos de objetos, y tiende a ser más eficiente desde el punto de vista computacional - sin embargo, a menudo es menos intuitivo y legible. Si quieres saber más sobre data.table, puedes consultar las múltiples viñetas disponibles en la página web del paquete. Por último, si data.table no es lo suficientemente eficiente para tratar tus datos, tenemos otra opción disponible. El paquete ff proporciona una solución interesante para el problema de las grandesbases de datos: en lugar de cargarlos en la RAM, como otros paquetes, ocupa directamente el disco duro (que normalmente tiene más espacio disponible). Aunque esto hace que la mayoría de las funciones tradicionales de R no funcionen, ff abre la puerta para usar bases de datos gigantes: el paquete trae toda una nueva familia de funciones de análisis ad hoc, inspiradas en R. Puede encontrar más información sobre ff, si es necesario, en su página web y en su R [archivo de ayuda] oficial(https://cran.r project.org/web/packages/ff/ff.pdf). Si quieres intentar cargar primero una base de datos con este paquete, puedes usar el siguiente comando22. -suponiendo que el paquete esté instalado, con install.packages(\"ff\"): library(ff) df_desiguales_large_ff &lt;- read.csv.ffdf(file = &quot;data/desiguales.csv&quot;) Ejercicio 4A. Desde la página web de Latinobarómetro, descarga la edición 2017 en formato SPSS (.sav) e impórtelas a R. Ten cuidado con las etiquetas. Ejercicio 4B. Ahora, repite el proceso de descarga de la base de datos de Stata (.dta). E-mail: snaraya@uc.cl↩︎ E-mail: arcruz@uc.cl↩︎ Esto no siempre es así en un archivo .csv, por ejemplo, que no contiene esta información almacenada. Lo que hace R para ese determinado formato -y otros- es inferir el tipo de formato↩︎ Aunque haven es una parte de tidyverse, no está incluido en el “núcleo” del mismo. El núcleo de tidyverse incluye algunos paquetes, que son los más usados. El resto de los paquetes, por ejemplo haven, necesitan ser cargados (pero no instalados) por separado↩︎ Esta es una forma de nombrar los números reales en la computación↩︎ Igual que haven, readxl es una parte de tidyverse, pero no un miembro principal de su núcleo. Por lo tanto, es necesario cargarlo por separado↩︎ Estos se encuentran en un formato .tsv, por lo que tendrás que hacer ligeras alteraciones en la sintaxis, tal y como aprendiste en la subsección de archivos .csv/.tsv (4.2)↩︎ Es importante añadir el argumento file =, ya que no es el primer argumento que recibe la función en cuestión↩︎ "],
["linear.html", "Capítulo 5 Modelos lineales 5.1 Mínimos cuadrados ordinarios en R 5.2 Modelo bivariado: regresión lineal simple 5.3 Modelo multivariado: regresión múltiple 5.4 Ajuste del modelo 5.5 Inferencia en los modelos lineales múltiples 5.6 Testeando los supuestos de MCO", " Capítulo 5 Modelos lineales Inés Fynn23 y Lihuen Nocetto24 Lecturas sugeridas Angrist, Joshua, Jörn-Steffen Pischke. (2008). Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton: Princeton University Press. Dunning, Thad. (2012). Natural Experiments in the Social Sciences. A design-based approach. Cambridge: Cambridge University Press. Lewis-Beck, C., &amp; Lewis-Beck, M. (2016). Applied Regression: An Introduction. SAGE Publications. Wooldridge, J. M. (2016). Introductory econometrics: A modern approach (6th ed.). Cengage Learning. Los paquetes que necesitas instalar tidyverse (Wickham 2019b), paqueteadp (Urdinez and Cruz 2020), skimr (Waring et al. 2020), car (Fox, Weisberg, and Price 2020), ggcorrplot (Kassambara 2019), texreg (Leifeld 2020), prediction (Leeper 2019), lmtest (Hothorn et al. 2019), sandwich (Zeileis and Lumley 2019), miceadds (Robitzsch, Grund, and Henke 2020). Introducción En este capítulo, aprenderemos a hacer regresiones lineales en R. Aquí la función a modelar es lineal, es decir, se estima por dos parámetros: la pendiente y el intercepto. Cuando nos enfrentamos a un análisis multivariado, la estimación se vuelve más compleja. Cubriremos en R cómo interpretar los diferentes coeficientes, cómo crear tablas de regresión, cómo visualizar los valores predichos, y profundizaremos en la evaluación de los supuestos de los mínimos cuadrados ordinarios (MCO), para que pueda evaluar lo bien que encajan tus modelos. 5.1 Mínimos cuadrados ordinarios en R En este capítulo, la base de datos que trabajaremos es una mezcla de dos bases de datos construidas por Evelyne Huber y John D. Stephens. Estas base de datos son: Base de datos bienestar en América Latina, 1960-2014 (Evelyne Huber y John D. Stephens, Latin American Welfare Dataset, 1960-2014, Universidad de Carolina del Norte en Chapel Hill, 2014): contiene variables sobre los Estados de bienestar en todos los países de América Latina y el Caribe entre 1960 y 2014. Base de datos política de América Latina y el Caribe, 1945-2012 (Evelyne Huber and John D. Stephens, Latin America and the Caribbean Political Dataset, 1945-2012, University of North Carolina at Chapel Hill, 2012): contiene variables políticas de todos los países de América Latina y el Caribe entre 1945 y 2012. La base de datos resultante tiene 1074 observaciones para 25 países entre 1970 y 2012 (se excluyeron los datos del decenio de 1960 porque contenían muchos valores perdidos o missing values). En primer lugar, cargamos el paquete tidyverse. library(tidyverse) Vamos a importar la base de datos del paquete del libro: library(paqueteadp) data(&quot;bienestar&quot;) Ahora, la base de datos ha sido cargada en nuestra sesión de R ls() ## [1] &quot;bienestar&quot; En el capítulo, utilizaremos el documento de Huber et al. (2006) como ejemplo para el análisis. En este artículo, se estiman los determinantes de la desigualdad en América Latina y el Caribe. Trabajar a partir de este artículo nos permite estimar un modelo con múltiples variables de control que ya han sido identificadas como relevantes para explicar la variación de la desigualdad en la región. Así, la variable dependiente que nos interesa explicar es la desigualdad de ingresos en los países de América Latina y el Caribe, operada de acuerdo al Índice de Gini (gini). Las variables independientes que incorporaremos al modelo son las siguientes: Dualismo sectorial (se refiere a la coexistencia de un sector tradicional de baja productividad y un sector moderno de alta productividad) - dualismo_sectorial PBI del país - pib Inversión Extranjera Directa (ingresos netos como % del PIB) - inversion_extranjera Diversidad étnica (variable dummy codificada como 1 cuando al menos el 20% pero no más allá del 80% de la población es étnicamente diversa) - diversidad_etnica Democracia (tipo de régimen) - tipo_regimen Gasto en educación (como porcentaje del PIB) - gasto_educ Gasto en salud (como porcentaje del PIB) - gasto_salud Gasto de seguridad social (como porcentaje del PIB) - gasto_segsocial Equilibrio legislativo - bal_legislativo En este capítulo se intentará estimar cuál es el efecto del gasto educativo en los niveles de desigualdad de los países de América Latina y el Caribe. Así, nuestra variable independiente de interés será gasto_educ. 5.1.1 Cómo obtener estadísticas descriptivas Antes de estimar un modelo lineal con mínimos cuadrados ordinarios (MCO) se recomienda identificar primero la distribución de las variables que te interesan: la variable dependiente \\(y\\) (también llamada variable de respuesta) y la variable independiente de interés \\(x\\) (también llamada variable explicativa o regresiva). En general, nuestros modelos tendrán, además de la variable independiente de interés, otras variables independientes (o explicativas) que llamaremos “controles”, ya que su tarea es hacer el escenario ceteris paribus lo más creíble posible. Es decir, los controles ayudarán a “mantener constantes todos los demás factores” para acercarnos lo más posible a un escenario en el que podamos controlar todas las variables que afectan a \\(y\\), y observar cómo la variación de una única variable independiente \\(x\\) afecta a la variación de la variable dependiente (\\(y\\)). Así, antes de estimar el modelo, observaremos las estadísticas descriptivas de las variables que se incluirán en el modelo (tanto las dependientes como las independientes). El objetivo es prestar atención a las siguientes cuestiones: Variación en \\(x\\): que las variables independientes (pero sobre todo la de interés) tengan variación en nuestra muestra. Si no hay variación en \\(x\\), no podemos estimar cómo esta variación afecta a la variación de \\(y\\). Variación en \\(y\\): si la variable dependiente no varía, no podemos explicar su variación según la variación de \\(x\\). Unidad de medida de las variables: aquí es donde evaluamos cómo se miden nuestras variables (además de revisar los libros de códigos que suelen venir con las bases de datos con los que estamos trabajando), para poder entender qué tipos de variables son (nominales, ordinales, continuas), y también para interpretar correctamente los resultados más adelante. Tipo de variables: en la estimación del MCO, la variable dependiente debe ser, en general, continua, (aunque es posible trabajar con variables dependientes dicotómicas). Por lo tanto, debemos asegurarnos de que la variable dependiente es continua y numérica. Además, es importante conocer el tipo de variable independiente y comprobar que su tipo es coherente con su formato de codificación (es decir, si tenemos una variable independiente de “rangos de edad”, la variable debe ser categórica o factorial, no numérica), para que nuestras interpretaciones de los resultados sean correctas. Los problemas con el formato de codificación son muy comunes, ¡ten cuidado! Identificar los valores perdidos: si nuestras variables tienen demasiados valores perdidos, debemos comprobar dónde están (en qué variables, para cuales observaciones) y, eventualmente, decidir si una imputación es deseable y posible (como se explica en el capítulo 11). 5.1.2 Estadísticas descriptivas y distribución de las variables en el modelo Una visualización inicial de nuestras variables de interés se puede hacer utilizando el comando skimr::skim() que nos da una idea de las variables (Figura 5.1): Figura 5.1: Skim de nuestra base de datos Notarás que los resultados están ordenados por el tipo de variable, e indica el número de valores perdidos de cada una de ellas, su respectiva media, su desviación estándar, los valores correspondientes de los percentiles y un pequeño histograma que muestra cómo está distribuida la variable. 5.1.3 Matriz de correlación de variables independientes Después de identificar todas las variables que incorporaremos al modelo, se recomienda observar cómo se relacionan entre sí. Para ello, crearemos una matriz de correlación de las variables independientes con el comando ggcorrplot() del paquete homónimo, con la que podremos evaluar la correlación de Pearson entre todas las variables. De todas formas, es importante recordar que la correlación no implica causalidad. Aquí, simplemente queremos entender si las variables del modelo están relacionadas de cierta manera. Este paso no sólo es importante para reconocer nuestros datos y variables, sino también porque queremos evitar tener multicolinealidad perfecta (que haya variables independientes que estén perfectamente correlacionadas) en nuestro modelo, ya que ésta es una suposición central de el MCO. library(ggcorrplot) corr_selected &lt;- bienestar %&gt;% select(gini, gasto_educ, dualismo_sectorial, inversion_extranjera, pib, diversidad_etnica, tipo_regimen, gasto_salud, gasto_segsocial, bal_legislativo, poblacion) %&gt;% # calcular la matriz de correlación y redondear a un decimal cor(use = &quot;pairwise&quot;) %&gt;% round(1) ggcorrplot(corr_selected, type = &quot;lower&quot;, lab = T, show.legend = F) Figura 3.3: Matriz de correlación entre las variables seleccionadas Ahora que conocemos todas las variables que se incorporarán al modelo, y cómo se correlacionan entre sí, profundizaremos en las variables clave de interés: las dependientes y las independientes. 5.1.4 Distribución de las variables de interés Como hemos mencionado anteriormente, queremos estimar cómo el cambio de una variable independiente (su variación) afecta a la variación de una variable dependiente. Es decir, cómo cambia \\(y\\) cuando cambia \\(x\\). En este caso, supongamos que nos interesa estimar cómo varían los niveles de desigualdad de un país (medidos como el porcentaje del PIB asignado al presupuesto de educación). Así, nuestra variable de interés independiente es el gasto en educación, mientras que la variable dependiente es la desigualdad. Observemos cómo se distribuyen estas variables en nuestra base de datos: ggplot(bienestar, aes(x = gini, na.rm = T)) + geom_histogram(binwidth = 1) + labs(x = &quot;Gini Index&quot;, y = &quot; Frecuencia&quot;, title = &quot; Distribución de la variable dependiente&quot;, caption = &quot;Fuente: Huber et al (2012)&quot;) ## Warning: Removed 649 rows containing non-finite values (stat_bin). Figura 5.2: Histograma de nuestra variable dependiente La variable independiente: Gasto en educación (% del PIB) ggplot(bienestar, aes(x = gasto_educ, na.rm = T))+ geom_histogram(binwidth = 1) + labs(caption = &quot;Fuente: Huber et al (2012))&quot;, x = &quot;Gasto en educacion&quot;, y = &quot;Frecuencia&quot;) ## Warning: Removed 325 rows containing non-finite values (stat_bin). Figura 5.3: Histograma de nuestra variable independiente: Gasto en educación como porcentaje del PIB 5.1.5 Relación entre la variable dependiente e independiente Después de observar cómo se distribuyen las variables de interés, podemos ver gráficamente cómo se relacionan. Es decir, graficamos la correlación entre estas dos variables: en el eje \\(x\\) (horizontal) ubicamos la variable independiente, mientras que en el eje \\(y\\) (vertical) la variable dependiente. Como resultado, cada “punto” de la Figura 5.4 representa una observación de nuestra muestra con un valor particular del gasto en educación (\\(x\\)) y un valor particular en el índice de Gini (\\(y\\)). ggplot(bienestar, aes(gasto_educ, gini)) + geom_point() + labs(x = &quot; Gasto en educación (% del PIB)&quot;, y = &quot;Gini&quot;, caption = &quot;Fuente: Huber and Stephens, 2012&quot;) ## Warning: Removed 718 rows containing missing values (geom_point). Figura 5.4: Scatter plot del gasto en educación y el índice de Gini Esta es una primera visualización de la relación entre nuestras variables que nos permite observar si hay algún tipo de relación entre ellas. Aquí vemos claramente una relación positiva (cuanto más alto es el gasto en educación, más alto es el Gini). De todos modos, hasta ahora no podemos decir nada concluyente sobre el efecto del gasto en educación en los niveles de desigualdad. Para ello es necesario estimar un modelo. Hasta ahora sólo hemos explorado nuestros datos. ¡Pasemos a las regresiones! Ejercicio 5A. Imagina que ahora estamos interesados en el efecto de la Inversión Extranjera Directa inversion_extranjera en la desigualdad económica (Gini). Analiza la distribución de esta variable y haga un gráfico para evaluar la relación entre esta variable y nuestra variable independiente (gini), ¿Hay algún signo de correlación entre las variables? ¿Cuál es la dirección (positiva/negativa) de la relación? 5.2 Modelo bivariado: regresión lineal simple El modelo lineal simple asume que una variable de respuesta aleatoria \\(y\\) es una función lineal de una variable independiente \\(x\\) más un término de error \\(u\\). También decimos que la variable dependiente \\(y\\) es el resultado de un proceso de generación de datos (DGP, por sus siglas en inglés) que puede escribirse como \\[ Y = \\beta_0 + \\beta_1x + u \\] Así, el modelo lineal implica definir que \\(Y\\) es generado por una función lineal de \\(x_1\\), además de un término constante \\(\\beta_0\\) y la variable \\(u\\) no observada. Otros dos supuestos son necesarios para derivar los estimadores del MCO. La primera se refiere a la expectativa de que \\(u\\) sea igual a 0 \\[E(u)=0\\] Esto implica que, a nivel de población, todos los factores no observados tienen un promedio de cero. La suposición más importante es que el promedio de \\(u\\) cada valor de \\(x\\) es cero: \\[E(u|x)=0\\] Esta suposición se conoce en la literatura econométrica como la media condicional cero, o independencia condicional. En la literatura experimentalista, se conoce como exogeneidad de \\(x\\), o que \\(x\\) y \\(u\\) son ortogonales. Todos estos términos implican la suposición de que, para cada valor de la variable independiente de interés \\(x\\), los factores no observados promediarán cero. En otras palabras, si la expectativa de los no observados es cero para cada valor \\(x\\), el valor esperado de \\(Y\\) sólo depende de \\(x\\) y de la constante. Cuando se cumplen estas suposiciones, podemos identificar el efecto de \\(x\\) en \\(y\\), manteniendo todo lo demás constante. Además, cuando \\(E(u|x) = 0\\), también es cierto que \\[cov(x,u) = 0\\] En resumen, bajo el supuesto de independencia condicional, \\(x\\) y \\(y\\) no se correlacionan y esto permite derivar los estimadores del MCO a través de las condiciones de primer orden. Las dos primeras condiciones de orden son que \\[E(u)=0\\] y \\[E(u|x)=0\\] Entender que estas son las condiciones que permiten derivar el estimador de MCO es clave para entender por qué no podemos probar la independencia del error a partir de una estimación. Esto significa que, por construcción, los residuos (\\(\\hat{u}\\)) de la regresión siempre promediarán cero en la muestra y en cada valor de \\(x\\). Exigir exogeneidad implica poder argumentar que \\(u\\) es efectivamente ortogonal a los \\(x\\), algo más creíble para los experimentos y cuasi-experimentos que los datos de observación (mira Gerber and Green 2012; Dunning 2012) 5.2.1 La función lm La función lm que forma parte de la base R es la principal herramienta para la estimación de los modelos lineales. La forma general que toma la función es lm(Y ~ 1 + X) A partir de la cual se entiende que un modelo lineal (lm, por sus siglas en inglés) se estima para una variable dependiente Y regresada (~) en una variable independiente X. El “1” no suele incluirse, pero lo añadimos para denotar la intercepción (\\(\\beta_0\\)). Basándonos en la investigación de Huber et al. (2006), nuestro modelo plantea que la desigualdad es una función lineal del gasto en educación, además de un término de error no observado \\(u\\) y una constante \\(\\beta_0\\). Formalmente: \\[ Desigualdad = \\beta_0 + \\beta_1 GastoEducacional + u \\] Por el momento, asumiremos que la base de datos contiene 1074 observaciones independientes.25. El supuesto de observaciones independientes y distribuidas de forma idéntica es lo que nos permite escribir el modelo para un individuo de \\(i\\) escogido al azar como \\[Desigualdad_i= \\beta_0 + \\beta_1GastoEducacional_i + u_i\\] Recuerda que la variable dependiente “Desigualdad” se representa con el índice de Gini y que el nombre de la variable es gini, mientras que la variable independiente “Gasto en educación” es gasto_educ. Dado que los datos se almacenan en un data.frame, necesitamos indicar su nombre en la función para traer los datos de la base de datos correspondiente. Eso es lo que ocurre después de la coma en el siguiente comando model_1 &lt;- lm(gini ~ 1 + gasto_educ, data = bienestar) # después de la coma indicamos el data.frame que contiene los datos class(model_1) # verificamos que la clase de objeto es &quot;lm&quot; ## [1] &quot;lm&quot; En la primera línea de código creamos un objeto (model_1) que guarda los resultados de la función lm. Esta función crea objetos de clase lm, que son vectores que incluyen los coeficientes estimados, los errores estándar, los residuos, los valores predichos, entre otros resultados de la estimación. Para ver los componentes del objeto, una forma rápida de hacerlo es usar la función summary. summary( model_1) ## ## Call: ## lm(formula = gini ~ 1 + gasto_educ, data = bienestar) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.04 -5.62 1.09 4.99 15.57 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 44.80 1.02 43.80 &lt; 2e-16 *** ## gasto_educ 1.23 0.25 4.93 1.3e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.7 on 354 degrees of freedom ## (718 observations deleted due to missingness) ## Multiple R-squared: 0.0642, Adjusted R-squared: 0.0615 ## F-statistic: 24.3 on 1 and 354 DF, p-value: 1.29e-06 Se pueden lograr presentaciones más elegantes con la función screenreg del paquete texreg. Esto se utilizará en todos los capítulos de los modelos econométricos. Veamos la presentación de los resultados con la función screenreg library(texreg) screenreg(model_1) ## ## ======================= ## Model 1 ## ----------------------- ## (Intercept) 44.81 *** ## (1.02) ## gasto_educ 1.23 *** ## (0.25) ## ----------------------- ## R^2 0.06 ## Adj. R^2 0.06 ## Num. obs. 356 ## ======================= ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Podemos agregar nombres a las variables con custom.coef.names library(texreg) screenreg(model_1, custom.model.names = &quot;Modelo 1&quot;, custom.coef.names = c(&quot;Constante&quot;, &quot;Gasto en educación&quot;)) ## ## ============================== ## Modelo 1 ## ------------------------------ ## Constante 44.81 *** ## (1.02) ## Gasto en educación 1.23 *** ## (0.25) ## ------------------------------ ## R^2 0.06 ## Adj. R^2 0.06 ## Num. obs. 356 ## ============================== ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Y podemos exportar la tabla en formato .doc para añadirla a nuestro manuscrito. El archivo se guardará en nuestra carpeta de trabajo. htmlreg(list(model_1), file = &quot;modelo_1.doc&quot;, custom.model.names = &quot;Modelo 1&quot;, custom.coef.names = c(&quot;Constante&quot;, &quot;Gasto en educación&quot;), inline.css = FALSE, doctype = T, html.tag = T, head.tag = T, body.tag = T) Como puedes notar, los resultados se muestran mejor con texreg que con summary(). Se ve claramente que gasto_educ –el gasto en educación– tiene un efecto positivo de magnitud 1.233. En particular, cuando el gasto en educación como porcentaje del PIB aumenta en una unidad, la desigualdad aumenta en 1,23%. El efecto del gasto en educación es significativo con un nivel de confianza del 99,9%. Lo sabemos porque además del coeficiente aparecen tres estrellas que se refieren a un nivel de significación del 0,01%. La significancia estadística es el resultado de un test-t. Esta prueba indica la distancia estandarizada, donde la beta estimada se encuentra en la distribución bajo la hipótesis nula de que \\(\\beta_1=0\\). El estimador tiene una distribución t-Student con grados de libertad iguales a \\(n-k-1\\), donde \\(k\\) es el número de variables independientes y se suma 1 para la estimación de la constante \\(\\beta_0\\). Una aproximación manual de la distancia beta estimada en la distribución del estimador bajo la hipótesis nula \\(\\beta_1=0\\) se obtiene cuando dividimos la estimación por su error estándar: 1.233 / 0.25 ## [1] 4.9 El mismo valor es entregado por la tercera columna de la sección “Coeficientes” en el summary() del modelo_1. El valor t siempre se interpreta como la distancia de la estimación \\(\\hat\\beta_1\\) de la media de la distribución del estimador bajo \\(H_0 = \\beta_1 = 0\\). En este caso, el valor 1,233 está a 4,93 desviaciones estándar de la distribución del estimador cuando H0 es verdadero (la media de la distribución es 0). Dado que las distribuciones t se superponen a la distribución normal a medida que aumentan los grados de libertad, y sabemos que aproximadamente hasta 2 desviaciones estándar el 95% de la probabilidad se encuentra en una distribución normal, podemos afirmar que la probabilidad de observar nuestra estimación, si H0 es verdadera, es menor de 0,05 cuando la estadística-t supera el valor de 2. Cuando esto ocurre, rechazamos la hipótesis nula con un nivel de confianza del 95%. En este caso, la estimación de \\(\\hat\\beta_1\\) es más de 4,93 desviaciones estándar de la media de la distribución bajo \\(H_0=\\beta_1=0\\), por lo que es poco probable que se haya observado un efecto de 1,23 si \\(H_0\\) fuera cierto. La probabilidad precisa se puede observar en la cuarta columna del modelo summary, que se puede solicitar a R con el siguiente comando. coef(summary(model_1))[, &quot;Pr(&gt;|t|)&quot;] ## (Intercept) gasto_educ ## 5.5e-145 1.3e-06 La probabilidad de observar una estimación de 1,23 si el H0 fuera cierto es de 0,00000128. Por lo tanto, podemos rechazar H0 incluso con un nivel de confianza del 99,9%. Ejercicio 5B. Utilizando los mismos datos, estima un modelo donde la variable independiente es Inversión Extranjera Directa (inversion_extranjera) y la variable dependiente es Desigualdad (gini) y exportarlo a un archivo .doc. ¿Es el efecto estadísticamente significativo? 5.2.2 Representación gráfica Como vimos anteriormente, una de las formas más fáciles de presentar la relación entre dos variables es a través de gráficos. Ya se ha dicho que ggplot2 es una herramienta conveniente para generar varios tipos de gráficos. Exploremos su uso para MCO. En el primer código, todas las observaciones se grafican según sus valores de variables independientes y dependientes. ggplot(data = bienestar, # seleccionamos la base de datos aes(x = gasto_educ, y = gini))+ # variables independientes y dependientes geom_point() + # los valores observados son graficados geom_smooth(method = &quot;lm&quot;, # La línea de regresión se superpone se = F, # el área de error no se grafica con un IC del 95% color = &quot;blue&quot;)+ # color de la línea labs (x = &quot;Gasto en educación&quot;, y = &quot;Desigualdad&quot;) ## Warning: Removed 718 rows containing non-finite values (stat_smooth). ## Warning: Removed 718 rows containing missing values (geom_point). Figura 5.5: Ajuste lineal entre el gasto en educación y la desigualdad Por lo general, también es útil mostrar una representación gráfica del error de predicción de la línea. ggplot2 nos permite editar un área sombreada donde los valores predichos con un cierto nivel de significación podrían haber sido localizados. Aunque el 95% de confianza es el valor por defecto, también podemos editar ese valor. El primer bloque muestra la línea de regresión y su error para un nivel de significancia estadística del 95%. Ten en cuenta que esta línea no representará el coeficiente que obtuviste a través de lm después de incluir controles a tu regresión. ggplot(data = bienestar, aes(x = gasto_educ, y = gini))+ geom_point() + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;, se = T) + # añadimos la predicción labs(x = &quot;Gasto en educación&quot;, y = &quot;Desigualdad&quot;, title = &quot; Ajuste lineal entre el gasto en educación y la desigualdad&quot;) ## Warning: Removed 718 rows containing non-finite values (stat_smooth). ## Warning: Removed 718 rows containing missing values (geom_point). Figura 5.6: Ajuste lineal en el que añadimos un intervalo de confianza del 95%. 5.3 Modelo multivariado: regresión múltiple Aunque tendemos a interesarnos por el efecto de una sola variable independiente sobre una dependiente, es común estimar modelos en los que \\(Y\\) es tanto el resultado de una variable independiente de interés como de un conjunto de variables de control. Formalmente, \\[Y= \\beta_0+\\beta_1x_{1}+\\beta_1x_{2}+...+\\beta_jx_{j}+u\\] En contraste con una simple regresión, ahora la variable aleatoria \\(Y\\) es una función de múltiples variables más un término de error \\(u\\). Como en las regresiones simples, la expectativa del error condicional en los valores de \\(x_j\\) tiene que ser igual a cero. Formalmente, \\(E(u|x_1, x_2, ..., x_j)=0\\). Para estimar un modelo lineal múltiple no sesgado, la suposición de una media condicional cero no es el único requisito, pero todas las suposiciones necesarias se presentarán en una sección posterior. Por el momento, estimaremos un modelo de población en el que la desigualdad social (gini) es una función lineal del gasto en educación como porcentaje del PIB (gasto_educ), de la inversión extranjera directa (inversion_extranjera), del gasto en salud como porcentaje del PIB (gasto_salud), del gasto en seguridad social como porcentaje del PIB. (gasto_segsocial), de la población joven (poblacion), del dualismo estructural de la economía (s_dualismo), de la división étnica (diversidad_etnica), del PIB real per cápita (pib), del tipo de régimen (tipo_regimen), y del equilibrio entre los poderes del Estado (bal_legislativo). Como puede verse, se han añadido diversas variables que se sospecha que predicen la desigualdad (Huber et al. 2006). El análisis de regresión múltiple nos permitirá estimar hasta qué punto nuestro modelo es correcto. La función lm también estima múltiples modelos, y la única diferencia es que las variables independientes deben ser añadidas. Antes de estimar el modelo, filtraremos la base de datos eliminando todos los casos con valores perdidos (NA)26. Aquí, por motivos prácticos, sólo consideraremos aquellos casos (país-año) que estén completos en todas las variables de nuestro modelo, eliminando los valores perdidos.27: bienestar_no_na &lt;- bienestar %&gt;% drop_na(gini, gasto_educ , inversion_extranjera , gasto_salud , gasto_segsocial , poblacion, dualismo_sectorial, diversidad_etnica, pib, tipo_regimen, bal_legislativo) Ahora podemos estimar el modelo 2: model_2 &lt;- lm(gini ~ 1 + gasto_educ + inversion_extranjera + gasto_salud + gasto_segsocial + poblacion + dualismo_sectorial + diversidad_etnica + pib + factor(tipo_regimen) + bal_legislativo, data = bienestar_no_na) Nota: Hemos indicado que la variable tipo_regimen es categórica a través de as.factor. Al hacer esto, cada categoría de régimen se mide por un coeficiente dummy. Ejercicio 5C. Recuerda: incluir el 1 no es necesario para estimar el modelo (lo colocamos ahí sólo para recordarte que también estamos estimando la intercepción). Intente probar el modelo sin él, y verá que los resultados no cambian. Como en el modelo simple, podemos mostrar y graficar los resultados de la estimación con summary() o screenreg(). screenreg(model_2) ## ## ================================= ## Model 1 ## --------------------------------- ## (Intercept) 85.94 *** ## (8.73) ## gasto_educ 1.59 *** ## (0.45) ## inversion_extranjera 0.24 ## (0.18) ## gasto_salud -0.83 ** ## (0.26) ## gasto_segsocial -0.83 *** ## (0.20) ## poblacion -0.93 *** ## (0.17) ## dualismo_sectorial -0.17 *** ## (0.03) ## diversidad_etnica 3.68 *** ## (1.04) ## pib -0.00 ** ## (0.00) ## factor(tipo_regimen)2 -2.29 ## (4.75) ## factor(tipo_regimen)3 -2.90 ## (4.70) ## factor(tipo_regimen)4 -5.14 ## (4.62) ## bal_legislativo -10.40 *** ## (2.22) ## --------------------------------- ## R^2 0.59 ## Adj. R^2 0.56 ## Num. obs. 167 ## ================================= ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Estas funciones también nos permiten comparar dos o más modelos. Cuando se presentan las conclusiones de una investigación, a menudo se recomienda mostrar cómo cambian (o no) los resultados bajo diferentes especificaciones. Primero, guardamos los modelos en una lista. Al comando screenreg añadimos los nombres de las variables, como hemos aprendido anteriormente. En este caso, la comparación de los modelos es la siguiente: models &lt;- list(model_1, model_2) screenreg(models, custom.model.names = c(&quot;Modelo 1&quot;, &quot;Modelo 2&quot;), custom.coef.names = c( &quot;Constante&quot;, &quot;Gasto en educación&quot;, &quot;IED&quot;, &quot;Gasto en salud&quot;, &quot;Gasto en seg. social&quot;, &quot;Población jóven&quot;, &quot;Dualismo en economía&quot;, &quot;División étnica&quot;, &quot;PBI pc&quot;, &quot;Reg. democrático&quot;, &quot;Reg. mixto&quot;, &quot;Reg. autoritario&quot;, &quot;Balance entre poderes&quot;) ) ## ## ============================================= ## Modelo 1 Modelo 2 ## --------------------------------------------- ## Constante 44.81 *** 85.94 *** ## (1.02) (8.73) ## Gasto en educación 1.23 *** 1.59 *** ## (0.25) (0.45) ## IED 0.24 ## (0.18) ## Gasto en salud -0.83 ** ## (0.26) ## Gasto en seg. social -0.83 *** ## (0.20) ## Población jóven -0.93 *** ## (0.17) ## Dualismo en economía -0.17 *** ## (0.03) ## División étnica 3.68 *** ## (1.04) ## PBI pc -0.00 ** ## (0.00) ## Reg. democrático -2.29 ## (4.75) ## Reg. mixto -2.90 ## (4.70) ## Reg. autoritario -5.14 ## (4.62) ## Balance entre poderes -10.40 *** ## (2.22) ## --------------------------------------------- ## R^2 0.06 0.59 ## Adj. R^2 0.06 0.56 ## Num. obs. 356 167 ## ============================================= ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Como puedes ver, la estimación del efecto del gasto en educación cambió ligeramente. Mientras que en el modelo simple el efecto es de 1,23, en el modelo múltiple este efecto crece a 1,59. En este caso, la interpretación es que cuando el gasto en educación aumenta en una unidad, la desigualdad aumenta en un promedio de 1,59 puntos porcentuales, manteniendo constantes todos los demás factores. Al igual que en el modelo 1, la variable sigue siendo significativa con una confianza del 99,9%, por lo que decimos que el efecto del gasto en educación es robusto en diferentes especificaciones. Cuando los investigadores incluyen nuevos controles al modelo y la principal variable de interés sigue siendo significativa y con magnitudes relativamente estables, ganamos evidencia a favor del efecto de esta última. En otras palabras, es cada vez más improbable que el efecto observado en primera instancia fuera espurio. Otra contribución del modelo 2 es la adición de variables nominales. Las variables dicotómicas y categóricas plantean un ligero desafío de interpretación. Observa la variable de diversidad étnica, que es dicotómica, es decir, el valor 1 implica que más del 20% de la población pertenece a una minoría étnica y el 0 que no existe una minoría tan relevante. El coeficiente de diversidad_etnica es 3,7, significativo en el 99,9%. ¿Cómo interpretamos este coeficiente? Simplemente, el valor predicho de la desigualdad es 3,7 puntos superior cuando existe una minoría étnica, para cualquier valor de los otros \\(x&#39;s\\). Para interpretar estos coeficientes, siempre necesitamos saber la categoría base. Como diversidad_etnica es igual a 0 cuando no hay minorías étnicas, el coeficiente se interpreta como el efecto de tener una minoría étnica. En el caso de la variable dualismo_sectorial, dado que la categoría base es 0 para “no dualismo”, el coeficiente se interpreta como el efecto de tener una economía dual disminuye (coeficiente negativo) la desigualdad en aproximadamente 0,17 puntos. Ejercicio 5D. Exporta la tabla con ambos modelos (con y sin controles) a un archivo .doc, te esperamos. El siguiente gráfico muestra la diferencia en el efecto del gasto en educación sobre la desigualdad según la presencia o ausencia de minorías étnicas en el país. Con fines didácticos, estimamos un nuevo modelo restringido a dos variables de interés: el gasto en educación y las minorías étnicas. model_2_restricted&lt;- lm(gini~1+gasto_educ + diversidad_etnica, data = bienestar_no_na) screenreg(model_2_restricted) # observamos nuevos coeficientes ## ## ============================= ## Model 1 ## ----------------------------- ## (Intercept) 45.40 *** ## (1.83) ## gasto_educ 0.67 ## (0.44) ## diversidad_etnica 6.82 *** ## (0.99) ## ----------------------------- ## R^2 0.24 ## Adj. R^2 0.23 ## Num. obs. 167 ## ============================= ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Para hacer el gráfico, necesitamos cargar el paquete de prediction que calcula los valores predichos de un modelo y que será usado para dibujar la línea predicha por el modelo. library(prediction) pred_model_2_restricted &lt;- as_tibble(prediction(model_2_restricted)) ggplot(data = pred_model_2_restricted) + # los nuevos valores predichos geom_point(mapping = aes(x = gasto_educ, y = gini, color = factor(diversidad_etnica))) + # se dibujan las líneas de regresión (diferenciadas por color): geom_line(mapping = aes(x = gasto_educ, y = fitted, color = factor(diversidad_etnica), group = factor(diversidad_etnica))) + labs(x = &quot;Gasto en educación&quot;, y = &quot;Desigualdad&quot;) Figura 5.7: Valores predichos por división étnica Como vemos claramente en el gráfico, el efecto del gasto en educación sobre la desigualdad es positivo porque, a medida que el gasto aumenta, el valor esperado del coeficiente de Gini también aumenta. Sin embargo, los países con minorías étnicas (valor de 1 en diversidad_etnica) tienen casi 7 puntos porcentuales más en su coeficiente de Gini en cualquier valor del gasto en educación. También se puede afirmar que el efecto medio de tener minorías étnicas en la desigualdad es de 6,82. Los modelos con más regresores (como el modelo 2 estimado anteriormente) son más complejos de graficar porque el valor predicho no sólo depende de dos variables (como en este gráfico), sino de todas las variables presentes en el modelo. En cualquier caso, como vimos anteriormente, la interpretación es la misma. 5.4 Ajuste del modelo La bondad del ajuste se define como la capacidad explicativa del modelo. Intuitivamente, se refiere a la porción de la variación de la variable dependiente \\(y\\) se explica por el modelo especificado. La medida de la bondad del ajuste es \\(R^2\\) y se define como 1- RSS/TSS, donde RSS es la Suma Residual de Cuadrados y TSS la Suma Total de Cuadrados. En términos sencillos, RSS es una medida de todo lo que no es explicado por el modelo, mientras que TSS es la variabilidad de \\(y\\). Un modelo que explica toda la variación de \\(y\\) tendrá un \\(R^2\\) de 1. Un modelo que no explica nada de la variabilidad de la variable dependiente tendrá un valor de 0. Como regla general, a medida que el número de variables independientes aumenta, la \\(R^2\\) nunca disminuye, por lo que la \\(R^2\\) ajustada se utiliza a menudo como una medida que penaliza la inclusión de variables sin fundamento. Como podemos observar en la comparación de los modelos previamente estimados, el modelo lineal simple tiene un \\(R^2\\) de 0,06. Esto puede leerse como el modelo 1 que explica el 6% de la variabilidad de la desigualdad. El modelo múltiple 2 aumenta su capacidad explicativa al 59%. Algunas investigaciones tratan de aumentar la bondad de ajuste del modelo. Sin embargo, para estimar el efecto de una sola variable no es necesario aumentar la bondad de ajuste sino simplemente cumplir con los supuestos del Modelo Lineal Clásico, tales como la media condicional cero, la linealidad en los parámetros y el resto de los supuestos que se describen en la sección correspondiente. 5.5 Inferencia en los modelos lineales múltiples Como en la regresión lineal simple, los estimadores de cada parámetro \\(\\beta_j\\) tiene una distribución t-Student, por lo que es posible hacer inferencias sobre las estimaciones puntuales de cada \\(\\hat{\\beta_j}\\) a través de un tes-t. Sin embargo, a menudo queremos imponer múltiples restricciones lineales al modelo del tipo \\(H_0= \\beta_1= \\beta_2 = 0\\). Aquí estamos afirmando que el efecto de las dos variables \\(x_1\\) y \\(x_2\\) es igual a cero. Un caso típico que requiere este tipo de hipótesis nula es el de las variables categóricas que entran en el modelo como variables dicotómicas ficticias.La variable dicotómica “educación secundaria” y la variable dicotómica “educación superior” son, en realidad, categorías de la variable nominal única “nivel de educación”, que sólo pueden entrar en el análisis de regresión en un formato dummy . La prueba que nos permite hacer inferencias para múltiples restricciones lineales es la prueba F. Esto significa que el \\(H_0\\) de una restricción múltiple se distribuye como la F de Fisher. Supongamos que queremos probar la hipótesis nula \\(H_0= \\beta_1= \\beta_2 = \\beta_3 =0\\). De acuerdo con esta hipótesis, las variables \\(x_1\\), \\(x_2\\) y \\(x_3\\) no afectan a \\(Y\\) cuando se consideran juntas. La hipótesis alternativa es que al menos una de las betas es diferente de 0. Si \\(H_0\\) es cierto, entonces un modelo que excluye estas variables debería explicar lo mismo que un modelo que las incluye, es decir, estas variables son redundantes. La forma de probar esta hipótesis es a través de una test-F, en la que se compara la suma de los residuos al cuadrado de los modelos completo y restringido. En términos sencillos, si las variables no explican la variabilidad de \\(Y\\), la suma de los residuos al cuadrado de ambos modelos (otra forma es considerar el \\(R^2\\)) no debería cambiar significativamente. En este caso, no tiene sentido mantener la variable en el modelo. Utiliza el hecho de que la comparación de los residuos al cuadrado distribuye F \\[F= \\frac{(RSS_r-RSS_c)/q}{RSS_c/(n-k-1)}\\] Donde \\(RSS_r\\) es la Suma Residual de Cuadrados del modelo restringido, \\(RSS_c\\) es la Suma Residual de Cuadrados del modelo completo, \\(q\\) es la cantidad de variables excluidas y \\(n-k-\\) son los grados de libertad del modelo completo. En R, podemos usar la función anova para comparar los modelos. Por ejemplo, supongamos que un colega afirma que el equilibrio legislativo (bal_legislativo), el tipo de régimen (tipo_regimen) y la diversidad étnica (diversidad_etnica) deben ser excluidos del modelo. Entonces, necesitamos estimar un modelo restringido de manera que model_2_restrained &lt;- lm(gini ~ 1 + gasto_educ + inversion_extranjera + gasto_salud + gasto_segsocial + poblacion+ dualismo_sectorial + pib, data = bienestar_no_na) Como podemos ver, las variables listadas fueron excluidas de la sintaxis. Ahora necesitamos comparar la capacidad explicativa de cada modelo anova(model_2, model_2_restrained) ## Analysis of Variance Table ## ## Model 1: gini ~ 1 + gasto_educ + inversion_extranjera + gasto_salud + ## gasto_segsocial + poblacion + dualismo_sectorial + diversidad_etnica + ## pib + factor(tipo_regimen) + bal_legislativo ## Model 2: gini ~ 1 + gasto_educ + inversion_extranjera + gasto_salud + ## gasto_segsocial + poblacion + dualismo_sectorial + pib ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 154 3148 ## 2 159 4737 -5 -1588 15.5 2.3e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La importancia de la última columna de la prueba (Pr(&gt;F)) muestra claramente que la hipótesis nula se rechaza ya que el valor p es inferior al umbral de 0,05, por lo que estas variables no deben ser excluidas del modelo. Ejercicio 5E. Estime un modelo en el que excluya el gasto en salud (gasto_salud) y el gasto en seguridad social (gasto_segsocial), y compare su capacidad explicativa con el modelo completo. De acuerdo con los resultados, ¿deberíamos excluir estas variables del modelo? 5.6 Testeando los supuestos de MCO Como habrás visto en tus manuales de econometría, el estimador MCO será útil (estimará sin sesgo el parámetro de población) si se cumplen los supuestos de Gauss-Markov. Esto permite que sea el mejor parámetro lineal sin sesgo (BLUE, por sus siglas en inglés). Para una discusión más profunda de las suposiciones, mira Wooldridge (2016) y Stock and Watson (2019). Es importante evaluar que estos supuestos se cumplen en nuestra estimación. Como veremos, esta evaluación es teórica y, en algunos casos, podría ser abordada empíricamente. Todas estas evaluaciones suelen añadirse a los artículos como apéndices o en los archivos de replicación de datos, y no necesariamente en el cuerpo del texto, pero puedes hacerlas en R. 5.6.0.1 Cero condicionalidad de la media Esta es la principal suposición para utilizar el estimador de mínimos cuadrados ordinarios. La premisa central es la independencia entre las variables independientes y el término de error. Esto nos permite aislar el efecto del \\(x\\) de los factores no observables (contenidos en el término de error \\(u\\)). Esta suposición no puede evaluarse empíricamente porque, por definición, no conocemos los factores contenidos en el término de error. Por lo tanto, la defensa de esta suposición siempre será teórica. 5.6.0.2 Muestra aleatoria Esta es una suposición sobre la generación de datos. Se supone una muestra aleatoria de tamaño \\(n\\), lo que implica que la muestra se tomó de tal manera que todas las unidades de la población tuvieron la misma probabilidad de ser seleccionadas. Es decir, no hay ningún sesgo de muestreo. 5.6.0.3 Linealidad en los parámetros MCO asume que la variable dependiente (\\(y\\)) tiene una relación lineal con la(s) variable(s) independiente(s) y el término de error (\\(u\\)). Es decir, un aumento de una unidad de \\(x\\) resulta en un efecto constante sobre la variable dependiente \\(y\\). De ahí la forma funcional de la ecuación de regresión: \\[Y = \\beta_0 + \\beta_1x + u\\] Si la relación no es lineal, entonces nos enfrentamos a un problema de especificación del modelo. Es decir, los valores predichos por nuestro modelo no encajan en la realidad de nuestros datos y, en consecuencia, las estimaciones estarán sesgadas. Por lo tanto, es fundamental evaluar si la relación que queremos estimar es lineal o si la forma funcional que caracteriza esa relación es otra (por ejemplo, podría ser cuadrática, cúbica, logarítmica, etc.). La buena noticia es que si tenemos razones teóricas y empíricas para creer que la relación no es lineal, es posible hacer transformaciones en nuestras variables para lograr una mejor especificación del modelo. Un ejemplo clásico es el de la relación parabólica entre edad e ingresos: a medida que aumenta la edad, los ingresos aumentan hasta que se alcanza un punto de inflexión en el que un aumento de la edad se relaciona con niveles de ingresos más bajos, como una U invertida. Para evaluar la linealidad, hacemos un gráfico de dispersión de los valores predichos contra los residuos de \\(u\\). El objetivo es evaluar si el promedio de los residuos tiende a situarse aleatoriamente por encima y por debajo de cero. Si los residuos presentan un patrón creciente o decreciente - o cualquier otro tipo - entonces la forma funcional de una de las variables en cuestión es no lineal. Para ello utilizamos ggplot2: ggplot(mapping = aes(x = model_1$fitted.values, y = model_1$residuals)) + labs(x = &quot;Valores predichos&quot;, y = &quot;Residuos&quot;) + geom_point() + geom_hline(mapping = aes(yintercept = 0), color = &quot;red&quot;) Figura 5.8: Prueba de linealidad en valores predichos También podemos hacer un gráfico de residuos parciales donde cada variable independiente del modelo se grafica contra los residuos. El objetivo es obtener un gráfico “parcial” para observar la relación entre la(s) variable(s) independiente(s) y la variable dependiente teniendo en cuenta (controlando) todas las demás variables del modelo. Una línea de puntos nos muestra la predicción del MCO, y otra línea (roja) nos muestra la relación “real”. Si observamos que una de nuestras variables no tiene una relación lineal podemos hacer transformaciones (¡a las variables!) para que la forma funcional se aproxime a la empírica. Hay que señalar que, además de la justificación empírica, esta transformación lineal debe siempre estar apoyada por un argumento teórico de por qué la relación entre las dos variables toma tal forma. Una transformación común que se verá regularmente en los trabajos es la de las transformaciones logarítmicas de las variables. Éstas están presentes tanto en las variables dependientes como en las independientes. Por esta razón, le ofrecemos una tabla que le será útil. Esto le permitirá saber cómo cambia la interpretación de los resultados cuando una de las variables (o ambas) se transforma. La tabla resume la interpretación de los coeficientes en presencia de transformaciones logarítmicas\". Por ejemplo, decidimos transformar nuestra variable dependiente de tal manera que model_1_log &lt;- lm(log(gini) ~ 1 + gasto_educ, data = bienestar) screenreg(model_1_log) ## ## ======================= ## Model 1 ## ----------------------- ## (Intercept) 3.78 *** ## (0.02) ## gasto_educ 0.03 *** ## (0.01) ## ----------------------- ## R^2 0.07 ## Adj. R^2 0.07 ## Num. obs. 356 ## ======================= ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 La interpretación sería la siguiente: si aumentamos el gasto en salud en una unidad, esperaríamos que el Gini aumente un 3%, ceteris paribus. Para saber cuándo transformar nuestras variables, veremos con un ejemplo cómo podemos diagnosticar un problema de medición en nuestras variables. library(car) crPlots(model_1) Figura 3.21: Test de linearidad La relación de nuestra variable de interés con las variables dependientes parece ser cada vez más cuadrática. Entonces, es razonable hacer una transformación cuadrática a la variable. Evaluemos esto gráficamente: bienestar_no_na &lt;- bienestar_no_na %&gt;% mutate(cseduc2 = gasto_educ*gasto_educ) model_1_quadratic &lt;- lm(gini ~ 1 + cseduc2 + gasto_educ, data = bienestar_no_na) crPlots(model_1_quadratic) Figura 3.22: Prueba de linealidad alternativa Basándonos en un diagnóstico visual, observamos una tendencia creciente en los residuos a medida que avanza en los valores predichos. También detectamos una relación no lineal entre el gasto en educación y los niveles de desigualdad. Sospechamos que esta relación podría ser cuadrática (parábola cuadrática creciente) y, según la gráfica de residuos parciales, parece que la variable transformada está mucho más cerca de la relación lineal estimada por MCO (marcada por la línea punteada). Observa que la escala de la figura de la izquierda es de 0 a 15, mientras que la de la derecha es de 0 a 20, lo que denota una pendiente más pronunciada. Para confirmar las observaciones visuales, a menudo se utiliza una prueba estadística para diagnosticar una especificación funcional deficiente del modelo: El test RESET de Ramsey. La idea es, precisamente, evaluar si hay un error de especificación en la ecuación de regresión. Lo que esta prueba hace es estimar de nuevo el modelo pero añadiendo los valores predichos del modelo original con algunas transformaciones no lineales de las variables. Luego, a partir de un test-F, el modelo se evalúa con las especificaciones no lineales para comprobar si tiene un mejor ajuste que el modelo original sin la transformación no lineal. La hipótesis nula establece que las nuevas variables (en este caso, csdeuc^2) no son significativas para explicar la variación de la variable dependiente; es decir, que su coeficiente es igual a cero (\\(beta=0\\)) library(lmtest) resettest(model_1, power = 2, type = &quot;fitted&quot;, data = bienestar_no_na) ## ## RESET test ## ## data: model_1 ## RESET = 9, df1 = 1, df2 = 353, p-value = 0.004 Según el resultado del test-F, confirmamos lo que observamos gráficamente: añadir un término cuadrático al gasto en educación mejora el ajuste de nuestra estimación. Llegamos a esta conclusión observando el valor p del test RESET de Ramsey: a un nivel de significancia estadística del 5%, se rechaza la hipótesis nula de que la incorporación del término cuadrático no mejora el ajuste del modelo. Nota: Esta evaluación se hizo para un modelo de regresión simple (bivariante), pero el mismo procedimiento puede hacerse para modelos multivariantes. 5.6.0.4 Variación en variables independientes y colinealidad no-perfecta Declaramos que es necesario que haya variación en la(s) variable(s) independiente(s). Una variable que no varía, ¡no es una variable! Si no tenemos variación, la estimación de los coeficientes será indeterminada. Además, una mayor variación en las variables independientes nos permitirá hacer estimaciones más precisas. Por otra parte, la ausencia de colinealidad perfecta implica que las variables independientes no están perfectamente correlacionadas linealmente. Es decir, aunque las variables independientes tienden a tener algún tipo de relación entre ellas, ¡no queremos que midan lo mismo! Esto se evaluará con pruebas de multicolinealidad. 5.6.0.4.1 Problemas de multicolinealidad A. Pérdida de eficiencia, porque sus errores estándar serán infinitos. Incluso cuando la multicolinealidad es menos que perfecta, los coeficientes de regresión tienen mayores errores estándar, lo que significa que no pueden ser estimados con gran precisión. Revisemos la fórmula del error estándar de los coeficientes \\[\\hat{\\sigma}_{\\hat{\\beta}_1} = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum(X_j – \\bar{X})^2(1 - R^2_j)}}\\] \\(\\hat{\\sigma}\\) Es la varianza del término del error: \\(\\frac{\\sum\\hat{u}}{n-k-1}\\) \\(\\sum(X_j – \\bar{X})^2\\) es la varianza de of \\(x_j\\) (\\(STCx_j\\)) \\(1 - R^2_j\\) Es la porción de \\(x_j\\) que no es explicada por el resto de x en el modelo (\\(R^2_j\\) indica que la varianza de \\(x_j\\) que se explica por el resto de las “x” del modelo). Es por este término que la colinearidad no perfecta es tan importante! B. Las estimaciones de los coeficientes pueden variar ampliamente dependiendo de qué otras variables independientes hay en el modelo. En una estimación MCO, la idea es que podemos cambiar el valor de una variable independiente y no el de las otras (esto es lo que significa ceteris paribus, para mantener constantes las otras co-variables). Sin embargo, cuando las variables independientes están correlacionadas, los cambios en una variable están vinculados a los cambios en otra variable. Cuanto más fuerte es la correlación, más difícil es cambiar una variable sin cambiar otra. Se hace difícil para el modelo estimar la relación entre cada variable independiente y la variable dependiente mientras se mantiene constante el resto, porque las variables independientes tienden a cambiar simultáneamente. Repasemos la fórmula para estimar el coeficiente en una regresión múltiple: \\[\\hat{\\beta_1} = \\frac{\\sum(\\hat{r_{i1}}\\hat{y_i})}{\\sum(\\hat{r^2_{i1}})}\\] donde: \\(\\hat{r_{i1}}\\) son los residuos de una regresión de \\(x_1\\) sobre el resto de los \\(x\\) en el modelo (es decir, la parte de \\(x_1\\) que no puede ser explicada - o que no está correlacionada - con el resto de los \\(x\\)) Así, \\(\\hat{\\beta_1}\\) mide la relación de la muestra entre \\(y\\) y \\(x_1\\) después de eliminar los efectos parciales de \\(x_2\\), \\(x_3\\)…\\(x_k\\). Para evaluar la multicolinealidad, el primer paso es observar la matriz de correlación de las variables de nuestro modelo (tal como hicimos en la etapa de análisis estadístico descriptivo) library(ggcorrplot) corr_selected &lt;- bienestar %&gt;% select(gini, gasto_educ, dualismo_sectorial, inversion_extranjera, pib, diversidad_etnica, tipo_regimen, gasto_salud, gasto_segsocial, bal_legislativo, poblacion) %&gt;% # calcular la matriz de correlaciones y redondear a un decimal cor(use = &quot;pairwise&quot;) %&gt;% round(1) ggcorrplot(corr_selected, type = &quot;lower&quot;, lab = T, show.legend = F) Figura 3.24: Matriz de correlación, donde diagnosticaremos los problemas de multicolinealidad Vemos que algunas de nuestras variables tienen fuertes correlaciones, como el gasto en seguridad social gasto_segsocial y la población (poblacion), que tiene una correlación negativa de 0,7. En cualquier caso, para detectar si la multicolinealidad es un problema, es necesario realizar un test de VIF (variance inflation factor), porque mirar los pares de correlaciones no nos ayuda a establecer si más de dos variables tienen una correlación lineal. Lo que el test VIF revela es cuánto “crecen” los errores de los coeficientes cuando el resto de las variables están presentes (cuánto aumenta la varianza del error). vif(model_2) ## GVIF Df GVIF^(1/(2*Df)) ## gasto_educ 1.8 1 1.4 ## inversion_extranjera 1.5 1 1.2 ## gasto_salud 1.8 1 1.3 ## gasto_segsocial 4.8 1 2.2 ## poblacion 5.0 1 2.2 ## dualismo_sectorial 1.2 1 1.1 ## [ reached getOption(&quot;max.print&quot;) -- omitted 4 rows ] A continuación, hacemos una consulta sobre si la raíz cuadrada de VIF para cada variable es inferior a 2 (la raíz cuadrada porque nos interesa el error estándar y no la varianza). Como regla general, la puntuación debe ser inferior a 2. Si es superior a 2, significa que la varianza es alta. Por lo tanto, hay problemas de multicolinealidad. sqrt(vif(model_2)) &gt; 2 ## GVIF Df GVIF^(1/(2*Df)) ## gasto_educ FALSE FALSE FALSE ## inversion_extranjera FALSE FALSE FALSE ## gasto_salud FALSE FALSE FALSE ## gasto_segsocial TRUE FALSE FALSE ## poblacion TRUE FALSE FALSE ## dualismo_sectorial FALSE FALSE FALSE ## [ reached getOption(&quot;max.print&quot;) -- omitted 4 rows ] Según la consulta, parece que no tenemos problemas de multicolinealidad. Sin embargo, si los tenemos, ¿deberíamos corregirlos? En general, los manuales de econometría coinciden en la necesidad de reducir la multicolinealidad en función de su gravedad y de cuál es el objetivo principal del modelo de regresión. Debemos considerar los tres puntos siguientes: La seriedad de las cuestiones aumenta el grado de multicolinealidad. Por lo tanto, si tenemos una multicolinealidad moderada, es plausible no resolverla. La multicolinealidad sólo afecta a las variables específicas independientes que están correlacionadas. Por lo tanto, si la multicolinealidad no está presente en las variables independientes de interés, es plausible no resolverla. La multicolinealidad sólo afecta a los coeficientes y a los errores estándar, pero no influye directamente en los valores predichos del modelo, ni en la precisión de estas predicciones y en la bondad de las estadísticas de ajuste. Si nuestro principal objetivo es hacer predicciones, y no necesitamos entender el papel de cada variable independiente, no necesitamos reducir la multicolinealidad. 5.6.0.4.2 Soluciones a la multicolinealidad Eliminando una de las variables independientes que está fuertemente correlacionada. Esto constituye una compensación, y es necesario que haya una justificación teórica que explique por qué se mantuvieron algunas variables y otras no, además de hacer evidente el alto grado de correlación. Combinando las variables que están fuertemente correlacionadas, por ejemplo, haciendo un índice (como mostramos en el Capítulo 15. Hasta ahora, hemos revisado cuatro supuestos que nos permiten afirmar que nuestros estimadores de MCO no están sesgados. Es decir, nos permite confiar en que la expectativa de la estimación hecha a través de MCO será igual al promedio de la población: \\(E(\\hat\\beta)=\\beta\\) 5.6.0.5 Homoscedasticidad El quinto supuesto está relacionado con la eficiencia. Es decir, con la variación del término de error de nuestra estimación. El término de error varianza es una constante. Es decir, dado cualquier valor de las variables explicativas, el error tiene la misma varianza. \\(Var(u\\mid{x})=\\sigma^2\\), that is \\(Var(u)=\\sigma^2\\) Así, la varianza del error inobservable, \\(u\\), condicionada a las variables explicativas, es constante. Como dijimos anteriormente, este supuesto no afecta al sesgo del estimador (es decir, que la distribución muestral de nuestra estimación \\(\\hat{\\beta_1}\\) está centrada en \\(\\beta_1\\)), sino a su eficiencia (cuánta dispersión hay alrededor de la estimación \\(\\hat{\\beta_1}\\) del parámetro \\(\\beta_1\\)) Este supuesto es crucial para calcular la varianza de los estimadores MCO, y es el que permite que sea el estimador de menor varianza entre los estimadores lineales no sesgados. Si evaluamos la fórmula de error estándar de los coeficientes, la necesidad de este supuesto se hace evidente: \\[\\hat{\\sigma}_{\\hat{\\beta}{_1}} = \\frac{\\hat{\\sigma}} {\\sqrt{\\sum(X_j – \\bar{X})^2(1 - R^2_j)}}\\] \\(\\hat{\\sigma}\\) Es la varianza del término del error: \\(\\frac{\\sum\\hat{u}}{n-k-1}\\) Para implementar esta fórmula, necesitamos que \\({\\sigma^2}\\) sea constante. Cuando esta suposición no se cumple, es decir, el término de error no permanece constante para diferentes valores de \\(x\\), nos enfrentamos a un escenario de heteroscedasticidad. Es bastante común tener heteroscedasticidad. La buena noticia es que esto no dificulta el uso del estimador MCO: ¡hay una solución! 5.6.0.5.1 Evaluando el supuesto Para evaluar este supuesto, se suelen seguir dos pasos: Diagnóstico visual: Queremos observar si los residuos (la distancia entre los puntos y la línea de regresión) son constantes para diferentes valores de x. Primero, hacemos un simple gráfico de dispersión de las variables independientes que nos interesan y la variable dependiente: ggplot(bienestar_no_na, aes(gasto_educ, gini)) + geom_point() + theme_bw()+ geom_smooth(method = &quot;lm&quot;) Figura 3.26: Evaluación visual de la suposición de homoscedasticidad Al parecer, en los niveles bajos de gasto en educación, la variabilidad de los niveles de desigualdad es significativamente mayor que en los niveles más altos de gasto en educación. Podemos hacer un mejor diagnóstico visual si utilizamos el modelo estimado (y no sólo la relación entre las dos variables) y graficamos los residuos. En primer lugar, lo hacemos para el modelo bivariante, simplemente usando plot() con el argumento which = 1 (hay algunos otros diagnósticos disponibles, que no discutiremos aquí): plot(model_1, which = 1) Figura 3.27: Evaluación de la homoscedasticidad para el modelo bivariado. Despues, lo hacemos para el modelo mutivariado: plot(model_1, which = 1) Figura 5.9: Evaluación de la homoscedasticidad para el modelo multivariado. Estos gráficos marcan los valores ajustados contra los residuos que se grafican. Recordemos que bajo el supuesto de homocedasticidad, ya que \\(Var(u\\mid{x})=\\sigma^2\\), entonces \\(Var(Y\\mid{x})=\\sigma^2\\). En otras palabras, la varianza de los residuales de los valores predichos basados en x’s debería ser constante. Por lo tanto, si no hay absolutamente ninguna heteroscedasticidad (es decir, estamos ante un escenario de homocedasticidad), debemos observar una distribución completamente aleatoria e igual de puntos a lo largo del rango del eje X y una línea roja constante. Sin embargo, observamos claramente que los residuos no son constantes para los diferentes valores de la variable gasto en educación. Estamos ante un caso de heteroscedasticidad. También podemos evaluar cada variable del modelo y así identificar en qué variables específicas está presente la heteroscedasticidad. De nuevo, lo que esperamos es que la línea roja coincida con la línea punteada (en cero). Usaremos la función residualPlots en el paquete car. Una forma de usar una función sin subir toda la librería es con :: como puedes ver aquí: car::residualPlots(model_2) ## Test stat Pr(&gt;|Test stat|) ## gasto_educ -1.80 0.074 . ## inversion_extranjera -0.09 0.928 ## gasto_salud -0.85 0.395 ## gasto_segsocial 0.99 0.325 ## poblacion -2.30 0.023 * ## dualismo_sectorial -0.19 0.849 ## [ reached getOption(&quot;max.print&quot;) -- omitted 5 rows ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Figura 3.28: Análisis de residuos para cada covariable 5.6.0.5.2 Diagnóstico estadístico En el segundo paso hacemos un diagnóstico estadístico. Hay diferentes maneras de evaluar la homoscedasticidad, pero el test de Breusch-Pagan es la más utilizada. La lógica de este test es la siguiente: se hace una regresión, donde la variable dependiente consiste en los residuos cuadrados para evaluar si las variables independientes del modelo tienen alguna relación con \\(u\\) o no. Lo que esperamos es que el efecto sea 0, porque si la varianza del error es constante, el error (residuos) no debería variar en relación con los valores de \\(x&#39;s\\). En resumen, ¡no queremos rechazar la hipótesis nula! bptest(model_2, studentize = T) ## ## studentized Breusch-Pagan test ## ## data: model_2 ## BP = 32, df = 12, p-value = 0.001 Como el valor p es menor de 0,05, la hipótesis nula es rechazada. Por lo tanto, estamos en un escenario de heteroscedasticidad. 5.6.0.5.3 Soluciones a la heteroscedasticidad Una vez que identificamos que tenemos heteroscedasticidad, es necesario resolverla. La primera alternativa es corregir la forma funcional. Puede estar frente a un caso en el que la no constancia del término de error es el resultado de una relación no lineal entre las variables, un problema que ya hemos aprendido a resolver, por ejemplo, exponiendo las variables. La segunda alternativa se produce a menudo cuando la naturaleza empírica de la relación hace que el error no sea constante. Sabemos que no podemos calcular los errores estándar de los estimadores como siempre lo hacemos en MCO: como la varianza del error es no constante, es necesario modificar la forma en que calculamos los errores. Así, para hacer una inferencia, necesitamos ajustar la estimación del error de tal manera que podamos hacer una estimación válida en presencia de heteroscedasticidad de la forma desconocida. Esto es, aunque no sepamos el tipo de heteroscedasticidad que tenemos, podemos mejorar nuestra precisión y hacer una inferencia estadística válida. La fórmula habitual del error estándar del estimador es: \\[ \\hat\\sigma_{\\hat\\beta_1} = \\frac{\\sum_{i=1}^{n}(x_{i}-\\overline{x})^2\\hat{\\sigma}} {\\sqrt{\\sum(X_j – \\bar{X})^2(1 - R^2_j)}} \\] Lo que pasa es que cuando se tiene homoscedasticidad, lo que hay en el nominador: \\(\\sum_{i=1}^{n}(x_{i}-\\overline{x})^2\\hat{\\sigma}=\\hat\\sigma\\). Dado que el significado ya no es constante, ¡la igualdad ya no está presente! Esto se debe a que el valor asumido por el signo depende de los diferentes valores de \\(x\\). Además, recuerda que al estimar una regresión múltiple, es necesario restar la variación de \\(x_1\\) en la estimación del error estándar, que se explica por el resto de los \\(x_k\\) del modelo. Así, en una regresión múltiple, un estimador válido de \\(\\hat{\\sigma}_{\\hat{\\beta}{_1}}\\) bajo heteroscedasticidad será: \\[\\hat{\\sigma}_{\\hat{\\beta}{_1}} = \\frac{\\sum_{i=1}^{n}r_{ij}^2\\hat{u}^2}{\\sqrt{\\sum(X_j–\\bar{X})^2(1 - R^2_j)}}\\] donde: \\(r_{ij}^2\\) Representa los residuos cuadrados de la regresión del resto de las variables independientes en la variable independiente \\(j\\). \\(\\sqrt{\\sum(X_j–\\bar{X})^2(1 - R^2_j)}\\) Representa la Varianza Total de x después de restar el efecto del resto de las x. Esta forma de estimar los errores estándar se llama errores estándar robustos, también conocidos como refuerzo del error, que no es otra cosa que abordar y permitir la heterocedasticidad haciendo que los errores sean más exigentes. 5.6.0.5.3.1 Errores Estándar Robustos Mientras que hay varias maneras de reforzar los errores (incluso podemos hacerlo a mano), R nos permite calcularlos fácilmente con el comando coeftest del paquete lmtest. Además, el paquete sandwich, con su función vcovHC, nos permite incorporar las especificaciones de la robusta matriz de varianza-covarianza. HC0 = es el original de White (Wooldridge 2016) HC1= Es el que usa el software de Stata HC3 =Es el más conservador, por lo tanto, es muy recomendable library(lmtest) library(sandwich) model_2_robust_3 &lt;- coeftest(model_2, vcov = vcovHC(model_2, &quot;HC3&quot;)) model_2_robust_1 &lt;- coeftest(model_2, vcov = vcovHC(model_2, &quot;HC1&quot;)) model_2_robust_0 &lt;- coeftest(model_2, vcov = vcovHC(model_2, &quot;HC0&quot;)) models_robust &lt;- list(model_2, model_2_robust_0, model_2_robust_1, model_2_robust_3) screenreg(models_robust, custom.model.names = c(&quot;sin ES robustos&quot;, &quot;robustos HC0&quot;, &quot;robustos HC1&quot;, &quot;robustos HC3&quot;)) ## ## ================================================================================ ## sin ES robustos robustos HC0 robustos HC1 robustos HC3 ## -------------------------------------------------------------------------------- ## (Intercept) 85.94 *** 85.94 *** 85.94 *** 85.94 ## (8.73) (8.77) (9.14) ## gasto_educ 1.59 *** 1.59 ** 1.59 ** 1.59 ## (0.45) (0.50) (0.52) ## inversion_extranjera 0.24 0.24 0.24 0.24 ## (0.18) (0.14) (0.14) ## gasto_salud -0.83 ** -0.83 *** -0.83 *** -0.83 ## (0.26) (0.22) (0.23) ## gasto_segsocial -0.83 *** -0.83 ** -0.83 ** -0.83 ## (0.20) (0.25) (0.26) ## poblacion -0.93 *** -0.93 *** -0.93 *** -0.93 ## (0.17) (0.20) (0.21) ## dualismo_sectorial -0.17 *** -0.17 *** -0.17 *** -0.17 ## (0.03) (0.03) (0.03) ## diversidad_etnica 3.68 *** 3.68 *** 3.68 *** 3.68 ## (1.04) (0.92) (0.96) ## pib -0.00 ** -0.00 * -0.00 * -0.00 ## (0.00) (0.00) (0.00) ## factor(tipo_regimen)2 -2.29 -2.29 -2.29 -2.29 ## (4.75) (1.36) (1.41) ## factor(tipo_regimen)3 -2.90 -2.90 * -2.90 * -2.90 ## (4.70) (1.16) (1.20) ## factor(tipo_regimen)4 -5.14 -5.14 *** -5.14 *** -5.14 ## (4.62) (0.84) (0.88) ## bal_legislativo -10.40 *** -10.40 *** -10.40 *** -10.40 ## (2.22) (2.29) (2.38) ## -------------------------------------------------------------------------------- ## R^2 0.59 ## Adj. R^2 0.56 ## Num. obs. 167 ## ================================================================================ ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Todas las alternativas dan lugar a errores sólidos similares. La diferencia viene dada por las diferentes especificaciones sobre la matriz robusta de varianza-covarianza (HC). Consejo: Para reproducir el comportamiento por defecto de Stata de usar la opción robusta en una llamada de regresión, necesitas solicitar a vcovHC usar la matriz de varianza-covarianza robusta HC1. 5.6.0.5.4 Un caso especial de Heteroscedasticidad: varianza de error asociada a los clusters Sabemos que hay observaciones que pueden relacionarse entre sí dentro de grupos (o clusters) específicos. Por ejemplo, los países de América Latina podrían relacionarse por pertenecer a regiones similares (América del Sur frente a América Central o el Caribe, regiones andinas frente a no andinas, etc.). Así pues, sus errores estándar podrían correlacionarse en función de la región a la que pertenecen. Entonces, tenemos que la varianza del error condicionada por la región no es constante. Mientras se trabaja con datos de panel, como es nuestro caso, esto es mucho más claro. Al tener el gasto en educación por país durante varios años, autocorrelación del error existe entre observaciones de un mismo país. Es decir, los errores se correlacionan entre las observaciones del mismo país para cada año (lo que se gasta en un año determinado está probablemente relacionado con lo que se gastó en el año anterior). Entonces, cuando nuestras observaciones pertenecen a clusters, la corrección será para clusters de los errores estándar. Lo que hacemos cuando agrupamos los errores estándar es permitir la existencia de una correlación de errores dentro del cluster (se afloja el supuesto de homoscedasticidad). Así, permitimos que la variación del error no sea constante, sino que sea diferente según los clusters. La selección de los clusters pertinentes se definirá teóricamente. En este caso, tiene sentido pensar que los clusters son los países. Recordemos que nuestro objetivo es estimar el efecto del gasto en educación en el Gini de los países latinoamericanos. Observemos esta relación en la región para evaluar si, a primera vista, existen clusters: library(ggplot2) ggplot(bienestar_no_na, aes(gasto_educ, gini)) + geom_point() + facet_wrap(~pais) Figura 5.10: Relación entre la educación y el Gini por país Parece que hay clusters por países. Es decir, el gasto en educación por país suele mantenerse dentro de un rango que varía ligeramente. Cuando lo observamos así no es obvio, ya que hay muchos países. Sin embargo, parece que todavía hay ciertos clusters por país (las observaciones se agrupan por país; no parecen ser independientes). ggplot(bienestar_no_na, aes(gasto_educ, gini, color = pais)) + geom_point() + theme_bw() Figura 5.11: Concretamos la figura anterior en una sola faceta Para hacer la estimación de MCO con el error de cluster, usaremos el comando lm.cluser del paquete miceadds. Este comando agrupa los errores estándar según la variable de agrupación indicada. En resumen, lo que estamos haciendo es permitir la presencia de una correlación de errores dentro de los clusters, en este caso, países (aflojando la suposición de homoscedasticidad). Los errores estándar robustos por clusters pueden aumentar o disminuir los errores estándar. Es decir, los errores estándar por conglomerados pueden ser mayores o menores que los errores estándar convencionales. La dirección en la que cambian los errores estándar depende del signo de la correlación de los errores intragrupo. Para este paso, es necesario instalar los paquetes miceadds y multiwayvcoc. Con la opción “cluster”, indicamos qué variable agrupará los errores: # install.packages(&quot;miceadds&quot;) library(miceadds) model_2_cluster &lt;- miceadds::lm.cluster( data = bienestar_no_na, formula = gini ~ 1 + gasto_educ + dualismo_sectorial + inversion_extranjera + pib + diversidad_etnica + tipo_regimen + gasto_salud + gasto_segsocial + bal_legislativo, cluster = &quot;pais&quot; ) summary(model_2_cluster) ## R^2= 0.51 ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.9e+01 3.56936 13.7775 3.5e-43 ## gasto_educ 1.1e+00 0.62151 1.8321 6.7e-02 ## dualismo_sectorial -1.5e-01 0.05207 -2.8472 4.4e-03 ## inversion_extranjera 6.1e-01 0.11008 5.5403 3.0e-08 ## pib 6.8e-05 0.00015 0.4576 6.5e-01 ## [ reached getOption(&quot;max.print&quot;) -- omitted 5 rows ] Al utilizar los clusters, el coeficiente de nuestra variable independiente gasto_educ disminuyó de 1,56 a 1,19, pero mantuvo una alta significancia estadística (valor t de &gt;12). 5.6.0.6 La normalidad en la distribución del error Hasta ahora, hemos revisado y evaluado empíricamente - cuando ha sido posible - los cinco supuestos del teorema de Gauss-Markov que aseguran que el estimador MCO es el mejor parámetro lineal imparcial (BLUE, por sus siglas en inglés). Sin embargo, estos no son suficientes para hacer inferencias estadísticas. Para ello, necesitamos una suposición adicional: Como hemos aprendido previamente, para probar una hipótesis de significancia individual de un coeficiente estimado por MCO, utilizamos la estadística \\(t\\) que nos permite contrastar el valor empírico \\(t\\) contra un valor teórico \\(t\\) (llamado “valor crítico”) dado un cierto nivel de significación (\\(\\alpha\\)). Se suele utilizar un alfa del 5% (por eso hablamos de significancia estadística con un nivel de confianza del 95%), Sin embargo, para realizar esta prueba de hipótesis, y así hacer una inferencia estadística, es necesario asumir que el coeficiente (\\(\\beta\\)) sigue una distribución T-Student. Sólo entonces podemos realizar la prueba de hipótesis utilizando la estadística \\(t\\). La suposición que permite esto es la de la normalidad en la distribución del error. Dado que el estimador MCO (\\(\\beta\\)) es una combinación lineal de los errores (\\(Y = \\beta_0 + \\beta_1x + u\\)), al asumir una distribución normal del error (\\(u\\)), podemos asumir una distribución normal del estimador MCO. Sin embargo, como se desconoce el error y su varianza, se estiman utilizando los residuos de la regresión (\\(\\hat{u}\\)), obteniendo así el error estándar de la estimación. No obstante, las estimaciones implican una pérdida de grados de libertad (por cada parámetro estimado perdemos un grado de libertad: n-k-1, n=tamaño de la muestra, k=número de parámetros estimados -variables del modelo-, 1=la estimación de interceptación, \\(\\beta_0\\)) y, por lo tanto, la distribución del error estándar y, por ende, del coeficiente, que ya no se distribuye normalmente sino como T-Student (\\(\\hat\\beta t_{n-k-1}\\)). Los dos comandos siguientes nos permiten comprobar que los residuos del modelo estimado a través del MCO siguen una distribución T-Student (aproximadamente normal). El comando qqPlot viene por defecto en R, y genera una gráfica de probabilidad normal que muestra la distribución de los datos frente a una distribución normal esperada teórica. Por lo tanto, lo que necesitamos examinar en el gráfico es que las observaciones (que son los residuos) no deben salir de las líneas punteadas (que delimitan la distribución normal): qqPlot(model_2$residuals) ## [1] 160 118 Figura 5.12: La normalidad de los residuos. Obsérvese que los países 160 y 118 son valores extremos El comando ggdensity del paquete ggpubr nos permite construir diagramas de densidad. Así, podemos trazar los residuos para evaluar visualmente si siguen una distribución aproximadamente normal. library(ggpubr) ggdensity(model_2$residuals, main = &quot; Gráfica de densidad de los residuos&quot;) Figura 5.13: Normalidad de la prueba de residuos Después de evaluar los supuestos y encontrar las soluciones (cuando sea necesario), podemos tener una mayor certeza en nuestra estimación y, como resultado, en la relación encontrada entre las variables. No obstante, una explicación completa de nuestro descubrimiento debe profundizar en el por qué y cómo se relacionan las dos variables entre sí. Todo lo que hemos aprendido será útil en el capítulo de selección de casos de estudio 6. En el próximo capítulo, estimaremos modelos con variables dependientes binarias mediante la Estimación de Máxima Verosimilitud (EMV, por sus siglas en inglés). Ejercicio 5F. - Antes de pasar al siguiente capítulo, haz un gráfico de dispersión de la relación entre la variable gini y la variable inversion_extranjera. Añade el nombre del país a cada observación. - Para el modelo 1, añade la variable inversion_extranjera como control e interpreta su coeficiente. - Haz las pruebas correspondientes para comprobar que no se violan las suposiciones del MCO. - Usando htmlreg, exporta la tabla de regresión a Word. E-mail: ifynn@uc.cl↩︎ E-mail: lnocetto@uc.cl↩︎ En realidad, la estructura es de datos de panel: el mismo país tiene diferentes observaciones a través del tiempo. Sin embargo, actualmente no somos capaces de acercarnos a estos datos. Lo trataremos en el capítulo de los datos de panel↩︎ para ello, puedes consultar el capítulo 11 sobre imputación de valores perdidos↩︎ Hacemos esto para facilitar el ejemplo, con tus datos no debes hacerlo ya que podrías crear sesgos no deseados↩︎ "],
["case-sel.html", "Capítulo 6 Selección de casos basada en regresiones 6.1 Selección de estudios de casos 6.2 ¿Qué estudio de caso debería seleccionar para la investigación cualitativa? 6.3 La importancia de la combinación de métodos", " Capítulo 6 Selección de casos basada en regresiones Inés Fynn28 y Lihuen Nocetto29 Lecturas sugeridas Gerring, J. (2006). Case Study Research: Principles and Practices. Cambridge: Cambridge University Press. Lieberman, E. S. (2005). Nested analysis as a mixed-method strategy for comparative research. American Political Science Review, 99(3), 435-452. Seawright, J. (2016). Multi-Method Social Science: Combining Qualitative and Quantitative Tools. Cambridge: Cambridge University Press. Los paquetes que necesitas instalar tidyverse (Wickham 2019b), paqueteadp (Urdinez and Cruz 2020), broom (Robinson and Hayes 2020). 6.1 Selección de estudios de casos Este capítulo le dará herramientas para, a partir de lo que hemos aprendido en el capítulo de los modelos lineales, utilizar las regresiones para seleccionar los estudios de casos. Estas técnicas serán útiles cuando se haga una investigación con métodos mixtos. Cuando trabajamos con datos de observación (en contraposición a los experimentales) las regresiones lineales no pueden, por sí mismas, responder a las preguntas de inferencia causal.30. Es decir, aunque pueden desvelar la existencia de una relación entre nuestras variables independientes y dependientes, nuestra investigación estaría incompleta si no somos capaces de demostrar, a través de otros métodos, cómo estas variables están causalmente conectadas. Un excelente libro para consultar y aprender más sobre este tema es \" Multi-Method Social Science\" de Jason Seawright (2016). La selección del método está guiada por la pregunta que queremos responder. Por ejemplo, si nuestro interés es entender cuáles son las causas de la desigualdad en América Latina y el Caribe, utilizaríamos un análisis estadístico de n-grande que nos permitiría analizar el mayor número de países posible. De esta manera, en el capítulo 5 encontramos que, en promedio, el gasto educativo tiene un efecto positivo sobre los niveles de desigualdad. Sin embargo, el descubrimiento de que un mayor gasto genera mayores niveles de desigualdad resulta un tanto intrigante y contrario a la intuición. Un hallazgo como éste podría tener importantes repercusiones en la elaboración de políticas públicas y consecuencias para la vida cotidiana de las personas. Por lo tanto, para avanzar en nuestra investigación es deseable responder, por ejemplo, ¿Por qué la educación afecta positivamente a los niveles de desigualdad? Es decir, cuál es el mecanismo causal que explica que en América Latina y el Caribe un mayor gasto en educación genere mayores niveles de desigualdad. Para responder a este tipo de preguntas, solemos recurrir a métodos cualitativos (como estudios de caso en profundidad o análisis de rastreo de procesos31 que nos permite comprender cuáles son los tipos de procesos que explican por qué y cómo se produce la relación causal. De esta manera, lo que intentamos hacer es integrar (Seawright 2016) dos métodos de investigación, en los que un método plantea la pregunta de investigación (derivada del análisis estadístico), y el otro busca responderla (a través de un estudio de caso). Otra alternativa para reforzar nuestra investigación podría ser a través de la triangulación, es decir, abordar la misma pregunta de investigación pero desde diferentes métodos que, tomados en su conjunto, pueden generar una respuesta más compleja y completa de nuestra pregunta de investigación. A pesar de su elección (integración o triangulación), el objetivo de un diseño de método de investigación mixto es combinar diferentes métodos para alcanzar una explicación más compleja de los fenómenos que estamos estudiando. El objetivo de los métodos mixtos es precisamente abordar los mismos fenómenos a través de diferentes metodologías que permitan captar diferentes ángulos o dimensiones de un problema de investigación. Aunque existen infinitas formas de combinar los métodos, algunos métodos son más compatibles entre sí que otros y, de hecho, algunas combinaciones pueden causar más confusión que claridad (Lieberman 2005). En esta sección, aprenderemos una combinación de métodos que Lieberman (2005) ha llamado análisis anidado (nested analysis), que es la combinación del análisis estadístico de una gran muestra con el estudio en profundidad de uno o más casos contenidos en esa muestra. En resumen, lo que haremos es seleccionar casos (en este caso, países) a partir de la estimación de nuestro modelo. Después de estimar el modelo, el primer paso para seleccionar los casos es estimar los residuos y los valores predichos del modelo para cada una de nuestras observaciones. Esto se debe a que, para seleccionar nuestros casos de estudio, compararemos lo que nuestro modelo predijo con los valores observados de cada uno de estos casos. Para obtener los residuos y los valores predichos en R utilizaremos el comando augment del paquete broom. Este comando crea una nueva base de datos sobre el modelo que añade variables a la base de datos original (para cada observación): valores predichos, errores estándar, los residuos y residuos estandarizados, entre otras estadísticas. Usaremos el modelo 2 que estimamos en el capítulo 5 con datos de Huber et al. (2006) como ejemplo. Residuos y valores predichos: library(tidyverse) library(broom) library(paqueteadp) data(&quot;bienestar&quot;) bienestar_sinna &lt;- bienestar %&gt;% drop_na(gini, gasto_educ, inversion_extranjera, gasto_salud, gasto_segsocial, poblacion, dualismo_sectorial, diversidad_etnica, pib, tipo_regimen, bal_legislativo, represion) model_2 &lt;- lm(gini ~ 1 + gasto_educ + inversion_extranjera + gasto_salud + gasto_segsocial + poblacion + dualismo_sectorial + diversidad_etnica + pib + tipo_regimen + bal_legislativo + represion, data = bienestar_sinna) model_aug &lt;- broom::augment(model_2, data = bienestar_sinna) model_aug ## # A tibble: 167 x 22 ## pais codigo_pais anio poblacion gini dualismo_sector… pib ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arge… ARG 1982 30.8 40.2 9.50 7711. ## 2 Arge… ARG 1983 30.9 40.4 8.36 7907. ## 3 Arge… ARG 1990 30.7 43.1 7.72 6823. ## # … with 164 more rows, and 15 more variables 6.2 ¿Qué estudio de caso debería seleccionar para la investigación cualitativa? Los casos seleccionados para un estudio cualitativo en profundidad se eligen de una población, y las razones de su selección dependen de su representatividad dentro de esa población. En este sentido, según (Gerring 2006), un estudio de caso no puede existir aislado de un análisis de casos cruzados relativamente grande. Su mejor estudio de caso dependerá de cuál es el objetivo para el que se seleccionó el caso. De esta manera, la selección del caso debe ser intencional y no aleatoria (Gerring 2006). A continuación, se presentan diferentes objetivos para los cuales se seleccionan y se implementan los casos en R, basados en nuestro modelo estadístico sobre los determinantes de la desigualdad en América Latina y el Caribe. 6.2.1 Casos típicos Uno de los objetivos de la selección de casos es examinar con mayor detalle los mecanismos que conectan la variable independiente con la variable dependiente. Si este es nuestro objetivo, entonces queremos seleccionar casos que sean ejemplos típicos de las relaciones que encontramos en el análisis estadístico. Por lo tanto, lo que buscamos es un caso con el menor residuo posible. Es decir, el caso en que nuestro modelo predijo el mejor. Estos son también llamados casos sobre la línea (casos que están sobre la línea de regresión). Para identificar este caso graficaremos, usando la base de datos creada con la función augment(), los residuos. Los transformaremos en valor absoluto porque, por defecto, siempre hay residuos negativos. Además, para identificar los casos, pediremos a ggplot() que añada las etiquetas de los cuatro (top_n(-4. -resid_abs)) países (mapping = aes(label = pais)) con los residuos más bajos. Incorporamos la línea horizontal (geom_hline(aes(yintercept = 0))) al gráfico para visualizar dónde los residuos son casi nulos (allí se encuentran los casos que el modelo predijo perfectamente: es decir, el caso típico). ggplot(data = model_aug, mapping = aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(aes(yintercept = 0)) + geom_text(data = model_aug %&gt;% mutate(.resid_abs = abs(.resid)) %&gt;% top_n(-4, .resid_abs), mapping = aes(label = pais)) Figura 3.3: Los 4 casos típicos más importantes Según el gráfico, el Brasil (dos veces), Honduras y el Uruguay son tres casos típicos del modelo estimado sobre los determinantes de la desigualdad en América Latina y el Caribe. Es decir, se trata de casos que explican sus niveles de desigualdad en base a las variables del Modelo 2, ya sea que la desigualdad sea alta (Brasil, Honduras) o baja (Uruguay). 6.2.2 Los valores atípicos Los valores atípicos son casos que, dado nuestro modelo, presentan un comportamiento no esperado; son valores atípicos porque no pueden ser bien explicados a través de nuestro modelo. En resumen, son “anomalías teóricas” (Gerring 2006, 106). En general, seleccionamos estos tipos de casos para explorar nuevas hipótesis, que pueden eventualmente arrojar luz sobre las variables omitidas en el modelo estadístico. La selección de los valores atípicos funciona de manera opuesta a la selección de los casos típicos: en lugar de seleccionar los que tienen el residuo más bajo, queremos seleccionar los casos cuyo valor predicho difiere más del valor real (es decir, que tienen los residuos más altos). ggplot(data = model_aug, mapping = aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(aes(yintercept = 0)) + geom_text(data = model_aug %&gt;% mutate(.resid_abs = abs(.resid)) %&gt;% top_n(4, .resid_abs), mapping = aes(label = pais)) Figura 3.4: Los 4 principales casos atípicos Jamaica (en varios años) aparece como un país mal explicado por nuestro modelo. Es un país que presenta niveles relativamente bajos de desigualdad, pero las variables independientes del modelo no dan cuenta de estos valores. Hay un año especialmente interesante, 1993, en el que su puntuación de Gini es de 35,7, lo que la convierte en una de las más equitativas de la muestra. Rodeado de algunos de los más desiguales países del mundo tendríamos que añadir una nueva variable a nuestro modelo para explicar el caso de Jamaica. 6.2.3 Casos influyentes Los casos influyentes son aquellos que presentan valores extremos ya sea en las variables independientes y tienen una fuerte influencia en la regresión. Es decir, son casos que influyen fuertemente en la pendiente de la regresión que observamos (recordemos que la pendiente viene dada por el coeficiente de regresión \\(\\beta_i\\)). Estos son casos que, como valores atípicos, también son “inusuales”. Mientras que la selección de un valor atípico se utiliza para explorar hipótesis alternativas, la selección de un caso influyente ayuda a confirmar nuestra hipótesis de base (Gerring 2006). Para identificar los casos influyentes, podemos tomar dos caminos: Por un lado, podemos utilizar los dfbetas, que son estadísticas que indican cuánto cambia el coeficiente de regresión \\(\\beta_i\\) en unidades de desviación estándar si la i-th observación fuera borrada. Por lo tanto, tendremos un dfbeta por cada observación que indica cuánto cambiaría el \\(\\beta_i\\) de la variable gasto_educ (gasto en educación) si no se incluyera esta observación en la muestra. Así, cuanto más varíe la pendiente (\\(\\beta_i\\)) con la ausencia de la observación, más influyente será esa observación. A continuación, queremos seleccionar los casos que producen mayores cambios en la desviación estándar de \\(\\beta_i\\) cuando se eliminan de la muestra. Como mencionamos, los casos influyentes se utilizan para confirmar nuestras teorías. Al mismo tiempo, si la eliminación de las observaciones influyentes de la muestra anula las relaciones que habíamos encontrado (si al eliminar el caso \\(\\beta_i\\) deja de ser significativo), estos casos también son útiles para explorar nuevas hipótesis o identificar las variables que se omitieron en el modelo. model_aug %&gt;% mutate(dfb_cseduc = as.tibble(dfbetas(model_2))$gasto_educ) %&gt;% arrange(-dfb_cseduc) %&gt;% slice(1:3) %&gt;% dplyr::select(pais, dfb_cseduc) ## Warning: `as.tibble()` is deprecated as of tibble 2.0.0. ## Please use `as_tibble()` instead. ## The signature and semantics have changed, see `?as_tibble`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## # A tibble: 3 x 2 ## pais dfb_cseduc ## &lt;chr&gt; &lt;dbl&gt; ## 1 Barbados 0.483 ## 2 Jamaica 0.298 ## 3 Venezuela 0.241 Usando la distancia de Cook, que se basa en una lógica similar a la de dfbetas. La distancia de Cook considera los valores que cada observación asume para las variables independientes y dependientes para calcular cuánto varían los coeficientes cuando cada observación está ausente de la muestra. En resumen, esta distancia nos dice cuánto influye cada observación en la regresión en su conjunto: cuanto mayor es la distancia de Cook, mayor es la contribución de la observación a las inferencias del modelo. Los casos con mayor distancia de Cook son centrales para mantener las conclusiones analíticas (especialmente con muestras relativamente pequeñas; con muestras grandes es menos probable que existan casos con tal influencia). Por lo tanto, el uso de la distancia de Cook para seleccionar casos para un estudio en profundidad puede ser relevante: si en el estudio cualitativo de un caso influyente no podemos confirmar nuestra teoría, es poco probable que podamos confirmarla en otros casos. ggplot(data = model_aug, mapping = aes(x = .fitted, y = .cooksd)) + geom_point() + geom_text(data = model_aug %&gt;% top_n(3, .cooksd), mapping = aes(label = pais)) + labs(title = &quot; Casos influyentes&quot;) Figura 3.6: Los 3 casos más influyentes Una vez más, Jamaica se destaca como un país para ser estudiado en profundidad.32 6.2.4 Casos extremos La selección de los casos extremos implica la identificación de observaciones que se ubican lejos de la media de la distribución de la variable independiente o dependiente. La utilidad de estas observaciones radica en la “rareza” de sus valores. Esta técnica se recomienda cuando no existe un supuesto teórico sólido y, por lo tanto, la investigación se centra en la construcción teórica. El estudio en profundidad de los casos extremos es más bien exploratorio: es una forma de evaluar y buscar las posibles causas de \\(y\\) o los posibles efectos de \\(x\\). Es importante señalar que un caso extremo puede coincidir tanto con un caso típico como con un caso atípico (Gerring 2006). Un trabajo clásico de selección de casos extremos en la variable dependiente es el de Theda Skocpol (1979) sobre revoluciones sociales, donde la teoría se desarrolla en base a tres casos que presentan el valor más extremo para una revolución (de hecho, son los únicos que presentan tal valor según Skocpol). 6.2.4.1 Casos extremos en la variable independiente: \\(x\\) Observemos cómo se comporta la variable independiente: ggplot(bienestar_sinna, aes(x = gasto_educ)) + geom_histogram(binwidth = 1,color=&quot;white&quot;, fill=&quot;black&quot;) + labs( subtitle = &quot;% del PIB gastado en Educación&quot;, caption = &quot;Fuente: Huber et al (2006))&quot;, x = &quot;Gasto en educación&quot;, y = &quot;Frecuencia&quot; ) Figura 3.7: Histograma de la Variable Independiente: Gastos de educación Para la selección de casos extremos sobre la variable independiente, a partir del modelo estadístico estimado, sólo hay que calcular la diferencia -en valores absolutos- entre el valor de cada observación y la media muestral del gasto en educación. Luego, se seleccionan los tres casos que presentan la mayor diferencia con la media de la muestra. Estos pasos se implementan en R de la siguiente manera: mean(model_aug$gasto_educ, na.rm = T) ## [1] 4 model_aug %&gt;% mutate(dif_cseduc = abs(gasto_educ - mean(gasto_educ, na.rm = T))) %&gt;% top_n(3, dif_cseduc) %&gt;% arrange(-dif_cseduc) %&gt;% dplyr::select(pais, anio, gasto_educ, dif_cseduc) ## # A tibble: 3 x 4 ## pais anio gasto_educ dif_cseduc ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Barbados 1981 0.8 3.16 ## 2 Honduras 2001 6.8 2.84 ## 3 Uruguay 1984 1.4 2.56 Graficamos los resultados para una mejor visualización: model_aug &lt;- model_aug %&gt;% mutate(dif_cseduc = abs(gasto_educ - mean(gasto_educ, na.rm = T))) ggplot(data = model_aug, mapping = aes(x = .fitted, y = dif_cseduc)) + geom_point() + geom_text(data = model_aug %&gt;% top_n(3, dif_cseduc), mapping = aes(label = pais)) Figura 3.9: Los tres casos extremos más importantes en los gastos de educación Barbados se destaca por ser un caso extremo, ya que está muy por debajo de la media de la muestra. Honduras, por el contrario, está muy por encima. Sería interesante comparar ambos. Considerando que el tercer país es Uruguay, y que los tres son economías relativamente pequeñas, surge una pregunta que seguramente nos hará mejorar el modelo: ¿no deberíamos controlar por el tamaño de la economía, medido por su PIB? Esta duda podría llevarnos a un nuevo modelo, donde la significancia estadística podría cambiar. Ejercicio 6A. Seleccione los casos extremos para la variable independiente Inversión Extranjera Directa inversion_extranjera. 6.2.4.2 Casos extremos en la variable dependiente \\(y\\) La selección de los casos extremos en la variable dependiente se hace de la misma manera que con los casos extremos en \\(x\\). La única diferencia es que ahora calculamos la diferencia -en valores absolutos- entre el valor observado de cada observación y la media de la muestra en la variable dependiente (Índice de Gini, en este ejemplo). Luego, se seleccionan los tres casos que presentan la mayor diferencia entre la media de la muestra y su valor de la variable dependiente. mean(model_aug$gini, na.rm = T) ## [1] 50 model_aug %&gt;% mutate(dif_gini = abs(gini - mean(gini, na.rm = T))) %&gt;% top_n(3, dif_gini) %&gt;% arrange(-dif_gini) %&gt;% dplyr::select(pais, gini, dif_gini) ## # A tibble: 3 x 3 ## pais gini dif_gini ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Barbados 28.9 21.4 ## 2 Jamaica 66 15.7 ## 3 Venezuela 65.8 15.5 También podemos graficarlo para una mejor visualización: model_aug &lt;- model_aug %&gt;% mutate(dif_gini = abs(gini - mean(gini, na.rm = T))) ggplot(data = model_aug, mapping = aes(x = .fitted, y = dif_gini)) + geom_point() + geom_text(data = model_aug %&gt;% top_n(2, dif_gini), mapping = aes(label = pais)) Figura 3.11: Los dos casos extremos más importantes según el índice de Gini Una vez más, Barbados y Jamaica aparecen como casos atípicos en la variable dependiente. Ambos tienen en común que fueron colonias caribeñas del Imperio Británico. ¿Tal vez, podríamos incluir este control para todos los países con este legado y ver cómo se ajusta al nuevo modelo? Si aparecen grandes cambios en los valores previstos, podríamos explorar con pruebas cualitativas el papel que tuvieron las instituciones coloniales del Imperio Británico sobre la desigualdad de estas colonias. 6.2.5 Casos más similares La selección de casos similares implica la identificación de dos casos que sean similares. Si estamos en la etapa exploratoria de nuestro proyecto de investigación y no tenemos fuertes supuestos teóricos (aún no hemos identificado una variable independiente clave en particular), buscamos un par de observaciones que sean más similares en las variables independientes pero que difieran en la variable dependiente. De esta manera, el objetivo será identificar uno o más factores que difieren entre los casos y que pueden explicar la divergencia del resultado. Esta estrategia es la del método directo de acuerdo de Stuart Mill. Sin embargo, cuando ya tenemos una teoría sobre cómo una variable independiente afecta a la variable dependiente, la selección de casos consiste en identificar dos casos que son similares en todas las covariables pero que difieren en la variable independiente de interés. En este caso, el enfoque será confirmar el argumento y explorar más a fondo los mecanismos causales que conectan la variable independiente con la dependiente. Para seleccionar casos similares, se recomienda utilizar una técnica de matching (Gerring 2006, 134). En palabras sencillas, esta técnica consiste en emparejar pares de observaciones que sean lo más similares posible en todas las covariables, pero que difieran en la variable independiente de interés. Para simplificar el análisis, estas variables independientes tienden a ser dicotómicas, emulando un escenario experimental en el que hay un tratamiento (1) y un placebo o control (0). De esta manera, el objetivo es hacer coincidir pares en los que una observación pertenece al grupo de tratamiento y la otra al grupo de control. Dado que fundar pares que coincidan en todas las covariables puede ser bastante difícil de lograr, se utiliza a menudo un procedimiento de emparejamiento basado en las puntuaciones de propensión. Este procedimiento consiste en fundar pares de observaciones que tienen probabilidades estimadas similares de estar en el grupo de tratamiento (con un valor de 1 en la variable independiente de interés), condicionadas por las variables de control. Para aplicar este método de selección de casos en nuestra investigación, crearemos una variable de tratamiento ficticia (para la variable de gasto en educación), en la que 0 es un gasto menor o igual a la media de la muestra y 1 si el gasto es mayor que la media. bienestar_sinna &lt;- bienestar_sinna %&gt;% mutate(treatment = if_else(gasto_educ &gt; mean(gasto_educ), 1, 0)) Ahora que tenemos una variable de tratamiento, podemos estimar las puntuaciones de propensión. Es decir, la probabilidad de estar en el grupo de tratamiento (gasto en educación mayor que la media de la muestra) condicionada por las variables de control del modelo. Esta estimación se hace en base a un modelo Logit, ya que nuestras variables dependientes son una variable dicotómica. m_propensityscore &lt;- glm(treatment ~ dualismo_sectorial + inversion_extranjera + pib + poblacion + diversidad_etnica + tipo_regimen + tipo_regimen * gasto_segsocial + gasto_salud + gasto_segsocial + bal_legislativo + represion, data = bienestar_sinna, family = binomial(link = logit), na.action = na.exclude) Como hicimos con el modelo general de los determinantes de la desigualdad, crearemos una base de datos con el comando augment para guardar algunas estadísticas que serán útiles para seleccionar los casos. propensity_scores&lt;- augment(m_propensityscore, data = bienestar_sinna, type.predict = &quot;response&quot;) %&gt;% dplyr::select(propensity_scores = .fitted, pais, treatment, anio, gini) Ahora, identificamos los casos con las puntuaciones de propensión más bajas tanto para el grupo de tratamiento (alto gasto en educación) como para el grupo de control (bajo gasto en educación) para decidir la selección de los casos. Cabe señalar que esto también puede hacerse para las puntuaciones de propensión altas: lo importante es que deben tener puntuaciones similares o “cercanas” (igual probabilidad de recibir tratamiento). Veamos los casos con puntuaciones de propensión bajas, en el grupo de países con gastos superiores a la media de la muestra: propensity_scores %&gt;% filter(treatment == 1) %&gt;% arrange(propensity_scores) %&gt;% dplyr::select(pais, anio, propensity_scores) %&gt;% slice(1:2) ## # A tibble: 2 x 3 ## pais anio propensity_scores ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Brasil 1984 0.0815 ## 2 México 2000 0.159 Por otra parte, observemos los casos con una baja puntuación de propensión entre los países con un gasto en educación inferior al promedio de la muestra: propensity_scores %&gt;% filter(treatment== 0) %&gt;% arrange(propensity_scores) %&gt;% dplyr::select(pais, anio, propensity_scores) %&gt;% slice(1:2) ## # A tibble: 2 x 3 ## pais anio propensity_scores ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Paraguay 1994 0.00309 ## 2 Argentina 1982 0.00673 De acuerdo con los resultados, tanto Brasil como México podrían ser seleccionados para ser comparados con Paraguay o Argentina para llevar a cabo estudios de casos más exhaustivos y similares. Teniendo en cuenta su proximidad geográfica, podríamos comparar Brasil con Argentina, e intentar explicar de qué manera el gasto en educación ha impactado en la equidad de los ingresos en ambos países. Ejercicio 6B. Seleccione pares de casos más similares tomando la Inversión Extranjera Directa inversion_extranjera como una variable independiente (tratamiento). 6.2.6 Casos mas diferentes El procedimiento para seleccionar los casos más diferentes implica una lógica opuesta a la de los casos más similares. Aquí buscamos observaciones que son diferentes en las variables de control, pero similares en el valor asumido por la variable independiente y la variable dependiente. En resumen, lo que buscamos son diferentes puntuaciones de propensión pero similares en la variable independiente y en la dependiente. Cabe señalar que este tipo de selección de casos es útil cuando se supone una “causa única” (Gerring 2006, pág. 143). Es decir, cuando la variación de la variable dependiente es causada por una sola variable (o cuando nos interesa explicar el efecto de un solo factor). Si lo que interesa es indagar sobre la combinación de diferentes factores causales, este procedimiento de selección de casos no es el más adecuado. Para seleccionar los casos “más diferentes”, también utilizaremos las puntuaciones de propensión, pero ahora estamos interesados en seleccionar pares con resultados iguales en la variable dependiente, así como en la variable independiente, y con puntuaciones de propensión muy diferentes. Veamos, entonces, cuáles son estos casos. Primero, creamos una variable ficticia para un Gini por encima o por debajo de la media. Luego, identificamos los casos tratados con bajas puntuaciones de propensión (baja probabilidad de tener un gasto por encima de la media) que tienen valores de Gini mayores que la media de la muestra y valores de gasto en educación también mayores que la media de la muestra: propensity_scores &lt;- propensity_scores %&gt;% mutate(gini = if_else(gini &gt; mean(gini, na.rm = T), 1, 0)) propensity_scores %&gt;% filter(gini == 1 &amp; treatment==0) %&gt;% arrange(propensity_scores) %&gt;% dplyr::select(pais, anio, propensity_scores) %&gt;% slice(1:2) ## # A tibble: 2 x 3 ## pais anio propensity_scores ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Paraguay 1999 0.00953 ## 2 Paraguay 1997 0.0221 A continuación, repetimos el mismo proceso para las puntuaciones de propensión más altas (es decir, cuando la probabilidad de recibir tratamiento - tener un gasto en educación mayor que la media - es muy alta). En otras palabras, identificamos los casos con la puntuación de propensión más alta para valores de Gini mayores que la media de la muestra y gastos en educación mayores que la media de la muestra: propensity_scores %&gt;% filter(gini == 1 &amp; treatment == 0) %&gt;% arrange(-propensity_scores) %&gt;% dplyr::select(pais, anio, propensity_scores) %&gt;% slice(1:2) ## # A tibble: 2 x 3 ## pais anio propensity_scores ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Honduras 1994 0.983 ## 2 Honduras 1996 0.969 Nuestros resultados sugieren que Paraguay podría ser seleccionado para ser comparado con Honduras para llevar a cabo un estudio de caso “muy diferente”. Ambos tienen un bajo gasto en educación como porcentaje de su PIB, y ambos son muy desiguales, aunque son muy diferentes en las variables de control. 6.3 La importancia de la combinación de métodos Para concluir, consideramos importante insistir en la pertinencia de la combinación de métodos para responder a una pregunta de investigación. Aunque los métodos apropiados dependerán de la pregunta de investigación, la respuesta a un fenómeno requiere tanto la identificación de una relación entre dos (o más) variables como una explicación detallada sobre la forma en que esas dos variables se vinculan entre sí y por qué se produce el efecto indicado. Para abordar estas dos dimensiones es necesario combinar diferentes estrategias empíricas, a fin de explotar las respectivas virtudes y complementar sus debilidades. Una estimación a través de MCO nos permite identificar las relaciones promedio entre dos variables en un gran número de observaciones, algo que la investigación cualitativa no puede realizar. Sin embargo, MCO no puede responder por qué o cómo se producen estas relaciones y, para ello, es necesario un diseño de investigación cualitativa que profundice en los procesos y agentes que “explican” estas relaciones. Por supuesto, el proceso también puede realizarse al revés: primero, podríamos identificar una relación entre dos variables a partir de un estudio de caso en profundidad, y luego, probar esta relación en otros casos a través de un estudio cuantitativo de n-grande para evaluar la generalización del descubrimiento. En cualquier caso, se recomienda la combinación de métodos para ofrecer explicaciones más complejas y completas sobre los fenómenos que nos interesa estudiar. Ejercicio 6C. - Estime de un modelo donde la variable dependiente es la puntuación de Gini (gini) y el gasto en educación de los independientes (gasto_educ), gasto en salud (gasto_salud), gasto en seguridad social (gasto_segsocial), PIB (pib), e inversión extranjera directa (inversion_extranjera). - Seleccione los casos típicos, atípicos e influyentes para este modelo. ¿Qué variables pueden ser importantes para entender los valores atípicos? - Ahora, supongamos que su variable de interés independiente es la Inversión Extranjera Directa. Seleccione los casos extremos en x, los casos extremos en y, los casos más similares y más diferentes. E-mail: ifynn@uc.cl↩︎ E-mail: lnocetto@uc.cl↩︎ Es posible que con buenas variables instrumentales, pero no las cubrimos en este libro↩︎ Un gran ejemplo de rastreo de procesos aplicado a la ciencia política latinoamericana es el libro de Pérez, Piñeiro y Rosenblatt en el caso del activismo partidario en Uruguay (https://www.cambridge.org/core/books/how-party-activism-survives/93C5584DB63DF0A80B51F3EEB68BC8E9)↩︎ si ya te estás preguntando qué ocurrió en Jamaica, te recomendamos este artículo: Handa, S., &amp; King, D. (1997). Structural adjustment policies, income distribution and poverty: a review of the Jamaican experience. World Development, 25(6), 915-930.↩︎ "],
["panel.html", "Capítulo 7 Modelos de panel 7.1 Introducción 7.2 Describiendo la base de datos en panel 7.3 Modelización de la variación a nivel de grupo 7.4 Efectos fijos vs. efectos aleatorios 7.5 Testeando raíces unitarias 7.6 Errores estándar robustos corregidos para panel", " Capítulo 7 Modelos de panel Francisco Urdinez33 Lecturas sugeridas Beck, N. (2008). Time‐Series Cross‐Section Methods. In J. M. Box-Steffensmeier, H. E. Brady, &amp; D. Collier (Eds.), The Oxford Handbook of Political Methodology (pp. 475–493). Oxford: Oxford University Press. Beck, N., &amp; Katz, J. N. (2011). Modeling dynamics in time-series–cross-section political economy data. Annual Review of Political Science, 14, 331-352. Croissant, Y., &amp; Millo, G. (2018). Panel Data Econometrics with R. John Wiley and Sons. Henningsen, A., &amp; Henningsen, G. (2019). Analysis of Panel Data Using R. In M. Tsionas (Ed.), Panel Data Econometrics: Theory (pp. 345–396). London: Academic Press. Los paquetes que necesitas instalar tidyverse (Wickham 2019b), paqueteadp (Urdinez and Cruz 2020), unvotes (Robinson 2017), lubridate (Spinu, Grolemund, and Wickham 2020), ggcorrplot (Kassambara 2019), plm (Croissant, Millo, and Tappe 2020). 7.1 Introducción En este capítulo, aprenderemos más sobre lo que hemos tocado en el capítulo de modelos lineales. Aprenderemos cómo trabajar con datos de panel, es decir, cómo añadir una dimensión temporal a nuestras regresiones lineales. Le mostraremos cómo diagnosticar si el tiempo tiene un efecto en nuestros resultados de MCO y de qué manera, y (a) estimar modelos con efectos fijos y efectos aleatorios, (b) ver cómo diagnosticar raíces unitarias, (c) cómo crear variables con desfases (llamados lags en inglés) o prospectivas (llamados leads en inglés), y (d) cómo calcular los errores estándar corregidos para panel. Recuerde que este es un capítulo introductorio, por lo que le recomendamos que consulte la bibliografía sugerida para responder a preguntas específicas. Trabajaremos con un ejemplo práctico de relaciones internacionales: Históricamente, la política exterior de América Latina ha estado fuertemente influenciada por los Estados Unidos, probablemente más que cualquier otra región del mundo. Sin embargo, en los últimos treinta años, esta tendencia se ha debilitado, y la influencia es menos evidente ahora que durante la Guerra Fría. ¿Cuál es la razón de esta tendencia? Esta pregunta fue abordada por Octavio Amorim Neto, un reconocido politólogo brasileño, en de Dutra a Lula: La formación y los determinantes de la política exterior brasileña* (De Dutra a Lula: a condução e os determinantes da política externa brasileira) (2012). Este libro tiene la ventaja de responder a esta pregunta con una metodología cuantitativa, algo inusual en un campo dominado por los trabajos historiográficos. Otros artículos pasaron luego a refinar más los argumentos del libro, como Neto and Malamud (2015) y Rodrigues, Urdinez, and de Oliveira (2019). Abramos la base de datos: library(tidyverse) library(paqueteadp) data(&quot;eeuu_brasil&quot;) Según Amorim Neto, a medida que Brasil se transformaba en una potencia regional, y su poder crecía, tenía un mayor margen para alejarse de los preceptos de los Estados Unidos. Una forma de abordar el concepto abstracto de “proximidad política” en las relaciones internacionales es a través de la convergencia de votos en la Asamblea General de las Naciones Unidas. Los países votan, digamos, 20 veces al año sobre diferentes temas. Así, sobre esos 20 votos, podemos calcular cuántas veces un país votó igual que otro (a favor, en contra o abstención) y expresar esta similitud como un porcentaje. La siguiente figura muestra el porcentaje de votos en común entre el Brasil y los Estados Unidos en la Asamblea de 1945: ggplot(eeuu_brasil) + geom_line(aes(x = anio, y = voto)) + labs(x = &quot;Year&quot;, y = &quot;Convergencia de votos&quot;) Figura 7.1: Convergencia de votos entre Brasil y los EE.UU. Hay una herramienta sobresaliente llamada unvotes en R para todos aquellos que estudian la historia del voto de los países en la Asamblea de las Naciones Unidas, que nos será útil en este capítulo.34. Usando unvotes podemos trazar la convergencia de los votos, y dividirlos por temas. library(unvotes) library(lubridate) Además, haremos uso del paquete lubridate para el análisis con fechas. El paquete unvotes proporciona tres bases de datos con los que podemos trabajar: un_roll_calls, un_roll_call_issues y eeuu_latam. Cada uno de estas bases de datos contiene una variable llamada rcid, el código de la votación, que puede ser usado como un identificador para unirlos con otras variables. Si recuerdas, en el capítulo de manejo avanzado te enseñamos cómo fusionar las bases de datos utilizando los códigos de país. La base de datos de eeuu_latam proporciona información sobre el historial de votación de la Asamblea General de las Naciones Unidas. Contiene una fila para cada par de países votantes. La base de datos un_roll_calls contiene información sobre cada votación nominal de la Asamblea General de las Naciones Unidas. La base un_roll_call_issues contiene clasificaciones de temas de las votaciones nominales de la Asamblea General de las Naciones Unidas. Muchos votos no tienen ningún tema, y algunos tienen más de uno. ¿Cómo ha cambiado el registro de votación de los Estados Unidos a lo largo del tiempo en una variedad de temas en comparación con Brasil? p_votos_eeuu_br &lt;- un_votes %&gt;% filter(country %in% c(&quot;United States of America&quot;, &quot;Brazil&quot;)) %&gt;% inner_join(un_roll_calls, by = &quot;rcid&quot;) %&gt;% inner_join(un_roll_call_issues, by = &quot;rcid&quot;) %&gt;% mutate(issue = if_else(issue == &quot;Nuclear weapons&quot;, &quot;Nuclear weapons&quot;, issue)) %&gt;% group_by(country, year = year(date), issue) %&gt;% summarize(votes = n(), percent_yes = mean(vote == &quot;yes&quot;)) %&gt;% filter(votes &gt; 5) ggplot(p_votos_eeuu_br, mapping = aes(x = year, y = percent_yes, color = country)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = F) + facet_wrap(~ issue, ncol = 2) + scale_color_grey() + labs(x = &quot;Year&quot;, y = &quot;% Yes&quot;, color = &quot;Country&quot;) Figura 3.4: Porcentaje de votos positivos en la Asamblea General de la ONU (1946 a 2015) Consideremos la hipótesis de Amorim Neto (2012). Su argumento es que este distanciamiento fue causado por un aumento del poder de Brasil, que le dio mayor autonomía. La discusión sobre cómo medir el poder en las relaciones internacionales es todo un tema en sí mismo. Una de las variables más populares para medirlo fue creada por Singer, Bremer, and Stuckey (1972)), conocida como el Índice CINC. Se trata de un índice relativo: para cada año, el índice mide la proporción de poder que cada país tiene del total del poder mundial. ¿Cómo ha cambiado Brasil desde 1945? ggplot(eeuu_brasil) + geom_line(aes(x = anio, y = poder_pais)) + labs(x = &quot;Año&quot;, y = &quot;Indice CINC&quot;) Figura 3.5: El poder internacional de Brasil ¡Muy bien! A priori, la hipótesis de Amorim Neto parece tener apoyo empírico: A medida que Brasil se volvió más poderoso, tuvo más margen para desarrollar una política exterior autónoma de los Estados Unidos. Si observamos la correlación entre ambas variables, observamos que ésta es alta y negativa (-0,89): los valores más altos de convergencia de votos están correlacionados con los valores más bajos de poder. Usaremos el paquete ggpubr para “Gráficas listas para publicar”: library(ggpubr) ggscatter(eeuu_brasil, x = &quot;poder_pais&quot;, y = &quot;voto&quot;, add = &quot;reg.line&quot;, add.params = list(color = &quot;black&quot;, fill = &quot;lightgray&quot;), conf.int = TRUE) + stat_cor(method = &quot;pearson&quot;, label.x = 0.015) Figura 3.6: Correlación entre la convergencia de los votos de la ONU y la participación en el poder internacional En conclusión, este es el argumento central del libro de Amorim Neto. A pesar de la gran influencia que este análisis tuvo sobre los estudios posteriores, hay algunas cuestiones que invalidan parcialmente la conclusión, que será útil ejemplificar en este capítulo. 7.2 Describiendo la base de datos en panel En ciencias políticas, es común encontrar la distinción entre datos de panel y series temporales transversales (TSCS, en inglés). Beck (2008) explica que un panel se refiere a un conjunto de datos con una N mucho más grande que su T, por ejemplo, una encuesta de 10.000 personas durante tres años consecutivos. Por otro lado, una base de datos en formato TSCS a menudo contiene una T más grande que N, y sus unidades son fijas, es decir, se repiten en el tiempo. La distinción está muy bien desarrollada en un texto de Beck en Annual Review of Political Science: “Los datos de panel son datos de secciones transversales repetidas, pero las unidades se muestrean (normalmente son encuestados obtenidos en algún esquema de muestreo aleatorio), y normalmente se observan sólo unas pocas veces. Las unidades de TSCS son fijas; no hay ningún esquema de muestreo para las unidades, y cualquier experimento de”remuestreo\" debe mantener las unidades fijas y sólo remuestrear las unidades completas (Freedman &amp; Peters 1984). En los datos en panel, las personas observadas no son de interés; todas las inferencias de interés se refieren a la población subyacente que fue objeto del muestreo, en lugar de estar condicionadas por la muestra observada. Los datos de la TSCS son exactamente lo contrario; todas las inferencias de interés son condicionales a las unidades observadas\". (2001, 273). A efectos prácticos, la notación de ambos es la misma y los conceptos tienden a utilizarse indistintamente: \\[ y_i,_t = x_i,_t\\beta + \\epsilon_i,_t \\] Donde \\[ x_i,_t \\] es un vector de variables exógenas y las observaciones son indexadas por ambas unidades (i) y tiempo (t). Para revisar las rutinas más comunes de los modelos de panel, usaremos una base de datos similar a la de Amorim Neto, pero con once países sudamericanos. Ahora tenemos una base en formato TSCS: una muestra de once países entre 1970 y 2007, 38 años. Si domina el contenido introductorio de este capítulo, el siguiente paso será trabajar a través del libro de Croissant and Millo (2018), que también son los creadores del paquete más utilizado para el panel en R, plm. data(&quot;eeuu_latam&quot;) eeuu_latam %&gt;% count(pais) ## # A tibble: 10 x 2 ## pais n ## &lt;chr&gt; &lt;int&gt; ## 1 Argentina 38 ## 2 Bolivia 38 ## 3 Brazil 38 ## # … with 7 more rows Observemos la generalizabilidad de la hipótesis. El libro sólo analiza un caso, pero queremos saber si un panel de once países puede ayudarnos a fortalecer esos hallazgos y ganar validez externa. Si observamos el comportamiento de voto de estos once países latinoamericanos en las Naciones Unidas, notaremos un patrón similar entre ellos. Parece que la convergencia de votos con EE.UU. cayó entre 1945 y 1990, luego subió durante los 90, y luego volvió a caer a principios de los 2000. Siempre se recomienda hacer este paso antes de pasar a las regresiones. Hay dos formas de trazar las variables independientes y dependientes a lo largo del tiempo usando “ggplot2”, como ya has aprendido. 7.2.1 Opción A. Gráfico de líneas ggplot(eeuu_latam, aes(x = anio, y = voto, color = pais, linetype = pais, group = pais)) + geom_line() + labs(x = &quot;Year&quot;, y = &quot;% Yes&quot;, color = &quot;&quot;, linetype = &quot;&quot;) Figura 3.9: Tendencias de línea para la evolución de la convergencia de los votos de la AGNU con los de EE.UU. por país 7.2.2 Option B. Gráfico de cajas ggplot(eeuu_latam, aes(x = factor(anio), y = voto)) + geom_boxplot() + scale_x_discrete(breaks = seq(1970, 2007, by = 5)) + labs(x = &quot;Year&quot;, y = &quot;% Convergencia con EEUU&quot;) Figura 3.10: Gráficos de recuadro para la evolución de la convergencia de los votos de la AGNU con los de EE.UU. por país Además, como observamos en el caso de Brasil, podemos ver la proximidad de los votos entre los 11 países y los Estados Unidos separando los votos por tema usando unvotes. p_votos_paises &lt;- un_votes %&gt;% filter(country %in% c(&quot;United States of America&quot;, &quot;Brazil&quot;, &quot;Bolivia&quot;, &quot;Argentina&quot;, &quot;Chile&quot;, &quot;Peru&quot;, &quot;Ecuador&quot;, &quot;Colombia&quot;, &quot;Venezuela&quot;, &quot;Paraguay&quot;, &quot;Uruguay&quot;)) %&gt;% inner_join(un_roll_calls, by = &quot;rcid&quot;) %&gt;% inner_join(un_roll_call_issues, by = &quot;rcid&quot;) %&gt;% mutate(issue = if_else(issue == &quot;Nuclear weapons&quot;, &quot;Nuclear weapons&quot;, issue)) %&gt;% group_by(country, year = year(date), issue) %&gt;% summarize(votes = n(), percent_yes = mean(vote == &quot;yes&quot;)) %&gt;% filter(votes &gt; 5) ggplot(p_votos_paises, mapping = aes(x = year, y = percent_yes, linetype = country, color = country)) + geom_smooth(method = &quot;loess&quot;, se = F) + facet_wrap(~issue, ncol = 2) + labs(x = &quot;Year&quot;, y = &quot;% Convergencia con EEUU&quot;, color = &quot;&quot;, linetype = &quot;&quot;) Figura 3.11: Porcentaje de votos positivos en la Asamblea General de la ONU (1946 a 2015) Una vez que haya observado el comportamiento de su variable dependiente, probablemente querrá replicar el mismo ejercicio para su variable independiente. En nuestro caso, el poder del país. Al observar el comportamiento de la variable independiente de nuestro ejemplo (la potencia del país) notaremos que, mientras que Colombia está en línea con lo que observamos para Brasil (crecimiento de poder a lo largo de los años), otros países se han debilitado desde 1970 (por ejemplo, Argentina), y la mayoría de ellos se han mantenido prácticamente estables (por ejemplo, Chile, Uruguay y Perú). Esta heterogeneidad de comportamientos desafía los hallazgos de Amorim Neto para Brasil. eeuu_latam %&gt;% filter(pais != &quot;Brazil&quot;) %&gt;% ggplot(aes(x = anio, y = poder_pais)) + geom_line() + facet_wrap(~pais, nrow = 3) Figura 3.12: Evolución del índice CINC en el tiempo para los países de América del Sur Ejercicio 7A. Usando la base de datos del capítulo de MCO (Latin America Welfare Dataset, 1960-2014, Huber et al. 2006), grafica el comportamiento del Índice de Gini (gini_slc) en América Latina a lo largo del tiempo. 7.3 Modelización de la variación a nivel de grupo Un panel de datos (o TSCS) contendrá datos para N unidades en T momentos. En este capítulo, N son países, pero en tu base de datos podrían ser partidos políticos, países, legisladores o cualquier otra unidad de observación de interés. En este capítulo, T son años, pero en tu base de datos podrían ser meses, semestres o décadas. Obviamente necesitamos regularidad en la periodización. Si tenemos años, es importante que el panel no mezcle dos unidades temporales diferentes (por ejemplo, datos por año y década). Si nuestra base de datos no contiene valores perdidos, diremos que nuestro panel está balanceado, ya que tenemos la misma cantidad de Ts para cada N. Si, por el contrario, hay valores perdidos, nuestro panel no estará balanceado.35. Al tener datos que varían a lo largo del tiempo y/o entre los individuos, nuestro modelo tendrá más información que en los modelos de corte transversal, obteniendo así estimadores más eficientes. Los datos de panel le permitirán controlar por medio de las heterogeneidades existentes entre las unidades que no varían en el tiempo, y reducir el sesgo de las variables omitidas, así como testear la hipótesis del comportamiento de las observaciones en el tiempo. Algunas de sus variables serán constantes a lo largo del tiempo, es decir, no variarán en T, sino en N. Por ejemplo, la superficie geográfica de un país o la distancia a otro país no cambiará en condiciones normales (podría variar si hubiera una guerra y las fronteras nacionales cambiaran, por ejemplo). En el caso de las personas, algunos atributos como la nacionalidad o el género tienden a permanecer iguales a lo largo del tiempo. En la ciencia política, normalmente tendremos más Ns que Ts en nuestro panel, aunque puede haber excepciones. Nos interesarán dos tipos de efectos sobre la variable dependiente: efectos sobre \\(Y\\) que varían en \\(t\\) pero no en \\(i\\), y efectos que varían en \\(i\\) pero no en \\(t\\). Los efectos que varían en \\(i\\) y en \\(t\\) se consideran en el término de error \\(\\epsilon_i,_t\\). Si ignoramos estos efectos, obtendremos coeficientes sesgados. ¡Observémoslo gráficamente! Este es un modelo en el que ignoramos la existencia de once países en la muestra, y tratamos cada observación como independiente. Por eso lo llamamos de pooled (agregado). pooled &lt;- lm(voto ~ poder_pais, data = eeuu_latam) summary(pooled) ## ## Call: ## lm(formula = voto ~ poder_pais, data = eeuu_latam) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.3194 -0.1465 -0.0538 0.0957 0.5657 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.381 0.012 31.67 &lt;2e-16 *** ## poder_pais -3.564 1.473 -2.42 0.016 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.19 on 378 degrees of freedom ## Multiple R-squared: 0.0152, Adjusted R-squared: 0.0126 ## F-statistic: 5.85 on 1 and 378 DF, p-value: 0.016 Este es un modelo en el que incorporamos un intercepto a cada país, asumiendo que nuestras variables varían entre las observaciones manual_fe &lt;- lm(voto ~ poder_pais + factor(pais), data = eeuu_latam) summary(manual_fe) ## ## Call: ## lm(formula = voto ~ poder_pais + factor(pais), data = eeuu_latam) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.3129 -0.1332 -0.0384 0.1002 0.5908 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.8145 0.0768 10.60 &lt; 2e-16 *** ## poder_pais -78.8200 12.5378 -6.29 9.2e-10 *** ## factor(pais)Bolivia -0.3637 0.0738 -4.93 1.2e-06 *** ## factor(pais)Brazil 1.3909 0.2313 6.01 4.4e-09 *** ## [ reached getOption(&quot;max.print&quot;) -- omitted 7 rows ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.18 on 369 degrees of freedom ## Multiple R-squared: 0.125, Adjusted R-squared: 0.101 ## F-statistic: 5.25 on 10 and 369 DF, p-value: 3.18e-07 Si extraemos los valores predichos de este modelo podemos comparar la diferencia que genera en el coeficiente poder_pais proporcionando esta información al modelo: eeuu_latam &lt;- eeuu_latam %&gt;% mutate(hat_fe = fitted(manual_fe)) Como se puede observar en la siguiente figura, el coeficiente del modelo combinado para poder_pais es de -3.56, mientras que para el modelo que tiene en cuenta la heterogeneidad de las unidades es de -78.8. Esta corrección se logra incorporando un intercepto para cada observación. En la figura se muestra la pendiente de cada país una vez añadidos los dummies, y la pendiente es menor cuando hacemos una regresión agregada (línea negra). La especificidad que incluyen los interceptos para cada observación se conoce como modelo de efectos fijos, y es el modelo más común para modelar datos en panel. ggplot(data = eeuu_latam, aes(x = poder_pais, y = hat_fe, label = pais, group = pais)) + geom_point() + # add pais-specific lines geom_smooth(method = &quot;lm&quot;, se = F, color = &quot;black&quot;) + # add pooled line geom_smooth(mapping = aes(x = poder_pais, y = hat_fe), inherit.aes = F, method = &quot;lm&quot;, se = T, color = &quot;black&quot;, linetype = &quot;dashed&quot;) + # label lines geom_text( data = eeuu_latam %&gt;% group_by(pais) %&gt;% top_n(1, poder_pais) %&gt;% slice(1), mapping = aes(label = pais), vjust = 1 ) Figura 3.16: Al añadir los dummies de países, la pendiente de la relación lineal cambia sustancialmente 7.4 Efectos fijos vs. efectos aleatorios En las ciencias políticas, las dos especificaciones para la modelización de la variación entre grupos en los datos de panel son modelos de efectos fijos o aleatorios. Todavía no está claro para los investigadores aplicados cuándo debe utilizarse uno u otro. Una buena discusión sobre este tema es presentada por Clark and Linzer (2015). Los modelos de efectos fijos añaden una variable dicotómica (dummy) para cada unidad. Los modelos de efectos aleatorios suponen que la variación entre unidades sigue una distribución de probabilidad, típicamente normal, con parámetros estimados a partir de los datos. El argumento index = es necesario para informar cuál es nuestro N y cuál es nuestro T, en este caso c(\"codigo\", \"anio\"). plm, por defecto, genera modelos de efectos fijos, por lo que no necesitamos usar la opción model. Si comparas el modelo fe que generaremos con el modelo manual_fe, notará que el coeficiente de poder_pais es el mismo. library(plm) fe &lt;- plm(voto ~ poder_pais, data = eeuu_latam, index = c(&quot;codigo&quot;, &quot;anio&quot;)) summary(fe) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = voto ~ poder_pais, data = eeuu_latam, index = c(&quot;codigo&quot;, ## &quot;anio&quot;)) ## ## Balanced Panel: n = 10, T = 38, N = 380 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -0.3129 -0.1332 -0.0384 0.1002 0.5908 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## poder_pais -78.8 12.5 -6.29 9.2e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 13.5 ## Residual Sum of Squares: 12.2 ## R-Squared: 0.0967 ## Adj. R-Squared: 0.0723 ## F-statistic: 39.5215 on 1 and 369 DF, p-value: 9.17e-10 Si quieres obtener los efectos fijos de los interceptos para compararlos con el modelo manual, necesitas usar la función fixef(). fixef(fe, type = &quot;dfirst&quot;) ## 2 3 4 5 6 8 9 10 11 ## -0.364 1.391 -0.232 -0.136 -0.352 -0.350 -0.204 -0.143 -0.385 Por el contrario, para los efectos aleatorios necesitamos especificar specify model = \"random\". re &lt;- plm(voto ~ poder_pais, data = eeuu_latam, index = c(&quot;codigo&quot;, &quot;anio&quot;), model = &quot;random&quot;) summary(re) ## Oneway (individual) effect Random Effect Model ## (Swamy-Arora&#39;s transformation) ## ## Call: ## plm(formula = voto ~ poder_pais, data = eeuu_latam, model = &quot;random&quot;, ## index = c(&quot;codigo&quot;, &quot;anio&quot;)) ## ## Balanced Panel: n = 10, T = 38, N = 380 ## ## Effects: ## var std.dev share ## idiosyncratic 0.03311 0.18195 0.99 ## individual 0.00017 0.01304 0.01 ## theta: 0.0853 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -0.3149 -0.1449 -0.0551 0.0974 0.5653 ## ## Coefficients: ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept) 0.3814 0.0131 29.10 &lt;2e-16 *** ## poder_pais -3.7487 1.6057 -2.33 0.02 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 13.9 ## Residual Sum of Squares: 13.7 ## R-Squared: 0.0142 ## Adj. R-Squared: 0.0116 ## Chisq: 5.45058 on 1 DF, p-value: 0.0196 7.4.1 Test de Hausman El Test de Hausman suele utilizarse para decidir si es preferible estimar el modelo con efectos fijos o con modelos aleatorios. En este test se asume que el modelo de efectos fijos es consistente para los parámetros verdaderos, y el modelo de efectos aleatorios es una especificación eficiente de los efectos individuales bajo el supuesto de que son aleatorios y siguen una distribución normal. Se supone que el modelo de efectos fijos calcula siempre el estimador consistente, mientras que el modelo de efectos aleatorios calculará el estimador que es consistente y eficiente bajo \\(H_0\\). El comando para realizar esta prueba es phtest, que es parte del paquete plm. phtest(fe, re) ## ## Hausman Test ## ## data: voto ~ poder_pais ## chisq = 36, df = 1, p-value = 2e-09 ## alternative hypothesis: one model is inconsistent Bajo la especificación actual, nuestra hipótesis inicial de que los efectos a nivel individual son modelados adecuadamente con un modelo de efectos aleatorios es claramente rechazada, con un valor p inferior al umbral de 0,05. En este caso, debemos quedarnos con el modelo de efectos fijos. Ejercicio 7B. Utilice la base de datos del capítulo de la MCO (Latin America Welfare Dataset, 1960-2014, de Evelyne Huber y John D. Stephens) para estimar un modelo de efectos fijos y otro de efectos aleatorios en el que su variable dependiente sea el índice de Gini (gini_slc) A continuación, realice un test de especificación de Hausman. 7.5 Testeando raíces unitarias Muchas series temporales de datos políticos exhiben un comportamiento con tendencias temporales, es decir, un comportamiento no estacionario en la media. Sin embargo, la existencia no estacionalidad no siempre se testea. Esto no es un detalle menor, ya que la no estacionalidad puede conducir fácilmente a regresiones espurias, y hay serias posibilidades de obtener resultados falsamente significativos. Dos procedimientos comunes de eliminación de tendencias temporales, también llamado de “desestacionalización” son las transformaciones de las variables a primeras diferencias y la inclusión como control de tendencias temporales. Un modelo de datos de panel dinámico es aquel que contiene (al menos) una variable dependiente con un lag (una variable desfazada en el tiempo, por ejemplo en t-1) y un modelo de primeras diferencias es aquel en el que tanto las variables dependientes como las independientes se expresan como \\(\\Delta\\), es decir, como \\(X_t-(X_{t-1})\\). En cierto sentido, una variable dependiente desfazada introduce el efecto del pasado en el modelo. Después de incluirlo, la variable dependiente es influenciada no sólo por el valor actual de la variable independiente (\\(X_t\\)), sino también por los valores de la variable independiente en el pasado, (\\(X_{t-1}\\), \\(X_{t-2}\\), etc.). Exploremos nuestros datos y testeemos las raíces unitarias en nuestra especificación de efectos fijos. fe &lt;- plm(voto ~ poder_pais, data = eeuu_latam, index = c(&quot;codigo&quot;, &quot;anio&quot;)) summary(fe) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = voto ~ poder_pais, data = eeuu_latam, index = c(&quot;codigo&quot;, ## &quot;anio&quot;)) ## ## Balanced Panel: n = 10, T = 38, N = 380 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -0.3129 -0.1332 -0.0384 0.1002 0.5908 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## poder_pais -78.8 12.5 -6.29 9.2e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 13.5 ## Residual Sum of Squares: 12.2 ## R-Squared: 0.0967 ## Adj. R-Squared: 0.0723 ## F-statistic: 39.5215 on 1 and 369 DF, p-value: 9.17e-10 Si exploramos cuán altamente correlacionadas están nuestras variables dependientes e independientes con nuestra variable de tiempo, podríamos tener una idea de cómo tendencias en el tiempo pueden sesgar nuestros hallazgos. La figura que creamos con ggcorrplot muestra que poder_pais tiene una correlación positiva muy fuerte con anio, mientras que la variable anio tiene una correlación muy negativa con la variable anio. library(ggcorrplot) corr_selected &lt;- eeuu_brasil %&gt;% select(anio, poder_pais, voto) %&gt;% # calcular la matriz de correlación y redondear a un decimal cor(use = &quot;pairwise&quot;) %&gt;% round(1) ggcorrplot(corr_selected, type = &quot;lower&quot;, lab = T, show.legend = F) Figura 3.22: Correlación con la variable de tiempo Esta es una advertencia, ya que podría ser que el poder_pais no explique el anio después de todo, lo que haría que nuestros hallazgos iniciales fueran espurios. Para diagnosticar raíces unitarias usaremos la función purtest. Hay múltiples tests disponibles, cada uno con diferentes especificaciones, por lo tanto, revisar todas sus diferencias excede el propósito de este capítulo. Utilizaremos dos, que son adecuadas para nuestro objetivo de detectar raíces unitarias en nuestras variables dependientes e independientes. Estos son los de Levin, Lin, and James Chu (2002) y Im, Pesaran, and Shin (2003). # opción del test de Levin et al. (2002) purtest(voto ~ 1, data = eeuu_latam, index = c(&quot;pais&quot;, &quot;anio&quot;), pmax = 10, test = &quot;levinlin&quot;, lags = &quot;AIC&quot;, exo = &quot;intercept&quot;) ## ## Levin-Lin-Chu Unit-Root Test (ex. var.: Individual Intercepts) ## ## data: voto ~ 1 ## z = -2, p-value = 0.03 ## alternative hypothesis: stationarity # opción del test de Im et al. (2003) purtest(voto ~ 1, data = eeuu_latam, index = c(&quot;pais&quot;, &quot;anio&quot;), pmax = 10, test = &quot;ips&quot;, lags = &quot;AIC&quot;, exo = &quot;intercept&quot;) ## ## Im-Pesaran-Shin Unit-Root Test (ex. var.: Individual ## Intercepts) ## ## data: voto ~ 1 ## Wtbar = -1, p-value = 0.1 ## alternative hypothesis: stationarity En este caso, sólo uno de los modelos, el de ips, indica la existencia de raíces unitarias. Aunque la evidencia no es absoluta, vale la pena corregir las posibles raíces unitarias en anio. Ahora, observemos nuestra variable dependiente. # test de Levin et al. (2002) purtest(poder_pais ~ 1, data = eeuu_latam, index = c(&quot;pais&quot;, &quot;anio&quot;), pmax = 10, test = &quot;levinlin&quot;, lags = c(&quot;AIC&quot;), exo = c(&quot;intercept&quot;)) ## ## Levin-Lin-Chu Unit-Root Test (ex. var.: Individual Intercepts) ## ## data: poder_pais ~ 1 ## z = -0.6, p-value = 0.3 ## alternative hypothesis: stationarity # test de Im et al. (2003) purtest(poder_pais ~ 1, data = eeuu_latam, index = c(&quot;pais&quot;, &quot;anio&quot;), pmax = 10, test = &quot;ips&quot;, lags = &quot;AIC&quot;, exo = &quot;intercept&quot;) ## ## Im-Pesaran-Shin Unit-Root Test (ex. var.: Individual ## Intercepts) ## ## data: poder_pais ~ 1 ## Wtbar = -0.4, p-value = 0.4 ## alternative hypothesis: stationarity En este caso, ambos modelos son claros al indicar las raíces unitarias en la variable de poder de los países. Intentaremos resolver el problema especificando dos modelos, uno dinámico y otro con primeras diferencias. En este capítulo no tenemos la extensión para cubrir cuál es el preferible en cada situación, por lo que asumiremos que ambos son igualmente válidos. Para especificar estos modelos, primero tenemos que aprender a crear variables rezagadas (t-1, t-2, etc.), variables prospectivas (t+1, t+2, etc.) y primeras diferencias (\\(\\Delta\\)). 7.5.1 Creación de variables rezagadas (lags) y variables prospectivas (leads) La siguiente porción de código crea variables que probablemente necesitemos. Primero, clasificamos (arrange) nuestro panel de acuerdo a nuestros Ns y Ts. Este es un paso importante para que la base de datos no confunda nuestras observaciones. Luego, las agrupamos según nuestro N, en este caso pais. Finalmente, creamos las variables. Después de la función (por ejemplo, lag) especificamos que es un rezago en relación a t-1. Para un rezago mayor, cambiaríamos el 1 por otro valor en el código. Hemos creado una de cada tipo para que sepas cómo hacerlo, pero no usaremos aquí la variable voto_lead1. eeuu_latam &lt;- eeuu_latam %&gt;% arrange(pais, anio) %&gt;% group_by(pais) %&gt;% mutate(voto_lag1 = dplyr::lag(voto, 1), voto_lead1 = dplyr::lead(voto, 1), voto_diff1 = c(NA, diff(voto))) %&gt;% ungroup() Ahora hacemos lo mismo con la variable poder_pais. Crearemos un rezago (t-1) y la diferencia (\\(Delta\\)) eeuu_latam &lt;- eeuu_latam %&gt;% arrange(pais, anio) %&gt;% group_by(pais) %&gt;% mutate(poder_pais_lag1 = dplyr::lag(poder_pais, 1), poder_pais_diff1 = c(NA, diff(poder_pais))) %&gt;% ungroup() Modelo dinámico Nuestro modelo dinámico incluirá la variable rezagada de la variable dependiente. Así, voto_lag1 es un predictor del voto en el presente. Notarán que, al incluir esta variable, la hipótesis de Amorim Neto de que cuanto mayor sea el poder del país, menor será la convergencia de los votos en la AGNU, queda sin sustento empírico. Ojalá esto sea una advertencia de que es esencial que cuando utilices datos en panel que siempre hagas este tipo de tests. fe_lag &lt;- plm(voto ~ voto_lag1 + poder_pais, data = eeuu_latam, index = c(&quot;codigo&quot;, &quot;anio&quot;)) summary(fe_lag) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = voto ~ voto_lag1 + poder_pais, data = eeuu_latam, ## index = c(&quot;codigo&quot;, &quot;anio&quot;)) ## ## Balanced Panel: n = 10, T = 37, N = 370 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -0.20668 -0.04013 -0.00833 0.02616 0.28549 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## voto_lag1 0.8912 0.0218 40.90 &lt;2e-16 *** ## poder_pais -3.0418 5.7186 -0.53 0.6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 12.3 ## Residual Sum of Squares: 1.98 ## R-Squared: 0.839 ## Adj. R-Squared: 0.834 ## F-statistic: 934.399 on 2 and 358 DF, p-value: &lt;2e-16 Modelo de primeras diferencias Lo mismo ocurre cuando se prueban los modelos con las primeras diferencias, por lo que la hipótesis del capítulo se queda sin sustento. fe_diff &lt;- plm(voto_diff1 ~ poder_pais_diff1, data = eeuu_latam, index = c(&quot;codigo&quot;, &quot;anio&quot;)) summary(fe_diff) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = voto_diff1 ~ poder_pais_diff1, data = eeuu_latam, ## index = c(&quot;codigo&quot;, &quot;anio&quot;)) ## ## Balanced Panel: n = 10, T = 37, N = 370 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -0.24860 -0.04107 -0.00426 0.03102 0.27316 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## poder_pais_diff1 12.6 15.3 0.83 0.41 ## ## Total Sum of Squares: 2.13 ## Residual Sum of Squares: 2.12 ## R-Squared: 0.00189 ## Adj. R-Squared: -0.0259 ## F-statistic: 0.68067 on 1 and 359 DF, p-value: 0.41 Por último, comparemos el modelo que no considera las raíces unitarias (fe) con el que lo corrige (fe_lag) a través de la prueba Wooldridge de errores AR(1) en los modelos de paneles de efectos fijos: pwartest(fe) ## ## Wooldridge&#39;s test for serial correlation in FE panels ## ## data: fe ## F = 4437, df1 = 1, df2 = 368, p-value &lt;2e-16 ## alternative hypothesis: serial correlation pwartest(fe_lag) ## ## Wooldridge&#39;s test for serial correlation in FE panels ## ## data: fe_lag ## F = 0.7, df1 = 1, df2 = 358, p-value = 0.4 ## alternative hypothesis: serial correlation Obviamente hemos cubierto estos contenidos sin entrar en muchos detalles o explicaciones. Uno podría escribir un libro entero sobre este capítulo. Para aprender más sobre este tema, sería genial si pudiera leer las lecturas sugeridas al principio de este capítulo, especialmente Beck y Katz (2011)36. Ejercicio 7C. Utiliza el Latin America Welfare Dataset para crear variables rezagadas en t-1 y t-10 del Índice Gini (gini_slc). Incorpore ambas variables en tu modelo y diagnostica las raíces unitarias. 7.6 Errores estándar robustos corregidos para panel 7.6.1 Errores estándar robustos Quizás has utilizado Stata en algún momento para hacer análisis de datos en panel, o tal vez uno de tus coautores quiere replicar tus resultados en Stata. Si deseas reportar errores estándar robustos equivalentes a los de la opción “robust” de Stata, usted necesita calcularlos con coeftest y definir type\"sss\", que corresponde a la misma corrección de muestra pequeña para datos de panel que hace Stata. library(lmtest) Si quisieras hacerlo para el modelo original de efectos fijos (fe) puedes con la siguiente línea de código: coeftest(fe, vcov. = function(x){vcovHC(x, type = &quot;sss&quot;)}) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## poder_pais -78.8 19.4 -4.06 6e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Y para compararlos con los errores estándar originales: summary(fe) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = voto ~ poder_pais, data = eeuu_latam, index = c(&quot;codigo&quot;, ## &quot;anio&quot;)) ## ## Balanced Panel: n = 10, T = 38, N = 380 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -0.3129 -0.1332 -0.0384 0.1002 0.5908 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## poder_pais -78.8 12.5 -6.29 9.2e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 13.5 ## Residual Sum of Squares: 12.2 ## R-Squared: 0.0967 ## Adj. R-Squared: 0.0723 ## F-statistic: 39.5215 on 1 and 369 DF, p-value: 9.17e-10 También es posible que quieras calcular los errores estándar corregidos para panel, equivalentes al comando xtpcse de Stata. Estos errores estándar se han popularizado en la ciencia política desde el clásico artículo de Beck y Katz (1995)37. En 2011 se publicó un paquete para calcularlos en R38. En el artículo, los autores explican que estos errores estándar son útiles para aquellos que trabajan con paneles de “estados o naciones”: &gt;“Los datos de series temporales transversales (TSCS) se caracterizan por tener observaciones repetidas a lo largo del tiempo, como puede ser datos de estados o naciones a lo largo de los años. Los datos TSCS típicamente sufren de correlación contemporánea entre las unidades y de heteroscedasticidad a nivel de unidad, haciendo que la inferencia de los errores estándar producidos por MCO sea incorrecta. Los errores estándar corregidos para panel (PCSE) dan cuenta de estas desviaciones en los errores y permiten una mejor inferencia de los modelos lineales estimados a partir de datos de tipo TSCS”. Para usar los errores estándar corregidos para Panel (PCSE) necesitas usar la opción vcov = \"vcovBK\" dentro de la función coeftest(). Debes fijar el argumento cluster = \"time\". coeftest(fe, vcov = vcovBK, type = &quot;HC1&quot;, cluster = &quot;time&quot;) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## poder_pais -78.8 11.3 -6.96 1.6e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Voilá! Hasta este punto, hemos cubierto las funciones básicas que necesitas tener en cuenta para analizar los datos de tu panel. Para ampliar tus conocimientos, consulta la bibliografía recomendada al principio del capítulo. Esperamos que este capítulo te haya resultado útil. Ejercicio 7D. En el modelo que estimaste en el ejercicio anterior, calcula los errores estándar corregidos para panel. E-mail: furdinez@uc.cl↩︎ Este ejemplo fue tomado de un curso de Mine Cetinkaya, su sitio web tiene más recursos útiles que vale la pena ver↩︎ En el capítulo de manejo avanzado de datos aprenderá cómo imputar los valores perdidos↩︎ Beck and Katz (2011)↩︎ Beck and Katz (1995)↩︎ Bailey and Katz (2011)↩︎ "],
["logit.html", "Capítulo 8 Modelos logísticos 8.1 Introducción 8.2 Uso de los modelos logísticos 8.3 ¿Cómo se estiman las probabilidades? 8.4 Estimación de los modelos 8.5 Creando tablas 8.6 Representación visual de los resultados 8.7 Medidas para evaluar el ajuste de los modelos", " Capítulo 8 Modelos logísticos Francisco Urdinez39 Lecturas sugeridas Agresti, A. (2007). An Introduction to Categorical Data Analysis, 2nd Ed. Hoboken: Wiley. Capítulos 3, 4 y 5 – “Generalized Linear Models”; “Logistic Regression”; “Building and Applying Logistic Regression Models.” Glasgow, G., &amp; Alvarez, R. M. (2008). Discrete Choice Methods. In J. M. Box-Steffensmeier, H. E. Brady, &amp; D. Collier (Eds.), The Oxford Handbook of Political Methodology (pp. 513–529). Oxford: Oxford University Press. Long, J. S. (1997). Regression models for categorical and limited dependent variables. Sage Publications. Capítulos 3 y 4 - “Binary Outcomes”; “Hyphotesis Testing and Goodness of Fit.” Los paquetes que necesitas instalar tidyverse (Wickham 2019b), paqueteadp (Urdinez and Cruz 2020), ggcorrplot (Kassambara 2019), margins (Leeper 2018), prediction (Leeper 2019), texreg (Leifeld 2020), jtools (Long 2020), skimr (Waring et al. 2020), pscl (Jackman et al. 2020), DescTools (Signorell 2020), broom (Robinson and Hayes 2020), plotROC (Sachs 2018), separationplot (Greenhill, Ward, and Sacks 2020). 8.1 Introducción En el capítulo anterior aprendiste a correr regresiones lineales cuando tienes variables dependientes continuas de una manera sencilla y cubriendo los paquetes más útiles disponibles en R. En este capítulo verás cómo estimar los modelos de regresión cuando tienes variables dependientes dicotómicas (también llamadas variables binarias o dummy). Estas variables asumen uno de dos valores, comúnmente 0 y 1. Al igual que en los capítulos anteriores, no trataremos aspectos sustanciales de la teoría que subyace a cada modelo, ni desglosaremos las fórmulas en detalle. Para ello, sugerimos las referencias anteriores que te ayudarán a acompañar lo que discutiremos. 8.2 Uso de los modelos logísticos Los modelos de variables dependientes dicotómicas se utilizan para estimar la probabilidad de que ocurra un evento. En nuestra base de datos, codificamos como ‘1’ a los casos en los que el evento ocurre, y ‘0’ cuando no ocurre. Por ejemplo, si mi variable fuera “países con acuerdos de libre comercio con los Estados Unidos”, Chile y México estarían codificados con un ‘1’, Argentina y Brasil con un ‘0’. Estos modelos estiman probabilidades, es decir, cuál es la probabilidad de que se observe un ‘1’ dadas ciertas características de las observaciones de nuestra muestra. Toda la discusión sobre cómo estimar la probabilidad de una muestra es muy interesante, y se puede profundizar en los capítulos 3, 4 y 5 de Agresti (2007). También es importante comprender la distinción entre probabilidad y verosimilitud (likelihood, en inglés). La probabilidad se estima a partir de una “población” de la que conocemos sus “parámetros”, mientras que la verosimilitud estima los valores de los parámetros para los que el resultado observado se ajusta mejor a ellos (ve la figura 8.1). Figura 8.1: El camino de doble sentido de la probabilidad y la verosimilitud Cuando se tiene una variable dependiente dicotómica que se quiere modelar a través de Logit, se asume que ésta tiene una distribución de Bernoulli con una probabilidad de \\(Y=1\\) que no conocemos. Así pues, estimamos nuestra probabilidad desconocida mediante la Estimación de Máxima Verosimilitud (Maximum Likelihood Estimation, en inglés), que nos dará una cierta combinación lineal de variables independientes de nuestra elección (ver la figura 8.2. Un muy buen ejercicio para entender cómo estimar un parámetro con una distribución binomial a través de Máxima Verosimilitud lo ofrece RPubs. Figura 8.2: Distribución de Bernoulli Los modelos logísticos han crecido enormemente en popularidad en la Ciencia Política. Google Scholar reporta que de la búsqueda de “logit” + “ciencias políticas” un total de 17100 referencias fueron publicadas entre 2008 y 2018. La década anterior (1998-2007) reporta 8900 artículos, y la anterior (1988-1997), 2100. La Ciencia Política ha extendido el uso de los modelos Logit, por encima de los modelos Probit, en gran medida porque el primero permite el cálculo de odds ratios. Casi todos los manuales econométricos discuten las diferencias y similitudes entre Probit y Logit, aunque a efectos prácticos de la estimación de un modelo, ambos son muy similares y reportan coeficientes casi idénticos. Por lo tanto, dado que son métodos que conducen a resultados muy similares, en este capítulo sólo utilizaremos Logit. Los modelos Logit y Probit difieren en sus funciones de enlace. Logit lleva su nombre porque la función viene dada por el logaritmo natural de las probabilidades (“log odds” \\(\\rightarrow\\) logit!). \\[ ln(odds) = ln(\\frac {p}{1 - p})\\] Al despejar los términos, podemos calcular el inverso de su función, de modo que tendremos \\[ logit^{-1}(\\alpha) = \\frac {1}{1+e^{-\\alpha}} = \\frac {e^\\alpha}{1+e^\\alpha}\\] Donde \\(\\alpha\\) es la combinación lineal de variables independientes y sus coeficientes. El inverso del Logit nos dará la probabilidad de que la variable dependiente sea igual a ‘1’ dada una cierta combinación de valores para nuestras variables independientes. Figura 8.3: La inversa de la función de Logit Si profundizas en la literatura recomendada, notarás que la función está indefinida en 0 y 1, es decir, que la probabilidad está infinitamente cerca del límite sin llegar a tocarlo. Si modeláramos las probabilidades por medio de los Mínimos Cuadrados Ordinarios, obtendríamos probabilidades mayores que 100% y menores que 0%, lo cual es conceptualmente imposible. La función sigmoide de Logit impide que esto suceda. Lo que vamos a estimar es la probabilidad de que, dada una cierta combinación de atributos (variables independientes y controles) de una cierta observación permita suponer que su variable dependiente es igual a 1. Esta probabilidad puede compararse con el valor real que ha asumido la variable dependiente y así saber qué tan bien nuestro modelo hace predicciones. Por esta razón, muchos manuales se refieren a los modelos logit como un modelo de “clasificación”, ya que lo que estimamos es la probabilidad de que cada observación sea clasificada correctamente como \\(Y= 1\\) o como \\(Y= 0\\). 8.3 ¿Cómo se estiman las probabilidades? Como mencionamos antes, los modelos probabilísticos han ganado una enorme preeminencia en la Ciencia Política en los últimos años, y es por eso que probablemente estés buscando una guía para saber qué hacer y qué no hacer cuando se tiene una variable dependiente dicotómica. Ilustraremos un paso a paso en R utilizando como ejemplo la base de datos del libro * Democracies and Dictatorships in Latin America: Emergence, Survival, and Fall* de Scott Mainwaring y Aníbal Pérez-Liñán (2013). A lo largo del libro, los autores analizan qué variables ayudan a explicar por qué y cuando se produjeron quiebres democráticos en América Latina a lo largo del siglo XX y principios del XXI. En el capítulo 4, los autores exploran qué factores explican la supervivencia de los regímenes políticos y, mientras que prueban varios modelos complejos, algunos logísticos y otros de supervivencia (los que trataremos en este libro en el próximo capítulo); ilustraremos un ejemplo muy sencillo para que nos acompañes desde tu computador paso a paso. Para cargar la base de datos del capítulo, utiliza nuestro paquete paqueteadp. Entonces, cargaremos la base de datos llamada, quiebre_democracia. library(paqueteadp) data(&quot;quiebre_democracia&quot;) Ahora, la base de datos se ha cargado en nuestra sesión R: ls() ## [1] &quot;quiebre_democracia&quot; Asumiendo que la variable dependiente tiene el valor ‘1’ si el país sufre el colapso de su régimen político democrático y ‘0’ si no, podemos hacer un par de preguntas. ¿Qué efecto tiene el hecho de tener una constitución nacional que otorga grandes poderes constitucionales al poder ejecutivo en la probabilidad de un colapso democrático? si esos poderes son pequeños, ¿la probabilidad de un colapso democrático es mayor o menor? Como argumentan los autores, estos poderes pueden ser medidos por medio de un índice creado por Shugart and Carey (1992)40 Primero que nada, quieres explorar tus datos. Queremos ver cuántos países han sufrido una quiebre democrático en la muestra y cómo se distribuye el índice de poderes del ejecutivo. ¿Recuerdas lo que se explicó en el capítulo 3.2.1.1? Empezamos cargando el tidyverse y luego usamos count(). El país con el mayor número de quiebres democráticos es Perú con 6, seguido de Argentina y Panamá, con 5 cada uno. library(tidyverse) quiebre_democracia %&gt;% filter(quiebre_democracia == 1) %&gt;% count(pais_nombre) ## # A tibble: 18 x 2 ## pais_nombre n ## &lt;chr&gt; &lt;int&gt; ## 1 Argentina 5 ## 2 Bolivia 2 ## 3 Brasil 1 ## # … with 15 more rows El primer modelo que vamos a probar tiene la variable dependiente de el quiebre democrático (quiebre_democracia) siendo predicha por el índice de Shugart and Carey (1992) de los poderes ejecutivos (poder_presid). La función glm requiere que definamos nuestros datos y el modelo, porque con la misma función para los Modelos Lineales Generalizados se puede usar Probit, Poisson y otras funciones menos comunes en Ciencia Política. model_1 &lt;- glm(quiebre_democracia ~ poder_presid, # variables dependientes e independientes separadas por ~ data = quiebre_democracia, #tus datos family = binomial(&quot;logit&quot;)) #usamos la función de enlace “logit” Como vimos en los capítulos anteriores, la función summary nos permite ver rápidamente los resultados de un modelo de regresión: summary(model_1) ## ## Call: ## glm(formula = quiebre_democracia ~ poder_presid, family = binomial(&quot;logit&quot;), ## data = quiebre_democracia) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.727 -0.295 -0.269 -0.223 2.792 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.2393 0.9638 -0.25 0.8039 ## poder_presid -0.1914 0.0648 -2.96 0.0031 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 217.84 on 643 degrees of freedom ## Residual deviance: 209.56 on 642 degrees of freedom ## (1572 observations deleted due to missingness) ## AIC: 213.6 ## ## Number of Fisher Scoring iterations: 6 El coeficiente de la variable poder_presid está asociado negativamente a la probabilidad de que ocurra un quiebre de régimen (-0.191), y es estadísticamente significativo (p=0.00312). Ahora bien, a diferencia de los modelos MCO -donde podíamos usar los coeficientes de regresión para interpretar directamente el efecto de la variable independiente sobre la variable dependiente- en el caso de las regresiones logísticas esto no es tan sencillo. Debemos transformar los coeficientes en probabilidades o chances. Dado que la función de enlace logístico es el logaritmo del cociente de chances (odds), tenemos que: \\[ln(\\frac {p}{1 - p}) = \\beta_{0} + \\beta_{1}x_{1}\\] Despejando \\(ln\\), obtenemos: \\[(\\frac {p}{1 - p}) = e^{\\beta_{0}+\\beta_{1}x_{1}}\\] Y despejando los términos, una vez más, obtenemos: \\[\\hat{p} = \\frac {e^{\\beta_{0}+\\beta_{1}x_{1}}}{1 + e^{\\beta_{0}+\\beta_{1}x_{1}}}\\] Lo que queremos, entonces, es transformar los coeficientes de la misma manera en que R los reporta en una probabilidad, con la variable dependiente asumiendo el valor ‘1’. Sabemos que la variable independiente poder_presid es un índice que a un valor más alto significa mayor concentración de poder para el ejecutivo vis a vis el legislativo, por lo tanto el coeficiente de regresión indica que a menor concentración de poder del ejecutivo, mayor probabilidad de quiebre de régimen. Los regímenes de presidentes fuertes serían más resistentes. La muestra del libro abarca 20 países de América Latina entre 1900 y 2010, y el índice oscila entre un mínimo de 5 (Haití, en varios años) y un máximo de 25 (Brasil en 1945, en años de Getúlio Vargas) (ver la figura 8.4. quiebre_democracia %&gt;% select(pais_nombre, anio, poder_presid) %&gt;% arrange(poder_presid) %&gt;% slice(1) ## # A tibble: 1 x 3 ## pais_nombre anio poder_presid ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Haití 1987 5 Consejo: Si ponemos un (-) antes de la variable dentro de la función arrange(), ordena de mayor a menor. quiebre_democracia %&gt;% select(pais_nombre, anio, poder_presid) %&gt;% arrange(-poder_presid) %&gt;% slice(1) ## # A tibble: 1 x 3 ## pais_nombre anio poder_presid ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Brasil 1945 25 Podemos ver la distribución de la variable trazando un histograma. La mayoría de las observaciones tienen valores de 16 en el índice y la variable asume una distribución normal. Haremos un histograma con ggplot2, como hemos tratado a fondo en el capítulo 3. Además, añadiremos una escala de colores para visualizar mejor los valores más frecuentes. shugart_plot&lt;- ggplot(quiebre_democracia, aes(poder_presid)) + geom_histogram(aes(fill = ..count..), binwidth = 2) + scale_x_continuous(name = &quot; Índice de Poder Presidencial&quot;) + scale_y_continuous(name = &quot;Conteo&quot;) shugart_plot Figura 8.4: Histograma del índice Shugart y Carey ¿Cómo podemos calcular cuánto cambia la probabilidad de un quiebre democrático si el nivel de concentración del poder ejecutivo “salta” de una puntuación de 5 (el mínimo observado) a una de 25 (el máximo observado), dado que no controlamos por nada más en la regresión? Para hacer este cálculo podemos sustituir los valores de nuestra última fórmula, en la que aislamos en el lado izquierdo de la fórmula un \\(\\hat{p}\\). Primero debemos calcular cuál es la probabilidad de sufrir un quiebre de régimen a un índice Shugart y Carey de 5 y a un nivel 25, respectivamente. Luego calculamos la diferencia de ambas probabilidades. Por lo tanto, tenemos para un índice de 5 que \\[\\hat{p} = \\frac {e^{(0+(-0.019*5))}}{1 + e^{(0+(-0.019*5))}}\\] Notarán que el valor correspondiente al intercepto es igual a 0 porque ese coeficiente no es estadísticamente significativo. Sabemos que para un índice Shugart y Carey de 5, después de hacer el cálculo en la fórmula anterior, la probabilidad de un quiebre es igual a 0,47 o 47%. Tip. Recomendamos que dejes de leer por un segundo y hagas el cálculo de la función en lápiz y papel. Si repetimos el proceso para un valor poder_presid de 25, la probabilidad baja al 38%. Con las probabilidades podemos calcular las chances o odds, que son simplemente \\(\\frac{p}{1-p}\\). Así, el impar para un valor 5 del índice Shugart y Carey es 0.0, mientras que para un índice Shugart y Carey de 25 es 0.62. \\[odd_a = \\frac {0.47}{1 - 0.47}\\] \\[odd_b = \\frac {0.38}{1 - 0.38}\\] La utilidad de los odds es que permiten calcular los odds ratios (cuya traducción es horrible: razón de momios) ¿Cuál es el punto de calcular un odds ratio? Veamos. Cuando calculamos la probabilidad de un cambio en el índice de Shugart y Carey de 23 a 24, la magnitud de esta probabilidad será diferente a un cambio en la probabilidad dado que el índice va de, supongamos, 12 a 13. Esto se debe a que los efectos de la variable independiente sobre la probabilidad de que la variable dependiente sea =1 no son lineales (recordemos la función “S” de la Figura 8.3). Sin embargo, las razones de probabilidad tienen la propiedad útil de poder reflejar los cambios independientemente de la curvatura de la función, es decir, son cambios “constantes”. Así, podemos expresar el efecto de la variable sin tener que especificar un valor determinado para ella. Por lo tanto, en muchos artículos se verán resultados expresados como odds ratios así como en cambios de probabilidad. Ambos se complementan bien, de hecho. Veamos cómo se vería el cálculo de odds ratios al seguir el ejemplo usando la base de datos de Mainwaring y Pérez-Liñan. Dijimos que las chances odds vienen dadas por \\(\\frac{p}{1-p}\\). Por lo tanto, una razón de chances odds ratio se expresa como \\(\\frac {p_1}{1-p_1}}{\\frac {p_2}{1-p_2}\\). Supongamos que en 1992 un país tenía un índice Shugart de 15, y que en 1993 ese índice pasó a 16. ¿Cuánto cambiará la probabilidad de un quiebre democrático si se asume que todo lo demás permaneció constante? \\[ Pr(quiebre){_{pais,1992}} = \\frac {e^{(0+(-0.019*15))}}{1 + e^{((0+(-0.019*15))}} = 0.42\\] \\[ Pr(quiebre){_{pais,1993}} = \\frac {e^{(0+(-0.019*16))}}{1 + e^{(0+(-0.019*16))}} = 0.43\\] La probabilidad difiere poco, cayendo un 2,4%, lo que parece ser un efecto pequeño. El odds ratio se calcula como la proporción de ambas chances, por lo que obtenemos \\[\\frac {0.42}{0.43}=0.97\\] De esta manera, un odds ratio mayor que 1 expresa un cambio positivo, mientras que si es menor que 1 (entre 0 y 1) representa un cambio negativo en las probabilidades estimadas. Si hiciéramos el mismo ejercicio para otros valores del índice de poderes presidenciales, por ejemplo, un cambio de 3 a 4 o de 23 a 24, el odds ratio también daría 0.97. He ahí su atractivo. Ejercicio 8A. Tomate un minuto para hacer un ejercicio antes de continuar. 1. Abre la base de datos latinobarometro del paquete del libro: data (latinobarometro). Esta es una encuesta de 2018 de la opinión pública latinoamericana sobre temas políticos. 2. La variable pro_dem es 1 si la persona cree que la democracia es, a pesar de sus problemas, la mejor forma de gobierno existente. Calcula cuánto cambia la probabilidad de que esta respuesta sea 1 dependiendo de los años de educación del encuestado (educ). 3. ¿Cuál es el odds ratio de un año más de educación? ¿En qué país es mayor el efecto, en Brasil o en México? R ofrece paquetes para facilitar este análisis. Podemos graficar fácilmente los odds ratios usando ggplot2 combinado con margins y prediction. Podemos calcular las probabilidades predichas, y también podemos realizar pruebas fáciles para saber la capacidad explicativa de nuestros modelos. Utilizando la misma base de datos, te daremos un ejemplo de una rutina típica, que puedes replicar en tu computador utilizando tus propios datos. Los pasos a seguir son: a) estimar tus modelos, b) crear tablas formateadas para incluirlas en tus artículos, c) crear figuras para visualizar la magnitud de los coeficientes por medio de odds ratios, d) graficar las probabilidades predichas para las variables de interés y explorar interacciones, y e) calcular la capacidad explicativa de los modelos (porcentaje correctamente previsto, AIC, BIC, curvas ROC, puntajes de Brier o separation plots, que explicaremos más adelante). 8.4 Estimación de los modelos Para ejemplificar este paso, estimaremos dos modelos más, aparte del modelo 1. El modelo 2 tendrá como variables independientes el índice de poder presidencial de Shugart y Carey y la edad (edad_regimen) del régimen político, medida en años. Veamos la distribución de la variable edad_regimen: age_plot &lt;- ggplot(quiebre_democracia, aes(edad_regimen)) + geom_histogram(aes(fill = ..count..), binwidth = 5) + scale_x_continuous(name = &quot; Edad del régimen político &quot;) + scale_y_continuous(name = &quot;Conteo&quot;) age_plot Figura 3.9: Histograma de la edad del régimen político Veamos el modelo con esta nueva variable: model_2 &lt;- glm(quiebre_democracia ~ poder_presid + edad_regimen, data = quiebre_democracia, family = binomial(&quot;logit&quot;)) Después de correr la regresión, con summary obtenemos los coeficientes: summary(model_2) ## ## Call: ## glm(formula = quiebre_democracia ~ poder_presid + edad_regimen, ## family = binomial(&quot;logit&quot;), data = quiebre_democracia) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.734 -0.302 -0.266 -0.218 2.828 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.22220 0.96764 -0.23 0.8184 ## poder_presid -0.18897 0.06581 -2.87 0.0041 ** ## edad_regimen -0.00408 0.01961 -0.21 0.8350 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 217.84 on 643 degrees of freedom ## Residual deviance: 209.52 on 641 degrees of freedom ## (1572 observations deleted due to missingness) ## AIC: 215.5 ## ## Number of Fisher Scoring iterations: 6 El modelo 3 añade una tercera variable, calidad_democracia, que corresponde al puntaje de Freedom House de calidad de la democracia. Cuanto más alta sea la puntuación, mejor el rendimiento de la democracia de un país. Nota: Estos son modelos de ejemplo; no nos preocupa demostrar causalidad. fh_plot &lt;- ggplot(quiebre_democracia, aes(calidad_democracia)) + geom_histogram(aes(fill = ..count..), binwidth = 1) + scale_x_continuous(name = &quot;Indice de Freedom House&quot;) + scale_y_continuous(name = &quot;Conteo&quot;) fh_plot Figura 8.5: Histograma del índice de democracia de Freedom House Seguramente querrás añadir una matriz de correlación a tu análisis para diagnosticar cómo se relacionan las variables independientes antes de incluirlas en el modelo. Para ello, podemos utilizar el paquete ggcorrplot. Vemos que la correlación entre edad_regimen, calidad_democracia y poder_presid es muy baja, pero ligeramente positiva. library(ggcorrplot) corr_selected &lt;- quiebre_democracia %&gt;% dplyr::select(edad_regimen, calidad_democracia, poder_presid) %&gt;% # calculate correlation matrix and round to 1 decimal place: cor(use = &quot;pairwise&quot;) %&gt;% round(1) ggcorrplot(corr_selected, type = &quot;lower&quot;, lab = T, show.legend = F) Figura 3.12: Matriz de correlación de las variables seleccionadas El modelo 3 se verá entonces de la siguiente manera: model_3 &lt;- glm(quiebre_democracia ~ poder_presid + edad_regimen + calidad_democracia, data = quiebre_democracia, family = binomial(&quot;logit&quot;)) summary(model_3) ## ## Call: ## glm(formula = quiebre_democracia ~ poder_presid + edad_regimen + ## calidad_democracia, family = binomial(&quot;logit&quot;), data = quiebre_democracia) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7864 -0.0008 -0.0001 0.0000 1.8940 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 15.360 6.584 2.33 0.020 * ## poder_presid -0.217 0.166 -1.31 0.189 ## edad_regimen 0.166 0.112 1.48 0.138 ## calidad_democracia -3.748 1.505 -2.49 0.013 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 71.271 on 421 degrees of freedom ## Residual deviance: 12.113 on 418 degrees of freedom ## (1794 observations deleted due to missingness) ## AIC: 20.11 ## ## Number of Fisher Scoring iterations: 12 Ahora la variable poder_presid ya no es estadísticamente significativa, sin embargo, ambas calidad_democracia y el intercepto son significativas. 8.5 Creando tablas Aquí veremos cómo crear tablas editables para artículos académicos usando el paquete texreg de Philip Leifeld. Básicamente, la función contiene tres paquetes, uno para exportar tablas a html (htmlreg()), otro para exportar tablas a  y un tercero para verlas en RStudio (llamado screenreg()). A través de htmlreg podemos exportar nuestras tablas formateadas en html y desde allí incorporarlas a nuestros artículos directamente. Una vez que estimamos los modelos que irán en la tabla, los agrupamos en una lista usando la función list. Esto ahorra tiempo, porque en lugar de tener que escribir el nombre de los modelos, simplemente nos referiremos a la lista mp_models: library(texreg) mp_models &lt;- list(model_1, model_2, model_3) Para exportar la tabla a html debemos definir la opción file y un nombre para el archivo html. En este ejemplo lo llamaremos “tabla_1”. Este archivo html puede ser arrastrado a tu Word para insertar las tablas directamente en el artículo. En el ejemplo, usaremos dos opciones de función para mejorar aún más la tabla. La primera es custom.model.names que permite nombrar los modelos y la segunda es custom.coef.names que permite nombrar los coeficientes. Entonces, el comando completo sería: htmlreg(mp_models, custom.model.names = c(&quot;Modelo 1&quot;, &quot;Modelo 2&quot;, &quot;Modelo 3&quot;), custom.coef.names = c(&quot;Intercepto&quot;, &quot;Poder presidencial (Shugart &amp; Carey)&quot;, &quot;Edad del régimen&quot;, &quot;Democracia (Freedom House&quot;), file=&quot;tabla_1.html&quot;) # nombre de su archivo html. Se guardará en tu #directorio de trabajo por defecto. Esta línea crea un archivo html en tu carpeta de proyecto. Si haces un clic derecho en el archivo, puedes abrirlo en Word o en cualquier procesador de texto. Antes de exportarlo, también podemos ver una versión en pantalla de la tabla usando screenreg. En este caso, veamos cómo se ve sin la opción custom.coef.names. Puedes usar este comando en tus proyectos de trabajo, y una vez que te decidas por la mejor tabla, la exportas con htmlreg. screenreg(mp_models, custom.model.names = c(&quot;Modelo 1&quot;, &quot;Modelo 2&quot;, &quot;Modelo 3&quot;), custom.coef.names = c(&quot;Intercepto&quot;, &quot; Poder presidencial (Shugart &amp; Carey)&quot;, &quot;Edad del régimen&quot;, &quot;Democracia (Freedom House)&quot;) ) ## ## ======================================================================= ## Modelo 1 Modelo 2 Modelo 3 ## ----------------------------------------------------------------------- ## Intercepto -0.24 -0.22 15.36 * ## (0.96) (0.97) (6.58) ## Poder presidencial (Shugart &amp; Carey) -0.19 ** -0.19 ** -0.22 ## (0.06) (0.07) (0.17) ## Edad del régimen -0.00 0.17 ## (0.02) (0.11) ## Democracia (Freedom House) -3.75 * ## (1.51) ## ----------------------------------------------------------------------- ## AIC 213.56 215.52 20.11 ## BIC 222.50 228.92 36.29 ## Log Likelihood -104.78 -104.76 -6.06 ## Deviance 209.56 209.52 12.11 ## Num. obs. 644 644 422 ## ======================================================================= ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Noten que poder_presid deja de ser estadísticamente significativa cuando controlamos por calidad_democracia en el Modelo 3. En este modelo, calidad_democracia se convierte en la única variable estadísticamente significativa. Además, noten que el número de observaciones disminuye significativamente al incluir la variable calidad_democracia, lo que dificulta la comparación de los modelos. La función skimr:skim()será muy útil para detectar valores perdidos, como podemos ver en la Figura 8.6. Las tres variables tienen un número muy alto de valores faltantes, para calidad_democracia de casi \\(2/3\\) del total de las observaciones (1436 de 2216).41 Figura 8.6: Skim de nuestra base de datos En el muy recomendable libro The Chicago Guide to Writing about Multivariate Analysis, Jane Miller (2013) pone mucho énfasis en la importancia de entender la diferencia entre la significancia estadística y la significancia sustantiva de los efectos cuando se interpretan las regresiones: no porque sea una variable estadísticamente significativa la magnitud del efecto será la esperada, ni significa que el hallazgo sea científicamente relevante. ¿Cuáles son algunas buenas opciones para comprender la magnitud de nuestros efectos? En MCO se suelen estandarizar los coeficientes. En regresiones logísiticas, los odds ratio y graficar las probabilidades predichas son buenas opciones. Para utilizar los odds ratios queremos mostrarte cómo puedes cambiar los coeficientes que obtienes directamente de la tabla, que son log odds, y reemplazarlos por odds ratios. Para ello, puedes usar los argumentos override.coef,override.se y override.pvalues de screenreg(). Los odds ratios, como vimos antes, son simplemente los coeficientes exponenciados. El cálculo de sus errores estándar y los valores p, mientras tanto, es un poco más complejo: necesitamos acceder a la matriz de varianza-covarianza del modelo. Afortunadamente esto ya lo ha hecho Andrew Heiss, a quien agradecemos por compartir la función. Adaptamos ligeramente estas funciones, que pueden ser introducidas fácilmente en el análisis siempre que se haya cargado el paquete de libro, paqueteadp. screenreg(model_3, custom.model.names = &quot;Modelo 3 - Odds Ratios&quot;, override.coef = exp(coef(model_3)), # la siguiente función, odds_*, están en el paquete del libro override.se = odds_se(model_3), override.pvalues = odds_pvalues(model_3), # además, omitiremos el coeficiente del intercepto omit.coef = &quot;Inter&quot;) ## ## ========================================== ## Modelo 3 - Odds Ratios ## ------------------------------------------ ## poder_presid 0.80 *** ## (0.13) ## edad_regimen 1.18 *** ## (0.13) ## calidad_democracia 0.02 ## (0.04) ## ------------------------------------------ ## AIC 20.11 ## BIC 36.29 ## Log Likelihood -6.06 ## Deviance 12.11 ## Num. obs. 422 ## ========================================== ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Al obtener una tabla como la que acabamos de crear, nos encontramos con dos retos que abordaremos en las próximas subsecciones: El primero es saber si la magnitud de los efectos es sustantiva desde el punto de vista científico. Por ejemplo, ¿qué diríamos si la variable calidad_democracia es estadísticamente significativa y sin embargo encontramos que si un país pasa de la peor puntuación (0) a la mejor puntuación (12) en calidad_democracia, la probabilidad de un quiebre democrático cae sólo en un 0,03%? Probablemente diríamos que, a pesar de tener un efecto estadísticamente significativo, nuestra variable no tiene un efecto sustantivo. El segundo desafío que enfrentaremos es comparar nuestros modelos para decidir cuál es el que mejor se ajusta, es decir, cuál tiene la mejor capacidad explicativa. A diferencia de MCO, no tenemos \\(R^2\\), por lo que se utilizan otras medidas. ¡Veamos! Ejercicio 8B. Usando la base de datos del Latinobarometro, escoge tres variables que creas que pueden predecir pro_dem e interpreta el modelo con summary. Si te atreves, crea tablas con texreg. Las variables disponibles son: edad_regimen (edad del encuestado), ideol (donde 1 es la extrema izquierda y 10 la extrema derecha), educ (años de educación del encuestado) y socioecon_status (1, muy bueno - 5, muy malo). 8.6 Representación visual de los resultados Podemos representar visualmente la tabla que hicimos con texreg usando los paquetes de prediction y margins. Si has usado Stata en el pasado, este comando será familiar. Para visualizar las magnitudes, una figura es mucho más intuitiva que una tabla. La figura 8.7 muestra la probabilidad predicha de un quiebre democrático según el índice de poderes presidenciales. library(margins) library(prediction) predict_model_2 &lt;- prediction::prediction( model_2, at = list(poder_presid = unique(model.frame(model_2)$poder_presid)) ) summary(predict_model_2) ## # A tibble: 15 x 7 ## `at(poder_presid)` Prediction SE z p lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 0.227 0.117 1.95 0.0512 -0.00119 0.456 ## 2 8 0.143 0.0587 2.43 0.0149 0.0279 0.258 ## 3 9 0.121 0.0449 2.71 0.00681 0.0335 0.209 ## # … with 12 more rows El resumen nos ofrece un tibble donde tenemos la probabilidad prevista de nuestra variable dependiente para cada valor observado de poder_presid y su significado estadístico. Esto se convierte fácilmente en una cifra, y para ello hay dos alternativas que recomendamos. #option 1 figure_op_1 &lt;- ggplot(summary(predict_model_2), aes(x = `at(poder_presid)`, y = Prediction, ymin = lower, ymax = upper, group = 1)) + geom_line() + geom_errorbar(width = 0.2) + theme(axis.text.x = element_text(angle = 90)) + labs(x = &quot; Poder presidencial&quot;, y = &quot;Pr(quiebre democrático)&quot;) figure_op_1 Figura 8.7: Opción 1. Modelo 2 basado en Mainwaring y Pérez Liñan (2013), probabilidad predicha de una ruptura democrática #option 2 cdat &lt;- cplot(model_2, &quot;poder_presid&quot;, what = &quot;prediction&quot;, main = &quot;Pr(quiebre democrático)&quot;, draw = F) ## xvals yvals upper lower ## 1 5.0 0.23 0.46 -0.0017 ## 2 5.7 0.20 0.40 0.0066 ## 3 6.4 0.18 0.35 0.0141 ## 4 7.1 0.16 0.31 0.0207 ## 5 7.8 0.15 0.27 0.0263 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 15 rows ] ggplot(cdat, aes(x = xvals)) + geom_line(aes(y = yvals)) + geom_line(aes(y = upper), linetype = 2)+ geom_line(aes(y = lower), linetype = 2) + geom_hline(yintercept = 0) + labs(title = &quot;Pr. Quiebre democrático&quot;, x = &quot;Poder presidencial&quot;, y = &quot;Prob. predicha&quot;) Figura 8.8: Opción 2. Modelo 2 basado en Mainwaring y Pérez Liñán (2013), probabilidad predicha de una ruptura democrática Ambas opciones conducen al mismo resultado, el formato de la figura puede ser personalizado a tu preferencia con la sintaxis de ggplot2. ¿Qué dirías sobre este efecto en términos de importancia científica? Si tuviéramos que interpretar la magnitud del índice de poderes presidenciales sobre la probabilidad de un quiebre de régimen, probablemente diríamos que su caída produce un efecto considerable en los países que se encuentran entre los valores 6 y 10 del índice, pero para los valores superiores a 10 el aumento del índice casi no afecta a la probabilidad de un quiebre. De hecho, para los países con un índice de poder presidencial superior a 10, la probabilidad de un quiebre democrático es muy baja. ¿Cuál es el efecto medio del aumento de una unidad de la variable independiente en la probabilidad de que se produzca la variable dependiente? El Efecto Marginal Promedio (AME, por sus siglas en inglés) se utiliza para ver estos efectos, y se logra con el comando plot del paquete de margins. marginal_ef &lt;- margins(model_2) plot(marginal_ef, labels = c(&quot;Edad del régimen&quot;,&quot;índice de poder presidencial&quot;), ylab = &quot;AME&quot;) Figura 8.9: Efecto marginal medio del Modelo 2 basado en Mainwaring y Pérez Liñán (2013) También podemos estar interesados, no en el efecto promedio, sino en el efecto marginal completo de una variable. Dada la no linealidad de estos modelos, el efecto marginal de una variable sobre la probabilidad de ocurrencia de la variable dependiente no es constante ni es significativo en toda la variable. Para ello utilizamos la función cplot del paquete margins y luego lo personalizamos con las opciones de ggplot2. marginal_shugart &lt;- cplot(model_2, &quot;poder_presid&quot;, what = &quot;effect&quot;, main = &quot;Ef.Marg(Quiebre)&quot;, draw = F) ggplot(marginal_shugart, aes(x = xvals)) + geom_line(aes(y = yvals)) + geom_line(aes(y = upper), linetype = 2)+ geom_line(aes(y = lower), linetype = 2) + geom_hline(yintercept = 0) + labs(title = &quot;Modelo 2&quot;, x = &quot;Indice de poder presidencial&quot;, y = &quot;Efecto marginal&quot;) Figura 8.10: Efecto marginal de poder_presid en el modelo 2 en Mainwaring y Perez Liñan (2013) También podemos graficar los coeficientes como odds ratios: recuerda que los odds ratios menores que 1 son efectos negativos y mayores que 1 son positivos. El coeficiente se expresa como su valor medio y su intervalo de confianza del 95%. Si el coeficiente es estadísticamente significativo, su intervalo de confianza no pasará de la línea en 1. Si, por el contrario, no son significativos, el efecto cruzará la línea. Comparemos los tres modelos: Para graficar los odds ratios usamos el paquete jtools de nuevo, esta vez con su función plot_summs. Como la opción exp=T es verdadera, estamos exponiendo los coeficientes. Si hiciéramos exp=F, veríamos las probabilidades logarítmicas de la tabla. # Es probable que para ejecutar este comando sea necesario #instalar estos dos paquetes antes. #install.packages(&quot;ggstance&quot;) #install.packages(&quot;huxtable&quot;) library(jtools) name_models &lt;- c(&quot;Modelo 1&quot;, &quot;Modelo 2&quot;, &quot;Modelo 3&quot;) odds_ratios &lt;- plot_summs(model_1, model_2, model_3, exp=T, scale = F, inner_ci_level = .9, coefs = c(&quot;Indice de poder presidencial&quot; = &quot;poder_presid&quot;, &quot;Edad del regímen&quot; = &quot;edad_regimen&quot;, &quot;Indice de democracia&quot; = &quot;calidad_democracia&quot;), model.names = name_models) odds_ratios + labs(x = &quot;Coeficientes exponenciados&quot;, y = NULL) Figura 8.11: Odds ratios del Modelo 3, basado en Mainwaring y Pérez Liñán (2013) Cada vez es más común encontrar este tipo de figuras en lugar de tablas, y personalmente creemos que son preferibles. Un precursor en la disciplina del uso de figuras e infografías fue Edward Tufte, quien fue mencionado en el Capítulo 3. Sin embargo, las ciencias políticas no habían prestado mucha atención a la presentación de los resultados a través de las figuras hasta hace unos dos decenios. Hoy en día, la tendencia en la disciplina es evitar el uso de tablas cuando no son esenciales. Paquetes como margins y jtools han facilitado esta tarea. Veamos las probabilidades predichas para la puntuación de democracia según Freedom House: observamos que la relación entre la probabilidad de que ocurra un colapso democrático y la puntuación de calidad democrática es negativa. Cuando un país tiene un puntaje de 2.5 en el índice, la probabilidad de un quiebre democrática es del 100%. Cuando la puntuación es de 5, la probabilidad es casi nula. Por lo tanto, un pequeño cambio en este índice afectará enormemente la probabilidad de un quiebre de régimen. cdat &lt;- cplot(model_3, &quot;calidad_democracia&quot;, what = &quot;prediction&quot;, main = &quot;Pr(Quiebre)&quot;) ## xvals yvals upper lower ## 1 2.0 1.00 1.0 0.99 ## 2 2.4 1.00 1.0 0.97 ## 3 2.8 0.98 1.1 0.89 ## 4 3.2 0.90 1.2 0.64 ## 5 3.7 0.65 1.1 0.19 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 15 rows ] ggplot(cdat, aes(x = xvals)) + geom_line(aes(y = yvals)) + geom_line(aes(y = upper), linetype = 2)+ geom_line(aes(y = lower), linetype = 2) + geom_hline(yintercept = 0) + labs(title = &quot; Probabilidad de quiebre democrático&quot;, x = &quot;Indice de Freedom House&quot;, y = &quot;Prob. predicha&quot;) Figura 8.12: Probabilidades predichas de FH en el Modelo 3, basadas en Mainwaring y Pérez Liñán (2013) Ejercicio 8C. Con las regresiones que has corrido usando los datos del Latinobarometro, crea tres gráficos para visualizar los efectos, ya sean probabilidades predichas, efectos marginales o coeficientes expresados como odds ratios. ¿Sus hallazgos tienen una significancia sustantiva? 8.6.1 Visualización de las interacciones entre las variables Este tema puede ser útil si ya se tienes algún conocimiento sobre lo que es una variable interactiva y para qué se utilizan. Si no es así, y no te interesa saberlo, puedes pasar a la siguiente sección sobre el ajuste de los modelos sin problemas. ¿Cómo se vería una hipótesis interactiva en los modelos logísticos? Supongamos que después de meses de explorar las variables políticas que predicen la ocurrencia de un quiebre de régimen en América Latina (edad_regimen, poder_presid y calidad_democracia), decidimos explorar si las variables económicas pueden tener poder explicativo. Después de estudiar extensamente la literatura existente, se formula la hipótesis de que la desigualdad económica en la sociedad (que mediremos con el índice de Gini como hicimos en el capítulo de MCO) afecta positivamente la probabilidad de un quiebre democrático. Para ello seguimos utilizando la base de datos quiebre_democracia, ahora con las variables económicas (crecim_10a, x_miner_petrol y gini). Construyamos un nuevo modelo, esta vez incorporando el índice de Gini: model_4 &lt;- glm(quiebre_democracia ~ gini, data = quiebre_democracia, family = binomial(&quot;logit&quot;)) summary(model_4) ## ## Call: ## glm(formula = quiebre_democracia ~ gini, family = binomial(&quot;logit&quot;), ## data = quiebre_democracia) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.334 -0.253 -0.226 -0.204 2.898 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.2591 2.4405 -0.52 0.61 ## gini -0.0460 0.0488 -0.94 0.35 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 110.28 on 442 degrees of freedom ## Residual deviance: 109.40 on 441 degrees of freedom ## (1773 observations deleted due to missingness) ## AIC: 113.4 ## ## Number of Fisher Scoring iterations: 6 Pero eso no es todo, porque también formulamos una hipótesis interactiva, que sostiene que el efecto de la desigualdad económica en la sociedad sobre la probabilidad de un quiebre democrático dependerá de cuánto haya crecido la “torta” para a distribuida (es decir, del crecimiento de la economía reciente). Por lo tanto, interactuaremos gini con crecim_10a que mide el crecimiento económico en los diez años anteriores al quiebre democrático. También añadiremos algunos controles, que no merecen demasiada discusión. model_5 &lt;- glm(quiebre_democracia ~ crecim_10a * gini + x_miner_petrol + poder_presid, data = quiebre_democracia, family = binomial(&quot;logit&quot;)) summary(model_5) ## ## Call: ## glm(formula = quiebre_democracia ~ crecim_10a * gini + x_miner_petrol + ## poder_presid, family = binomial(&quot;logit&quot;), data = quiebre_democracia) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.581 -0.226 -0.178 -0.143 3.009 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 6.5797 4.8787 1.35 0.177 ## crecim_10a -256.1668 128.7769 -1.99 0.047 * ## gini -0.1371 0.0788 -1.74 0.082 . ## x_miner_petrol -1.6673 4.7359 -0.35 0.725 ## [ reached getOption(&quot;max.print&quot;) -- omitted 2 rows ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 94.80 on 425 degrees of freedom ## Residual deviance: 87.54 on 420 degrees of freedom ## (1790 observations deleted due to missingness) ## AIC: 99.54 ## ## Number of Fisher Scoring iterations: 7 Como pueden ver, la interacción de crecim_10a*gini es estadísticamente significativa. ¿Cómo interpretamos este coeficiente? Allí vamos: La necesidad de incluir variables interactivas para probar las hipótesis condicionales en los modelos binarios ha sido objeto de un gran debate académico en los últimos años. A diferencia de los modelos lineales estimados a partir de MCO, donde hay consenso sobre esta necesidad (en parte a partir de Brambor, Clark, and Golder 2006). En el MCO, debido a la construcción del propio estimador y al supuesto de linealidad en los parámetros, los efectos de las variables independientes sobre la variable dependiente son siempre efectos constantes e independientes del valor tomado por las otras covariables del modelo. Los modelos logísticos, por otra parte, son interactivos por naturaleza: el efecto de una variable independiente sobre la probabilidad de que ocurra la variable dependiente depende de los valores de las otras covariables del modelo, como se ejemplifica con el Modelo 1 al principio del capítulo. Hay dos opiniones al respecto en la ciencia política: Según Berry, DeMeritt, and Esarey (2010), la inclusión o no de un término interactivo en una regresión logística dependerá en gran medida de la hipótesis o teoría que prediga el efecto de la interacción. En particular, dependerá de si la hipótesis predice un efecto sobre la variable dependiente latente, o si se trata de la probabilidad de que ocurra el evento. Si la hipótesis se refiere al primer escenario, entonces no hay más discusión en la literatura, ya que la situación es análoga al caso de una variable dependiente continúa estimada con una regresión MCO. Por lo tanto, es necesario incorporar un término de multiplicación con su respectivo coeficiente para evaluar la interacción. Rainey (2016), por su parte, sostiene que incluso cuando la hipótesis de interacción sobre la probabilidad de ocurrencia de la variable dependiente se basa en la noción de compresión, es necesario incorporar términos de interacción en los modelos. La recomendación de Rainey (2016) es incorporar siempre términos de interacción porque si no incluyen el modelo de regresión logística, no puede representar situaciones razonables que serían inconsistentes con la teoría interactiva. Aunque este debate sigue abierto, en este capítulo recomendamos que siempre se incluyan las interacciones si se tienen razones teóricas para hacerlo. A efectos prácticos, el paquete de margins que utilizaremos para visualizar este tipo de efectos interactivos requiere este término. ¿Cómo interpretamos este coeficiente? Los resultados muestran que la desigualdad de la sociedad aumenta la probabilidad de un quiebre democrático sólo cuando la economía ha crecido en los últimos diez años. También confirmamos que en las recesiones económicas la probabilidad de un quiebre democrático aumenta fuertemente pero sólo si la sociedad tiene un índice de Gini de menos de 50 puntos, es decir, en sociedades más equitativas. Para graficar estos hallazgos utilizamos la función persp() del paquete de margins de nuestro modelo que tiene la interacción. persp(model_5, &quot;crecim_10a&quot;, &quot;gini&quot;, what = &quot;prediction&quot;, ylab = &quot;Desigualdad&quot;, xlab = &quot;Crecimiento económico&quot;, zlab = &quot;Probabilidad predicha&quot; ) Figura 8.13: Interacción entre el coeficiente de Gini y el crecimiento del PBI en tres dimensiones Una forma alternativa de ver los resultados de la interacción es en dos dimensiones. Para ello utilizamos la función image. Ambas son equivalentes, dependerá de nuestra preferencia estética. image(model_5, &quot;crecim_10a&quot;, &quot;gini&quot;, xlab = &quot;Crecimiento económico&quot;, ylab = &quot;Desigualdad&quot;, zlab = &quot;Probabilidad predicha&quot;) Figura 8.14: Interacción entre el coeficiente de Gini y el crecimiento del PIB en dos dimensiones 8.7 Medidas para evaluar el ajuste de los modelos Una vez que se haya analizado la importancia sustantiva de los modelos mediante tablas y trazando las probabilidades previstas y los efectos marginales, podría interesarte el ajuste de los modelos. Al igual que en la MCO, cuando se utilizan \\(R^2\\) y el error medio de la raíz cuadrada, hay una serie de estadísticas diseñadas para saber cuál de los modelos logísticos tiene el mejor ajuste. 8.7.0.1 Pseudo-\\(R^2\\) Muchas veces, verás que las regresiones logísticas van acompañadas de Pseudo-\\(R^2\\). No es nuestra medida favorita para los modelos logísticos, pero se usa habitualmente. Para entender cómo se interpreta el Pseudo-\\(R^2\\) (normalmente se usa el de McFadden) es importante entender cómo se diferencia de un \\(R^2\\) en las regresiones lineales (puedes usar este enlace para acompañar su interpretación en el capítulo de MCO). La fórmula, en este caso es \\(psR^2 = 1-\\frac{ln\\hat{L}(Modelo completo)}{ln\\hat{L}(Modelo solo con intercepto)}\\) Donde \\(\\hat{L}\\) es la probabilidad estimada por el modelo. Básicamente, lo que hace la fórmula es comparar el modelo con todas nuestras covariables con el modelo que apenas tiene el intercepto, para ver cuánto mejora la capacidad explicativa del mismo si incluimos variables independientes. Dado que \\(L\\) oscila entre 0 y 1, su logaritmo es menor o igual a 0. Por lo tanto, cuanto menor sea la relación, mayor será la diferencia entre el modelo elegido y el modelo que sólo tiene intercepto. La función se toma del paquete pscl. Los modelos 1 y 2 tienen casi la misma capacidad explicativa, aunque se ha incorporado una variable más en el segundo modelo. En cambio, al añadir calidad_democracia al modelo 3, nuestro Pseudo-\\(R^2\\) salta a 0,96. library(pscl) pR2(model_1)[[&quot;McFadden&quot;]] ## fitting null model for pseudo-r2 ## [1] 0.038 pR2(model_2)[[&quot;McFadden&quot;]] ## fitting null model for pseudo-r2 ## [1] 0.038 pR2(model_3)[[&quot;McFadden&quot;]] ## fitting null model for pseudo-r2 ## [1] 0.83 También se podría implementar un \\(pseudo-R^2\\) ajustado. Ajustado significa que es una versión que penaliza la cantidad de covariables. Al igual que en el \\(R^2\\) ajustado, añadimos a la fórmula el término \\(c\\), que es la cantidad de covariables, y por lo tanto tenemos \\(Pseudo-R^2=1-\\frac{ln\\hat{L}(Modelo completo)-c}{ln\\hat{L}(Modelo solo con intercepto)}\\) Aunque el \\(R^2\\) siempre aumenta cuando se añaden covariables al modelo, si utilizamos medidas ajustadas, tendremos en cuenta este aspecto. library(DescTools) PseudoR2(model_1, c(&quot;McFadden&quot;)) ## McFadden ## 0.43 PseudoR2(model_2, c(&quot;McFadden&quot;)) ## McFadden ## 0.43 PseudoR2(model_3, c(&quot;McFadden&quot;)) ## McFadden ## 0.97 8.7.0.2 AIC Casi siempre, las tablas con modelos de regresión logística informan sobre el Criterio de Información de Akaike (AIC, por sus siglas en inglés). Éstos permiten la comparación de diferentes modelos ejecutados para la misma muestra y decidir cuál tiene un mejor poder explicativo. El AIC, al igual que el \\(Pseudo-R^2\\), utiliza información de \\(ln(\\hat{L})\\). Lo que hace la AIC es medir la “distancia” que existe entre los parámetros verdaderos y los estimadores del modelo, mediante una distancia matemática llamada divergencia Kullback-Leibler. Cuanto más pequeña sea esta distancia, mejor será el modelo. Es muy útil cuando se comparan diferentes modelos para argumentar cuál debe tomarse como referencia, y se calcula como \\(AIC = 2p-2ln(\\hat {L})\\) \\(p\\) es el número de regresores incluyendo la intercepción, y \\(hat{L}\\) es la probabilidad estimada por el modelo. De los tres modelos, claramente el que mejor se ajusta a este criterio es el tercero. AIC(model_1) ## [1] 214 AIC(model_2) ## [1] 216 AIC(model_3) ## [1] 20 8.7.0.3 BIC El Criterio de Información Bayesiana (BIC, por sus siglas en inglés), al igual que el AIC, es un criterio para comparar los modelos según su ajuste. A efectos prácticos, y para no entrar en las diferencias entre el AIC y el BIC, es importante saber que el BIC penaliza la complejidad del modelo más rigurosamente que el AIC, ya que su fórmula es \\(BIC=ln(n)p-2ln(\\hat {L})\\) donde se añade a la fórmula \\(n\\), que es el número de observaciones en la muestra. Los resultados son similares a los que obtuvimos a través de la AIC, es decir, el tercer modelo muestra el mejor ajuste. BIC(model_1) ## [1] 222 BIC(model_2) ## [1] 229 BIC(model_3) ## [1] 36 A efectos prácticos, se recomienda utilizar el AIC o el BIC, pero no es necesario informar de ambos en un cuadro con varios modelos. 8.7.0.4 Porcentaje de predicciones correctas Tal vez, un nombre mejor para ello sería “Porcentaje de clasificaciones correctas”. Este es uno de nuestros diagnósticos favoritos por la bondad de encajar en los modelos Logit, ya que es el más intuitivo de todos. Para entender el porcentaje de predicciones correctas en un modelo, es importante tener claro que un modelo produce cuatro combinaciones posibles para cada observación: Figura 8.15: Tabla de clasificación a partir de la cual se calcula el porcentaje de predicciones correctas Cualquier observación se clasificará como “correcta” si corresponde al cuadro superior izquierdo (verdadero positivo) o al inferior derecho (verdadero negativo). El porcentaje de observaciones que pertenecen a estos dos recuadros determina el porcentaje de predicciones correctas en el modelo. Como criterio estándar, si la probabilidad estimada de una observación es mayor o igual al 50%, se estima que es una probabilidad positiva, y si es menor del 50%, será una probabilidad negativa. Para calcular las predicciones correctas, necesitamos hacer varios pasos, pero si utilizas los comandos que te ofrecemos a continuación, podrás hacerlo con tu propia base de datos cuando trabajes solo. En primer lugar, necesitamos usar el paquete broom, que transforma los modelos que hemos guardado en bases de datos en formato tidy, como vimos en el Capítulo 5. Calcularemos los valores predichos del modelo 3 como un ejemplo que puede ser repetido para los otros modelos. Lo primero que necesitamos son los valores predichos, es decir, la probabilidad asignada a cada observación de que Y=1. En el orden que obtuvimos usando broom esta variable se llama .fitted. library(broom) pred_model_3 &lt;- augment(model_3, type.predict = &quot;response&quot;) pred_model_3 ## # A tibble: 422 x 12 ## .rownames quiebre_democra… poder_presid edad_regimen calidad_democra… ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 75 0 19 1 8 ## 2 76 0 19 2 8 ## 3 77 1 19 3 3 ## # … with 419 more rows, and 7 more variables La función dplyr nos permite transformar nuestra nueva base obtenida a través de broom. Una de las funciones básicas de dplyr es select, que nos permite elegir las variables por sus nombres. Necesitamos identificar los valores predichos y la variable dependiente (en este caso quiebre_democracia) para comparar la probabilidad asignada por el modelo con el valor real de la variable. La función mutate de dplyr nos permitirá crear una variable binaria para saber si el modelo ha predicho correctamente cada observación. El punto de corte es 0,5, es decir, si la probabilidad estimada es igual o superior a 0,5, se considera que el modelo ha predicho la ocurrencia del evento, y si es inferior a ese valor, se considera que ha predicho la no ocurrencia del evento. Trabajar con valores de corte arbitrarios tiene desventajas, que como veremos a continuación se resuelven con los ROC. Finalmente, creamos una variable que llamamos pcp (porcentaje de correctamente predicho) que muestra la proporción de verdaderos positivos y verdaderos negativos estimados por el modelo 3. El resultado muestra que de las 422 observaciones del modelo 3, el 99,5% se han predicho correctamente. pred_model_3 %&gt;% dplyr::select(.rownames, quiebre_democracia, .fitted) %&gt;% mutate(predict_binary = if_else(.fitted &gt;= 0.5, 1, 0), predict_binary = if_else(quiebre_democracia == predict_binary, 1, 0)) %&gt;% summarize(pcp = mean(predict_binary)) ## # A tibble: 1 x 1 ## pcp ## &lt;dbl&gt; ## 1 0.995 8.7.0.5 Puntaje Brier Esta es otra medida de ajuste, menos frecuente en Ciencias Políticas pero muy útil. Cuanto más se acerque el puntaje Brier a 0, mejor será el ajuste del modelo. En general, no se utilizan todas las medidas que estamos revisando (AIC, BIC, Brier, etc.), sino que se eligen dos o tres que sean de su agrado. Pensamos que las situaciones en las que se quiere “castigar” las predicciones erróneas es una alternativa ideal ya que su fórmula viene dada por \\[BS=\\frac{1}{N}\\sum(\\hat{p}-x)^2\\] donde \\(N\\) es el número de observaciones, \\(hat{p}\\) es la probabilidad predicha para cada observación, y \\(x\\) es el valor real de la observación en nuestra base de datos. La puntuación es el promedio de todas las observaciones de la muestra. ¿Cuál de los tres modelos tiene la puntuación más baja? BrierScore(model_1) ## [1] 0.038 BrierScore(model_2) ## [1] 0.038 BrierScore(model_3) ## [1] 0.0048 8.7.0.6 ROC Plot Otra de nuestras medidas favoritas por su versatilidad es el ROC. Comparadas con el porcentaje de observaciones correctamente predichas, las curvas ROC tienen la ventaja de no definir un límite arbitrario a partir del cual se pueda decidir si la observación ha sido clasificada correcta o incorrectamente. Su desventaja es que se trata de una figura adicional que debe incluirse en el documento, que tal vez prefieras añadir a un apéndice. Para interpretar estas figuras, lo que más importa es el área debajo de la curva diagonal que cruza la figura. Cuanto mayor sea el área bajo la curva, mejor será el ajuste del modelo. Si quieres leer más sobre ello, el área forma una puntuación llamada puntuación AUC, por sus siglas en inglés (que significa Area Bajo la Curva). Construyámosla con la función geom_roc() del paquete plotROC. Lo que haremos primero es crear una base de datos con los resultados de los tres modelos de regresión. Llamaremos a la base pred_models. library(plotROC) library(broom) pred_models &lt;- bind_rows(augment(model_1, response.type = &quot;pred&quot;) %&gt;% mutate(model = &quot;Modelo 1&quot;), augment(model_2, response.type = &quot;pred&quot;) %&gt;% mutate(model = &quot;Modelo 2&quot;), augment(model_3, response.type = &quot;pred&quot;) %&gt;% mutate(model = &quot;Modelo 3&quot;)) Una vez creado el objeto con la información de los tres modelos, procedemos a crear la figura usando ggplot2, que aprendiste en el capítulo 3. En el eje vertical tenemos la sensibilidad del modelo, mientras que en el eje horizontal tenemos (1-especificidad) del modelo. La sensibilidad es la relación entre los verdaderos positivos (es decir, las observaciones predichas como “1”, que en realidad eran “1” en la base de datos), y la suma de los verdaderos positivos más los falsos negativos (los predichos como “0”, que en realidad eran “1”). La especificidad es la relación entre los verdaderos negativos (las observaciones predichas como “0” que eran “0” en la base de datos) y la suma de los falsos positivos (las observaciones predichas como “1” que realmente eran “0”) sumados a los verdaderos negativos. roc &lt;- ggplot(pred_models, aes(d = quiebre_democracia, m = .fitted, color = model)) + geom_roc(n.cuts = 0) + geom_abline(slope = 1) + labs(x = &quot;1 - especificidad&quot;, y = &quot;Sensibilidad&quot;) roc Figura 8.16: ROCs de nuestros modelos Para obtener el AUC, usamos la función calc_auc encima del objeto que llamamos roc. calc_auc(roc) ## PANEL group AUC ## 1 1 1 0.64 ## 2 1 2 0.64 ## 3 1 3 1.00 Ejercicio 8D. 1. Usa la base de datos latinobarometro del paquete del libro: data (latinobarometro). 2. La variablepro_dem asume 1 si la persona cree que la democracia es, a pesar de sus problemas, la mejor forma de gobierno existente. Dependiendo de los años de educación del encuestado educ, calcula cuánto cambia la probabilidad de que esta respuesta sea 1. 3. Estima un modelo que prediga, lo mejor que puedas, la variable dependiente. 4. 4. Traza la curva ROC del modelo. 8.7.0.7 Plots de separación Los separation plots fueron propuestos por tres politólogos en 2011 (Greenhill, Ward, and Sacks 2011) y se refiere a la idea que mencionamos al principio del capítulo. Esta idea es que los modelos logísticos son modelos de clasificación. El plot de separación mezcla la utilidad tanto de los porcentajes correctamente predichos como de los ROC. Son cifras que nos permiten ver la información en una tabla de clasificación, pero donde también podemos identificar qué observaciones son las peor predichas. ¿Cómo interpretamos la figura? Si el modelo predijera perfectamente nuestra variable dependiente, todas las observaciones en amarillo estarían en el lado izquierdo, y todas las observaciones en rojo en el lado derecho, ya que en el eje horizontal de la figura tenemos la probabilidad predicha para cada observación. library(separationplot) separationplot( pred = predict.glm(model_1, type = &quot;response&quot;), actual = as.vector(model_1$y), type = &quot;line&quot;, show.expected = T, newplot = F, line = F, heading = &quot;Model 1&quot; ) separationplot( pred = predict.glm(model_3, type = &quot;response&quot;), actual = as.vector(model_3$y), type = &quot;line&quot;, show.expected = T, newplot = F, line = F, heading = &quot;Model 3&quot; ) Figura 8.17: Separation plots de modelos 1 y 3 La ventaja del plot de separación es que cada observación se incluye en la figura en forma de una línea vertical que será amarilla si la observación asume quiebre_democracia = 0 y roja si quiebre_democracia = 1. Fíjate en cómo usamos el argumento type = \"line\" en las opciones, de modo que cada observación se incluye como una línea. Una crítica que se puede hacer a esta hermosa herramienta visual es que cuanto mayor es el número de observaciones en nuestra muestra, más difícil se hace interpretar la figura. Si las observaciones fueran demasiadas, sería aconsejable utilizar el argumento type = \"band\", que facilita la lectura para los modelos con muestras muy grandes. El pequeño triángulo negro que vemos debajo de la figura es el punto desde el que deberían comenzar las observaciones rojas si estuvieran perfectamente clasificadas de las amarillas. Si observas el plot de separación del modelo 1, verás que hay líneas rojas en el lado izquierdo de la figura. Estas son observaciones para las cuales quiebre_democracia = 1 y sin embargo se ha estimado que tienen una probabilidad muy baja de un desglose democrático. El modelo 1, sólo con una variable independiente (poder_presid) predice bastante mal la realidad de los quiebres democráticos en América Latina. Notarán que el modelo 3 ha calificado las observaciones mucho mejor que el modelo 1. De hecho, el modelo 3, con sólo 3 variables independientes, ha sido capaz de clasificar la variable dependiente casi perfectamente. Ejercicio 8E. - Añade dos variables independientes al modelo 3 que revisamos en el capítulo e interpreta los coeficientes como odds ratios. - Gráfica estos coeficientes usando ggplot2. - Diagnostica el ajuste del modelo con un ROC y un plot de separación. E-mail: furdinez@uc.cl↩︎ En el capítulo 15 ejemplificaremos cómo crear índices en R↩︎ Esto podría ser problemático, pero a veces no hay nada que podemos hacer con los datos que faltan. Sin embargo, cubrimos algunas soluciones posibles en el capítulo 11.↩︎ "],
["surv.html", "Capítulo 9 Modelos de supervivencia 9.1 Introducción 9.2 ¿Cómo interpretamos las tasas de riesgo? 9.3 El modelo Cox de riesgos proporcionales 9.4 Estimación de los modelos de Cox en R 9.5 Modelos de supervivencia e interpretación sus coeficientes como Hazard Ratios.", " Capítulo 9 Modelos de supervivencia Francisco Urdinez42 Lecturas sugeridas Allison, P. D. (2014). Event history and survival analysis (2nd ed.). Thousand Oaks: SAGE. Box-Steffensmeier, J. M. &amp; Jones, B. S. (2004). Event history modeling: A guide for social scientists. Cambridge: Cambridge University Press. Broström, G. (2018). Event history analysis with R. Boca Raton: CRC Press. Golub, J. (2008). Discrete Choice Methods. In J. M. Box-Steffensmeier, H. E. Brady, &amp; D. Collier (Eds.), The Oxford Handbook of Political Methodology (pp. 530–546). Oxford: Oxford University Press. Los paquetes que necesitas instalar tidyverse (Wickham 2019b), paqueteadp (Urdinez and Cruz 2020), skimr (Waring et al. 2020), countrycode (Arel-Bundock 2020), ggalt (Rudis, Bolker, and Schulz 2017), survival (Therneau 2020), survminer (Kassambara, Kosinski, and Biecek 2020), texreg (Leifeld 2020). 9.1 Introducción Hay una serie de cuestiones recurrentes en relación con el análisis de datos políticos que aún no hemos abordado. En muchas ocasiones estamos interesados en saber por qué ciertos eventos duran lo que duran, o por qué tardan más que otros en ocurrir. ¿Por qué la paz es tan duradera en ciertos países, mientras que otros están en guerra constante? ¿Cuál era la posibilidad de que se produjeran disturbios sociales en Venezuela en 2018? ¿Por qué algunos legisladores permanecen en el cargo durante muchos períodos consecutivos, mientras que otros ni siquiera son reelegidos una vez? ¿Cuánto tiempo tarda un sindicato en hacer huelga durante una crisis económica? Todas estas preguntas son sobre la duración de un evento. El momento de ocurrencia de un evento es parte de la respuesta que buscamos, necesitamos de un modelo que nos permita llegar a esta respuesta. (2004), una de las principales referencias en Ciencia Política de este método, se refiere a ellos como “modelos de eventos históricos” aunque buena parte de la literatura los llama modelos de supervivencia o modelos de duración. Si bien en la Ciencia Política no son modelos tan utilizados como uno creería (en el fondo, casi todas las preguntas que nos hacemos pueden ser reformuladas en una pregunta sobre la duración del evento), las ciencias médicas han explorado estos métodos en profundidad, y muchas las referencias que uno encuentra en R sobre paquetes accesorios a estos modelos son de departamentos bioestadísticos y médicos. De allí que “modelos de supervivencia” sea el nombre más frecuentemente utilizado para estos modelos, ya que en medicina comenzó a utilizárselos para modelar qué variables afectaban la sobrevida de sus pacientes enfermos. Podemos tener dos tipos de bases de datos para estos problemas. Por un lado, podemos tener una base en formato de panel en el que para un momento dado nuestra variable dependiente codifica si el evento ha ocurrido (=1) o no (=0). Así, por ejemplo, podemos tener una muestra de veinte países para cincuenta años (1965-2015) en los que nuestra variable de interés es si el país ha implementado una reforma constitucional. La variable independiente asumirá el valor 1 para el año 1994 en Argentina, pero será 0 para el resto de los años en este país. Por otro lado, podemos tener una base de datos transversal en la que cada observación aparece codificada apenas una vez. En este caso necesitamos, además de la variable que nos dirá si en el periodo de interés el evento ocurrió o no para cada observación (por ejemplo, Argentina debería ser codificada como “1”), una variable extra que codifique el tiempo de “supervivencia” de cada observación, es decir, cuánto tiempo pasó hasta que finalmente el evento sucedió. Para el caso de Argentina, esta variable codificará 29 (años), que es lo que demoró en implementarse una reforma constitucional desde 1965. La elección del año de partida, como podrá sospechar, es decisión del investigador, pero tiene un efecto enorme sobre nuestros resultados. Además, muchas veces la fecha de inicio acaba determinada por la disponibilidad de datos y se alejan del ideal que quisiéramos modelar. Supongamos que nos hacemos la pregunta que se hizo David Altman (2019) “¿Por qué algunos países demoran menos que otros en implementar instancias de democracia directa?”. Para ello tenemos una base de datos en formato de panel que parte del año 1900 y que llega a 2016 para 202 países (algunas observaciones, como la Unión Soviética se transforman en otras observaciones a partir de un determinado año en que dejan de existir). Al observar sus datos uno nota algo que probablemente también te suceda en tu base de datos. Para el año 2016 apenas un pequeño porcentaje de países había implementado este tipo de mecanismos (27% para ser más precisos) pero la base está censurada ya que a partir de ese año no sabemos que ha ocurrido con los países que aún no habían implementado mecanismos de democracia directa. No todas las observaciones han “muerto” aún, ¿cómo saber cuándo lo harán? Ésta es una pregunta válida, que podremos responder con este tipo de modelos, ya que podemos calcular el tiempo que demorará cada uno de los países censurados en nuestra muestra (con la información que le damos al modelo, que siempre es incompleta). En nuestra base de datos tendremos, al menos, cuatro tipos de observaciones (ver figura 9.1): (a) aquellas que, para el momento en que tenemos datos ya estaban en la muestra, aunque no siempre sabremos hace cuanto que “existen”. Son, en la figura, las observaciones B y C. En la base de datos de Altman, por ejemplo, México ya existía como entidad política en 1900, cuando su base de datos parte (sabemos que la Primera República Federal existió como entidad política desde octubre de 1824, por lo que México sería codificado como existente a partir de esa fecha). También sabemos que en 2012, por primera vez, México implementó una iniciativa de democracia directa, lo que define como positiva la ocurrencia del evento que nos interesa medir. Así, México sería como la observación B de la figura; (b) Algunas observaciones estarán desde el comienzo de la muestra, y existirán hasta el último momento sin haber registrado el evento de interés. Tal es el caso, de la observación C en la figura. En la muestra de Altman un ejemplo sería Argentina, que desde 1900 está registrado en la base (ya había “nacido”), y hasta el último año de la muestra no había registrado instancias de democracia directa (no “murió”), lo que la transforma en una observación censurada. Por razones prácticas, no cambia saber qué ocurrió a partir del año en que nuestra base termina. Por ejemplo, en la figura, nuestra base cubre hast \\(t_7\\), y sabemos que en \\(t_8\\) la observación C aún no había muerto, y la observación D lo había hecho en \\(t_8\\). En nuestra base, C y D serán ambas observaciones censuradas en \\(t_7\\); (c) Algunas observaciones pueden entrar “tarde” en la muestra, como es el caso de las observaciones A y D. Por ejemplo, Eslovenia entra a la muestra de Altman en 1991, que es cuando se independiza de Yugoslavia y “nace” como país; (d) Algunas observaciones, independientemente de cuando entren a la muestra, “moriran” durante el periodo analizado. Por ejemplo, A y B mueren dentro del periodo que hemos medido entrte \\(t_1\\) y \\(t_7\\). Ya para la observación D, no registramos su muerte. Hay un caso no considerado en el ejemplo, de observaciones que nacen y mueren sucesivamente a lo largo del periodo de estudio. Para ellas, deberemos decidir si las tratamos como observaciones independientes, o si modelamos la posibilidad de morir más de una vez. Si es así, la probabilidad de morir por segunda vez deberá estar condicionada por la probabilidad de haber muerto (y cuando!) por primera vez. Este es un tipo de caso algo más complejo que no cubriremos en este capítulo. Figura 9.1: Ejemplo de observaciones presentes en una base de datos de supervivencia 9.2 ¿Cómo interpretamos las tasas de riesgo? Los modelos de supervivencia se interpretan a partir de la probabilidad de que en un momento dado el evento de interés ocurra siendo que no ha ocurrido aún. Esta probabilidad recibe el nombre de tasa de riesgo o Hazard rate. Partimos sabiendo que tenemos una variable, que llamaremos \\(T\\), y que representa un valor aleatorio positivo y que tiene una distribución de probabilidades (correspondiente a la probabilidad del evento ocurrir en cada uno de los momentos posibles) que llamaremos \\(f(t)\\). Esta probabilidad se puede expresar de manera acumulada, como una densidad acumulada \\(F(t)\\). Como se expresa en la siguiente formula, \\(F(t)\\) viene dada por la probabilidad de que el tiempo de supervivencia \\(T\\) sea menor o igual a un tiempo específico \\(t\\) : \\[F(t)=\\int\\limits_0^t f(u)d(u)=Pr(T)\\leq t)\\] La función de supervivencia \\(\\hat S(t)\\), que es un concepto clave en estos modelos, está relacionado con \\(F(t)\\), ya que \\[\\hat S(t)= 1-F(t)=Pr(T\\geq t)\\] En otras palabras, la función de supervivencia es la probabilidad inversa de \\(F(t)\\), pues dice respecto a la probabilidad de que el tiempo de supervivencia \\(T\\) sea mayor o igual un tiempo \\(t\\) de interés. Para el ejemplo concreto de Altman, uno podría preguntarse cuál es la probabilidad de que un país no implemente un mecanismo de democracia directa (lo que sería equivalente a “sobrevivir” a dicha implementación) siendo que ya ha sobrevivido a los mismos por 30 años. A medida que más y más países en la muestra van implementando iniciativas de democracia directa, la probabilidad de supervivencia va disminuyendo. Los coeficientes de los modelos de supervivencia se suelen interpretar como una tasa de riesgo, que es el cociente de la probabilidad de que el evento suceda, y su función de supervivencia \\[h(t)=\\frac{f(t)}{S(t)}\\] Así, la tasa de riesgo indica la tasa a la que las observaciones “mueren” en nuestra muestra en el momento \\(t\\), considerando que la observación ha sobrevivido hasta el momento \\(t\\). Veremos más adelante cómo en el ejemplo de Altman podemos interpretar los coeficientes de nuestras regresiones como tasas de riesgo. En definitiva, la tasa de riesgo \\(h(t)\\) es el riesgo de que el evento ocurra en un intervalo de tiempo determinado, que viene dado por \\[f(t)=\\lim_{\\bigtriangleup x \\to 0} \\frac {P(t+\\bigtriangleup t &gt; T \\geq t)}{\\bigtriangleup t}\\] 9.3 El modelo Cox de riesgos proporcionales Hay dos tipos de modelos de supervivencia, los llamados modelos paramétricos y los llamados semi-parametricos. Los primeros son aquellos que hacen supuestos sobre las características de la población a la que la muestra pertenece. En este caso, los supuestos son sobre el riesgo de base o “baseline hazard”, es decir, sobre el riesgo de que el evento ocurra cuando todas nuestras variables independientes sean iguales a cero. El tipo de modelo de supervivencia más común para esta categoría es el modelo de Weibull. Por otro lado, los modelos semi-parametricos no hacen ningún tipo de supuestos sobre la función de base, ya que ésta es estimada a partir de los datos. El ejemplo más famoso de esta especificación es la del modelo de Cox. El Oxford Handbook sobre metodología política dedica un capítulo entero a discutir modelos de supervivencia, y en él se toma una posición fuerte en favor de los modelos semi-parametricos. Aquí seguiremos dicha recomendación ya que las ventajas son varias. Por un lado, como no se hacen presupustos sobre la función del riesgo de base, su estimación es mucho más precisa. En una estimación paramétrica, elegir un “baseline hazard” equivocado siginificará que todo nuestro trabajo analítico estará sesgado. La decisión de la forma que adopta la curva de base en un modelo de Weibull debería estar orientado por razones teóricas de cuál es el efecto de nuestra variable independiente sobre la probabilidad de supervivencia de la observación (ver figura 9.2). Sin embargo, no siempre nuestra teoría define tales presupuestos. Elegir una especificación por Cox nos ahorra tomar una decisión tan costosa. Figura 9.2: Los diferentes peligros de la línea de base en el modelo de Weibull Una segunda ventaja de los modelos semi-parametricos sobre los paramétricos tiene que ver con el presupuesto de riesgos proporcionales. Ambos, modelos paramétricos y semi-parametricos, asumen que los riesgos entre dos individuos cualquiera de la muestra se mantienen constantes a lo largo de todo su periodo de supervivencia. Es decir, se asume que la curva de riesgo de cada individuo sigue la misma curva en el tiempo. Este es un presupuesto fuerte para trabajos en ciencia política, ya que las observaciones cambian en el tiempo y se diferencian unas de otras. Piénsense en el trabajo de Altman, por ejemplo. Uno puede teorizar que la probabilidad de una iniciativa de democracia directa en un determinado año en un determinado país estará afectada por el nivel de solidez de las instituciones democráticas, que podemos medir con algún tipo de variable estándar como los 21 puntos de Polity IV o la más reciente medición de V-Dem. Podemos, entonces, teorizar que a mayor solidez institucional mayor probabilidad de implementar mecanismos de democracia directa. Sin embargo, los valores de estas variables no solo difieren ente países, sino que a lo largo del tiempo estas variables cambian mucho para un mismo país. Piénsese en Colombia, por ejemplo, en que la variable de V-Dem “vdem” sufrió avances y retrocesos entre 1900 y 2016 (ver figura 3). Cada vez que el valor de esta variable cambia, necesariamente cambia la tasa de riesgo de democracia directa para Colombia, rompiendo el presupuesto de proporcionalidad de los riesgos. Figura 9.3: Valores de poliarquía según V-Dem La ventaja del modelo de Cox sobre sus contrapartes paramétricas es que existen tests para saber si alguna variable de nuestro modelo rompe el presupuesto de proporcionalidad de los riesgos, y de esa forma podremos corregirlo generando interacciones entre estas variables y variables temporales. De esta forma, permitimos que en nuestro modelo haya dos tipos de coeficientes: coeficientes constantes en el tiempo, y coeficientes cambiantes en el tiempo. Por ejemplo, podemos imaginar que, ante un aumento brusco en la calidad de las instituciones democráticas de un país, la tasa de riesgo de implementar democracia directa se dispare y que dicho efecto de desvanezca en el lapso de cuatro o cinco años. Cuando definas tu modelo, es importante que reflexiones sobre qué variables puede asumirse que permanezcan constantes en los riesgos y cuáles no. La recomendación dada por el Oxford Handbook para una buena implementación de modelos de supervivencia es la siguiente: (a) Primero, dada las ventajas de los modelos semi-paramétricos sobre los paramétricos, se recomienda el uso de Cox por sobre Weibull u otro modelo paramétrico. (b) Una vez que hemos definido nuestra variable dependiente (el evento), el tiempo de “nacimiento” y de “muerte” de cada observación, podemos especificar nuestro modelo. (c) Los coeficientes deben ser interpretados en tasas de riesgo (hazard rates), lo que exige exponenciar los coeficientes brutos que obtenemos en R. (d) Una vez que tenemos el modelo que creemos correcto, en función de nuestras intuiciones teóricas, es necesario testear que ninguno de los coeficientes viole el presupuesto de proporcionalidad de los riesgos. Para ello ejecutamos un test de Grambsch y Therneau, o mediante el análisis de los residuos de Schoenfeld. (e) Una vez identificados los coeficientes problemáticos, permitimos que estos interactúen con el logaritmo natural de la variable que mide la duración del evento. De esta forma, permitimos que haya coeficientes cuyo efecto se desvanece o se potencia con el tiempo. Una vez corregidos los coeficientes problemáticos, podemos si, proceder a interpretar nuestro modelo y la función de supervivencia del modelo. 9.4 Estimación de los modelos de Cox en R Volvamos, entonces, a la pregunta que se hizo David Altman en el capítulo 3 de [Citizenship and Contemporary Direct Democracy (2019), “Catching on: waves of adoption of citizen-initiated mechanisms of direct democracy since World War I”. La pregunta aquí es: ¿Por qué algunos países demoran menos que otros en implementar instancias de democracia directa? Comencemos por cargar el tidyverse y nuestra base (esta última, desde nuestro paquete paqueteadp): library(tidyverse) library(survival) library(paqueteadp) data(&quot;democracia_directa&quot;) Ahora la base se ha cargado en nuestra sesión de R: ls() ## [1] &quot;democracia_directa&quot; Las variables que tenemos en la base son las siguientes: Variable dependiente, que registra la ocurrencia del evento, que en este caso es la adopción de un mecanismo de democracia directa - dem_directa Año - anio Nombre del país - pais_nombre El país sufre un proceso de rápida democratización - dem_rapida_positiva El país sufre un proceso rápido de deterioro de la democracia - dem_rapida_negativa Memoria de instancias previas de democracia directa - memoria Score de democracia del país - vdem Efecto de la difusion de capacidades - difusion_cap Efecto de la difusión de ocurrencias - difusion_ocurr Logaritmo natural de la población total del país - log_poblacion Dummy para ex colonias británicas - colonia_gb Dummy para ex miembros de la URSS - colonia_urss A lo largo del ejemplo usaremos los paquetes skimr, countrycode, survival, rms, survminer, ggalt, tidyversey texreg, pero los iremos cargando uno a uno para que veas para que sirven. Si utilizamos skim, como ya hicimos en otros capítulos, podemos ver que es una base en formato de panel balanceado. Es decir, tenemos una variable “país” (pais_nombre), que se repite a lo largo de una variable “año” (anio). Figura 9.4: Skim de nuestra base de datos Los países \"“entran” a la base cuando comienzan a existir como países independientes. Veamos el caso de Albania, por ejemplo, que nace como país en 1912 luego de las Guerras los Balcanes: democracia_directa %&gt;% filter(pais_nombre == &quot;Albania&quot;) ## # A tibble: 105 x 23 ## pais_nombre anio dem_directa dem_rapida_posi… dem_rapida_nega… memoria ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 1912 0 0 0 0 ## 2 Albania 1913 0 0 0 0 ## 3 Albania 1914 0 0 0 0 ## # … with 102 more rows, and 17 more variables For the correct functioning of the models in R, countries should exit the analysis (and the dataset!) when they “die”. In this case, death occurs when countries adopt mechanisms of direct democracy. Albania, following the example, should stop existing in 1998, and do not last until 2016 as it does in the dataset. Thus, we need to create a second version of our dataset where this has already been corrected: Note: If your dataset is in this format since the beginning, you can skip this step. Para que los modelos funcionen correctamente en R, los países deberían salir del análisis (¡y de la base!) cuando “mueren”. En este caso la muerte se da cuando los países adoptan mecanismos de democracia directa. Albania, siguiendo el ejemplo, debería dejar de existir en 1998, y no perdurar en la base hasta 2016 como sucede ahora. Entonces crearemos una segunda versión de nuestra base de datos donde esto ya ha sido corregido: Figura 9.5: Cómo debe verse tu base de datos democracia_directa_b &lt;- democracia_directa %&gt;% group_by(pais_nombre) %&gt;% # pedimos que la suma acumulada del dummy es como máximo 1 filter(cumsum(dem_directa) &lt;= 1) %&gt;% ungroup() Lo que estamos haciendo es filtrar los datos para que cuando ocurra eñ evento de interés (en este caso es la variable dem_directa) lo que viene después en el tiempo es reemplazado por NAs. Si comparamos el caso de Albania para la base original y para la base actual veremos la diferencia. En la primera base de datos tenemos observaciones para 1999 y 2000, después de que ocurriera el evento. En la segunda base, Albania “muere” en 1998 y allí se cortan los datos para el país: democracia_directa %&gt;% filter(pais_nombre == &quot;Albania&quot; &amp; dem_directa == 1) ## # A tibble: 19 x 23 ## pais_nombre anio dem_directa dem_rapida_posi… dem_rapida_nega… memoria ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 1998 1 0 0 1 ## 2 Albania 1999 1 0 0 1 ## 3 Albania 2000 1 0 0 1 ## # … with 16 more rows, and 17 more variables democracia_directa_b %&gt;% filter(pais_nombre == &quot;Albania&quot; &amp; dem_directa == 1) ## # A tibble: 1 x 23 ## pais_nombre anio dem_directa dem_rapida_posi… dem_rapida_nega… memoria ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 1998 1 0 0 1 ## # … with 17 more variables En resumen, ahora tenemos un panel desbalanceado, en el que los países entran a la base cuando comienzan a existir como tales y salen, o bien cuando adoptan mecanismos de democracia directa, o bien cuando la base termina en su extensión temporal (en 2016). De esta forma nuestra base se acerca mucho a la figura 9.1 con la que ejemplificamos los distintos tipos de observaciones. Ejercicio 9A. Tómese un minuto para hacer un ejercicio antes de continuar. En el capítulo anterior utilizamos la base de datos Democracies and Dictatorships in Latin America: Emergence, Survival, and Fall (Mainwaring and Pérez-Liñán 2013). Utiliza la base de datos para ver si está lista para ser usada con modelos de supervivencia o si necesitas transformarla. Qué tal si probamos hacer algo similar a la figura 1 pero con los datos de David Altman? Este tipo de figuras se llaman gráficos de Gantt, y pueden recrearse con ggplot2 aunque es justo decir que hay que seguir unos cuantos pasos, y puede ser algo difícil. Ojalá que con este ejemplo puedas recrearlo con tus propios datos porque una figura así es de mucha utilidad para el lector. Primero debemos crear una base de datos que, para cada país, registre el año de entrada y el año de salida. También nos interesa por qué sale el país: ¿adopta democracia directa o la base termina? Vamos a crear un subonjunto que llamaremos gantt_plot_df donde nos quedamos solamente con tres variables de la base, que son el nombre del país pais_nombre, el año anio, y la variable dependiente dem_directa. También quitaremos de la base aquellas observaciones que para el primer año de la base ya han “muerto”. Por ejemplo, Suiza había implementado mecanismos de democracia directa mucho antes que 1900, así que desde el primer año de la base hasta el último la variable dependiente será “1”: gantt_chart_df &lt;- democracia_directa_b %&gt;% # las variables que nos interesan dplyr::select(pais_nombre, anio, dem_directa) %&gt;% group_by(pais_nombre) %&gt;% filter(anio == min(anio) | anio == max(anio)) %&gt;% # necesitamos eliminar las observaciones para los países que &quot;nacen&quot; con #democracia directa ya implementada: filter(!(anio == min(anio) &amp; dem_directa == 1)) %&gt;% summarize(anio_enters = min(anio), anio_exits = max(anio), exits_bc_dd = max(dem_directa)) %&gt;% ungroup() gantt_chart_df ## # A tibble: 193 x 4 ## pais_nombre anio_enters anio_exits exits_bc_dd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afganistán 1919 2016 0 ## 2 Albania 1912 1998 1 ## 3 Andorra 1900 2016 0 ## # … with 190 more rows Los países que salen debido a la democracia directa (“mueren”) son: gantt_chart_df %&gt;% filter(exits_bc_dd == 1) ## # A tibble: 48 x 4 ## pais_nombre anio_enters anio_exits exits_bc_dd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 1912 1998 1 ## 2 Belarús 1991 1995 1 ## 3 Belice 1981 2008 1 ## # … with 45 more rows Podemos identificar en una nueva variable la región geopolítica de cada país, gracias a la función countrycode::countrycode() (Esto lo explicamos en detalle en el Capítulo 11). Este paquete es de gran utilidad para quienes hacen política comparada o relaciones internacionales porque facilita mucho darle códigos a los países. Lo que nos permite el paquete es asignar a cada país su región de pertenencia de manera casi automática: library(countrycode) gantt_chart_df_region &lt;- gantt_chart_df %&gt;% mutate(region = countrycode(pais_nombre, origin = &quot;country.name&quot;, dest = &quot;region&quot;)) ## Warning in countrycode(pais_nombre, origin = &quot;country.name&quot;, dest = &quot;region&quot;): Some values were not matched unambiguously: Afganistán, Arabia Saudita, Argelia, Azerbaiyán, Bahrein, Belarús, Bélgica, Belice, Bhután, Brasil, Camboya, Camerún, Canadá, Chipre, Comoras, Corea del Norte, Corea del Sur, Costa de Marfil, Croacia, Dinamarca, Egipto, Emiratos Árabes Unidos, Eslovaquia, España, Estados Unidos, Etiopía, Filipinas, Francia, Gabón, Gran Bretaña, Granada, Grecia, Haití, Hungría, Irán, Irlanda, Islandia, Islas Salomón, Italia, Japón, Kazajstán, Kirguistán, Letonia, Líbano, Libia, Lituania, Lituania 2, Malasia, Maldivas, Malí, Marruecos, Mauricio, México, Mónaco, Noruega, Nueva Zelandia, Omán, Países Bajos, Pakistán, Panamá, Papua Nueva Guinea, Perú, Polonia, República Centroafricana, República Checa, República Dominicana, Rusia, San Vicente y las Granadinas, Santa Lucía, Singapur, Siria, Suazilandia, Sudáfrica, Sudán, Sudán del Sur, Suecia, Tailandia, Taiwán, Tayikistán, Timor Oriental, Túnez, Turquía, Ucrania ## Warning in countrycode(pais_nombre, origin = &quot;country.name&quot;, dest = &quot;region&quot;): Some strings were matched more than once, and therefore set to &lt;NA&gt; in the result: Papua Nueva Guinea,Sub-Saharan Africa,East Asia &amp; Pacific gantt_chart_df_region ## # A tibble: 193 x 5 ## pais_nombre anio_enters anio_exits exits_bc_dd region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afganistán 1919 2016 0 &lt;NA&gt; ## 2 Albania 1912 1998 1 Europe &amp; Central Asia ## 3 Andorra 1900 2016 0 Europe &amp; Central Asia ## # … with 190 more rows Como dice el warning, algunos países no fueron encontrados. Estos son los países que countrycode no pudo encontrar, seguramente porque los países están escritos de otra manera: gantt_chart_df_region %&gt;% filter(is.na(region)) ## # A tibble: 83 x 5 ## pais_nombre anio_enters anio_exits exits_bc_dd region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afganistán 1919 2016 0 &lt;NA&gt; ## 2 Arabia Saudita 1946 2016 0 &lt;NA&gt; ## 3 Argelia 1962 2016 0 &lt;NA&gt; ## # … with 80 more rows Podemos corregir esto a mano ex-post o correr countrycode::countrycode() de nuevo, pero con el argumento custom_match: gantt_chart_df_region &lt;- gantt_chart_df %&gt;% mutate(region = countrycode( pais_nombre, origin = &quot;country.name&quot;, dest = &quot;region&quot;, custom_match = c(&quot;Korea, North&quot; = &quot;Eastern Asia&quot;, &quot;Kosovo&quot; = &quot;Eastern Europe&quot;, &quot;South Yemen&quot; = &quot;Western Asia&quot;, &quot;Vietnam, Democratic Republic of&quot; = &quot;South-Eastern Asia&quot;, &quot;Vietnam, Republic of&quot; = &quot;South-Eastern Asia&quot;))) ## Warning in countrycode(pais_nombre, origin = &quot;country.name&quot;, dest = &quot;region&quot;, : Some values were not matched unambiguously: Afganistán, Arabia Saudita, Argelia, Azerbaiyán, Bahrein, Belarús, Bélgica, Belice, Bhután, Brasil, Camboya, Camerún, Canadá, Chipre, Comoras, Corea del Norte, Corea del Sur, Costa de Marfil, Croacia, Dinamarca, Egipto, Emiratos Árabes Unidos, Eslovaquia, España, Estados Unidos, Etiopía, Filipinas, Francia, Gabón, Gran Bretaña, Granada, Grecia, Haití, Hungría, Irán, Irlanda, Islandia, Islas Salomón, Italia, Japón, Kazajstán, Kirguistán, Letonia, Líbano, Libia, Lituania, Lituania 2, Malasia, Maldivas, Malí, Marruecos, Mauricio, México, Mónaco, Noruega, Nueva Zelandia, Omán, Países Bajos, Pakistán, Panamá, Papua Nueva Guinea, Perú, Polonia, República Centroafricana, República Checa, República Dominicana, Rusia, San Vicente y las Granadinas, Santa Lucía, Singapur, Siria, Suazilandia, Sudáfrica, Sudán, Sudán del Sur, Suecia, Tailandia, Taiwán, Tayikistán, Timor Oriental, Túnez, Turquía, Ucrania ## Warning in countrycode(pais_nombre, origin = &quot;country.name&quot;, dest = &quot;region&quot;, : Some strings were matched more than once, and therefore set to &lt;NA&gt; in the result: Papua Nueva Guinea,Sub-Saharan Africa,East Asia &amp; Pacific gantt_chart_df_region ## # A tibble: 193 x 5 ## pais_nombre anio_enters anio_exits exits_bc_dd region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afganistán 1919 2016 0 &lt;NA&gt; ## 2 Albania 1912 1998 1 Europe &amp; Central Asia ## 3 Andorra 1900 2016 0 Europe &amp; Central Asia ## # … with 190 more rows Ahora no tenemos NA! Hemos logrado asignar una región a cada país de la muestra. gantt_chart_df_region %&gt;% filter(is.na(region)) ## # A tibble: 83 x 5 ## pais_nombre anio_enters anio_exits exits_bc_dd region ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afganistán 1919 2016 0 &lt;NA&gt; ## 2 Arabia Saudita 1946 2016 0 &lt;NA&gt; ## 3 Argelia 1962 2016 0 &lt;NA&gt; ## # … with 80 more rows Con nuestra base ya armada, podemos hacer el plot con facilidad, gracias a ggalt::geom_dumbbell(): library(ggalt) gantt_chart &lt;- ggplot(data = gantt_chart_df_region, mapping = aes(x = anio_enters, xend = anio_exits, y = fct_rev(pais_nombre), color = factor(exits_bc_dd))) + geom_dumbbell(size_x = 2, size_xend = 2) gantt_chart Figura 3.12: Gráfico de Gantt para todas las observaciones, versión simple En el eje vertical tenemos los países ordenados alfabéticamente, y en el eje x hay dos variables que informan al gráfico, por un lado, el comienzo de la línea (anio_enters) y por otro su fin (anio_exits). Además, hay una tercera variable informativa que es el color de la línea, que denota si el país implementó o no una instancia de democracia directa (exits_bc_dd). Los países en azul son los que implementaron dicha instanca entre 1900 y 2016. Si bien la figura es de una enorme utilidad visual, hay que reconocer que son demasiados países para incluirla en el cuerpo de un artículo. Un enfoque posible es concentrarnos en ciertas regiones. Recordarás la función filter del Capítulo 2. Filtremos Sudamérica, por ejemplo: gantt_chart_sa &lt;- ggplot(data = gantt_chart_df_region %&gt;% filter(region == &quot;Latin America &amp; Caribbean&quot;), mapping = aes(x = anio_enters, xend = anio_exits, y = fct_rev(pais_nombre), color = fct_recode(factor(exits_bc_dd)))) + geom_dumbbell(size_x = 2, size_xend = 2) gantt_chart_sa Figura 3.13: Gráfico de Gantt para Latinoamérica, versión simple Podemos agregarle los años como texto para mejorar aún más la lectura de la figura: gantt_chart_sa &lt;- gantt_chart_sa + geom_text(aes(label = anio_enters), vjust = -0.4) + geom_text(aes(x = anio_exits, label = anio_exits), vjust = -0.4) gantt_chart_sa Figura 3.14: Gráfico de Gantt para Latinoamérica con los años de inicio y fin Finalmente, algunos retoques estéticos, con todo lo que hemos aprendido en el Capítulo 3: library(ggplot2) gantt_chart_sa &lt;- gantt_chart_sa + labs(x = &quot;año&quot;, y = &quot;&quot;, title = &quot; Años de entrada y salida, América del Sur &quot;, color = &quot;¿Adoptan instancias de democracia directa?&quot;) + theme(axis.text.x = element_blank()) gantt_chart_sa Figura 3.15: Versión pulida del gráfico de Gantt para Latinoamérica Ejercicio 9B. Usando la base de datos de Mainwaring &amp; Pérez-Liñán (2013) grafica un diagrama de Gantt como el anterior, mostrando las rupturas democráticas en México. Además de gráficos de Gantt, es muy común que quien trabaja con modelos de supervivencia muestre gráficos con las curvas de supervivencia comparando dos grupos de interés. Por ejemplo, David Altman se pregunta si hubo una diferencia en el siglo XX entre países que se democratizaron rápidamente y aquellos que demoraron décadas en hacerlo respecto a la rapidez con que implementaron mecanismos de democracia directa. Este tipo de figuras no tiene valor inferencial, pero si gran valor descriptivo. Tenemos que estimar una curva de supervivencia no paramétrica, usando el método de Kaplan-Meier. A partir de este punto debemos hacer una modificación más a nuestra base. Seguramente tu también debas hacerlo con tus propios datos, así que presta atención. Los modelos de supervivencia no trabajan con datos de panel tradicionales, como el de nuestra base. Tenemos que convertirlos a “tiempo en riesgo”. ¿Qué quiere decir esto? Quiere decir que necesitamos dos variables nuevas, una que nos diga el tiempo que la observación lleva en riesgo de morir al inicio de cada t risk_time_at_starty otra variable que haga lo mismo al final de cada t risk_time_at_end. Estas dos variables serán utilizadas a partir de ahora en los scripts del análisis de supervivencia: democracia_directa_c &lt;- democracia_directa_b %&gt;% group_by(pais_nombre) %&gt;% # eliminaremos el primer año para cada país; # ya que por lógica no corre riesgo de morir ese primer año filter(anio != min(anio)) %&gt;% mutate(risk_time_at_end = c(1:n()), risk_time_at_start = c(0:(n() - 1))) %&gt;% ungroup() %&gt;% dplyr::select(pais_nombre, anio, risk_time_at_start, risk_time_at_end, everything()) democracia_directa_c ## # A tibble: 11,995 x 25 ## pais_nombre anio risk_time_at_st… risk_time_at_end dem_directa ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afganistán 1920 0 1 0 ## 2 Afganistán 1921 1 2 0 ## 3 Afganistán 1922 2 3 0 ## # … with 11,992 more rows, and 20 more variables Tracemos la curva de supervivencia no paramétrica, usando el método de Kaplan-Meier: km &lt;- survfit(Surv(time = risk_time_at_start, time2 = risk_time_at_end, event = dem_directa) ~ dem_rapida_positiva, type = &quot;kaplan-meier&quot;, conf.type = &quot;log&quot;, data = democracia_directa_c) Ahora podemos armar nuestro plot con survminer::ggsurvplot() library(survminer) ggsurvplot(km, conf.int = T, risk.table=T, legend.title = &quot;&quot;, break.x.by = 20, legend.labs = c(&quot;Democratización rápida = 0&quot;, &quot; Democratización rápida = 1&quot;), data = democracia_directa_c) + labs(title = &quot; Estimaciones de supervivencia de Kaplan-Meier &quot;) ## Warning: Vectorized input to `element_text()` is not officially supported. ## Results may be unexpected or may change in future versions of ggplot2. Figura 9.6: Kaplan-Meier Curve Altman tiene la hipótesis de que países que sufrieron “shocks” democratizadores fueron mucho más rápidos en implementar mecanismos directos. La figura confirma su intuición, pues vemos que la probabilidad de supervivencia de un país (léase como la probabilidad de que persista sin implementar mecanismos de democracia directa) se reduce a la mitad en los primeros cuatro años que siguen al shock democratizador. Por el contrario, países que se democratizan de a poco, no sufren este efecto. Ejercicio 9C. Utilizando los mismos datos del ejercicio anterior: ¿Cómo se compara la curva de Kaplan-Meier entre los países que recibieron un alto apoyo político de los Estados Unidos y los que recibieron un bajo apoyo político? Para ello, utilice la variable us_t, que es un índice de 0 a 1, donde 1 denota un apoyo constante de los Estados Unidos en un país, y 0 denota que no se ofreció ningún apoyo de los Estados Unidos. Para comparar los dos grupos de la curva de Kaplan-Meier, cree un dummy que asuma 1 si el apoyo es mayor que un 0.75 y 0 en caso contrario. 9.5 Modelos de supervivencia e interpretación sus coeficientes como Hazard Ratios. Vamos a usar la base de Altman para estimar algunos modelos y correr los tests de Grambsch y Therneau para testear que ninguno de los coeficientes viole el presupuesto de proporcionalidad de los riesgos. No estamos replicando los modelos de su capítulo porque son algo más complejos (incluyen varias interacciones), simplemente usamos su base como referencia. Nota: Aquí vale la pena hacer una aclaración respecto a replicabilidad: Si el autor utiliza Stata (por ejemplo con el comando stcox) van a haber algunas diferencias menores en los resultados obtenidos utilizando R. Primer modelo cox_m1 &lt;- coxph(Surv(risk_time_at_start, risk_time_at_end, dem_directa) ~ dem_rapida_positiva + dem_rapida_negativa + memoria + vdem, data = democracia_directa_c, method =&quot;breslow&quot;) Si lo desea, puede elegir expresar su modelo como una figura. Dado que estimaremos cinco modelos, usaremos una tabla en su lugar, pero es bueno para usted conocer ggforest ggforest(cox_m1) ## Warning in .get_data(model, data = data): The `data` argument is not ## provided. Data will be extracted from model fit. Figura 9.7: Representación gráfica de las tasas de riesgo del Modelo 1 Usando la función cox.zph hacemos el test de riesgo proporcional: test_cox_m1 &lt;- cox.zph(cox_m1) Miremos el valor global del test. Su p-valor es superior al punto de corte de 0.05. Si lo desea, puede elegir graficar los resultados del test usando ggcoxzph. No lo haremos en los siguientes modelos, pero estas figuras pueden ir perfectamente a un apéndice en tu artículo. ggcoxzph(test_cox_m1) Estimemos un segundo modelo: cox_m2 &lt;- coxph(Surv(risk_time_at_start, risk_time_at_end, dem_directa) ~ dem_rapida_positiva + dem_rapida_negativa + memoria + vdem + difusion_cap, data = democracia_directa_c, method =&quot;breslow&quot;) Y su test de riesgo proporcional: cox.zph(cox_m2) ## chisq df p ## dem_rapida_positiva 1.693 1 0.1932 ## dem_rapida_negativa 0.718 1 0.3968 ## memoria 1.623 1 0.2026 ## vdem 6.594 1 0.0102 ## difusion_cap 8.135 1 0.0043 ## GLOBAL 14.825 5 0.0111 En este caso, el test global es apenas superior al p de corte de 0.05, aún no se viola el supuesto de proporcionalidad. Sin embargo, hay una variable significativa en su Chi cuadrado:difusion_cap, con un p de 0.02. Ahora, un tercer modelo: cox_m3 &lt;- coxph(Surv(risk_time_at_start, risk_time_at_end, dem_directa) ~ dem_rapida_positiva + dem_rapida_negativa + memoria + vdem + difusion_cap + difusion_ocurr, data = democracia_directa_c, method =&quot;breslow&quot;) Y su test de riesgo proporcional: cox.zph(cox_m3) ## chisq df p ## dem_rapida_positiva 2.092 1 0.148 ## dem_rapida_negativa 0.473 1 0.491 ## memoria 1.697 1 0.193 ## vdem 4.515 1 0.034 ## difusion_cap 4.264 1 0.039 ## difusion_ocurr 2.089 1 0.148 ## [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ] Aquí, el test revela un escenario similar al del primer modelo, es decir, sin problemas. El cuarto modelo: cox_m4 &lt;- coxph(Surv(risk_time_at_start, risk_time_at_end, dem_directa) ~ dem_rapida_positiva + dem_rapida_negativa + memoria + vdem + difusion_cap + difusion_ocurr + log_poblacion, data = democracia_directa_c, method = &quot;breslow&quot;) Y su test de riesgo proporcional: cox.zph(cox_m4) ## chisq df p ## dem_rapida_positiva 2.084 1 0.149 ## dem_rapida_negativa 0.445 1 0.505 ## memoria 1.677 1 0.195 ## vdem 4.533 1 0.033 ## difusion_cap 4.224 1 0.040 ## difusion_ocurr 2.059 1 0.151 ## [ reached getOption(&quot;max.print&quot;) -- omitted 2 rows ] Note como el p-valor global está muy cerca del punto de corte pero aún se mantiene sobre 0.05. Las variables vdem y log_poblacion están violando el presupuesto de proporcionalidad de los riesgos, pues sus p-valores son menores a 0.05. El quinto modelo: cox_m5 &lt;- coxph(Surv(risk_time_at_start, risk_time_at_end, dem_directa) ~ dem_rapida_positiva + dem_rapida_negativa + memoria + vdem + difusion_cap + difusion_ocurr + log_poblacion + colonia_gb, data = democracia_directa_c, method =&quot;breslow&quot;) Su test de riesgo proporcional: cox.zph(cox_m5) ## chisq df p ## dem_rapida_positiva 1.883 1 0.170 ## dem_rapida_negativa 0.380 1 0.538 ## memoria 1.274 1 0.259 ## vdem 4.640 1 0.031 ## difusion_cap 3.960 1 0.047 ## difusion_ocurr 1.798 1 0.180 ## [ reached getOption(&quot;max.print&quot;) -- omitted 3 rows ] El quinto modelo presenta el mismo escenario que el cuarto modelo. Tenemos dos variables violando el presupuesto de proporcionalidad. El test global tiene un p-valor de 0.16 por lo que no deberíamos preocuparnos por resolver la violación. Sin embargo, caso que en tu trabajo tengas un p-valor global menor a 0.05 te mostramos como abordar el problema tal como recomienda el texto del Oxford Handbook sobre el que basamos la discusión teórica al inicio del capítulo. Una forma de resolverlo es interactuando las variables problemáticas con el logaritmo natural de la variable temporal que creamos anteriormente. Si el p-valor del test global fuese menor a 0.05, el quinto modelo corregido se vería así: cox_m5_int &lt;- coxph(Surv(risk_time_at_start, risk_time_at_end, dem_directa) ~ dem_rapida_positiva + dem_rapida_negativa + memoria + vdem + difusion_cap + difusion_ocurr + log_poblacion + colonia_gb + vdem:log(risk_time_at_end) + log_poblacion:log(risk_time_at_end), data = democracia_directa_c, method = &quot;breslow&quot;) Verás que el test ya no muestra problemas con la proporcionalidad de los riesgos. cox.zph(cox_m5_int) ## chisq df p ## dem_rapida_positiva 1.13e+00 1 0.287 ## dem_rapida_negativa 3.37e-01 1 0.562 ## memoria 5.39e-01 1 0.463 ## vdem 5.88e-05 1 0.994 ## difusion_cap 4.16e+00 1 0.041 ## difusion_ocurr 1.15e+00 1 0.284 ## [ reached getOption(&quot;max.print&quot;) -- omitted 5 rows ] Veamos todos los modelos juntos con texreg: library(texreg) Para obtener hazard ratios en R necesitamos exponenciar los coeficientes y luego calcular los errores estándar y valores-p a partir de una transformación de la matriz varianza-covarianza del modelo. En el Capítulo 8 vimos cómo hacer esto para modelos logísticos cuando queremos odds ratios (utilizando las opciones override.coef, override.se y override.pvalues de texreg). Para los modelos de supervivencia este paso es idéntico. La única diferencia para nuestro caso actual es la siguiente: ahora tenemos varios modelos para los que queremos hazard ratios, por lo que utilizaremos la función de iteración map() para que las transformaciones de coeficientes, errores estándar y valores-p se apliquen en cada modelo: list_models &lt;- list(cox_m1, cox_m2, cox_m3, cox_m4, cox_m5, cox_m5_int) screenreg(l = list_models, custom.model.names = c(&quot;Modelo 1&quot;, &quot;Modelo 2&quot;, &quot;Modelo 3&quot;, &quot;Modelo 4&quot;, &quot;Modelo 5&quot;, &quot;Modelo 5b&quot;), override.coef = map(list_models, ~ exp(coef(.x))), override.se = map(list_models, ~ odds_se(.x)), override.pvalues = map(list_models, ~ odds_pvalues(.x)) ) ## ## ===================================================================================================================== ## Modelo 1 Modelo 2 Modelo 3 Modelo 4 Modelo 5 Modelo 5b ## --------------------------------------------------------------------------------------------------------------------- ## dem_rapida_positiva 5.48 * 5.43 * 6.14 * 6.07 * 5.48 * 3.93 * ## (2.28) (2.27) (2.60) (2.59) (2.36) (1.81) ## dem_rapida_negativa 2.26 2.37 2.35 2.36 2.22 2.30 ## (1.69) (1.78) (1.77) (1.78) (1.67) (1.74) ## memoria 5.79 ** 5.35 ** 5.55 ** 5.51 ** 5.04 ** 5.17 ** ## (2.07) (1.92) (2.00) (1.99) (1.84) (1.90) ## vdem 4.03 3.07 3.81 3.91 5.20 379.22 ## (2.45) (1.93) (2.41) (2.60) (3.59) (636.11) ## difusion_cap 1.60 *** 2.95 ** 2.94 ** 2.77 ** 2.70 ** ## (0.34) (0.93) (0.93) (0.88) (0.88) ## difusion_ocurr 0.03 0.03 0.02 0.04 ## (0.05) (0.06) (0.04) (0.08) ## log_poblacion 1.01 *** 1.01 *** 1.57 *** ## (0.09) (0.10) (0.37) ## colonia_gb 0.45 * 0.49 * ## (0.21) (0.24) ## vdem:log(risk_time_at_end) 0.26 * ## (0.12) ## log_poblacion:log(risk_time_at_end) 0.88 *** ## (0.06) ## --------------------------------------------------------------------------------------------------------------------- ## AIC 377.71 374.17 370.81 372.70 371.44 365.19 ## R^2 0.00 0.00 0.01 0.01 0.01 0.01 ## Max. R^2 0.04 0.04 0.04 0.04 0.04 0.04 ## Num. events 47 47 47 47 47 47 ## Num. obs. 11441 11269 11260 11229 11229 11229 ## Missings 554 726 735 766 766 766 ## PH test 0.05 0.01 0.05 0.01 0.03 0.19 ## ===================================================================================================================== ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Agreguemos nombres a las variables para que esté lista para ser exportada: screenreg(l = list_models, custom.model.names = c(&quot;Modelo 1&quot;, &quot;Modelo 2&quot;, &quot;Modelo 3&quot;, &quot;Modelo 4&quot;, &quot;Modelo 5&quot;, &quot;Modelo 5b&quot;), custom.coef.names = c(&quot;Democratización Rápida&quot;, &quot;Rápido Retroceso Democrático&quot;, &quot;Memoria&quot;, &quot;Democracia&quot;, &quot;Difusión de Capacidades&quot;, &quot;Difusión de Ocurrencias&quot;, &quot;Población(ln)&quot;, &quot;Fue Colonia Británica&quot;, &quot;Democracia x tiempo en riesgo(ln)&quot;, &quot;Población(ln) x tiempo en riesgo(ln)&quot;), override.coef = map(list_models, ~ exp(coef(.x))), override.se = map(list_models, ~ odds_se(.x)), override.pvalues = map(list_models, ~ odds_pvalues(.x)) ) ## ## ====================================================================================================================== ## Modelo 1 Modelo 2 Modelo 3 Modelo 4 Modelo 5 Modelo 5b ## ---------------------------------------------------------------------------------------------------------------------- ## Democratización Rápida 5.48 * 5.43 * 6.14 * 6.07 * 5.48 * 3.93 * ## (2.28) (2.27) (2.60) (2.59) (2.36) (1.81) ## Rápido Retroceso Democrático 2.26 2.37 2.35 2.36 2.22 2.30 ## (1.69) (1.78) (1.77) (1.78) (1.67) (1.74) ## Memoria 5.79 ** 5.35 ** 5.55 ** 5.51 ** 5.04 ** 5.17 ** ## (2.07) (1.92) (2.00) (1.99) (1.84) (1.90) ## Democracia 4.03 3.07 3.81 3.91 5.20 379.22 ## (2.45) (1.93) (2.41) (2.60) (3.59) (636.11) ## Difusión de Capacidades 1.60 *** 2.95 ** 2.94 ** 2.77 ** 2.70 ** ## (0.34) (0.93) (0.93) (0.88) (0.88) ## Difusión de Ocurrencias 0.03 0.03 0.02 0.04 ## (0.05) (0.06) (0.04) (0.08) ## Población(ln) 1.01 *** 1.01 *** 1.57 *** ## (0.09) (0.10) (0.37) ## Fue Colonia Británica 0.45 * 0.49 * ## (0.21) (0.24) ## Democracia x tiempo en riesgo(ln) 0.26 * ## (0.12) ## Población(ln) x tiempo en riesgo(ln) 0.88 *** ## (0.06) ## ---------------------------------------------------------------------------------------------------------------------- ## AIC 377.71 374.17 370.81 372.70 371.44 365.19 ## R^2 0.00 0.00 0.01 0.01 0.01 0.01 ## Max. R^2 0.04 0.04 0.04 0.04 0.04 0.04 ## Num. events 47 47 47 47 47 47 ## Num. obs. 11441 11269 11260 11229 11229 11229 ## Missings 554 726 735 766 766 766 ## PH test 0.05 0.01 0.05 0.01 0.03 0.19 ## ====================================================================================================================== ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Si bien no estamos replicando la especificación de Altman, a simple vista nuestros resultados confirmarían la hipótesis de que países que sufrieron “shocks” democratizadores fueron mucho más rápidos en implementar mecanismos directos de democracia. Hemos terminado el ejercicio, ahora te invitamos a que hagas la lista de ejercicios antes de pasar al próximo capítulo. Ejercicio 9D. - Utilizando survminergrafique una curva de Kaplan-Meier para la variable colonia_gb. La variable colonia_urssindica aquellos países que fueron parte de la Unión Soviética. Grafique una curva de Kaplan-Meier para la variable. Incorpore esta variable a un sexto modelo, haga su test de Grambsch y Therneau y rehaga la tabla de los modelos con texreg ¿Tenés tu propia base de datos de supervivencia? Sería genial que repitieras todo el ejercicio usando tus datos y compartas dudas en nuestro GitHub. Si no tienes base, usa la de Mainwaring y Pérez-Liñán (2013) para intentar identificar la variable que más aumenta los riesgos de quiebre democrático en América Latina. E-mail: furdinez@uc.cl↩︎ "],
["causal-inf.html", "Capítulo 10 Inferencia causal 10.1 Introducción 10.2 Causalidad y gráficos causales 10.3 Midiendo efectos causales 10.4 DAGs y asociaciones estadísticas 10.5 Puertas traseras y cálculos “hacer” 10.6 Dibujar y analizar DAGs 10.7 Haciendo ajustes causales 10.8 CConsejos finales", " Capítulo 10 Inferencia causal Andrew Heiss43 Lecturas sugeridas Elwert, F. (2013). Graphical Causal Models. In S. L. Morgan (Ed.), Handbook of Causal Analysis for Social Research (pp. 245–273). Springer. Hernán, M. A., &amp; Robbins, J. M. (2020). Causal Inference: What If. CRC Press. Morgan, S. L., &amp; Winship, C. (2007). Counterfactuals and Causal Inference: Methods and Principles for Social Research. Cambridge University Press. Pearl, J., Glymour, M., &amp; Jewell, N. P. (2016). Causal Inference in Statistics: A Primer. Wiley. Pearl, J., &amp; Mackenzie, D. (2018). The Book of Why: The New Science of Cause and Effect. Basic Books. Rohrer, J. M. (2018). Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data. Advances in Methods and Practices in Psychological Science, 1(1), 27–42. Los paquetes que necesitas instalar tidyverse, ggdag, dagitty, MatchIt, broom, texreg 10.1 Introducción Una de las frases más repetidas en cualquier clase de estadística introductoria es la advertencia de que “la correlación no es causalidad”. En las investigaciones de Ciencia Política, sin embargo, a menudo nos preocupamos por las causas de los fenómenos sociales y políticos. ¿El gasto del gobierno en educación disminuye la desigualdad social? ¿El aumento del poder ejecutivo causa el colapso de un régimen? ¿El aumento de la fragmentación étnica causa genocidio? ¿Los proyectos de desarrollo internacional reducen la pobreza o aumentan la salud? Estas son preguntas importantes, pero usando los métodos estadísticos que se tratan en este libro, sólo podemos hablar de estas relaciones usando un lenguaje asociativo. En el capítulo 6 encontramos que los gastos en educación están asociados con el aumento de la desigualdad, y en el capítulo 9 encontramos que el aumento de la concentración del poder ejecutivo está asociado con una menor probabilidad de que el régimen se derrumbe. Aunque tuvimos mucho cuidado de no utilizar un lenguaje causal al interpretar estos coeficientes de regresión, en última instancia nos preocupa la causalidad, especialmente si tenemos la capacidad de influir en la política. Si una intervención de desarrollo causa mejoras en la salud, sería valioso desplegarla a gran escala. Es bastante fácil hacer una regresión que incluya dos variables aparentemente no relacionadas y encontrar que en realidad están significativamente correlacionadas. Por ejemplo, en los Estados Unidos, el consumo per cápita de queso de mozzarella se correlaciona fuertemente (\\(r = 0,959\\)) con el número de doctorados de ingeniería civil (ver Figura 10.1). Matemáticamente no hay diferencia entre la fuerte relación entre el consumo de queso y los doctorados en ingeniería civil y la fuerte relación entre el gasto en educación y la desigualdad social. Ambas relaciones están definidas por un único número: un coeficiente de regresión. Sin embargo, la relación entre el queso y los títulos no significa que el aumento del consumo de queso creará nuevos doctores, ni que el aumento de las filas de ingenieros civiles provoque un aumento de la cantidad de queso que comen los estadounidenses. Podemos fácilmente descartar esta relación como espuria. Figura 10.1: La alta correlación entre el queso y la ingeniería civil Es menos probable que califiquemos de espuria la relación entre el gasto en educación y la desigualdad. No nos reímos inmediatamente de la correlación que encontramos en el capítulo 6, y hay muchos trabajos académicos publicados que investigan la relación genuina entre ellos. ¿Por qué? La diferencia entre la posibilidad de las dos relaciones va más allá de las estadísticas. No hay ninguna explicación creíble que vincule el consumo de queso con los títulos de doctorado, a pesar de la alta correlación entre ambos. Hay una historia causal creíble entre la desigualdad y la educación, ya que el aumento de la calidad de la educación mejora las oportunidades de empleo disponibles para los estudiantes después de la graduación. En este capítulo, presentamos un nuevo lenguaje no estadístico para crear, medir y evaluar situaciones y relaciones causales utilizando datos observacionales (es decir, no experimentales). Introducimos el concepto de gráficos acíclicos dirigidos por la causalidad (Directed Acyclical Graphs o DAGs en inglés) que nos permiten codificar formalmente nuestra comprensión de las historias causales. Con los DAGs bien elaborados, podemos utilizar un conjunto de reglas llamadas cálculos do (hacer) para lograr ajustes específicos a los modelos estadísticos y aislar o identificar relaciones causales entre las variables de interés. 10.2 Causalidad y gráficos causales A diferencia de la correlación, que en su núcleo es simplemente una fórmula matemática, no existe una función de “causalidad” en R, algo como causation(). La causalidad es una cuestión de filosofía y teoría, no de matemáticas. Los debates sobre la definición de la causalidad se han llevado a cabo durante miles de años - Platón y Aristóteles escribieron sobre la metafísica y los componentes de las relaciones causales. En aras de la simplicidad, en este capítulo utilizaremos una definición bastante universal: se puede decir que \\(X\\) causa \\(Y\\) si: Asociación: \\(X\\) e \\(Y\\) están relacionadas entre sí Orden temporal: \\(X\\) precede a \\(Y\\) La relación entre \\(X\\) e \\(Y\\) no es espúrea Más simplemente, podemos colapsar las tres condiciones en una sola definición: Una variable \\(X\\) es una causa de una variable \\(Y\\) si \\(Y\\) de alguna manera depende de \\(X\\) para su valor… \\(X\\) es una causa de \\(Y\\) si \\(Y\\) escucha a \\(X\\) y decide su valor en respuesta a lo que escucha (Pearl, Glymour, and Jewell 2016, 5–6) El concepto de variables que se “escuchan” simultáneamente incorpora la relación, el ordenamiento en el tiempo y la relación no espuria. Considere la relación entre encender un interruptor de luz y que la luz se encienda. El acto de accionar un interruptor de luz se asocia con una bombilla que emite luz, lo que indica una asociación entre ambos. Una bombilla no puede emitir luz antes de ser encendida, por lo que se debe asegurar el orden correcto del tiempo: el encendido del interruptor debe preceder a la luz. Por último, la asociación no es espuria ya que existe un vínculo plausible entre ambos: los impulsos eléctricos viajan a través de los cables hasta el equipo que transforma la energía en electricidad que, en última instancia, puede alimentar la bombilla. También podemos decir más simplemente que la bombilla “escucha” al interruptor de luz. Los cambios en el estado de encendido y apagado del interruptor influyen en el estado de la bombilla más abajo en la cadena causal. Una bombilla escucha muchos otros factores - la electricidad debe fluir en el edificio, los transformadores deben funcionar correctamente, y múltiples interruptores podrían controlar la misma luz - pero un solo interruptor es definitivamente una de las causas de la emisión de luz. Figura 10.2: Varios DAGs Podemos codificar la filosofía o la teoría de una cadena causal de eventos en un gráfico acíclico dirigido, o DAG. Los DAGs son modelos gráficos del proceso que genera los datos y describen cómo \\(X\\) causa \\(Y\\) (ver panel A de la Figura @ref(fig: general-examples)) Estos gráficos consisten en tres elementos centrales: nodos, bordes (o flechas) y dirección. Los nodos representan fenómenos que tienen relaciones causales con otros elementos del sistema. Estos pueden ser cosas que se pueden medir y que podrían incluir una base de datos, como el PIB, la población, el partido político, el tiempo de permanencia en el cargo, los ingresos, la ubicación geográfica o la situación socioeconómica. Los nodos no tienen por qué ser necesariamente medibles; las limitaciones en la recopilación de datos o la abstracción de variables pueden hacer imposible la recopilación de mediciones fiables sobre diferentes fenómenos. Si éste es el caso, el nodo debe seguir estando incluido en el gráfico, pero debe ser considerado no observado o latente. Los nodos también representan las variables de tratamiento y de resultado (véase el panel B de la figura @ref(fig: general-examples)). Por ejemplo, si nos interesa el efecto causal de los gastos en educación sobre la desigualdad (como en el capítulo 6), los gastos en educación serían el tratamiento o la exposición, y la desigualdad social basada en Gini sería el resultado. Las flechas transmiten asociaciones entre nodos. Por ejemplo, si un gráfico muestra que \\(X \\rightarrow Y\\), \\(X\\) tiene una asociación causal con \\(Y\\), o \\(Y\\) “escucha” o responde a \\(X\\). Crucialmente, la ausencia de una flecha entre los nodos implica que no hay relación causal entre los nodos. Las asociaciones causales entre los nodos sólo pueden fluir en una dirección- las flechas nunca pueden ir en ambas direcciones (por ejemplo, \\(X \\leftrightarrow Y\\)), y debería ser imposible volver a cualquier nodo dado mientras se camina por el gráfico (por ejemplo, \\(X \\rightarrow Z \\rightarrow Y \\rightarrow Z \\rightarrow X\\)). A menudo hay una razón válida para incluir flechas de loop o de retroalimentación. Por ejemplo, los gastos en educación pueden conducir a la desigualdad, lo que a su vez conduce a cambios en los gastos en educación. En estas situaciones, en lugar de incluir flechas bidireccionales, es mejor distinguir entre los dos períodos de gastos en educación, a menudo utilizando el subíndice \\(t\\) para el tiempo: \\(X_{t-1} \\rightarrow Y_{t-i} \\rightarrow X_t \\rightarrow Y_t\\) (ver panel C en la figura -(fig:general-examples)). La presencia y ausencia de nodos, flechas y dirección en un DAG comunica nuestra teoría y nuestra filosofía de la causalidad. El uso de los DAGs deja claro a su audiencia cuáles son tus supuestos. Vincular los nodos entre sí implica que hay una relación estadística entre dos fenómenos, mientras que omitir un nodo o una flecha implica que el nodo no juega ningún papel en la historia causal. Si los lectores quieren discutir con su teoría de la causalidad, pueden hacerlo fácilmente refiriéndose al DAG y señalando qué nodos y flechas deben ser incluidos o excluidos. 10.3 Midiendo efectos causales Como la causalidad significa que \\(Y\\) escucha a \\(X\\), medir el efecto de \\(X\\) en \\(Y\\) requiere que se pueda manipular \\(X\\) directamente para ver qué pasa con \\(Y\\). Podemos usar una pieza especial de notación matemática para representar intervenciones directas: el operador hacer, o \\(hacer(\\cdot)\\). Si \\(Y\\) escucha a \\(X\\), podemos decir que hay algún valor esperado de \\(Y\\) cuando “hacemos” \\(X\\), o \\(E(Y | hacer(X))\\) (léase esto como “el valor esperado de Y dado que tienes X”).44 Podemos hacer que esta notación sea un poco más fácil de entender con algunos ejemplos de preguntas causales que interesan a los cientistas sociales: \\(E(\\text{Votos obtenidos}\\ | \\ hacer(\\text{Recaudación de fondos de la campaña}))\\): El efecto de hacer (es decir, participar en) la recaudación de fondos de la campaña en el número de votos ganados en una elección \\(E(\\text{Desigualdad}\\ | \\ hacer(\\text{Gastos en educación}))\\): El efecto de “hacer” gastos de educación (es decir, gastar dinero) en el nivel de desigualdad social de un país \\(E(\\text{Calidad del aire}\\ | \\ hacer(\\text{Impuesto sobre el carbono}))\\): Tel efecto de “hacer” (es decir, aplicar) un impuesto sobre el carbono en la calidad del aire de un país \\(E(\\text{Tasa de infección de malaria}\\ | \\ hacer(\\text{redes para mosquitos}))\\): El efecto de “hacer” (es decir, usar) un mosquitero en la tasa de infección de malaria de una aldea Para medir el efecto causal de una intervención, necesitamos comparar el valor esperado del resultado (\\(E(Y)\\)) cuando intervenimos y cuando no intervenimos con \\(X\\). Por ejemplo, podemos comparar lo que sucede con una bombilla cuando accionamos un interruptor con lo que sucede con una bombilla cuando no accionamos el interruptor. Restringiendo la diferencia entre estos dos estados del mundo se obtiene un efecto causal: \\[ \\text{Efecto causal del encendido de la luz} = E(\\text{Luz}\\ | \\ hacer(\\text{Interruptor = Encendido})) - E(\\text{Luz}\\ | \\ hacer(\\text{Interruptor = Apagado})) \\] Podemos escribir esta definición de manera más general usando unos pocos símbolos matemáticos más. Aquí \\(\\delta\\) representa el efecto causal y el subíndice \\(i\\) representa un individuo específico (es decir, comparando los efectos en la misma bombilla): \\[ \\delta_i = E(Y\\ | \\ hacer(X = 1))_i - E(Y\\ | \\ hacer(X = 0))_i \\] El cálculo del efecto causal \\(\\delta_i\\) en el mundo físico es bastante sencillo. Encuentra dos bombillas codigoénticas, enciende una ( \\(hacer(\\text{Interruptor = Encendido})\\)), deja una apagada ( \\(hacer(\\text{Interruptor= Apagado})\\)), y compara la luz que se emite. O, como una sola bombilla no cambia con el tiempo, medir el cambio de luz en la misma bombilla en dos momentos diferentes. Sin embargo, en las ciencias sociales es mucho más difícil medir el efecto causal de las intervenciones en personas, provincias o países concretos. No podemos medir \\(E(Y | hacer( X = 1))_i\\) en un individuo y luego retroceder en el tiempo para medir \\(E(Y | hacer(X = 0))_i\\) en ese mismo individuo. Esto plantea el problema fundamental de la inferencia causal: los efectos causales a nivel individual son imposibles de observar porque no podemos ver contrafactuales a nivel individual. En lugar de medir los efectos a nivel individual, es más fácil (y realmente factible) medir los resultados medios de cada unidad de un grupo. Podemos encontrar el efecto promedio del tratamiento (ATE) calculando la diferencia entre el valor esperado del resultado promedio (\\(\\bar{Y}\\)) para los que \\(hacen(X)\\) y los que no: \\[ \\delta_\\text{ATE} = E(\\bar{Y}\\ | \\ hacer(X = 1)) - E(\\bar{Y}\\ | \\ hacer(X = 0)) \\] En condiciones experimentales, como en los ensayos controlados aleatorios en los que los participantes se asignan al azar para recibir tratamiento o no, los grupos que \\(hacen(X = 1)\\) y \\(hacen(X = 0)\\) serán generalmente comparables porque ningún individuo se seleccionó voluntariamente dentro o fuera del tratamiento. Mientras la aleatorización se haga bien, el ATE que calculamos debe ser imparcial (sin sesgo) y preciso. Sin embargo, la mayoría de los datos de la ciencia política son de naturaleza observacional. El PIB nacional, los resultados de las elecciones, el gasto del gobierno, la matrícula en las escuelas primarias y el número de golpes de Estado se generan a partir de procesos no experimentales. Si estamos interesados en el efecto de ser una democracia (es decir, “hacer” democracia) en el PIB, podríamos tratar de definir el efecto causal como: \\[ \\delta_\\text{ATE} = E(\\text{PBI promedio}\\ | \\ hacer(\\text{Democracia} = 1)) - E(\\text{PBI promedio}\\ | \\ hacer(\\text{Democracia} = 0)) \\] Sin embargo, el simple hecho de restar el PIB medio de las no democracias del PIB medio de las democracias dará lugar a una estimación sesgada e incorrecta. Los países no se asignan al azar para ser democracias. Hay innumerables factores económicos, históricos, sociológicos y políticos que influyen en la capacidad de un país para \\(hacer(\\text{democracia})\\), lo que garantiza que habrá un sesgo sistemático en la selección del sistema político de un país. Cualquier correlación que pudiéramos encontrar no implicaría causalidad. Existen muchas técnicas econométricas para identificar y estimar los efectos causales a partir de datos de observación, incluidos los experimentos naturales, el análisis de diferencias en las diferencias, el análisis de regresión discontinua y las variables instrumentales (véase Cunningham (2018), Angrist and Pischke (2008), Angrist and Pischke (2015) para una visión general completa de todos estos métodos). En general, estos métodos tratan de aproximarse a los grupos de tratamiento y control, organizando a las personas o los países en grupos comparables y eliminando el sesgo de selección. Además de estos enfoques, podemos utilizar la lógica de los modelos causales para identificar las relaciones causales, eliminar la confusión estadística y utilizar datos de observación para estimar efectos causales válidos. 10.4 DAGs y asociaciones estadísticas Dibujar nodos y flechas es útil para comprender los diversos elementos de un fenómeno social, pero por sí solos, los nodos y bordes no identifican relaciones causales. Se considera que un efecto causal está identificado si la asociación entre el nodo de tratamiento y el nodo de resultado se aísla y se despoja de las asociaciones estadísticas que provienen de otros nodos del gráfico. Figura 10.3: Los tipos básicos de relaciones en DAGs La dirección de las flechas a lo largo de las trayectorias entre nodos en un gráfico determina cómo se transmiten las asociaciones estadísticas entre los nodos. Para ilustrar cómo ciertas relaciones direccionales pueden pasar asociaciones estadísticas entre nodos en un gráfico, supongamos que dos variables \\(X\\) e \\(Y\\) no están relacionadas entre sí. Podemos decir esto con notación matemática usando el símbolo \\(\\perp\\): \\(X \\perp Y\\) significa que \\(X\\) es independiente de \\(Y\\) (y \\(X \\not\\perp Y\\) significa que las dos variables no son independientes). En el lenguaje de los gráficos causales, podemos decir que si \\(X\\) e \\(Y\\) son independientes la una de la otra, están d-separadas (el d- significa “dirección”). Si \\(X\\) e \\(Y\\) no son independientes una de la otra, están d-conectadas: las características de las relaciones direccionales en el gráfico causal conectan las dos variables entre sí y permiten que la información pase entre ellas. La figura 10.3 muestra cómo la inclusión de un tercer nodo \\(Z\\) en la vía entre \\(X\\) y \\(Y\\) cambia la relación estadística entre los dos e influye en si están d-separadas o d- conectadas. Mediadores: En el panel A, \\(X\\) e \\(Y\\) están correlacionadas entre sí por la variable mediadora \\(Z\\). Cualquier cambio en \\(X\\) causará cambios en \\(Y\\) a través de \\(Z\\). El camino entre \\(X\\) e \\(Y\\) es por lo tanto abierto y \\(X \\not\\perp Y\\). \\(X\\) e \\(Y\\) están dconectadas. variable confusora: En el panel B, aunque \\(X\\) e \\(Y\\) no se causan mutuamente, \\(Z\\) es una causa común de ambos y confunde la relación. Cualquier cambio en \\(Z\\) causará cambios tanto en \\(X\\) como en \\(Y\\), eliminando así cualquier independencia estadística entre los dos. El camino entre \\(X\\) e \\(Y\\) está así abierto y de nuevo, \\(X \\not\\perp Y\\). \\(X\\) e \\(Y\\) están dconectadas. En realidad, \\(X\\) e \\(Y\\) deberían ser independientes entre sí, pero en el caso de los mediadores y los las variables de confusión, \\(X\\) e \\(Y\\) están dconectadas porque \\(Z\\) abre un camino entre los dos y pasa información entre ellos. Un elemento poderoso de la lógica de los diagramas causales es que podemos controlar el flujo de información estadística bloqueando y abriendo vías entre los nodos. Tanto para mediar como para confundir, si podemos detener la asociación entre ambos \\(X\\) y \\(Z\\) y \\(Z\\) e \\(Y\\), entonces \\(X\\) e \\(Y\\) serán una vez más independientes entre sí y estarán d-separadas. Para ello, podemos ajustar para o condicionar sobre \\(Z\\) quitando las partes de \\(X\\) e \\(Y\\) que se explican por \\(Z\\). Hay muchas maneras de ajustar las variables, y se incluyen ejemplos más adelante en este capítulo. Una forma básica de pensar en el ajuste de variables es con la regresión: si ejecutáramos una regresión e incluyéramos \\(Z\\) como variable de control (por ejemplo, lm(Y ~ X + Z)), el coeficiente para \\(Z\\) explicaría (y eliminaría) la variación conjunta de \\(Z\\) a través de \\(X\\) y \\(Y\\). Una vez que ajustamos para \\(Z\\), el camino entre \\(X\\) e \\(Y\\) se bloquea y los dos se dseparan. Matemáticamente, podemos escribir esto como \\(X \\perp Y | Z\\), o decir que \\(X\\) es independiente de \\(Y\\) condicionado a \\(Z\\). El concepto de bloqueo o condicionamiento nos ayuda a entender el tipo de relación final entre los nodos: Colisionadores: En el panel C de la figura @ref(fig: association-examples), \\(X\\) e \\(Y\\) son causas comunes de \\(Z\\), pero cada una es independiente de la otra. El camino entre \\(X\\) e \\(Y\\) está cerrado porque está bloqueado por \\(Z\\). \\(X \\perp Y\\) y \\(X\\) e \\(Y\\) están dseparados. Si controlamos por \\(Z\\) en un modelo de regresión, abrimos inadvertidamente el camino entre \\(X\\) e \\(Y\\) y creamos una relación estadística espuria entre los dos. Figura 10.4: DAG simplificado que muestra la relación entre los gastos de campaña y los votos ganados en una elección Hasta ahora, hemos hablado de DAGs y d-separación en abstracto con \\(X\\), \\(Y\\), y \\(Z\\). La figura 10.4 proporciona un gráfico causal más concreto que ilustra las tres relaciones simultáneamente. Supongamos que estamos interesados en el efecto causal de la recaudación de fondos de la campaña (\\(X\\)) sobre el total de votos (\\(Y\\)). En este caso, ya no suponemos que \\(X\\) e \\(Y\\) son independientes la una de la otra (es decir, \\(X \\perp Y\\)), sino que queremos medir la relación entre ambas. Sin embargo, para aislar esa relación, debemos asegurarnos de que la vía entre el dinero de la campaña y el total de votos sea la única vía conectada en el gráfico. Cada uno de los otros nodos del gráfico - “director de campaña contratado”, “calidad del candidato” y “elección ganada” - pasa por diferentes tipos de asociaciones estadísticas entre el dinero y el éxito electoral. Podemos examinarlos a su vez: La relación entre el dinero y el total de votos es mediada por la contratación de un director de campaña. Cualquier cambio en la recaudación de fondos tiene un efecto en el número de votos ganados, pero la recaudación de fondos también influye en las posibilidades de contratar a un director de campaña, lo que a su vez tiene un efecto en el total de votos. El camino entre el dinero y los votos está así abierto debido a la variable de director de campaña mediador. La relación entre el dinero y el éxito electoral se confunde con la calidad del candidato. Los candidatos de alta calidad son más propensos a recaudar más dinero y ganar elecciones, lo que significa que la relación entre el dinero y el éxito electoral ya no está aislada. Si ajustamos la calidad del candidato y comparamos los candidatos de la misma calidad (o controlar por la calidad en una regresión, manteniendo la calidad constante), podemos cerrar el camino entre el dinero y el éxito electoral. Ganar una elección es un colicionador en el camino entre el dinero y el éxito electoral. Si ajustamos por ganar las elecciones (es decir, sólo miramos a los candidatos ganadores), la relación real entre el dinero y el éxito electoral se distorsionará. El sesgo colisionador es un tipo de sesgo de selección: si sólo miramos las campañas exitosas, no vemos la relación entre el dinero y el éxito de los candidatos que no ganaron, y nuestras estimaciones serán erróneas. No deberíamos controlar por el hecho de ganar una elección. Conocer la dirección de las flechas entre los nodos de un DAG, por lo tanto, proporciona una guía útil sobre qué controlar o ajustar. Si queremos aislar la relación entre la recaudación de fondos de campaña y el éxito electoral, debemos controlar por la calidad del candidato (ya que es un confusor) y no debemos controlar por el hecho de ganar la elección (ya que es un colisionador). La decisión de ajustar por tener un director de campaña dependerá de nuestra pregunta de investigación. Si lo ajustamos, eliminaremos el efecto de director de campaña del efecto total de la recaudación de fondos sobre los votos, y el efecto restante de \\(X\\) sobre \\(Y\\) será realmente de \\(X\\) sin el efecto de director de campaña. Si estamos interesados en el efecto total de la recaudación de fondos, incluyendo cualquier efecto que provenga de tener un director de campaña, no deberíamos controlar el tener un director de campaña. 10.5 Puertas traseras y cálculos “hacer” Following the fundamental problem of causal inference (i.e. due the the fact that we do not have a time machine), answering causal questions without an experiment appears impossible. However, if we apply a special set of logical rules called do-calculus to our causal graph, we can strip away any confounding relationships between our tratamiento and outcome nodes and isolate the causal effect between them using only observational data. Figura 10.5: Las flechas en un DAG se borran cuando usamos el operador \\(hacer(\\cdot)\\) El operador hacer representa una intervención directa en un DAG y nos permite “congelar” un nodo en un valor específico. Por ejemplo, en un ensayo controlado aleatorio, nosotros como investigadores tenemos control sobre quién es asignado al tratamiento y a los grupos de control en la intervención. El efecto causal (o \\(delta\\)) que encontramos en el ensayo es el efecto de \\(X\\) en nuestro resultado \\(Y\\) cuando \\(hacer(X = \\ text{tratamiento})\\). En los entornos experimentales, una intervención \\(hacer(\\cdot)\\) implica que todas las flechas que entran en el nodo de tratamiento se eliminan (ver Figura 10.5). Esto asegura que \\(X\\) esté d-separada de \\(Y\\) y que la flecha entre \\(X\\) y \\(Y\\) esté completamente aislada e identificada. Sin embargo, con datos de observacionales, no es posible estimar \\(E(Y | hacer(X))\\) porque como investigadores, no tenemos control sobre \\(X\\). No podemos asignar a algunos países a que aumenten y otros a que disminuyan el gasto en educación como parte de un experimento para ver cómo cambia la desigualdad social, ni asignar a algunas campañas a que hagan un esfuerzo extra en la recaudación de fondos para ver el efecto del dinero en los votos. En lugar de \\(E(Y | hacer(X))\\), que es nuestro valor de interés, sólo podemos estimar \\(E(Y | X)\\), o el valor esperado de \\(Y\\) dados los niveles existentes de \\(X\\) (es decir, la correlación entre ambos). Desafortunadamente, la frase con la que se abrió este capítulo, “la correlación no es causalidad”, se mantiene aquí: \\(E(Y | X) \\neq E(Y | hacer(X))\\). Para estimar un efecto causal a partir de datos observacionales, necesitamos transformar \\(E(Y | hacer(X))\\) en algo que esté liberado del término hacer, ya que no podemos realmente intervenir sobre cuándo hacer ocurre y cuando no. Un conjunto de tres reglas lógicas especiales denominadas “cálculos hacer” nos permite hacer precisamente eso: mediante la aplicación de diferentes reglas, podemos eliminar los operadores \\(hacer(\\cdot)\\) de los DAG e identificar los efectos causales con sólo datos observacionales. Cubrir el conjunto completo de reglas de cálculo hacer va más allá del alcance de este capítulo; puedes consultar Pearl, Glymour, and Jewell (2016), Pearl and Mackenzie (2018), Shpitser and Pearl (2008) para más detalles y ejemplos. Una derivación particular de las reglas de cálculos hacer define un especial criterio de puerta trasera que nos permite eliminar los operadores \\(hacer(\\cdot)\\) ajustando las variables de confusión a lo largo del camino entre \\(X\\) y \\(Y\\). El criterio de puerta trasera establece que el efecto causal de \\(X\\) sobre \\(Y\\) es identificable (es decir, puede ser aislado) después de ajustar el conjunto de variables de confusión \\(Z\\), usando la fórmula: \\[ P(Y | hacer(X)) = \\sum_Z P(Y | X, Z) \\times P(Z) \\] En este capítulo, la derivación e interpretación exacta de esta fórmula es menos importante que la intuición. De acuerdo con este criterio de puerta trasera, el operador hacer del lado izquierdo de la ecuación puede transformarse en una expresión libre de hacer del lado derecho, estimada con \\(P(Y | X, Z)\\) (la distribución de \\(Y\\) condicionada tanto a \\(X\\) como a \\(Z\\)) y \\(P(Z)\\) (la distribución de \\(Z\\)), que son ambas estimables utilizando sólo datos de observación. En la práctica, el ajuste de la puerta trasera suele ser mucho más intuitivo que tratar de derivar fórmulas largas. En su lugar, es posible encontrar y ajustar caminos de puertas traseras confusores usando un DAG siguiendo este proceso: Enumera todos los caminos entre \\(X\\) e \\(Y\\), sin importar la dirección de las flechas. Identifica cualquier camino que tenga flechas que apunten hacia atrás, hacia \\(X\\). Los nodos que apuntan hacia atrás a \\(X\\) son confusores y por lo tanto abren caminos de vuelta. Estos deben ser ajustados. Podemos aplicar este proceso al DAG en la figura 10.4. Estamos interesados en el efecto de la recaudación de fondos de campaña en la cantidad de votos obtenidos en una elección, o \\(E(\\text{Votos obtenidos}\\ | \\ hacer(\\text{campaña de recaudación de fondos}))\\). Como no asignamos experimentalmente algunas campañas para recaudar dinero y otras no, sólo podemos trabajar con datos observacionales, dejándonos sólo con la correlación entre la recaudación de fondos de la campaña y los votos, o \\(E(\\text{Votos obtenidos}\\ |\\ \\text{campaña de recaudación de fondos})\\). Si miramos el gráfico causal hay cuatro caminos entre “Dinero recaudado” y “Total de votos”: \\(\\text{Recaudación} \\rightarrow \\text{Cantidad de votos}\\) \\(\\text{Recaudación} \\leftarrow \\text{Calidad del candidato} \\rightarrow \\text{Cantidad de votos}\\) \\(\\text{Recaudación} \\rightarrow \\text{Contratar gerente de campaña} \\rightarrow \\text{Cantidad de votos}\\) \\(\\text{Recaudación} \\rightarrow \\text{Ganar la elección} \\leftarrow \\text{Cantidad de votos}\\) En el segundo camino, la calidad del candidato apunta hacia atrás hacia “dinero recaudado” y es un confusor que abre un camino secundario entre la recaudación de fondos y los votos. El primero, tercero y cuarto caminos sólo tienen flechas hacia la derecha y no producen ninguna confusión. Si ajustamos por la calidad del candidato y la mantenemos constante, nos aseguramos de que la relación entre la recaudación de fondos y los votos estará d-separada de todos los demás nodos y por lo tanto esté aislada e identificada. Figura 10.6: Un DAG más complicado que muestra la relación entre los gastos de campaña y los votos ganados en una elección La misma lógica se aplica también a DAGs más complejos. Consideremos la figura 10.6, que es una versión amplificada de la figura 10.4 con tres nuevos nodos: el partido político del candidato, el distrito en el que se presenta el candidato y la historia no observada tanto del distrito como del partido, que influyen en la toma de decisiones del partido y en la dinámica demográfica del distrito. Estamos interesados en identificar o aislar el camino entre la recaudación de fondos y el total de votos, pero hay caminos de confusión que d-separan el camino causal que nos interesa. Podemos cerrar estas puertas traseras para aislar el efecto causal. Primero, enumeramos todos los caminos entre “Dinero recaudado” y “Total de votos”: \\(\\text{Recaudación} \\rightarrow \\text{Cantidad de votos}\\) \\(\\text{Recaudación} \\rightarrow \\text{Contratar gerente de campaña} \\rightarrow \\text{Cantidad de votos}\\) \\(\\text{Recaudación} \\rightarrow \\text{Ganar elección} \\leftarrow \\text{Cantidad de votos}\\) \\(\\text{Recaudación} \\leftarrow \\text{Calidad del candidato} \\rightarrow \\text{Cantidad de votos}\\) \\(\\text{Recaudación} \\leftarrow \\text{Distrito} \\rightarrow \\text{Cantidad de votos}\\) \\(\\text{Recaudación} \\leftarrow \\text{Partido} \\rightarrow \\text{Cantidad de votos}\\) \\(\\text{Recaudación} \\leftarrow \\text{Distrito} \\leftarrow \\text{Historia} \\rightarrow \\text{Partido} \\rightarrow \\text{Cantidad de votos}\\) \\(\\text{Recaudación} \\leftarrow \\text{Partido} \\leftarrow \\text{Historia} \\rightarrow \\text{Distrito} \\rightarrow \\text{Cantidad de votos}\\) De estos ocho caminos posibles, los últimos cinco tienen flechas que apuntan hacia la izquierda en “Dinero recaudado” desde tres nodos únicos: calidad del candidato, partido y distrito. Por lo tanto, debemos ajustar por la calidad del candidato, el partido y el distrito para cerrar estos caminos de puerta trasera y asegurarnos de que se identifique la conexión entre dinero y votos. Es importante destacar que la historia, que no es medible, también es un factor de confusión. Sin embargo, debido a que cerramos el partido y el distrito mediante ajustes, la historia no confunde estadísticamente la relación entre el dinero y los votos. Aunque es latente e inconmensurable, los nodos posteriores que son medibles nos permiten cerrar sus efectos de puerta trasera. También ten en cuenta que medir por “haber ganado la elección” es un colisionador, no un factor de confusión, y no debe ajustarse para evitar crear conexiones espurias entre la recaudación y los votos, y tener un gerente de campaña es un mediador y no debe ajustarse si estamos interesados en el efecto total del dinero en los votos. El criterio de puerta trasera no es el único método para encontrar qué nodos deben ajustarse. Otro método común es el criterio de la puerta delantera, que se basa en variables mediadoras d-separadas para identificar la relación entre $ X $ e $ Y $ (consulta Rohrer (2018), Pearl, Glymour, and Jewell (2016), Pearl and Mackenzie (2018), Elwert (2013) y Hernán and Robbins (2020) para detalles y ejemplos). Los DAGs complejos que no se ajustan fácilmente a los criterios de puerta trasera o puerta delantera pueden usar la aplicación de las reglas de cálculo hacer para encontrar ajustes apropiados y factibles. Se han desarrollado algoritmos especiales para trabajar a través de las reglas de cálculo hacer para determinar si un efecto es identificable: el paquete causaleffect en R incluye funciones para ejecutar estos algoritmos. Es importante destacar que no siempre es posible identificar los efectos causales en un DAG. Si no hay forma de traducir $ P (Y | hacer (X)) $ a una expresión libre usando cálculos hacer, entonces es imposible aislar e identificar la relación causal usando datos observacionales. 10.6 Dibujar y analizar DAGs Como los DAGs son un conjunto de nodos y flechas, son fáciles de dibujar. Recomiendo esbozarlos a mano en papel o en una pizarra cuando estés mapeando por primera vez tu teoría causal, y luego transfiere el borrador escrito a mano a una computadora. Si bien es posible usar muchos programas diferentes para dibujar DAGs, incluidos Microsoft PowerPoint o Adobe Illustrator, es mejor usar un software de gráficos como DAGitty o el paquete ggdagen R. 10.6.1 Dibujar un DAG con DAGitty DAGitty (https://www.dagitty.red) is an in-browser graphical editor for creating and analyzing causal diagrams. By pointing and clicking, you can create nodes and link them together with edges. You can also assign specific nodes to be tratamiento, outcome, and latent/unobserved variables, which each have their own coloring scheme. You can export DAGs as PNG, JPG, and SVG files for inclusion in other documents. Figure 10.7 shows an example of a causal graph made in DAGitty. Consult the online manual for more details about DAGitty’s features, or spend a few minutes playing around to get used to adding and connecting nodes. DAGitty ([https://www.dagitty.red ] (https://www.dagitty.red )) es un editor gráfico online para crear y analizar diagramas causales. Con el mouse, y sin código, puede crear nodos y vincularlos con flechas. También puede asignar nodos específicos para que sean tratamiento, resultado o variables latentes / no observadas, y a cada una darle su propia escala de colores. Puedes exportar los DAGs como archivos PNG, JPG y SVG para incluirlos en otros documentos. La figura 10.7 muestra un ejemplo de un gráfico causal realizado en DAGitty por mí. Ejercicio 10A. Consulta [el manual en línea] (http://dagitty.red /manual-3.x.png) para obtener más detalles sobre las características de DAGitty, o pasa unos minutos jugando para acostumbrarte a agregar y conectar nodos. Figura 10.7: Ejemplo de un DAG dibujado en DAGitty 10.6.2 Encontrar sets de ajuste con DAGitty DAGitty colorea automáticamente los nodos y las flechas según estén abiertos o no. Por ejemplo, en la Figura 10.7, los nodos de la temperatura nocturna, ingresos y condiciones de salud son rojos, lo que indica que confunden la relación entre el tratamiento (red) y el resultado (malaria). Puedes especificar que estos nodos se ajustarán y así despejar ej camino causal. En la barra lateral derecha, DAGitty incluye una lista de nodos que deben ajustarse para identificar el efecto causal. Si el efecto es identificable, enumerará todos los nodos mínimamente suficientes; y si la causalidad no es identificable, DAGitty se lo informará. Para el DAG en la Figura 10.7), el conjunto de ajustes mínimamente suficientes incluye las temperaturas nocturnas, los ingresos y las condiciones de salud subyacentes (see Figure 10.8). Figura 10.8: Sets con ajustes mínimos suficientes e implicaciones comprobables para un modelo en DAGitty La barra lateral derecha también incluye una lista de todas las implicaciones comprobables del DAG. Recuerde que después del ajuste, ciertos nodos se dseparan y, por lo tanto, no pasan asociaciones estadísticas entre sí. Según este DAG, después de ajustar las temperaturas nocturnas, los ingresos y las condiciones de salud subyacentes, algunas de las siguientes afirmaciones deberían ser ciertas: \\(\\text{Ingresos} \\perp \\text{Resistencia al insecticida}\\): Los ingresos deben ser independientes de la resistencia a los insecticidas. \\(\\text{Ingresos} \\perp \\text{Temperaturas nocturas}\\):Los ingresos deben ser independientes de las temperaturas nocturnas. \\(\\text{Gente en el hogar} \\perp \\text{Condiciones de salud subyacente}\\ |\\ \\text{Ingresos}\\): El número de personas en un hogar debe ser independiente de las condiciones de salud subyacentes en niveles similares de ingresos. Cada una de estas afirmaciones es comprobable con datos observacionales. Si tiene columnas en una base de datos para estos nodos diferentes, puede usar R para verificar las correlaciones entre ellos (es decir, cor(data$ingresos, data$temperatura) o lm(gente_hogar ~ salud + ingresos)) y ver si en realidad son independientes entre sí. 10.6.3 Diseñar DAGs en R El paquete ggdag te permite usar ggplot2 para crear y analizar DAGs en R. La documentación del paquete está llena de ejemplos útiles del rango completo de funciones del paquete. A continuación se presentan algunos ejemplos de las cosas más comunes que puede hacer con él En general, diseñas un DAG con dagify () y lo graficas con ggdag () o ggplot (). La sintaxis para diseñar un DAG en dagify () es similar a la que usas cuando creas modelos con lm () o glm (): crearás sistemas de fórmulas que indican relaciones entre los nodos. Por ejemplo, en el DAG a continuación, y es causado por x, a y b (y ~ x + a + b), y x es causado por a yb (x ~ a + b), que hace que a y b sean variables de confusión. library(ggdag) simple_dag &lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = &quot;x&quot;, outcome = &quot;y&quot; ) # theme_dag() coloca la trama en un fondo blanco sin etiquetas en los ejes ggdag(simple_dag) + theme_dag() Figura 10.9: DAG básico diseñado con Establecer x e y como tus variables independiente y dependiente es opcional si lo que deseas es un gráfico simple, pero si lo configuras así, puedes colorear los puntos por tipo de nodo: ggdag_status(simple_dag) + theme_dag() Figura 10.10: DAG con nodos coloreados por estatus Observe cómo el diseño es diferente en ambos gráficos. Por defecto, ggdag () coloca los nodos aleatoriamente cada vez que usa un algoritmo de red. Puedes cambiar el algoritmo utilizando el argumento layout:ggdag (simple_dag, layout = \"nicely\"). Puedes ver una lista completa de los posibles algoritmos ejecutando ?layout_tbl_graph_igraph en la consola. Alternativamente, puedes especificar tus propias coordenadas para que los nodos se coloquen en el mismo lugar cada vez. Logras esto con el argumento coords endagify (): simple_dag_with_coords &lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = &quot;x&quot;, outcome = &quot;y&quot;, coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords) + theme_dag() Figura 10.11: DAG con coordenadas manuales Esto ya lo sabrás de los capítulos previos, pero es bueno aclarar que los nombres de variables que uses no tienen que limitarse a solo x, y y otras letras minúsculas. Puedes usar cualquier nombre que desees, siempre que no haya espacios. dag_with_var_names &lt;- dagify( resultado ~ tratamiento + confundidor1 + confundidor2, tratamiento ~ confundidor1 + confundidor2, exposure = &quot;tratamiento&quot;, outcome = &quot;resultado&quot; ) ggdag_status(dag_with_var_names) + theme_dag() Figura 10.12: DAG con nombres en los nodos Sin embargo, a menos que uses nombres muy cortos, es probable que el texto no se ajuste dentro de los nodos. Para evitar esto, puede agregar etiquetas a los nodos utilizando el argumento labels en dagify (). Trace las etiquetas configurando use_labels =\" label \" en ggdag (). Puedes desactivar el texto en los nodos con text = FALSE enggdag (). simple_dag_with_coords_and_labels &lt;- dagify( y ~ x + a + b, x ~ a + b, exposure = &quot;x&quot;, outcome = &quot;y&quot;, labels = c(y = &quot;Resultado&quot;, x = &quot;Tratamiento&quot;, a = &quot;Confundidor 1&quot;, b = &quot;Confundidor 2&quot;), coords = list(x = c(x = 1, a = 2, b = 2, y = 3), y = c(x = 2, a = 1, b = 3, y = 2)) ) ggdag_status(simple_dag_with_coords_and_labels, use_labels = &quot;label&quot;, text = FALSE) + guides(fill = FALSE, color = FALSE) + # Quitar la leyenda theme_dag() Figura 10.13: DAG con etiquetas en los nodos 10.6.4 Encontrar caminos y sets de ajustes con R R también puede realizar análisis en objetos DAG. Por ejemplo, podemos encontrar todas las implicaciones comprobables del DAG utilizando la función impliedConditionalIndependencies () del paquete dagitty. Para este DAG simple, solo hay uno: a debe ser independiente deb. Si tuviéramos una base de datos con columnas para cada una de estas variables, podríamos verificar si esto es cierto ejecutando cor (a, b) para ver si las dos están relacionadas. library(dagitty) impliedConditionalIndependencies(simple_dag) ## a _||_ b También podemos encontrar todas las rutas entre x ey usando la función paths () del paquete dagitty. Podemos ver que hay tres caminos abiertos entre x ey: paths(simple_dag) ## $paths ## [1] &quot;x -&gt; y&quot; &quot;x &lt;- a -&gt; y&quot; &quot;x &lt;- b -&gt; y&quot; ## ## $open ## [1] TRUE TRUE TRUE El primer camino abierto está bien: queremos una única relación dconectada entre el tratamiento y el control, pero las otras dos indican que hay confusión entre a yb. Podemos ver cuáles son cada una de estas rutas con la función ggdag_paths () del paquete ggdag: ggdag_paths(simple_dag_with_coords) + theme_dag() Figura 10.14: Todos los caminos posibles entre e En vez de enumerar todos los caminos posibles e identificar puertas traseras a mano, puede usar la función settingsSets () en el paquete dagitty para buscar todos los nodos que necesitan ser ajustados. Aquí vemos que tanto a comob deben controlarse para aislar la relación x -&gt; y. adjustmentSets(simple_dag) ## { a, b } También puedes visualizar los sets de ajustes con ggdag_adjustment_set () en el paquete ggdag. Asegúrate de establecer shadow = TRUE para dibujar las flechas que salen de los nodos ajustados ya que de manera predeterminada, no están incluidos. ggdag_adjustment_set(simple_dag_with_coords, shadow = TRUE) + theme_dag() Figura 10.15: Sets de ajuste en un DAG R encontrará conjuntos de ajustes mínimamente suficientes, que incluyen el menor número de ajustes necesarios para cerrar todas las puertas traseras entre x ey. En este ejemplo, el DAG solo tiene un set de variables (a yb), pero en otras situaciones podría haber muchos sets posibles, o ninguno si el efecto causal no es identificable. Ejercicio 10B. En el Capítulo 6, hiciste una regresión múltiple para estimar los determinantes de la desigualdad en América Latina y el Caribe (Huber et al. (2006)). Para este ejercicio, dibujarás un DAG que modele el efecto causal de la diversidad étnica sobre la desigualdad social. Haz lo siguiente: Enumera todas las variables que utilizaste en ese capítulo (PIB, inversión extranjera directa, gasto en salud, etc.) y cualquier otra variable que parezca relevante para explicar la desigualdad social. Dibuja un DAG inicial a mano en papel o en una pizarra y considera cuidadosamente las relaciones causales entre los diferentes nodos. Dibuja el DAG con DAGitty. Asigne la desigualdad como el resultado y la diversidad étnica como la variable independiente. Si alguno de tus nodos no es observado, asígnelos como latentes. Determine para qué nodos deben ajustarse. Dibuja el DAG en R con ggdag(). 10.7 Haciendo ajustes causales A lo largo de este capítulo, hemos hablado sobre el ajuste de los confusores para cerrar los caminos de las puertas traseras, pero hasta ahora no hemos explorado cómo hacer realmente estos ajustes. No hay un método único y correcto para ajustar los nodos. Matemáticamente, el ajuste significa eliminar la variación que proviene de los confusores fuera de los nodos de tratamiento y control. Por ejemplo, en el DAG de la figura @ref(fig: money-votes-simple), eliminamos el efecto de la calidad del candidato sobre el dinero recaudado, eliminamos el efecto de la calidad del candidato sobre el total de votos, y luego comparamos el efecto sin confusión del dinero recaudado sobre el total de votos. Podemos explicar el efecto de la calidad del candidato estratificando nuestra muestra en candidatos de alta y baja calidad, realizando una regresión que incluya la calidad del candidato como covariable, encontrando pares coincidentes en los datos que tengan valores de calidad similares, o ponderando las observaciones por calidad. En esta sección, recorreremos tres formas comunes de hacer ajustes: regresión múltiple, emparejamiento y ponderación de probabilidad inversa. 10.7.1 Datos de redes para mosquitos En lugar de utilizar una base de datos real de ciencia política, exploraremos diferentes métodos para hacer ajustes utilizando datos simulados que generé sobre un programa de desarrollo internacional diseñado para reducir el riesgo de malaria utilizando mosquiteros. Los mismos métodos y principios se aplican al análisis utilizando datos reales, pero es imposible conocer el verdadero efecto causal preexistente en los datos reales de observación, por lo que no hay forma de comparar las estimaciones con la “verdad”. Sin embargo, debido a que nuestros datos son simulados, sabemos que la (hipotética) verdad de que el uso de mosquiteros causa que el riesgo de malaria disminuya, en promedio, en 10 puntos. A los investigadores les interesa saber si el uso de mosquiteros disminuye el riesgo de que una persona contraiga malaria. Para ello han recopilado datos de 1.752 hogares en un país sin nombre y tienen variables relacionadas con los factores ambientales, salud individual y las características de los hogares. Además, este país tiene un programa especial que proporciona mosquiteros gratuitos a los hogares que cumplen con requisitos específicos: para calificar para el programa, debe haber más de 4 miembros del hogar, y el ingreso mensual del hogar debe ser inferior a 700 dólares al mes. Los hogares no se inscriben automáticamente en el programa, y muchos no lo usan. Los datos no son experimentales - los investigadores no tienen control sobre quién usa los mosquiteros, y los hogares individuales toman sus propias decisiones sobre si solicitan mosquiteros gratuitos o compran sus propios mosquiteros, así como si usan los mosquiteros si los tienen. mosquito_dag &lt;- dagify( riesgo_malaria ~ red + ingreso + salud + temperatura + resistencia, red ~ ingreso + salud + temperatura + elegible + hogar, elegible ~ ingreso + hogar, salud ~ ingreso, exposure = &quot;red&quot;, outcome = &quot;riesgo_malaria&quot;, coords = list(x = c(riesgo_malaria = 7, red = 3, ingreso = 4, salud = 5, temperatura = 6, resistencia = 8.5, elegible = 2, hogar = 1), y = c(riesgo_malaria = 2, red = 2, ingreso = 3, salud = 1, temperatura = 3, resistencia = 2, elegible = 3, hogar = 2)), labels = c(riesgo_malaria = &quot;Riesgo de malaria&quot;, red = &quot;Mosquitero&quot;, ingreso = &quot;Ingreso&quot;, salud = &quot;Salud&quot;, temperatura = &quot;temperaturas nocturnas&quot;, resistencia = &quot;Resistencia a insecticidas&quot;, elegible = &quot;Eligible para el programa&quot;, hogar = &quot;Miembros en el hogar&quot;) ) ggdag_status(mosquito_dag, use_labels = &quot;label&quot;, text = FALSE) + theme_dag() Figura 10.16: DAG del efecto de un hipotético programa de mosquiteros sobre el riesgo de malaria El gráfico causal de la figura 10.16 esboza la relación completa entre el uso de mosquiteros y el riesgo de malaria. Cada nodo en el DAG es una columna en la base de datos recogida por los investigadores, e incluye lo siguiente: Riesgo de malaria (riesgo_malaria): La probabilidad de que alguien en el hogar se infecte con malaria. Se mide en una escala de 0-100, con valores más altos que indican un mayor riesgo. Mosquitero (red y red_cant): Una variable binaria que indica si el hogar utiliza mosquiteros. Elegible para el programa (elegible): Una variable binaria que indica si el hogar es elegible para el programa de mosquiteros gratuitos. Ingresos (ingreso): Los ingresos mensuales del hogar, en dólares americanos. Temperaturas nocturnas (temperatura): La temperatura media nocturna, en grados centígrados. Salud (salud): La salud de la familia se declarada por sus propios miembros. Medido en una escala de 0-100, con valores más altos que indican mejor salud. Número en el hogar (hogar): Número de personas que viven en el hogar. Resistencia a los insecticidas (resistencia): Algunas cepas de mosquitos son más resistentes a los insecticidas y, por lo tanto, suponen un mayor riesgo de infectar a las personas con malaria. Esto se mide en una escala de 0-100, con valores más altos que indican una mayor resistencia. Según el DAG, el riesgo de contraer malaria es causado por los ingresos, las temperaturas, la salud, la resistencia a los insecticidas y el uso de mosquiteros. Las personas que viven en regiones más cálidas, tienen ingresos más bajos, tienen peor salud, están rodeadas de mosquitos con alta resistencia a los insecticidas y los que no utilizan mosquiteros corren un riesgo mayor de contraer malaria que los que los utilizan. El uso de los mosquiteros se explica por los ingresos, las temperaturas nocturnas, la salud, el número de personas que viven en la vivienda y la elegibilidad para el programa de mosquiteros gratuitos. Las personas que viven en zonas más frescas por la noche, tienen ingresos más altos, mejor salud, tienen más personas en el hogar, y son elegibles para los mosquiteros gratuitos del gobierno, tienen más probabilidades de usar regularmente los mosquiteros. El DAG también muestra que la elegibilidad para el programa de mosquiteros gratuitos se explica por los ingresos y el tamaño de los núcleos familiares, ya que los hogares deben cumplir con umbrales específicos para calificar. Podemos cargar los datos y usar glimpse() para ver las primeras observaciones de cada columna: library(tidyverse) library(paqueteadp) data(red_mosquitos) glimpse(red_mosquitos) ## Rows: 1,752 ## Columns: 10 ## $ codigo &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, … ## $ red &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, F… ## $ red_cant &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,… ## $ riesgo_malaria &lt;dbl&gt; 33, 42, 80, 34, 44, 25, 19, 35, 32, 40, 30, 14,… ## $ ingreso &lt;dbl&gt; 781, 974, 502, 671, 728, 1050, 1146, 1093, 1037… ## $ salud &lt;dbl&gt; 56, 57, 15, 20, 17, 48, 65, 75, 60, 36, 75, 62,… ## $ hogar &lt;dbl&gt; 2, 4, 3, 5, 5, 1, 3, 5, 3, 3, 6, 3, 4, 3, 1, 5,… ## $ elegible &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE,… ## $ temperatura &lt;dbl&gt; 21, 26, 26, 21, 19, 25, 27, 30, 28, 21, 17, 19,… ## $ resistencia &lt;dbl&gt; 59, 73, 65, 46, 54, 34, 45, 65, 55, 54, 33, 39,… 10.7.2 Verificar las independencias condicionales Antes de proceder a identificar la relación causal entre el uso de mosquiteros y el riesgo de malaria, debemos comprobar primero si las relaciones definidas por nuestro DAG reflejan la realidad de los datos. Recordemos que la dseparación implica que los nodos son estadísticamente independientes entre sí y no transfieren información asociativa. Si dibujas el DAG en la Figura 10.16 en DAGitty, o si ejecutas impliedConditionalIndependencies() en R, puedes ver una lista de todas las independencias condicionales implícitas. impliedConditionalIndependencies(mosquito_dag) ## elegible _||_ resistencia ## elegible _||_ riesgo_malaria | ingreso, red, salud, temperatura ## elegible _||_ salud | ingreso ## elegible _||_ temperatura ## hogar _||_ ingreso ## hogar _||_ resistencia ## hogar _||_ riesgo_malaria | ingreso, red, salud, temperatura ## hogar _||_ salud ## hogar _||_ temperatura ## ingreso _||_ resistencia ## ingreso _||_ temperatura ## red _||_ resistencia ## resistencia _||_ salud ## resistencia _||_ temperatura ## salud _||_ temperatura Por motivos de espacio aquí, no verificaremos todas estas independencias implícitas, pero podemos probar algunas de ellas: \\(\\text{Salud} \\perp \\text{Miembros en el hogar}\\): La salud debe ser independiente del número de personas en cada hogar. En los datos, las dos variables no deben estar correlacionadas. Este es el caso: cor(red_mosquitos$salud, red_mosquitos$hogar) ## [1] 9.8e-05 \\(\\text{Income} \\perp \\text{Insecticide resistencia}\\): Income should be independent of insecticide resistencia. This is again true: cor(red_mosquitos$ingreso, red_mosquitos$resistencia) ## [1] 0.014 \\(\\text{Riesgo de malaria} \\perp \\text{Miembros en el hogar}\\ |\\ \\text{Salud, Ingreso, Uso de la red, Temperatura}\\): El riesgo de malaria debe ser independiente del número de miembros de la familia, dado que los niveles de salud, ingresos, uso de mosquiteros y temperaturas nocturnas son similares. No podemos utilizar cor() para probar esta implicación, ya que hay muchas variables en juego, pero podemos utilizar un modelo de regresión para comprobar si el número de miembros del hogar está significativamente relacionado con el riesgo de malaria. No es significativo (\\(t = -0.17\\), \\(p = 0.863\\)), lo que significa que los dos son independientes, como se esperaba. lm(riesgo_malaria ~ hogar + salud + ingreso + red + temperatura, data = red_mosquitos) %&gt;% broom::tidy() ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 76.2 0.966 78.9 0. ## 2 hogar -0.0155 0.0893 -0.173 8.63e- 1 ## 3 salud 0.148 0.0107 13.9 9.75e-42 ## # … with 3 more rows Después de comprobar todas las demás dependencias condicionales, podemos saber si nuestro DAG captura la realidad del sistema completo de factores que influyen en el uso de mosquiteros y el riesgo de malaria. Si hay correlaciones sustanciales y significativas entre los nodos que deberían ser independientes, es probable que haya un problema con la especificación del DAG. Si es así, vuelves a la teoría y refinas más tu DAG. 10.7.3 Encuentra el set de ajuste Existe una vía directa entre el uso de mosquiteros y el riesgo de contraer malaria, pero el efecto no se identifica causalmente debido a otras varias vías abiertas. Podemos enumerar todos los caminos y encontrar cuáles tienen flechas que apuntan hacia atrás en el nodo del mosquitero (ejecuta paths(mosquito_dag) para ver estos resultados), o podemos dejar que R encuentre los sets de ajuste apropiados automáticamente: adjustmentSets(mosquito_dag) ## { ingreso, salud, temperatura } Basándose en las relaciones entre todos los nodos del DAG, el ajuste por salud, ingresos y temperatura es suficiente para cerrar todas las puertas traseras e identificar la relación entre el uso de la red y el riesgo de malaria (ver Figura 10.17). Es importante destacar que no tenemos que preocuparnos por ninguno de los nodos relacionados con el programa gubernamental de redes gratuitas, ya que esos nodos no están d conectados con el riesgo de malaria. Sólo tenemos que preocuparnos por las relaciones de confusión. Podemos confirmar esto gráficamente con ggdag_adjustment_set(): ggdag_adjustment_set(mosquito_dag, shadow = TRUE, use_labels = &quot;label&quot;, text = FALSE) Figura 10.17: Adjustment set to identify the relationship between mosquito red use and malaria risk 10.7.4 Estimación ingenua no ajustada Como base de referencia para los otros enfoques de ajuste que intentaremos, podemos ver primero cuál es la relación entre el uso de mosquiteros y el riesgo de malaria en ausencia de cualquier ajuste. Si creamos un cuadro de la distribución del riesgo de malaria entre las personas que usan y no usan mosquiteros, vemos que el riesgo medio es sustancialmente menor entre los que usan mosquiteros (véase la figura 10.18). ggplot(red_mosquitos, aes(x = red , y = riesgo_malaria)) + geom_boxplot() + cowplot::theme_cowplot(font_size = 11, font_family = &quot;LM Roman 10&quot;) Figura 10.18: Distribución del riesgo de malaria entre quienes usaron y no usaron mosquiteros Podemos ejecutar un simple modelo de regresión para medir la diferencia promedio exacta: model_naive &lt;- lm(riesgo_malaria ~ red , data = red_mosquitos) texreg::screenreg(model_naive) ## ## ======================== ## Model 1 ## ------------------------ ## (Intercept) 41.94 *** ## (0.40) ## redTRUE -16.33 *** ## (0.65) ## ------------------------ ## R^2 0.27 ## Adj. R^2 0.27 ## Num. obs. 1752 ## ======================== ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Según este modelo, parece que el uso de un mosquitero está asociado con una disminución de 16 puntos en el riesgo de malaria. Sin embargo, este no es el efecto causal. Este es un caso en el que la correlación no es igual a la causalidad. Otros factores como los ingresos, la salud y las temperaturas confunden la relación entre el uso de los mosquiteros y el riesgo. 10.7.5 Regresiones Como ya está familiarizado con los modelos de regresión múltiple del capítulo 6, una forma rápida y fácil de tratar de ajustar las variables de confusión es incluirlas como covariables en una regresión lineal. A primera vista, esto tiene sentido intuitivo: el objetivo del ajuste es comparar los nodos de tratamiento y de resultado con los mismos valores de los diversos confusores, y el propósito de la regresión múltiple es explicar la variación del resultado manteniendo constantes las diferentes variables explicativas. Sin embargo, hacer ajustes de confusión con la regresión lineal dará como resultado relaciones causales correctamente identificadas sólo en circunstancias muy específicas. Para que el ajuste basado en la regresión funcione, las relaciones entre todos los nodos de tratamiento, resultado y confusión deben ser lineales, lo cual es difícil de probar y verificar con datos de observación reales. Casi siempre es mejor utilizar una de las otras técnicas de ajuste que se describen a continuación (matching o ponderación de probabilidad inversa), ya que esos métodos no se basan en el supuesto de la linealidad. Con esta importante advertencia, podemos ajustar nuestras puertas traseras incluyéndolas en un modelo de regresión: model_regression &lt;- lm(riesgo_malaria ~ red + ingreso + temperatura + salud, data = red_mosquitos) texreg::screenreg(model_regression) ## ## ======================== ## Model 1 ## ------------------------ ## (Intercept) 76.16 *** ## (0.93) ## redTRUE -10.44 *** ## (0.26) ## ingreso -0.08 *** ## (0.00) ## temperatura 1.01 *** ## (0.03) ## salud 0.15 *** ## (0.01) ## ------------------------ ## R^2 0.89 ## Adj. R^2 0.88 ## Num. obs. 1752 ## ======================== ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Según estos resultados, el uso de un mosquitero causa una disminución de 10.44 puntos en el riesgo de paludismo, en promedio. Nótese que debido a que hemos ajustado los confusores, ahora podemos usar justificadamente el lenguaje causal en lugar de hablar simplemente de asociaciones. 10.7.6 Matching El principal problema de utilizar datos de observación en lugar de datos experimentales es que las personas que utilizaron un mosquitero lo hicieron sin ser asignadas a un grupo de tratamiento. Las características individuales llevaron a las personas a auto-seleccionarse para el tratamiento, lo que hace que las personas que usaron mosquiteros sean esencialmente diferentes de las que no lo hicieron. Gracias al DAG, conocemos muchos de los factores que causaron que la gente eligiera usar los mosquiteros: ingresos, salud y temperatura. Si pudiéramos agrupar las observaciones que son similares entre sí en cuanto a ingresos, salud y temperatura, pero que difieren en el uso de los mosquiteros, podríamos simular el tratamiento experimental y los grupos de control y, posiblemente, calcular un efecto causal más exacto, ya que estamos encontrando la estimación entre grupos comparables. En el capítulo 7 se exploró la idea de matching para identificar estudios de casos relevantes en una investigación de métodos mixtos. Allí se crearon puntuaciones de propensión que estimaban la probabilidad de tratamiento, y luego se identificaron casos con puntuaciones de propensión similares (o muy diferentes). Si bien el emparejamiento con los puntajes de propensión es popular, puede causar problemas cuando se utiliza para la identificación causal (véase King and Nielsen (2019)). Sin embargo, existen alternativas. Una técnica común es encontrar coincidencias que minimicen la distancia entre los diferentes factores de confusión para cada observación. Para ilustrar este concepto muy brevemente, considere un modelo causal en el que la edad y la educación son los únicos factores de confusión de la relación entre un tratamiento y un resultado. Algunas de las observaciones se autoseleccionaron en el tratamiento, mientras que el resto no. El panel A de la figura 10.19 muestra la distribución del tratamiento auto-seleccionado a través de diferentes valores de edad y educación. Hay algunos patrones notables: sólo una observación tratada tiene más de 25 años de educación, y sólo una tiene menos de 15 años de edad. Si queremos crear grupos sintéticos de tratamiento y control a partir de estos datos, no tendría sentido comparar la única observación tratada de alto nivel educativo con observaciones no tratadas de bajo nivel educativo, ya que las dos observaciones tienen niveles muy diferentes de educación. Idealmente, queremos encontrar la observación altamente educada no tratada más cercana y usarla como comparación con la única observación altamente educada tratada. Si podemos encontrar un conjunto de observaciones sin tratar que estén cerca en distancia de las observaciones tratadas, podemos crear un grupo equilibrado de observaciones tratadas y no tratadas. El panel B de la figura 10.19 muestra cada uno de los pares más cercanos de observaciones tratadas y no tratadas. Podemos descartar cualquier observación sin tratar y sólo usar el grupo emparejado para hacer nuestra inferencia causal, ya que ahora aparentemente tenemos grupos de tratamiento y control comparables. Hay múltiples maneras de medir esta distancia - Mahalanobis y distancia euclidiana son comunes, pero no son los únicos métodos. También hay múltiples métodos para definir pares. En la figura 10.19, cada observación tratada se empareja con una única observación no tratada, pero también es posible permitir la duplicación y triplicación de los emparejamientos. El ejemplo de la figura 10.19 minimiza la distancia entre dos dimensiones de las variables de confusión (edad y educación), pero se pueden utilizar tantas variables de confusión como sea necesario para crear pares de coincidencia. Figura 10.19: Matching basado en la distancia Mahalanobis del vecino más cercano Podemos usar matching para ajustar los factores de confusión en nuestro DAG con la relación entre usar la red de mosquitos y el riesgo de malaria. La función matchit() en el paquete Matchit proporciona muchos métodos de emparejamiento diferentes, incluyendo la concordancia de Mahalanobis del vecino más cercano. No hay un método de concordancia que sea necesariamente el mejor, y deberás jugar con las diferentes opciones para los argumentos de method y distance. El argumento replace = TRUE permite que las observaciones tratadas se emparejen con más de una observación no tratada. La función matchit() utiliza una sintaxis familiar basada en fórmulas para definir cómo hacer coincidir la asignación con los mosquiteros. Debido a que nuestro set de ajuste incluye ingresos, temperatura y salud, usamos las tres para crear las coincidencias. library(MatchIt) matched &lt;- matchit(red ~ ingreso + temperatura + salud, data = red_mosquitos, method = &quot;nearest&quot;, distance = &quot;mahalanobis&quot;, replace = TRUE) matched ## ## Call: ## matchit(formula = red ~ ingreso + temperatura + salud, ## data = red_mosquitos, ## method = &quot;nearest&quot;, distance = &quot;mahalanobis&quot;, replace = TRUE) ## ## Sample sizes: ## Control Treated ## All 1071 681 ## Matched 439 681 ## Unmatched 632 0 ## Discarded 0 0 Según el resultado de matched, las 681 observaciones tratadas (es decir, las que utilizaron mosquiteros) se emparejaron con 439 observaciones no tratadas (es decir, personas que son similares en cuanto a ingresos, temperatura nocturna y salud, pero que no utilizaron mosquiteros). 632 observaciones no tratadas no fueron emparejadas y serán descartadas. Si hubiéramos usado replace = FALSE, habría habido un número igual de observaciones tratadas y no tratadas; hay menos observaciones no tratadas aquí porque algunas se duplican o triplican. Podemos crear una nueva base de datos basada en este matching con la función match.data(). Observa cómo ahora hay sólo 1120 filas en lugar de 1752, ya que hemos descartado las observaciones no emparejadas. También nota que hay una nueva columna llamada weights. La función matchit() asigna a los pares de observaciones emparejadas diferentes pesos dependiendo de lo cerca o lejos que estén los emparejamientos en un intento de controlar la variación de la distancia. Podemos utilizar estos pesos en nuestro modelo de regresión para mejorar nuestra estimación del efecto causal. red_mosquitos_matched &lt;- match.data(matched) glimpse(red_mosquitos_matched) ## Rows: 1,120 ## Columns: 12 ## $ codigo &lt;dbl&gt; 1, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 16, 17, 20… ## $ red &lt;lgl&gt; TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, F… ## $ red_cant &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,… ## $ riesgo_malaria &lt;dbl&gt; 33, 80, 34, 44, 25, 19, 35, 30, 14, 31, 34, 54,… ## $ ingreso &lt;dbl&gt; 781, 502, 671, 728, 1050, 1146, 1093, 889, 1133… ## $ salud &lt;dbl&gt; 56, 15, 20, 17, 48, 65, 75, 75, 62, 42, 53, 29,… ## $ hogar &lt;dbl&gt; 2, 3, 5, 5, 1, 3, 5, 6, 3, 4, 3, 5, 2, 6, 4, 3,… ## $ elegible &lt;lgl&gt; FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE,… ## $ temperatura &lt;dbl&gt; 21, 26, 21, 19, 25, 27, 30, 17, 19, 28, 24, 26,… ## $ resistencia &lt;dbl&gt; 59, 65, 46, 54, 34, 45, 65, 33, 39, 37, 53, 55,… ## $ distance &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ weights &lt;dbl&gt; 1.00, 0.64, 1.00, 0.64, 1.29, 1.00, 1.93, 0.64,… Por último, podemos hacer una regresión usando los datos emparejados: model_matched &lt;- lm(riesgo_malaria ~ red , data = red_mosquitos_matched, weights = weights) texreg::screenreg(model_matched) ## ## ======================== ## Model 1 ## ------------------------ ## (Intercept) 36.09 *** ## (0.60) ## redTRUE -10.49 *** ## (0.76) ## ------------------------ ## R^2 0.14 ## Adj. R^2 0.14 ## Num. obs. 1120 ## ======================== ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Según estos resultados, el uso de un mosquitero causa una disminución de 10.49 puntos en el riesgo de malaria, en promedio. Una vez más, podemos utilizar el lenguaje causal ahora porque hemos ajustado los confusores al hacer la comparación, identificando así el camino causal entre el uso de mosquiteros y el riesgo de malaria. 10.7.7 Ponderación de probabilidad inversa Una desventaja del uso de matching es que desechamos mucha información: las observaciones no coincidentes se descartan, y el tamaño de nuestra muestra puede reducirse significativamente. El emparejamiento también tiende a ser muy riguroso, ya que cada observación tratada debe ser emparejada con una (o más) observaciones no tratadas. Miren el Panel B en la Figura 10.19 y noten que algunas observaciones no tratadas están en realidad muy cerca de las observaciones tratadas, pero aún así se descartan porque fueron superadas por observaciones que tienen una distancia ligeramente menor. En lugar de desechar datos potencialmente útiles, podemos utilizar otros métodos para crear coincidencias que sean menos rigurosas pero más informativas. Un método común en epidemiología y bioestadística es la ponderación de probabilidad inversa (IPW, en inglés). En la IPW, a cada observación se le asigna un peso basado en lo bien que su asignación real al tratamiento coincide con la probabilidad prevista del tratamiento, y esos pesos se utilizan luego en un modelo de regresión para estimar el efecto causal del tratamiento en el resultado. Podemos ilustrar este proceso con el ejemplo de educación y edad de la figura 10.19. En lugar de emparejar por matching, utilizamos una regresión logística para crear puntuaciones de propensión al tratamiento: data(&quot;edad_educ&quot;) # desde el paquete &quot;paqueteadp&quot; edad_educ &lt;- edad_educ %&gt;% mutate(tratamiento = factor(tratamiento)) model_tratamiento &lt;- glm(tratamiento ~ educacion + edad, data = edad_educ, family = binomial(link = &quot;logit&quot;)) edad_educ_propensiones &lt;- broom::augment_columns( model_tratamiento, edad_educ, type.predict = &quot;response&quot; ) %&gt;% rename(propension = .fitted) Podemos mirar unas pocas filas en los datos para ver estos puntajes de propensión (o propensity scores). La persona 59 tenía una probabilidad del 14% de auto-seleccionarse para el tratamiento dada su educación y edad, pero terminó no haciendo el tratamiento, como se predijo. La persona 27, por otro lado, también tenía una probabilidad del 14% de elegir el tratamiento y lo hizo, lo cual es un resultado bastante improbable. Esa elección es inesperada! edad_educ_propensiones %&gt;% select(codigo, tratamiento, educacion, edad, propension) %&gt;% slice(59, 27) ## # A tibble: 2 x 5 ## codigo tratamiento educacion edad propension ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 59 Control 28.9 47.0 0.147 ## 2 27 Tratamiento 21.7 63.2 0.148 Con matching, estábamos interesados en emparejar las observaciones inesperadas y esperadas, recordemos que necesitábamos encontrar una observación no tratada altamente educada para emparejarla con la única observación tratada altamente educada. Hacemos lo mismo aquí. Las observaciones que es improbable que reciban tratamiento y luego no lo reciben siguen nuestras expectativas y deberían tener menos peso. Por el contrario, las observaciones que con poca probabilidad recibirán tratamiento deberían tener más peso. La ponderación de la probabilidad inversa nos permite asignar un valor numérico a lo inesperado de las observaciones. Utilizamos la siguiente fórmula para calcular los pesos de probabilidad inversa para un efecto medio del tratamiento, donde \\(\\text{Tratamiento}\\) es una variable binaria 0/1:45 \\[ \\frac{\\text{Tratamiento}}{\\text{Propensión}} + \\frac{1 - \\text{Tratamiento}}{1 - \\text{Propensión}} \\] Podemos agregar pesos de probabilidad inversa a nuestros puntajes de propensión pronosticados con mutate(). Compara los valores de ipw de las personas 59 y 27. Recuerde que la persona 59 tenía una baja probabilidad de seleccionar en el tratamiento y no lo hizo, como se esperaba. Su peso de probabilidad inversa es sólo 1.17. La persona 27, por otro lado, tenía una baja probabilidad de estar en el tratamiento y sin embargo superó las probabilidades y se unió al tratamiento, en contra de lo esperado. En consecuencia, tienen un alto peso de probabilidad inversa de 6.78. edad_educ_peso_invertido &lt;- edad_educ_propensiones %&gt;% mutate(peso_invertido = (num_tratamiento / propension) + (1 - num_tratamiento) / (1 - propension)) edad_educ_peso_invertido %&gt;% select(codigo, tratamiento, educacion, edad, propension, peso_invertido) %&gt;% slice(59, 27) ## # A tibble: 2 x 6 ## codigo tratamiento educacion edad propension peso_invertido ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 59 Control 28.9 47.0 0.147 1.17 ## 2 27 Tratamiento 21.7 63.2 0.148 6.78 Dado que cada observación en la base de datos tiene una puntuación de probabilidad inversa, no necesitamos desechar ningún dato. En su lugar, podemos ponderar cada observación por su puntuación de probabilidad inversa. La figura 10.20 muestra los datos completos con los puntos dimensionados por su IPW. Las observaciones que cumplen las expectativas reciben menos peso que las que se comportan de forma contraria a las expectativas. ggplot(edad_educ_peso_invertido, aes(x = educacion, y = edad, color = tratamiento, size = peso_invertido)) + geom_point() Figura 10.20: Observaciones dimensionadas por pesos de probabilidad inversa Podemos ajustar los factores de confusión de nuestro mosquitero y el ejemplo de riesgo de malaria usando una ponderación de probabilidad inversa. Primero, usamos una regresión logística para predecir la propensión a usar un mosquitero usando los ingresos, la temperatura y la salud. Luego usamos los puntajes de propensión para calcular los pesos de probabilidad inversa usando esta fórmula: \\[ \\frac{\\text{Mosquitero}}{\\text{Propensión}} + \\frac{1 - \\text{Mosquitero}}{1 - \\text{Propensión}} \\] model_red_mosquitos &lt;- glm(red ~ ingreso + temperatura + salud, data = red_mosquitos, family = binomial(link = &quot;logit&quot;)) red_mosquitos_ipw &lt;- broom::augment_columns(model_red_mosquitos, red_mosquitos, type.predict = &quot;response&quot;) %&gt;% rename(propension = .fitted) %&gt;% mutate(peso_invertido = (red_cant / propension) + (1 - red_cant) / (1 - propension)) Ahora que tenemos pesos de probabilidad inversa, podemos usarlos en una regresión: model_peso_invertido &lt;- lm(riesgo_malaria ~ red , data = red_mosquitos_ipw, weights = peso_invertido) texreg::screenreg(model_peso_invertido) ## ## ======================== ## Model 1 ## ------------------------ ## (Intercept) 39.68 *** ## (0.47) ## redTRUE -10.13 *** ## (0.66) ## ------------------------ ## R^2 0.12 ## Adj. R^2 0.12 ## Num. obs. 1752 ## ======================== ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Según este modelo de IPW, el uso de un mosquitero causa una disminución de 10.13 puntos en el riesgo de malaria, en promedio. Una vez más, podemos utilizar con seguridad el lenguaje causal porque hemos identificado el camino causal entre el uso de mosquiteros y el riesgo de malaria teniendo en cuenta los factores de confusión en los pesos de probabilidad inversos. 10.7.8 Comparando todos los métodos Ahora que hemos ejecutado varios modelos de regresión que se ajustan a los factores de confusión de diferentes maneras, podemos comparar los resultados todos juntos. La ingenua estimación de -16 parece ser definitivamente una sobreestimación: después de ajustar con la regresión, el matching y la ponderación de probabilidad inversa, el efecto causal del uso de un mosquitero en el riesgo de malaria es consistentemente de alrededor de -10. Asumiendo que nuestro DAG está correcto, encontramos con éxito un efecto causal a partir de datos no experimentales y observacionales. texreg::screenreg(list(model_naive, model_regression, model_matched, model_peso_invertido), custom.model.names = c(&quot;Naive&quot;, &quot;Regresión&quot;, &quot;Matching&quot;, &quot;IPW&quot;)) ## ## =============================================================== ## Naive Regresión Matching IPW ## --------------------------------------------------------------- ## (Intercept) 41.94 *** 76.16 *** 36.09 *** 39.68 *** ## (0.40) (0.93) (0.60) (0.47) ## redTRUE -16.33 *** -10.44 *** -10.49 *** -10.13 *** ## (0.65) (0.26) (0.76) (0.66) ## ingreso -0.08 *** ## (0.00) ## temperatura 1.01 *** ## (0.03) ## salud 0.15 *** ## (0.01) ## --------------------------------------------------------------- ## R^2 0.27 0.89 0.14 0.12 ## Adj. R^2 0.27 0.88 0.14 0.12 ## Num. obs. 1752 1752 1120 1752 ## =============================================================== ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Ejercicio 10B: En el ejercicio 10A, dibujaste un DAG que modeló la relación causal entre la diversidad étnica y la desigualdad social. En este ejercicio utilizarás el set de ajustes de ese DAG para intentar estimar el efecto causal de esa relación. Haz lo siguiente: Carga la base de datos de bienestar del paquete del libro: library(paqueteadp) data(&quot;bienestar&quot;) Tu variable dependiente es el índice de Gini (gini). Tu variable de tratamiento es la diversidad étnica (diversidad_etnica), una variable binaria que es 1 si entre el 20-80% de la población es étnicamente diversa (y 0 si no). Use el DAG que hiciste anteriormente para determinar el ajuste mínimo suficiente. ¿Qué nodos deben ser ajustados para asegurar que se identifique el camino entre la diversidad étnica y la desigualdad? Construye un modelo de correlación no causal (que llamamos de ingenuo o naive) para probar la relación entre la diversidad y la desigualdad (es decir, lm(gini ~ diversidad_etnica, data = bienestar)). ¿Cómo se asocia la diversidad con la desigualdad? ¿Por qué esta estimación no es causal? Usar una regresión múltiple para cerrar las puertas traseras. Incluya las variables de tu set de ajuste como variables explicativas en un modelo de regresión. Usa matching para cerrar las puertas traseras. Usa las variables de tu set de ajuste para hacer coincidir la asignación de las observaciones con el tratamiento, luego usa las observaciones emparejadas en un modelo de regresión. Usa la ponderación de probabilidad inversa para cerrar las puertas traseras. Usa las variables de tu conjunto de ajustes para generar puntuaciones de propensión para la asignación al tratamiento, y luego crea ponderaciones de probabilidad inversas y usa esas ponderaciones en un modelo de regresión. ¿Cómo se comparan estos efectos ajustados con el modelo “ingenuo”? ¿Cuánta confianza tienes en que estos son efectos causales? ¿Por qué? ¿Qué podría hacer para mejorar su identificación causal? 10.8 CConsejos finales Los DAG son herramientas poderosas para incorporar la teoría y la filosofía de tu modelo causal en la estimación estadística de la relación entre tratamiento y control. Sin embargo, diseñar un DAG no te garantiza automáticamente la capacidad de hablar con lenguaje causal. Los gráficos causales no resuelven el problema fundamental de la inferencia causal, no crean una máquina del tiempo que permita ver los resultados en un mundo contrafactual. El supuesto central cuando se utilizan los DAG es que es posible identificar y aislar completamente una relación causal utilizando sólo datos observables. Los críticos de este supuesto a menudo desprecian este enfoque de selección de datos observables. Si se omite un nodo importante porque no es observable, la estimación causal será incorrecta y sesgada, independientemente de si se utiliza matching, la ponderación de probabilidad inversa o cualquier otro método para aislar relaciones entre nodos. Los nodos en los DAG pueden representar variables observables reales en una base de datos, pero no siempre es así. Recordemos que los gráficos causales codifican nuestra filosofía del proceso de generación de datos: cada nodo representa algún fenómeno que tiene alguna influencia en algún punto de la cadena de eventos que finalmente causa el tratamiento y el resultado. Los nodos no necesitan ser medibles. En la figura 10.6 incluimos un nodo para la historia del distrito electoral, que influye tanto en la dinámica de los partidos como en las características del distrito. Es imposible destilar toda la historia de un distrito en un solo número en un conjunto de datos. Podríamos intentar hacerlo dividiendo el nodo de “historia” en componentes más pequeños (y más mensurables), como el partido de los candidatos anteriores, las decisiones sobre los límites del distrito o la riqueza de los dirigentes y partidarios del partido, pero incluso así, debemos sentirnos cómodos declarando que estos nodos más pequeños representan toda la historia de un distrito. Es una suposición difícil. Contrariamente a las críticas al enfoque de selección sobre observables inherente a los gráficos causales, la presencia de nodos inobservables en un DAG no impide que los utilicemos para aislar e identificar los efectos causales. Debido a la lógica de la dseparación - y a la forma en que hemos dibujado las flechas en el DAG, podemos excluir la historia inobservable de un distrito en la Figura 10.6 porque su nodo está bloqueado por los nodos de distrito y partido. Si creemos que nuestro DAG es correcto y que estos dos nodos son de hecho suficientes para bloquear toda la variación no medida del nodo de la historia, podemos proceder con seguridad a estimar el efecto causal. Si la historia inobservable de un distrito pasa de las asociaciones estadísticas a la recaudación de fondos de campaña o el total de votos a través de otras vías no bloqueadas, no podremos identificar el efecto causal con datos observacionales. Al final, tus estimaciones causales son tan buenas como sea tu DAG. Si accidentalmente omites nodos de varaibles de confusión, o si no puede medir los nodos de confusión, o si incluyes nodos que son colisionadores, los resultados serán erróneos. Además, las relaciones entre los nodos observados y no observados podrían ser demasiado complejas para ser resueltas usando el ajuste de cálculo hacer. A menos que estés completamente seguro de que tu DAG refleja el proceso real de generación de datos y la teoría subyacente del fenómeno que estás estudiando y que los ajustes que hagas son suficientes para identificar la relación causal, evita siempre confiar demasiado en los resultados y mantén una actitud de incertidumbre. E-mail: aheiss@gsu.edu↩︎ Esto también puede escribirse como \\(P(Y | hacer(X))\\), de la distribución de probabilidad de \\(Y\\) dado que “haces” \\(X\\).↩︎ Hay muchas otras versiones de ponderaciones de probabilidad inversa que tienen por objeto estimar otras cantidades causales, como el promedio de tratamiento en el efecto tratado, el promedio de tratamiento entre el efecto igualado y el promedio de tratamiento entre el efecto de la población superpuesta. Vale la pena ver esta entrada en el blog de Lucy D’Agostino McGowan para más detalles: https://livefreeordichotomize.com/2019/01/17/understanding-propension-score-weighting/↩︎ "],
["adv-data.html", "Capítulo 11 Manejo avanzado de datos políticos 11.1 Introducción 11.2 Uniendo bases de datos 11.3 Estandarizando los códigos de país 11.4 La unión borrosa o inexacta de los datos 11.5 Gestión de valores perdidos 11.6 Imputación de valores perdidos", " Capítulo 11 Manejo avanzado de datos políticos Andrés Cruz46 y Francisco Urdinez47 Lecturas sugeridas Graham, B. A., &amp; Tucker, J. R. (2019). The international political economy data resource. The Review of International Organizations, 14(1), 149-161. Lall, R. (2016). How multiple imputation makes a difference. Political Analysis, 24(4), 414-433. Los paquetes que necesitas instalar tidyverse (Wickham 2019b), paqueteadp (Urdinez and Cruz 2020), skimr (Waring et al. 2020), countrycode (Arel-Bundock 2020), stringdist (van der Loo 2019), naniar (Tierney et al. 2020), mice (van Buuren and Groothuis-Oudshoorn 2020), remotes (Hester et al. 2020), inexact (Cruz 2020). 11.1 Introducción En el presente capítulo abordaremos dos problemas comunes que surgen cuando se utilizan datos de fuentes diferentes. El primero de ellos consiste en unir bases de datos, proceso que a menudo se aborda utilizando una o varias variables de identificación (por ejemplo, el nombre de un país, el código oficial de una unidad subnacional, un código creado por uno mismo). En algunos casos, cuando las variables de identificación están bien normalizadas, el proceso se vuelve sencillo. Por ejemplo, en las bases de datos oficiales del gobierno, los países tienen un código único. En todos ellos el Brasil será “BRA”, y Chile será “CHI”, lo que facilita el trabajo en R. Sin embargo, en el mundo real, las diferentes fuentes de datos no están hechas para trabajar juntas. Después de mirar una herramienta para estandarizar los códigos de país, countrycode, mostraremos una solución general, la unión fuzzy o inexacta de datos. Considere el siguiente escenario: su principal base de datos tiene una variable llamada country, donde Brasil está codificado en mayúsculas como “BRAZIL”. Entonces, quieres unir tus datos con los del Banco Mundial, donde la información de Brasil está etiquetada como “Brazil”. Hay casos extremos, como la diferencia entre “Venezuela” y su nombre oficial, “República Bolivariana de Venezuela”. Es probable que se enfrente a este tipo de diferencias en los países de su base de datos. ¿Cómo podemos resolver este rompecabezas de una manera rápida y efectiva? El segundo problema es que, en la mayoría de los casos, nuestras bases de datos contienen valores perdidos. Esto ocurre por múltiples razones: error de codificación, gobiernos que no registran la información, etcétera. En la subsección, aprenderá cómo R registra y trabaja con los valores perdidos, además de algunas herramientas que permiten explorarlos en sus bases de datos. Un tema específico que abordaremos es el de las imputaciones. Cuando en una regresión hay valores perdidos, R simplemente elimina la observación que tiene el valor perdido, ya sea en su variable dependiente, variable independiente o controles (esta eliminación se llama listwise deletion). Imaginemos que queremos comparar diez países de América Latina en la evolución de sus tasas de desempleo entre 2008 y 2018, y sin embargo uno de ellos no tiene datos para el trienio 2011-2013. ¿Podemos, tal vez, llenar esos valores “adivinando” los valores no observados? Este proceso recibe el nombre de imputación. La decisión de imputar o no es una decisión del investigador. La imputación será adecuada dependiendo de si los datos faltan de forma aleatoria o no. Este dilema está presente cuando los datos se usan para hacer gráficos de tendencia temporal y no queremos que estén incompletos. Además, en algunos modelos avanzados, como los espaciales, el hecho de tener valores perdidos puede inhibirnos de usar comandos. Para estos casos, podemos considerar la imputación de datos. Hacer esto tiene un costo ya que, como toda solución, no es perfecta. A lo largo de este capítulo utilizaremos la base de datos de tratados internacionales creada por Carsten Schulz basada en el [repositorio de tratados internacionales existentes de las Naciones Unidas] (https://tratados.un.org/). Este repositorio alberga todos los tratados internacionales celebrados entre Estados, con sus textos e información sobre los firmantes. La base de datos del ejemplo se simplifica, ya que sólo tiene cuatro tratados internacionales en lugar de los cientos que utiliza Schulz para estudiar las razones que motivan la decisión de un país de adherirse a un acuerdo internacional. Comencemos cargando la base de datos de nuestro paquete, paqueteadp: library(tidyverse) library(paqueteadp) data(&quot;tratados&quot;) Ahora, la base de datos ha sido cargado en nuestra sesión de R. ls() ## [1] &quot;tratados&quot; La unidad de análisis de la base de datos es el país del tratado. Cada observación contiene información sobre el proceso de incorporación del tratado a nivel nacional, es decir, de su firma y ratificación. Estas acciones se clasifican en la variable accion_tipo, que va acompañada de la variable accion_anio. Esta última registra el año en que se llevó a cabo cada acción. Echemos un vistazo a la base de datos con glimpse(): glimpse(tratados) ## Rows: 248 ## Columns: 5 ## $ nombre_tratado &lt;chr&gt; &quot;Tratado de Prohibición Completa de los Ensayos… ## $ anio_adopcion &lt;dbl&gt; 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996,… ## $ pais_nombre &lt;chr&gt; &quot;Antigua y Barbuda&quot;, &quot;Antigua y Barbuda&quot;, &quot;Arge… ## $ accion_tipo &lt;chr&gt; &quot;Ratificación&quot;, &quot;Firma&quot;, &quot;Ratificación&quot;, &quot;Firma… ## $ accion_anio &lt;dbl&gt; 2006, 1997, 1998, 1996, 2007, 2005, 2008, 2008,… Nuestra base de datos sólo contiene información de 31 países americanos y sus respuestas (firma, ratificación) a cuatro tratados pertinentes de los años 90. El Tratado de Prohibición Completa de los Ensayos Nucleares (1996). El Protocolo de Kyoto de la Convención Marco de las Naciones Unidas sobre el Cambio Climático (1997). El Convenio de Rotterdam sobre el procedimiento de consentimiento fundamentado previo aplicable a ciertos plaguicidas y productos químicos peligrosos objeto de comercio internacional (1998) El Estatuto de Roma, acto constitutivo de la Corte Penal Internacional (1998). Así, para cada tratado tenemos 62 observaciones (31 países, 2 respuestas posibles para cada uno): tratados %&gt;% count(nombre_tratado) ## # A tibble: 4 x 2 ## nombre_tratado n ## &lt;chr&gt; &lt;int&gt; ## 1 &quot;Convenio de Rotterdam sobre el Procedimiento de Consentimiento … 62 ## 2 &quot;Estatuto de Roma de la Corte Penal Internacional&quot; 62 ## 3 &quot;Protocolo de Kioto de la Convención Marco de las Naciones Unida… 62 ## # … with 1 more row 11.2 Uniendo bases de datos Un ejercicio común en los estudios de observación es unir bases de datos de diferentes fuentes. Supongamos que, por ejemplo, tenemos el siguiente resumen sobre cuántos tratados de la muestra han sido firmados o ratificados por cada país: resumen_tratados &lt;- tratados %&gt;% group_by(pais_nombre) %&gt;% summarize( # Encontraremos los que no faltan con !is.na() sum_signed = sum(accion_tipo == &quot;Firma&quot; &amp; !is.na(accion_anio)), sum_ratif = sum(accion_tipo == &quot;Ratificación&quot; &amp; !is.na(accion_anio)) ) resumen_tratados ## # A tibble: 31 x 3 ## pais_nombre sum_signed sum_ratif ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Antigua y Barbuda 3 3 ## 2 Argentina 4 4 ## 3 Bahamas 2 1 ## # … with 28 more rows Al explorar la base de datos, un caso interesante es el de los Estados Unidos, que firmó los cuatro tratados pero no los ratificó: resumen_tratados %&gt;% filter(pais_nombre == &quot;Estados Unidos&quot;) ## # A tibble: 0 x 3 ## # … with 3 variables Sería interesante explorar la relación entre las respuestas de los países a los tratados y algunas otras variables sobre ellos. A modo de ejemplo, cargaremos datos en panel con información sobre el PIB per cápita con paridad de cambio, según el Banco Mundial. data(&quot;pib_pc_america&quot;) Tenemos información de los 31 países de interés entre 1996 y 1998: pib_pc_america %&gt;% count(pais_nombre) ## # A tibble: 31 x 2 ## pais_nombre n ## &lt;chr&gt; &lt;int&gt; ## 1 Antigua and Barbuda 3 ## 2 Argentina 3 ## 3 Bahamas (the) 3 ## # … with 28 more rows pib_pc_america %&gt;% count(anio) ## # A tibble: 3 x 2 ## anio n ## &lt;int&gt; &lt;int&gt; ## 1 1996 31 ## 2 1997 31 ## 3 1998 31 Con esta base de datos, obtenemos el cambio medio del PIB per cápita con paridad de poder adquisitivo para el período 1996-1998: resumen_pib &lt;- pib_pc_america %&gt;% group_by(pais_nombre) %&gt;% summarize(mean_pib_pc = mean(pib_pc)) Ahora tenemos dos bases de datos de resumen, resumen_tratados y resumen_pib, ambos a nivel de país. ¿Cómo podemos unir su información? En este caso particular, ambos tienen 31 filas (observaciones). Una opción que hay que evitar es simplemente pegar una base de datos junto a la otra (por ejemplo, con la función bind_cols()). En algunos casos, esta opción puede ser una buena idea, pero suele ser una elección arriesgada: es difícil saber si ambas bases de datos están ordenadas exactamente de la misma manera necesaria para unirlas correctamente, especialmente cuando el número de observaciones es grande. Por lo tanto, normalmente queremos guiar la unión por una o más variables de identificación presentes en ambas bases de datos. En este caso, la columna pais_nombre puede guiar la unión: queremos añadir información sobre el PIB a nuestra base de datos si y sólo si hay un match exacto entre sus nombres de países. En el código, podemos añadir nuevas variables a nuestra base de datos de otro con left_join()48: resumen_completo &lt;- left_join(x = resumen_tratados, y = resumen_pib, # Podemos proporcionar explícitamente la identificación #nombre variable: by = &quot;pais_nombre&quot;) Este código dará los mismos resultados usando pipes: resumen_completo &lt;- resumen_tratados %&gt;% left_join(resumen_pib, by = &quot;pais_nombre&quot;) resumen_completo ## # A tibble: 31 x 4 ## pais_nombre sum_signed sum_ratif mean_pib_pc ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Antigua y Barbuda 3 3 NA ## 2 Argentina 4 4 15362. ## 3 Bahamas 2 1 NA ## # … with 28 more rows También es posible unir bases de datos utilizando más de una variable de identificación. Por ejemplo, podríamos unir nuestras bases de datos originales, tratados y pib_pc_america, ambos con la unidad de observación del año del país. Sin embargo, hay una particularidad. En ambos casos, la variable país se llama pais_nombre, pero el nombre de la variable año no es el mismo: en el primero, el nombre es anio_adopcion (para la fecha original del tratado), y en el segundo es anio. Podemos proporcionar fácilmente esta información a left_join() con el argumento by =: tratados_con_pib &lt;- tratados %&gt;% left_join(pib_pc_america, by = c(&quot;pais_nombre&quot;, &quot;anio_adopcion&quot; = &quot;anio&quot;)) Así pues, a la base de datos original de los “tratados”, que tenía 248 observaciones y 5 columnas, añadimos la columna adicional con información sobre el PIB per cápita con paridad de tipos de cambio para cada año de participación en el tratado: tratados_con_pib ## # A tibble: 248 x 6 ## nombre_tratado anio_adopcion pais_nombre accion_tipo accion_anio pib_pc ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Tratado de Pr… 1996 Antigua y … Ratificaci… 2006 NA ## 2 Tratado de Pr… 1996 Antigua y … Firma 1997 NA ## 3 Tratado de Pr… 1996 Argentina Ratificaci… 1998 14557. ## # … with 245 more rows Ejercicio 11A. Descargue el World Economics and Politics (WEP) Dataverse y elija diez variables de país-año, incluyendo tanto las características institucionales como económicas de los estados como nuevas variables que se añadirán a tratados_con_pib y unirlas. ¿Fue fácil encontrar los identificadores únicos (códigos, nombres)? 11.3 Estandarizando los códigos de país Puedes notar que los nombres de los países en las bases de datos con los que estamos trabajando son muy particulares. Por ejemplo, Bolivia es “Bolivia (Estado Plurinacional de)” y la República Dominicana es “República Dominicana (la)”. Estos son sus nombres oficiales para la comunidad internacional. Probablemente queramos unirlos con datos de otras fuentes que no tienen los mismos nombres: podemos tener “Bolivia”, “Estado Plurinacional de Bolivia”, “Plurinational State of Bolivia”, etcétera. Para estos casos, que son muy comunes para los que estudian política comparada y relaciones internacionales, la mejor opción es tener códigos estandarizados, algo que también podemos encontrar para las divisiones subnacionales y otros tipos de unidades de análisis. La mayor parte de la codificación reduce sustancialmente los nombres de los países a un puñado de caracteres y/o números. En general, éstos provienen de organizaciones internacionales o de grandes proyectos académicos que los apoyan. El paquete de countrycode nos permite transformar diferentes codificaciones y estándares con facilidad, lo que nos ayuda en las operaciones posteriores con las bases de datos. library(countrycode) Después de cargar el paquete, podemos comprobar en el archivo de ayuda ?codelist las diferentes codificaciones permitidas. Después de esto, usando nuestra variable inicial y la función countrycode(), crearemos una variable con códigos estandarizados. iso2c y iso3c son algunas de las codificaciones más comunes, que reducen los países a 2 o 3 caracteres, según la ISO (Organización Internacional de Normalización). Mientras tanto, cown y imf utilizan codificaciones numéricas relativamente comunes en las ciencias sociales, que provienen del proyecto Correlates of War y del Fondo Monetario Internacional, respectivamente[^countrycode permite muchas otras codificaciones utilizadas en nuestra disciplina, algunas más completas que otras. Dos de ellas, que prevalecen en las ciencias políticas, son las de Polity IV y el proyecto Varieties of Democracy (V-Dem). Le recomendamos que revise el archivo de ayuda ?codelist para más información.] resumen_completo_con_codigos &lt;- resumen_completo %&gt;% mutate( pais_iso2c = countrycode(pais_nombre, origin = &quot;un.name.es&quot;, destination = &quot;iso2c&quot;, custom_dict = codelist), pais_iso3c = countrycode(pais_nombre, origin = &quot;un.name.es&quot;, destination = &quot;iso3c&quot;, custom_dict = codelist), pais_iso3c = countrycode(pais_nombre, origin = &quot;un.name.es&quot;, destination = &quot;cown&quot;, custom_dict = codelist), pais_iso3c = countrycode(pais_nombre, origin = &quot;un.name.es&quot;, destination = &quot;imf&quot;, custom_dict = codelist), ) resumen_completo_con_codigos %&gt;% select(starts_with(&quot;pais_&quot;)) ## # A tibble: 31 x 3 ## pais_nombre pais_iso2c pais_iso3c ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Antigua y Barbuda AG 311 ## 2 Argentina AR 213 ## 3 Bahamas BS 313 ## # … with 28 more rows Tener una base de datos con al menos un código de normalización es particularmente útil, ya que a menudo reduce la fricción de unir nuestros datos con otros. El countrycode, entonces, es una buena herramienta para pre-procesar las bases de datos con la información del país antes de unirlos. 11.4 La unión borrosa o inexacta de los datos Aunque el country code ayuda enormemente en algunos casos, no siempre es lo suficientemente flexible. A veces, en lugar de necesitar traducir entre codificaciones, no tenemos un estándar para guiar la unión, es todo un desastre. Piensa en los nombres de las personas: en una base datos de ex-presidentes de Brasil, la misma persona puede ser codificada como Lula, Luiz Inácio Lula da Silva, Lula da Silva. ¿Y si la persona se cambió el nombre o se le conoce por un apodo? El legislador chileno José Manuel Ismael Edwards Silva es conocido como Rojo Edwards. Estos problemas del mundo real pueden causar grandes dolores de cabeza a los que trabajan con datos políticos. La unión difusa es la solución para estas situaciones. Carguemos una base de datos igual a nuestro anterior resumen_pib, pero con nombres desordenados en los países: data(&quot;resumen_desordenado&quot;) unique(resumen_desordenado$pais_nombre) ## [1] &quot;Antigua &amp; Barbuda&quot; &quot;Argentina&quot; &quot;Bahamas&quot; ## [4] &quot;Barbados&quot; &quot;Belice&quot; &quot;Bolivia&quot; ## [7] &quot;Brasil&quot; &quot;Canadá&quot; &quot;Chile&quot; ## [10] &quot;Colombia&quot; &quot;Costa Rica&quot; &quot;Dominica&quot; ## [13] &quot;Rep. Dominicana&quot; &quot;Ecuador&quot; &quot;El Salvador&quot; ## [16] &quot;Granada&quot; &quot;Guatemala&quot; &quot;Guyana&quot; ## [19] &quot;Haití&quot; &quot;Honduras&quot; ## [ reached getOption(&quot;max.print&quot;) -- omitted 11 entries ] Los nombres de esta base de datos no corresponden a ninguna normalización, sino que son el resultado de una codificación manual. Un ejemplo claro es la abreviatura de “Rep. Dominicana”. ¿Qué pasaría si unimos nuestra base de datos resumen_tratados con una base de datos confuso como este? resumen_tratados %&gt;% left_join(resumen_desordenado, by = &quot;pais_nombre&quot;) ## # A tibble: 31 x 4 ## pais_nombre sum_signed sum_ratif pib_pc_prom ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Antigua y Barbuda 3 3 NA ## 2 Argentina 4 4 15362. ## 3 Bahamas 2 1 29530. ## # … with 28 more rows Como pueden observar, la unión falla para las observaciones múltiples porque no se encuentra una coincidencia exacta para la variable de identificación. Ese es el caso de “Antigua y Barbuda” en la base de datos original, y “Antigua &amp; Barbuda” en el adicional. R considera que son unidades diferentes. Intuitivamente, estos valores son similares, pero R no puede adivinar o asumir sin nuestras instrucciones precisas. Afortunadamente, tenemos una solución para este tipo de casos, así como con situaciones similares para nombres de políticos, partidos políticos, empresas o regiones. La intuición inicial es la siguiente: Las cadenas de texto Antigua y Barbuda y Antigua &amp; Barbuda son similares. Comparten letras, tienen longitudes similares, etcétera. Ambas tienen más en común que, por ejemplo, el par “Antigua y Barbuda” y “Argentina”. Los científicos de la computación, reconociendo este hecho, han desarrollado múltiples algoritmos para asignar un puntaje de diferencia entre dos cadenas de texto, cualquiera que sea. La mayoría de estos algoritmos asignan un valor de 0 cuando las cadenas son iguales, y luego aumenta de acuerdo a su disimilitud. Basándonos en lo anterior, podríamos calcular las distancias entre todos los posibles pares de cadenas en nuestras dos variables de identificación. Luego, podríamos recuperar los valores más bajos para hacer los pares, y finalmente, podríamos realizar un fuzzy join. Como ejemplo, usaremos el paquete stringdist para generar una matriz con todas estas distancias para nuestros dos bases de datos: library(stringdist) stringdistmatrix(resumen_tratados$pais_nombre, resumen_desordenado$pais_nombre, # Usaremos el algoritmo por defecto, llamado &quot;osa&quot; o # &quot;Alineación óptima de las cuerdas&quot; method = &quot;osa&quot;, useNames = T) ## Antigua &amp; Barbuda Argentina Bahamas Barbados ## Antigua y Barbuda 1 14 15 13 ## Argentina 14 0 9 9 ## Bahamas 15 9 0 4 ## Barbados 13 9 4 0 Mirando la primera fila, podemos ver que la cadena “Antigua y Barbuda” obtiene una puntuación de 1 en comparación con “Antigua &amp; Barbuda”, mientras que recibe una puntuación de 14 en comparación con “Argentina”. Aunque los algoritmos disponibles en stringdist son robustos, no son infalibles, y la supervisión humana es a menudo necesaria. Para realizar un fuzzy join utilizaremos un paquete de trabajo en curso desarrollado por Andrés Cruz, inexact, que combina los algoritmos de stringdist con la supervisión humana. Permite hacer una unión borrosa supervisada por un humano. Después de instalar inexact, podemos usarlo en la pestaña Addins de RStudio, o ejecutando el siguiente comando: inexact::inexact_addin() Aparecerá una ventana como la de la figura 11.1, en la que tendremos que proporcionar las características de la unión que estamos tratando de realizar. Figura 11.1: Primer panel de inexact, donde hay que seleccionar las opciones de unión. Después de hacer clic en Next, aparecerá una ventana de supervisión de inexact, como podemos observar en la Figura 11.2. En esta ventana sólo aparecerán los pares que tengan una distancia superior a 0 (es decir, los imperfectos). Por defecto, estos estarán ordenados desde el más conflictivo al menos conflictivo, en términos de las distancias calculadas. Puede comprobar cada par y modificarlos manualmente según sus conocimientos. En este caso, uno de los seis pares por defecto del algoritmo está equivocado: se emparejó “Bolivia (Estado Plurinacional de)” con “Estados Unidos”. Podemos corregir esto fácilmente seleccionando “Bolivia” en la lista de opciones. Figura 11.2: Segundo panel de inexact, donde supervisamos los pares por defecto. Por último, después de hacer clic en Next, llegaremos a una ventana final, donde se presenta un código que nos permite hacer la unión fuzzy, tal como muestra la Figura 11.3. (ref:cap-inexact-3) Tercer panel de inexact, con el código final. Figura 11.3: (ref:cap-inexact-3) El código y sus resultados se presentan a continuación. Fíjate en cómo el argumento custom_match = es el que permite a inexact::inexact_join() modificar los pares por defecto del algoritmo. Ahora podemos unir ambas bases de datos perfectamente, ¡aunque no tengan variables completamente estandarizadas en común! # You added custom matches: inexact::inexact_join( x = resumen_tratados, y = resumen_desordenado, by = &#39;pais_nombre&#39;, method = &#39;osa&#39;, mode = &#39;left&#39;, custom_match = c( &#39;Bolivia (Estado Plurinacional de)&#39; = &#39;Bolivia&#39; ) ) ## pais_nombre sum_signed sum_ratif ## 1: Antigua y Barbuda 3 3 ## 2: Argentina 4 4 ## 3: Bahamas 2 1 ## 4: Barbados 3 2 ## 5: Belice 2 2 ## pib_pc_prom ## 1: 18965 ## 2: 15362 ## 3: 29530 ## 4: 14863 ## 5: 5980 ## [ reached getOption(&quot;max.print&quot;) -- omitted 27 rows ] Ejercicio 11B. La siguiente base de datos tiene los nombres e información de los diputados argentinos en 2019. Descárguelo y combínelo con el siguiente base de datos que contiene la información de las comisiones de las que cada diputado forma parte. Utilice códigos y nombres únicos para unir ambas variables y comparar los resultados. 11.5 Gestión de valores perdidos Ya nos hemos encontrado con valores perdidos (NAs) en algunas ocasiones, incluyendo la sección introductoria. En el contexto de R, se trata de valores especiales que pueden asumir vectores que designan que hay información faltante. Sin embargo, los valores perdidos no son sólo una rareza de la programación. Tomarlos de forma suelta puede implicar serios sesgos en nuestra investigación. Por lo tanto, abordaremos el tema con mayor detalle. Discutiremos las imputaciones, una posible técnica para modificar los sesgos producidos por los valores perdidos. 11.5.1 Tipos de valores perdidos Aunque R simplemente registra NA en la célula perdida, es importante entender teóricamente el tipo de valor perdido con el que estamos tratando. En resumen, la literatura de imputación identifica cuatro tipos diferentes. Los valores faltantes pueden ser estructurales, faltantes completamente al azar (MCAR, en inglés), faltantes al azar (MAR, en inglés) y faltantes no al azar (MNAR, en inglés). Primero, examinaremos el caso de los datos estructurales faltantes. Veamos el siguiente ejemplo de la base de datos de nuestros tratados: tratados %&gt;% filter(pais_nombre == &quot;Estados Unidos de América&quot; &amp; nombre_tratado == &quot;Tratado de Prohibición Completa de los Ensayos Nucleares&quot;) %&gt;% select(pais_nombre, accion_tipo, accion_anio) ## # A tibble: 2 x 3 ## pais_nombre accion_tipo accion_anio ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Estados Unidos de América Ratificación NA ## 2 Estados Unidos de América Firma 1996 La variable accion_anio codifica el año en que, en este caso los Estados Unidos, tomó medidas sobre el Tratado de Prohibición Completa de los Ensayos Nucleares (1996). Estados Unidos firmó el tratado en 1996, pero nunca lo ratificó, como observamos anteriormente. En otras palabras, el año de la ratificación simplemente no existe para los Estados Unidos, como podría ser el caso de otros países. Por lo tanto, los datos que faltan son estructurales cuando faltan datos porque no existen. En segundo lugar, tenemos datos que faltan completamente al azar (MCAR) si hubo algún proceso de aleatorización en el trabajo cuando se generaron los datos. Por ejemplo, si hemos hecho ocho preguntas a cada persona al azar en una encuesta de diez preguntas. Las dos preguntas que faltan para cada persona no se explican por variables relacionadas con el encuestado (su ideología, edad, género, religión, etc.), porque por diseño elegimos no hacerlas al azar. Otros casos de datos que faltan completamente al azar pueden ocurrir sin la interferencia del diseño, pero debido a eventos no planeados que afectan nuestro proceso de recolección de datos. Por otro lado, nuestros datos podrían faltar al azar (MAR), además del resto de las variables que nos permitirán modelar la distribución de las celdas perdidas. Por ejemplo, pensemos en el Índice Compuesto de Capacidad Nacional (CINC, en inglés) del proyecto Correlatos de Guerra (COW, en inglés). Este índice es un proxy del poder nacional tal y como lo entiende la escuela de realismo. Combina seis indicadores de poder duro: consumo de energía primaria, población total, población urbana, producción de acero y hierro, gasto militar y número de tropas militares. El índice varía de 0 a 1, ya que representa la participación de cada país en el poder mundial total en un año determinado. Supongamos que tenemos veinte valores faltantes para algunos países-años. Probablemente tenemos otras variables para describir estos países-años que teóricamente se correlacionan con el índice. Podríamos hacer un modelo para tratar de “adivinar” nuestros valores perdidos a través de la información que observamos. Esto es lo que llamamos una “imputación”. Finalmente, nuestros datos podrían faltar no al azar (MNAR). Este es posiblemente el escenario más frecuente en el que los datos se imputan incorrectamente en la ciencia política, cuando en realidad no deberían. Los valores faltantes no aleatorios son aquellos cuya condición faltante está correlacionada con otra variable de tal manera que hay un patrón en los datos faltantes. Cuando nos enfrentamos a un escenario en el que hay datos perdidos, debemos considerar cuidadosamente qué variables pueden estar explicando estos datos perdidos, y si existe un sesgo de selección. Por ejemplo, si utilizamos datos del Banco Mundial sobre el crecimiento del PIB, es posible que los países pobres no presenten ningún dato. Esto se debe a que la calidad de las mediciones en estos países no es óptima. También es posible que no se hayan hecho mediciones si el país está pasando por un momento difícil en su economía, lo que podría tener efectos negativos en sus estadísticas nacionales. Por ejemplo, desde 2016 el Banco Mundial no reporta las tasas de inflación de Venezuela. Idealmente, es necesario corregir este sesgo a través de una co-variable en el modelo utilizando modelos de selección (desafortunadamente, no los abordamos en el libro). Después de explicar el caso de ejemplo y presentar las estadísticas descriptivas de los valores faltantes, haremos dos escenarios para las imputaciones. El primero es una imputación con fines descriptivos (creación de gráficos). El segundo, quizás más útil, será para corregir posibles sesgos en el modelo de regresión. 11.5.2 Descripción de los valores que faltan en la base de datos Comencemos cargando algunos datos sobre la capacidad del estado recogidos del proyecto Correlatos de Guerra, que hemos mencionado anteriormente. Nuestra muestra tiene sólo dos países, Estados Unidos y China, con datos anuales desde 1860 hasta 2012. Por lo tanto, la unidad de observación de la base de datos es el país-año. library(paqueteadp) data(&quot;indice_cinc&quot;) Tenemos dos variables con códigos estandarizados de países, pais_iso3c (categórico) y pais_cown (numérico), además de anio, que denota el año. A continuación, gasto_militar es el gasto militar del país, cuerpo_militar es el tamaño del ejército en hombres activos, hierro_acero denota la producción de acero y hierro, cons_energia corresponde al consumo de energía, poblacion es la población total, y poblacion_urb es la población urbana. Por último, indice_capacidades es el índice compuesto por las capacidades materiales que nos interesan. Esto refleja la parte de poder global que cada país posee. Si observamos el valor de los dos países más poderosos del mundo, notaremos que China superó a los Estados Unidos hace un par de años: indice_cinc %&gt;% filter(anio == 2012) %&gt;% select(pais_iso3c, indice_capacidades) ## # A tibble: 2 x 2 ## pais_iso3c indice_capacidades ## &lt;chr&gt; &lt;dbl&gt; ## 1 USA 13.9 ## 2 CHN 21.8 En la literatura de Relaciones Internacionales existe un debate sobre el alcance de este índice en relación con la participación real en el poder mundial total de los países, que se puede observar en Chan (2005) y Xuetong (2006). Empecemos describiendo los valores que faltan de la base de datos. Usando el resumen rápido de skimr::skim(), observamos en la Figura 11.4 que gasto_militar y poblacion_urb son las dos variables en las que están presentes, teniendo 49 y 68 valores perdidos, respectivamente. Figura 11.4: Skim de nuestra base de datos El paquete naniar contiene herramientas visuales para ayudarnos a entender mejor los posibles patrones de nuestros valores perdidos. Una forma de explorar visualmente estos últimos viene con la función gg_miss_var(): library(naniar) gg_miss_var(indice_cinc) Figura 5.9: Los valores perdidos por variable en nuestra base de datos De manera similar, sólo podemos presentar los porcentajes con el argumento show_pct = T: gg_miss_var(indice_cinc, show_pct = T) Figura 3.28: Valores perdidos por variable en nuestra base de datos, expresados como porcentaje de las observaciones ¿Dónde están nuestros valores perdidos? Una inspección visual de la base de datos, por ejemplo, con View(), nos permite tener una idea en bases de datos de tamaño pequeño y mediano. En cualquier caso, la función gg_mis_fct() de naniar es a menudo útil, que nos permite separar las NA según una variable categórica de nuestra base de datos. Hagámoslo por país: gg_miss_fct(x = indice_cinc, fct = pais_iso3c) Figura 3.30: Gráfica alternativa. Valores perdidos por variable en nuestra base de datos, expresados como un porcentaje de las observaciones Rápidamente observamos que todos los valores que nos faltan corresponden a China. En contraste, sólo hay observaciones completas para los Estados Unidos. También puede ser interesante explorar la relación entre los valores perdidos de una variable y una columna numérica de la base de datos, en lugar de una categórica. Para esta operación ligeramente más compleja, naniar proporciona la función bind_shadow(), que genera variables shadow a partir de las de nuestra base de datos. Estas variables contienen sólo la información binaria de los valores que faltan o no faltan para cada observación. indice_cinc_perdidos &lt;- indice_cinc %&gt;% bind_shadow(only_miss = T) Con esta nueva base de datos podemos generar gráficos con ggplot2 que pueden ayudarnos a explorar la distribución de los valores perdidos (según nuestras nuevas variables de las shadows) en relación con el año (variables del anio). ggplot(indice_cinc_perdidos, aes(x = anio, fill = poblacion_urb_NA)) + geom_histogram(binwidth = 1) + # el resto es para hacer la gráfica más legible: scale_x_continuous(breaks = seq(1860, 2020, 10)) + scale_y_continuous(breaks = c(1, 2)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) Figura 5.10: Diagnosticar para responder a la pregunta: \"¿En qué años hay NAs para la población urbana? ggplot(indice_cinc_perdidos, aes(x = anio, fill = gasto_militar_NA)) + geom_histogram(binwidth = 1) + # el resto es para hacer la gráfica más legible: scale_x_continuous(breaks = seq(1860, 2020, 10)) + scale_y_continuous(breaks = c(1, 2)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) Figura 5.11: Diagnosticar para responder a la pregunta: \"¿En qué años hay NAs para la población urbana? En el caso de poblacion_urb, la población urbana del país, los valores que faltan se recogen claramente en los primeros años de la base de datos. Lo mismo se aplica para el gasto_militar, aunque ocasionalmente hay datos completos para unos pocos años más que en el poblacion_urb. 11.6 Imputación de valores perdidos Observemos en un diagrama de dispersión la evolución del Índice Compuesto de Capacidad Nacional de China y Estados Unidos: ggplot(indice_cinc) + geom_point(aes(x = anio, y = indice_capacidades, group = pais_iso3c, color = pais_iso3c)) + scale_x_continuous(breaks = seq(1860, 2020, 10)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) Figura 11.5: En lugar de mirar el índice de capacidades materiales como una línea, vemos cada año como un punto para observar mejor los valores que faltan ¿Cómo se vería el gráfico si los Estados Unidos no tuviera datos entre 1950 y 1970? indice_cinc2 &lt;- indice_cinc %&gt;% mutate(indice_capacidades = if_else(pais_iso3c == &quot;USA&quot; &amp; anio %in% 1950:1970, NA_real_, indice_capacidades)) Entonces, observamos el gráfico: ggplot(indice_cinc2) + geom_point(aes(x = anio, y = indice_capacidades, group = pais_iso3c, color = pais_iso3c)) + scale_x_continuous(breaks = seq(1860, 2020, 10)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) ## Warning: Removed 21 rows containing missing values (geom_point). Figura 5.13: A la figura le faltan datos de los Estados Unidos entre 1950 y 1970 Ahora, es importante pensar en el tipo de valores perdidos que tenemos. Supongamos que no hay motivos declarados para asumir los valores perdidos en la base de datos, por lo tanto, se pierden al azar. Para hacer la imputación usaremos mice (abreviatura en inglés para Multivariate Imputation via Chained Equations). Esta es una de las muchas opciones que existen para el ecosistema de los paquetes de R, que también incluye el popular paquete Amelia, creado por los cientistas políticos Gary King, James Honaker y Matthew Blackwell. Elegimos ejemplificar con mice, considerando que es uno de los paquetes que se ha mantenido más actualizado. Por cierto, recuerda que estos paquetes asumen que los datos perdidos son MAR. El paquete está diseñado para que podamos indicar cuántas imputaciones queremos hacer por cada valor que falta (con la opción m=, que por defecto hace 5 imputaciones), y nos permite trabajar tanto con datos transversales (como encuestas) como con datos de panel, como es nuestro caso con el índice CINC, que contiene datos de muchos años para cada país. Después de comprender la estructura de los valores que faltan en la base de datos y exponer las hipótesis sobre su aleatoriedad, el segundo paso para la imputación es seleccionar las variables que se utilizarán. Sólo se necesita el id de cada observación (pais_iso3c o code_cowc), la variable temporal (anio) y las variables que se utilizarán para modelar los valores. Nuestra base de datos no tiene variables adicionales que nos gustaría eliminar, pero es una gran idea si quieres aplicar estos pasos en tus propios ejemplos. Llamaremos a la imputación imputacion_inicial. Para ejecutarla, necesitamos indicar a mice una serie de parámetros. Como nuestros datos son de panel, necesitamos declarar cuál es la variable de las unidades de análisis y cuál es la variable temporal. Haremos esto por medio de la función predictor(): pais_cown es nuestra ID de países (numérica) y anio nuestra variable de tiempo. library(mice) imputacion_inicial &lt;- mice(indice_cinc2, maxit = 0) ## Warning: Number of logged events: 1 predictor &lt;- imputacion_inicial$predictorMatrix # fijar pais_cown como la variable de ID predictor[, &quot;pais_cown&quot;] &lt;- -2 # fijar el año como variable temporal (efectos aleatorios) predictor[, &quot;anio&quot;] &lt;- 2 También necesitamos especificar el método por el cual se realizará la imputación, usando el argumento method =. “pmm” (Predictive Mean Matching) funciona para variables numéricas, “logreg” (Logistic Regression) funciona para variables binaria y “polyreg” (Bayesian polytomous regression) es lo recomendado para variables de factores. En este caso, como el índice CINC es una variable continua, usaremos “pmm”. También definiremos un modelo normal en dos niveles con “2l.lmer” (tendría sentido usar “2l.bin”, un modelo logístico de dos niveles, si estuviéramos imputando variables categóricas). metodo_1 &lt;- imputacion_inicial$method metodo_1[which(metodo_1 == &quot;pmm&quot;)] &lt;- &quot;2l.lmer&quot; Una vez que la imputación está creada, la guardaremos como imputacion_mice. La imputación está más o menos escondidas en la lista de objetos que creamos, por lo que le asignaremos un nombre directamente. imputacion_mice &lt;- mice(indice_cinc2, m = 5, seed = 1, method = metodo_1, predictorMatrix = predictor) ## ## iter imp variable ## 1 1 gasto_militar poblacion_urb indice_capacidades ## 1 2 gasto_militar poblacion_urb indice_capacidades ## 1 3 gasto_militar poblacion_urb indice_capacidades ## 1 4 gasto_militar poblacion_urb indice_capacidades ## 1 5 gasto_militar poblacion_urb indice_capacidades ## 2 1 gasto_militar poblacion_urb indice_capacidades ## 2 2 gasto_militar poblacion_urb indice_capacidades ## 2 3 gasto_militar poblacion_urb indice_capacidades ## 2 4 gasto_militar poblacion_urb indice_capacidades ## 2 5 gasto_militar poblacion_urb indice_capacidades ## 3 1 gasto_militar poblacion_urb indice_capacidades ## 3 2 gasto_militar poblacion_urb indice_capacidades ## 3 3 gasto_militar poblacion_urb indice_capacidades ## 3 4 gasto_militar poblacion_urb indice_capacidades ## 3 5 gasto_militar poblacion_urb indice_capacidades ## 4 1 gasto_militar poblacion_urb indice_capacidades ## 4 2 gasto_militar poblacion_urb indice_capacidades ## 4 3 gasto_militar poblacion_urb indice_capacidades ## 4 4 gasto_militar poblacion_urb indice_capacidades ## 4 5 gasto_militar poblacion_urb indice_capacidades ## 5 1 gasto_militar poblacion_urb indice_capacidades ## 5 2 gasto_militar poblacion_urb indice_capacidades ## 5 3 gasto_militar poblacion_urb indice_capacidades ## 5 4 gasto_militar poblacion_urb indice_capacidades ## 5 5 gasto_militar poblacion_urb indice_capacidades Podemos mirar los datos imputados, guardados dentro de imp: imputacion_mice$imp$indice_capacidades ## 1 2 3 4 5 ## 91 21 22 19 26 29 ## 92 23 27 24 22 22 ## 93 16 33 18 26 25 ## 94 14 28 25 26 28 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 17 rows ] Observemos visualmente los valores imputados en la distribución de la variable “indice_capacidades”: stripplot(imputacion_mice, indice_capacidades, pch = 20) Para trabajar con las imputaciones, debemos extraerlas del objeto imputacion_mice. Recuerda que generamos cinco imputaciones (m = 5). Usando la función mice::complete() extraeremos cada una, juntándolas al mismo tiempo con bind_rows(): datos_completos &lt;- bind_rows( mice::complete(imputacion_mice, 1) %&gt;% mutate(num_imp = 1), mice::complete(imputacion_mice, 2) %&gt;% mutate(num_imp = 2), mice::complete(imputacion_mice, 3) %&gt;% mutate(num_imp = 3), mice::complete(imputacion_mice, 4) %&gt;% mutate(num_imp = 4), mice::complete(imputacion_mice, 5) %&gt;% mutate(num_imp = 5), ) %&gt;% select(num_imp, everything()) %&gt;% mutate(fuente = &quot;Imputación específica&quot;) %&gt;% filter(pais_iso3c == &quot;USA&quot; &amp; anio %in% 1950:1970) Calcularemos la media de las imputaciones, para analizar los resultados en un gráfico bivariado. media_imp &lt;- datos_completos %&gt;% group_by(pais_iso3c, pais_cown, anio) %&gt;% summarize(indice_capacidades = mean(indice_capacidades)) %&gt;% ungroup() %&gt;% mutate(fuente = &quot;Average Imp.&quot;) %&gt;% filter(pais_iso3c == &quot;USA&quot; &amp; anio %in% 1950:1970) La siguiente figura muestra las imputaciones individuales en un color claro y la media de estas en un tono más oscuro: ggplot(mapping = aes(x = anio, y = indice_capacidades, group = pais_iso3c, color = pais_iso3c)) + geom_point(data = indice_cinc2) + # add imputed data and its average values: geom_point(data = datos_completos, color = &quot;darkgray&quot;) + geom_point(data = media_imp, color = &quot;black&quot;) + # add vertical lines for emphasis: geom_vline(xintercept = c(1950, 1970), linetype = &quot;dashed&quot;) + scale_x_continuous(breaks = seq(1860, 2020, 10)) + scale_color_manual(values = c(&quot;lightgray&quot;, &quot;black&quot;)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) ## Warning: Removed 21 rows containing missing values (geom_point). Figura 11.6: Cuatro imputaciones (tono claro) más su media (tono oscuro). Es importante remarcar que usar imputaciones para estudios descriptivos es distinto a hacerlo para regresiones, como veremos a continuación. Figura 11.7: El proceso de las imputaciones para un análisis visual se realiza luego de tomar el promedio de las imputaciones individuales de mice. 11.6.1 Regresiones luego de imputar datos A diferencia de cuando visualizamos datos, al hacer regresiones con datos imputados lo recomendado no es tomar el promedio de las imputaciones, sino que obtener el promedio de los coeficientes obtenidos para regresiones hechas por separado con las distintas imputaciones. El proceso es el siguiente: Figura 11.8: El proceso de imputación para regresiones consiste en obtener el promedio de los coeficientes entre múltiples regresiones, no usar los datos promediados en una sola regresión. Primero veamos cómo se vería usando una regresión usando datos incompletos. Usando un modelo muy sencillo, estimaremos el índice de capacidades de cada país de acuerdo a las variables cons_energia y poblacion_urb. modelo_incompleto &lt;- lm(indice_capacidades ~ cons_energia + poblacion_urb, data = indice_cinc2) texreg::screenreg(modelo_incompleto) ## ## ========================= ## Model 1 ## ------------------------- ## (Intercept) 15.32 *** ## (0.54) ## cons_energia 0.00 ** ## (0.00) ## poblacion_urb -0.01 * ## (0.00) ## ------------------------- ## R^2 0.04 ## Adj. R^2 0.03 ## Num. obs. 217 ## ========================= ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 A continuación generaremos cinco regresiones, una por cada base imputada que generamos antes: modelo_imp1 &lt;- lm(indice_capacidades ~ cons_energia + poblacion_urb, data = mice::complete(imputacion_mice, 1)) modelo_imp2 &lt;- lm(indice_capacidades ~ cons_energia + poblacion_urb, data = mice::complete(imputacion_mice, 2)) modelo_imp3 &lt;- lm(indice_capacidades ~ cons_energia + poblacion_urb, data = mice::complete(imputacion_mice, 3)) modelo_imp4 &lt;- lm(indice_capacidades ~ cons_energia + poblacion_urb, data = mice::complete(imputacion_mice, 4)) modelo_imp5 &lt;- lm(indice_capacidades ~ cons_energia + poblacion_urb, data = mice::complete(imputacion_mice, 5)) lista_modelos &lt;- list(modelo_incompleto, modelo_imp1, modelo_imp2, modelo_imp3, modelo_imp4, modelo_imp5) texreg::screenreg( lista_modelos, custom.model.names = c(&quot;M incomp&quot;, &quot;M imp 1&quot;, &quot;M imp 2&quot;, &quot;M imp 3&quot;, &quot;M imp 4&quot;, &quot;M imp 5&quot;) ) ## ## ===================================================================================== ## M incomp M imp 1 M imp 2 M imp 3 M imp 4 M imp 5 ## ------------------------------------------------------------------------------------- ## (Intercept) 15.32 *** 15.02 *** 15.87 *** 15.26 *** 15.81 *** 15.25 *** ## (0.54) (0.37) (0.46) (0.39) (0.44) (0.39) ## cons_energia 0.00 ** 0.00 *** 0.00 *** 0.00 *** 0.00 *** 0.00 *** ## (0.00) (0.00) (0.00) (0.00) (0.00) (0.00) ## poblacion_urb -0.01 * -0.01 ** -0.01 ** -0.01 ** -0.01 *** -0.01 ** ## (0.00) (0.00) (0.00) (0.00) (0.00) (0.00) ## ------------------------------------------------------------------------------------- ## R^2 0.04 0.06 0.07 0.07 0.08 0.07 ## Adj. R^2 0.03 0.06 0.07 0.06 0.08 0.06 ## Num. obs. 217 306 306 306 306 306 ## ===================================================================================== ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 mice nuevamente hace bastante simple el proceso de combinar los valores de las cinco regresiones: modelo_combinado_form &lt;- with( imputacion_mice, lm(indice_capacidades ~ cons_energia + poblacion_urb) ) modelo_combinado &lt;- summary(pool(modelo_combinado_form)) modelo_combinado ## term estimate std.error statistic df p.value ## 1 (Intercept) 15.4407 0.58274 26.5 14 1.4e-13 ## 2 cons_energia 0.0015 0.00036 4.1 59 1.4e-04 ## 3 poblacion_urb -0.0103 0.00355 -2.9 214 4.1e-03 Mostrar los resultados con texreg será un poco más complicado, pues no hay una implementación por defecto. Crearemos una personalizada usando texreg::createTexreg(), especificando manualmente los coeficientes, errores estándar, \\(R^2\\), etc. tr_modelo_combinado &lt;- texreg::createTexreg( # nombres de los coeficientes: coef.names = as.character(modelo_combinado$term), # coeficientes, errores estándar y valores-p: coef = modelo_combinado$estimate, se = modelo_combinado$std.error, pvalues = modelo_combinado$p.value, # R^2 y número de observaciones: gof.names = c(&quot;R^2&quot;, &quot;Num. obs.&quot;), gof = c(pool.r.squared(modelo_combinado_form)[1, 1], nrow(imputacion_mice$data)), gof.decimal = c(T, F) ) Así podemos usar el texreg::screenreg() tradicional con nuestro nuevo objeto: texreg::screenreg(tr_modelo_combinado) ## ## ========================= ## Model 1 ## ------------------------- ## (Intercept) 15.44 *** ## (0.58) ## cons_energia 0.00 *** ## (0.00) ## poblacion_urb -0.01 ** ## (0.00) ## ------------------------- ## R^2 0.07 ## Num. obs. 306 ## ========================= ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Finalmente conseguimos lo que buscábamos. Una regresión que tiene más observaciones, gracias al proceso de imputación. En este caso, la diferencia entre las observaciones es pequeña, cambiando de 12.165 a 12.187, casi sin diferencias en los coeficientes. Notarás que las imputaciones generan diferencias más importantes cuando el porcentaje de valores perdidos es mayor. texreg::screenreg(list(modelo_incompleto, tr_modelo_combinado), custom.model.names = c(&quot;M incomp&quot;, &quot;M combinado&quot;)) ## ## ====================================== ## M incomp M combinado ## -------------------------------------- ## (Intercept) 15.32 *** 15.44 *** ## (0.54) (0.58) ## cons_energia 0.00 ** 0.00 *** ## (0.00) (0.00) ## poblacion_urb -0.01 * -0.01 ** ## (0.00) (0.00) ## -------------------------------------- ## R^2 0.04 0.07 ## Adj. R^2 0.03 ## Num. obs. 217 306 ## ====================================== ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 ¡Esperamos que hayas encontrado esta sección útil! Si es que te interesa aprender más sobre el trabajo con valores perdidos, recomendamos el trabajo de James Honaker y Gary King (Honaker and King 2010). Ejercicio 11C. En vez de cinco imputaciones, repite el ejercicio de regresión usando diez imputaciones. E-mail: arcruz@uc.cl↩︎ E-mail: furdinez@uc.cl↩︎ Esta forma de unir bases de datos, conocida como left join, es particularmente común. En algunos casos específicos, podríamos necesitar otros tipos de uniones, más avanzadas. Te recomendamos que revises este prolífico resumen escrito por Jenny Bryan↩︎ "],
["web-mining.html", "Capítulo 12 Minería de datos web 12.1 Introducción 12.2 Formas de hacer web scraping 12.3 Web scraping en R 12.4 Usando APIs y extrayendo datos de Twitter", " Capítulo 12 Minería de datos web Gonzalo Barría49 Lecturas sugeridas Calvo, E. (2015). Anatomía política de Twitter en Argentina. Tuiteando #Nisman. Buenos Aires: Capital Intelectual. Steinert-Threlkeld, Z. (2018). Twitter as Data (Elements in Quantitative and Computational Methods for the Social Sciences). Cambridge: Cambridge University Press. Los paquetes que necesitas instalar tidyverse (Wickham 2019b), glue (Hester 2020), rvest (Wickham 2019a), rtweet (Kearney 2020). 12.1 Introducción La información en Internet crece exponencialmente cada día. Si el problema de la ciencia política en el siglo XX fue la falta de datos para probar hipótesis, el siglo XXI presenta otro desafío: la información es abundante y está al alcance de la mano, pero hay que saber cómo recopilarla y analizarla. Esta enorme cantidad de datos está generalmente disponible pero de forma no estructurada, por lo que cuando te encuentres con la información tendrás dos retos principales: extraerla y luego clasificarla (es decir, dejarla como datos organizados (tidy)). Una de las técnicas más utilizadas para extraer información de los sitios web es la técnica conocida como web scraping. El web scraping se está convirtiendo en una técnica cada vez más popular en el análisis de datos debido a su versatilidad para tratar con diferentes sitios web. Como podemos ver en el siguiente gráfico, las búsquedas en Google del término “web scraping” han crecido constantemente año tras año desde 200: Figura 12.1: Búsquedas de ‘web scraping’ en Google Esto consiste en obtener datos no muy estructurados (HTML) desde un sitio web, que usualmente luego transformamos a un formato estructurado con filas y columnas, con el que es más fácil trabajar. Nos permite obtener datos de fuentes no tradicionales (¡prácticamente cualquier página web!) La información que podemos obtener es la misma que podríamos hacer manualmente (copiar y pegar a un documento de Excel, por ejemplo), pero podemos automatizar tareas muy tediosas. ¿Para qué podríamos usar el web scraping? Como ejemplo práctico, vamos a realizar 2 simples ejercicios de extracción de datos. Pero antes de eso es bueno estar familiarizado con las fuentes de información y los múltiples usos que se le pueden dar a los datos extraídos. Algunas otras posibles aplicaciones para las que se puede usar el web scraping son: Por ejemplo, pueden utilizarse para clasificar productos o servicios para crear motores de recomendación, para obtener datos de texto, como en la Wikipedia, para hacer sistemas basados en el Procesamiento del Lenguaje Natural. Generar datos a partir de etiquetas de imágenes, de sitios web como Google, Flickr, etc. para entrenar modelos de clasificación de imágenes. Consolidar los datos de las redes sociales: Facebook y Twitter, para realizar análisis de sentimientos u opiniones. Extraer comentarios de usuarios y sitios de comercio electrónico como Alibaba, Amazon o Walmart. 12.2 Formas de hacer web scraping Podemos raspar (scrape) los datos, es decir, obtenerlos, de diferentes maneras: Copiar y pegar: obviamente esto debe ser hecho por un humano, es una manera lenta e ineficiente de obtener datos de la web. Uso de APIs: API (por sus siglas en inglés) significa interfaz de programación de aplicaciones. Sitios web como Facebook, Twitter, LinkedIn, entre otros, ofrecen una API pública y/o privada, a la que se puede acceder mediante programación, para recuperar datos en el formato deseado. Una API es un conjunto de definiciones y protocolos utilizados para desarrollar e integrar programas informáticos de aplicación. En pocas palabras, es un código que indica a las aplicaciones cómo pueden comunicarse entre sí. Las API permiten que sus productos y servicios se comuniquen con otros, sin necesidad de saber cómo se implementan. Esto simplifica el desarrollo de aplicaciones y ahorra tiempo y dinero. Análisis DOM: DOM significa Documento Objeto Modelo y es esencialmente una interfaz de plataforma que proporciona un conjunto estándar de objetos para representar documentos HTML y XML. El DOM permite el acceso dinámico a través de la programación para acceder, añadir y cambiar dinámicamente el contenido estructurado en documentos con lenguajes como JavaScript. A través de algunos programas es posible recuperar el contenido dinámico, o partes de sitios web generados por scripts de un cliente. Los objetos DOM modelan tanto la ventana del navegador como el historial, el documento o la página web, y todos los elementos que la propia página puede tener, como párrafos, divisiones, tablas, formularios y sus campos, etc. A través del DOM se puede acceder, mediante Javascript, a cualquiera de estos elementos, es decir, a sus correspondientes objetos para alterar sus propiedades o invocar sus métodos. Sin embargo, a través del DOM, cualquier elemento de la página está disponible para los programadores de Javascript, para modificarlos, borrarlos, crear nuevos elementos y colocarlos en la página. 12.2.1 Estándar de exclusión de robot Antes de entrar en la práctica del web scraping tenemos que entender mejor qué es y cómo funciona el archivo robots.txt presente en la mayoría de los sitios web. Este archivo contiene el llamado estándar de exclusión de robot que es una serie de instrucciones especialmente dirigidas a los programas que buscan indexar el contenido de estas páginas (por ejemplo el bot de Google que “guarda” las nuevas páginas que se crean en Internet). Este método se utiliza para evitar que ciertos bots que analizan sitios de Internet añadan información “innecesaria” a los resultados de las búsquedas. Un archivo robots.txt en una página web funcionará como una petición para que ciertos bots ignoren archivos o directorios específicos en su búsqueda. Esto es importante cuando hablamos de web scraping ya que siempre es aconsejable revisar el archivo robots.txt de una página web antes de iniciar el scraping ya que puede incluir información que necesitaremos más adelante. Un ejemplo de cómo es uno de estos archivos se puede encontrar en el archivo robots.txt de Google https://www.google.com/robots.txt Figura 7.1: Vistazo a los estándares de exclusión de Google En esta imagen encontramos la expresión Usuario-agente: . Esto permite a todos los robots acceder a los archivos que se almacenan en el código principal de la página web ya que el comodín () significa “TODO”. A continuación vemos que los robots no pueden indexar o visitar páginas web del tipo /index.html?* o /grupos por ejemplo. Si encontramos la expresión Rechazar: / significa que se le niega el acceso a todos los bots (el comodín / se aplica automáticamente a todos los archivos almacenados en el directorio raíz del sitio web). 12.3 Web scraping en R Iremos cargando los paquetes que necesitamos. Ya estás familiarizado con algunos de ellos como ggplot2, que es parte del tidyverse. El paquete rvest es el que nos permitirá hacer el scraping de datos con R. Es esencialmente una librería que nos permite traer y manipular datos de una página web, usando HTML y XML. Finalmente, el paquete glue está diseñado para hacer más fácil interpolar (pegar) datos en strings¨ library(tidyverse) library(glue) library(rvest) Para “leer” datos de diferentes sitios web necesitaremos la ayuda de una herramienta de código abierto (open source tool), un plugin llamado “selectorgadget”. Este se utiliza para extraer información de un sitio web. En este caso lo usaremos para seleccionar y resaltar las partes del sitio web que queremos extraer. Se puede encontrar más información en este enlace https://selectorgadget.com/ y aquí https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html 12.3.1 Ejemplo aplicado: los comunicados de prensa de la Organización de Estados Americanos (OEA) Como primer ejemplo haremos un web scraping de un sitio estático. Es decir, un sitio web que tiene texto en HTML y que no cambia. Supongamos que estamos trabajando en un proyecto de diplomacia y relaciones internacionales en el que debemos sistematizar la información sobre la interacción entre los países de América Latina. Uno de los repositorios más útiles para iniciar este tipo de investigación es el sitio web de la Organización de Estados Americanos (OEA). Se puede encontrar en el siguiente link: https://www.oas.org/es/. Este sitio ofrece información muy pertinente para los analistas de datos políticos, ya que se trata de datos no estructurados que permiten identificar, por ejemplo, las redes de países, las alianzas, las fuentes de conflicto y las cuestiones que los ministerios de relaciones exteriores consideran pertinentes. Antes de comenzar con el web scraping en sí mismo analizaremos el archivo robots-txt del sitio web de la OEA http://oas.org/robots.txt Figura 3.4: Vistazo al archivo robots.txt de la OEA Encontramos que algunos directorios están prohibidos de ser indexados, pero todos los bots y usuarios están autorizados a visitar el sitio. Lo más importante es la expresión Crawl-delay: 3 que básicamente nos dice que por cada petición hecha por un robot es aconsejable esperar 3 segundos entre una consulta y otra para no saturar el sitio, lo que puede resultar en que te bloquee. En este ejemplo en particular, queremos extraer los títulos de los comunicados de prensa que se encuentran en este sitio web. Para simplificar, se recomienda utilizar Google Chrome (https://chrome.google.com/) y la extensión Selector Gadget para este buscador.50 La extensión Selector Gadget también funciona en Firefox y en Safari (así como en otros navegadores), sólo tienes que arrastrar el marcador a tu barra de marcadores. Primero cargamos la página como un objeto para que luego podamos leerla como un archivo html. En este caso queremos que los títulos de los comunicados de prensa de la Organización de Estados Americanos (OEA) para octubre de 2019 download_html(url = &quot;https://www.oas.org/es/centro_noticias/ comunicados_prensa.asp?nMes=10&amp;nAnio=2019&quot;, file = &quot;webs/comunicados_oea_10_2019.html&quot;) 12.3.2 Caraga el html para trabajar con R Una vez que descargamos los datos, los importamos en R usando read_html() web_comunicados_10_2019 &lt;- read_html(&quot;webs/comunicados_oea_10_2019.html&quot;, encoding = &quot;UTF-8&quot;) 12.3.3 Extraer la información con html_nodes() + html_text() A continuación, abrimos la página en Google Chrome donde lo primero que veremos es el sitio con las noticias. Allí hacemos clic en la extensión Chrome del Selector Gadget como se indica en la figura de abajo (es la pequeña lupa en la esquina superior derecha). Figura 3.7: Pantallazo del sitio de los comunicados de prensa de la OEA El primer paso es encontrar el selector de CSS que contiene nuestra información. En la mayoría de los casos sólo usaremos SelectorGadget. Empecemos con los títulos. Como podemos ver en la figura, seleccionamos la parte del sitio web que queremos extraer. En este caso queremos los títulos de los comunicados de prensa que después de ser seleccionados se destacan en amarillo. Observarán que al hacer clic en uno de ellos, aparece un mensaje “.itemmenulink” en el espacio vacío que hay en el Selector. Estos son los caracteres designados para los títulos en este sitio web Figura 12.2: Títulos de los comunicados de prensa que queremos extraer A continuación creamos un objeto para leer esta información como un objeto html en R. Lo llamaremos titulos_web_comunicados_10_2019. Dentro de la función html_nodes() añadimos el carácter “.itemmenulink” ya que representa los títulos según el selector. titulos_web_comunicados_10_2019 &lt;- web_comunicados_10_2019 %&gt;% html_nodes(&quot;.itemmenulink&quot;) %&gt;% html_text() Así es como obtuvimos los titulares de todo el sitio web. Ahora revisaremos los primeros 10 resultados para ver cómo se clasificaron los datos. La idea más adelante sería transformar este objeto en un marco de datos. También podemos extraer el contenido de cada noticia, pero para ello necesitaríamos la URL en inglés (o en el idioma que prefieras) de cada comunicado de prensa. head(titulos_web_comunicados_10_2019, n = 10) ## [1] &quot;Ministros de Seguridad de las AmÃ©ricas adoptan recomendaciones de Quito para fortalecer la prevenciÃ³n y lucha contra la delincuencia organizada&quot; ## [2] &quot;La SecretarÃ­a General de la OEA inicia hoy el anÃ¡lisis de integridad electoral y auditorÃ­a del cÃ³mputo oficial en Bolivia&quot; ## [3] &quot;El Salvador serÃ¡ sede de la VIII ReuniÃ³n de Ministros de Seguridad PÃºblica de la OEA en 2021&quot; ## [4] &quot;Pronunciamiento de los Ministros en Materia de Seguridad PÃºblica de las AmÃ©ricas&quot; ## [5] &quot;Recomendaciones de Quito para el Fortalecimiento de la CooperaciÃ³n Internacional en Materia de Seguridad PÃºblica en la PrevenciÃ³n y Lucha Contra la Delincuencia&quot; ## [6] &quot;Secretario General de la OEA rechaza masacre de indÃ­genas en Cauca, Colombia&quot; ## [7] &quot;Â¿QuÃ© es la ReuniÃ³n de Ministros en Materia de Seguridad PÃºblica de las AmÃ©ricas (MISPA)?&quot; ## [8] &quot;Informe Preliminar de la MisiÃ³n de VeedurÃ­a Electoral de la OEA para las Elecciones de Autoridades Territoriales en Colombia&quot; ## [9] &quot;OEA recibe contribuciÃ³n de Estados Unidos al Programa de CapacitaciÃ³n sobre Delito CibernÃ©tico&quot; ## [10] &quot;Consejo Permanente de la OEA celebra DÃ­a de la Visibilidad Intersexual y recibe Informe de la Ombudsperson&quot; Para eliminar los enlaces de un elemento, en lugar de su texto, debemos sustituir html_text()por html_attr(\"href\"): links_web_comunicados_10_2019 &lt;- web_comunicados_10_2019 %&gt;% html_nodes(&quot;.itemmenulink&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_c(&quot;https://www.oas.org/es/centro_noticias/&quot;,.) links_web_comunicados_10_2019 ## [1] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-092/19&quot; ## [2] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-091/19&quot; ## [3] &quot;https://www.oas.org/es/centro_noticias/fotonoticia.asp?sCodigo=FNC-97905&quot; ## [4] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=D-023/19&quot; ## [5] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=D-022/19&quot; ## [6] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-090/19&quot; ## [7] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=D-021/19&quot; ## [8] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=D-020/19&quot; ## [9] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-089/19&quot; ## [10] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=AVI-209/19&quot; ## [11] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=AVI-206/19&quot; ## [12] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-087/19&quot; ## [13] &quot;https://www.oas.org/es/centro_noticias/fotonoticia.asp?sCodigo=FNC-97847&quot; ## [14] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=D-019/19&quot; ## [15] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=AVI-203/19&quot; ## [16] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-086/19&quot; ## [17] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=AVI-200/19&quot; ## [18] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-085/19&quot; ## [19] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-083/19&quot; ## [20] &quot;https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-082/19&quot; ## [ reached getOption(&quot;max.print&quot;) -- omitted 19 entries ] Ahora podemos crear un marco de datos (data frame) con toda la información: df_web_comunicados_10_2019 &lt;- tibble( titulo = titulos_web_comunicados_10_2019, link = links_web_comunicados_10_2019) df_web_comunicados_10_2019 ## # A tibble: 39 x 2 ## titulo link ## &lt;chr&gt; &lt;chr&gt; ## 1 Ministros de Seguridad de las AmÃ©r… https://www.oas.org/es/centro_not… ## 2 La SecretarÃ­a General de la OEA in… https://www.oas.org/es/centro_not… ## 3 El Salvador serÃ¡ sede de la VIII R… https://www.oas.org/es/centro_not… ## # … with 36 more rows Ejercicio 12A. Consigue la fecha de cada comunicado de prensa de la OEA para octubre de 2019. Llama al vector “web_date_releases_10_2019”. Ejercicio 12B. Consigue los títulos de las noticias de la página web de la revista The Economist como su sección internacional: https://www.economist.com/international/?page=1 12.3.4 Iteraciones Las iteraciones nos permiten repetir la misma operación para un conjunto de elementos. Analicemos la sintaxis en el siguiente ejemplo: print(10) [1] 10 walk(.x = 1:10, .f = ~ { print(.x) }) [1] 1 [1] 2 [1] 3 [1] 4 [1] 5 [1] 6 [1] 7 [1] 8 [1] 9 [1] 10 Podríamos descargar varias páginas del sitio web de la OEA con una iteración. Por ejemplo, descarguemos los comunicados de prensa de todos los meses de 2016 a 2018. Para ello, utilizamos la función cross_df() para crear un data.frame que contiene todas las combinaciones posibles entre meses y años: iteration_df &lt;- cross_df(list(month = 1:12,year = 2016:2018)) Entonces, usamos la función walk2() que recibirá 2 argumentos que se iterarán, los meses y los años. walk2(.x = iteration_df$month, .y = iteration_df$year, .f = ~ { Sys.sleep(2) # stops download_html( url = glue(&quot;https://www.oas.org/es/centro_noticias/ comunicados_prensa.asp?nMes={.x}&amp;nAnio={.y}&quot;), file = glue(&quot;webs/comunicados_oea_{.x}_{.y}.html&quot;)) }) Nuestros próximos pasos te mostrarán cómo procesar estos sitios web (en este caso haremos 37 URL, pero podrían ser muchos más) para crear una base de datos único. Utilizaremos funciones e iteraciones personalizadas en el proceso. 12.3.5 Funciones personalizadas (recetas) Lo que hicimos hasta ahora para obtener nuestro marco de datos con información del sitio web de la OEA puede resumirse en los siguientes pasos: Descargar el sitio Sube el html a la ‘R’ Extraer los vectores para el título y el enlace usando selectores Crear el marco de datos con estos vectores Esto puede ser pensado como una “receta”, que debería funcionar para cualquier archivo. Por lo tanto, lo crearemos como una función personalizada para que trabajes con ella: f_procesar_sitio &lt;- function(file){ web &lt;- read_html(file, encoding = &quot;UTF-8&quot;) titulos &lt;- web %&gt;% html_nodes(&quot;.itemmenulink&quot;) %&gt;% html_text() links &lt;- web %&gt;% html_nodes(&quot;.itemmenulink&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_c(&quot;https://www.oas.org/es/centro_noticias/&quot;,.) df_info &lt;- tibble( titulo = titulos, link = links ) return(df_info) # what returns/delivers the function } Ahora la función toma cualquier archivo como argumento y nos da lo que esperamos: f_procesar_sitio(file = &quot;webs/comunicados_oea_10_2016.html&quot;) ## # A tibble: 24 x 2 ## titulo link ## &lt;chr&gt; &lt;chr&gt; ## 1 &quot;Asamblea General Extraordinaria de… https://www.oas.org/es/centro_not… ## 2 &quot;ATENCIÃ\\u0093N - ACTUALIZA FECHA Y… https://www.oas.org/es/centro_not… ## 3 &quot;La OEA y la UNESCO buscan reducir … https://www.oas.org/es/centro_not… ## # … with 21 more rows Así, podemos iterar esta función en nuestros 37 archivos para crear una base de datos completo. En este caso no usaremos walk(), sino map_dfr() –esta función espera que cada iteración devuelva un marco de datos, y (debajo) los pega en orden con bind_rows(). archivos &lt;- list.files(&quot;webs/&quot;, full.names = T) archivos ## [1] &quot;webs//comunicados_oea_1_2016.html&quot; ## [2] &quot;webs//comunicados_oea_1_2017.html&quot; ## [3] &quot;webs//comunicados_oea_1_2018.html&quot; ## [4] &quot;webs//comunicados_oea_10_2016.html&quot; ## [5] &quot;webs//comunicados_oea_10_2017.html&quot; ## [6] &quot;webs//comunicados_oea_10_2018.html&quot; ## [7] &quot;webs//comunicados_oea_10_2019.html&quot; ## [8] &quot;webs//comunicados_oea_11_2016.html&quot; ## [9] &quot;webs//comunicados_oea_11_2017.html&quot; ## [10] &quot;webs//comunicados_oea_11_2018.html&quot; ## [11] &quot;webs//comunicados_oea_12_2016.html&quot; ## [12] &quot;webs//comunicados_oea_12_2017.html&quot; ## [13] &quot;webs//comunicados_oea_12_2018.html&quot; ## [14] &quot;webs//comunicados_oea_2_2016.html&quot; ## [15] &quot;webs//comunicados_oea_2_2017.html&quot; ## [16] &quot;webs//comunicados_oea_2_2018.html&quot; ## [17] &quot;webs//comunicados_oea_3_2016.html&quot; ## [18] &quot;webs//comunicados_oea_3_2017.html&quot; ## [19] &quot;webs//comunicados_oea_3_2018.html&quot; ## [20] &quot;webs//comunicados_oea_4_2016.html&quot; ## [ reached getOption(&quot;max.print&quot;) -- omitted 53 entries ] df_web_comunicados_2016_2018 &lt;- map_dfr(.x = archivos, .f = ~ { f_procesar_sitio(.x) }) df_web_comunicados_2016_2018 ## # A tibble: 1,397 x 2 ## titulo link ## &lt;chr&gt; &lt;chr&gt; ## 1 &quot;MisiÃ³n Especial de la OEA llegarÃ… https://www.oas.org/es/centro_not… ## 2 &quot;OEA enviarÃ¡ MisiÃ³n Especial a Ha… https://www.oas.org/es/centro_not… ## 3 &quot;ATENCIÃ\\u0093N CAMBIO DE HORA: Con… https://www.oas.org/es/centro_not… ## # … with 1,394 more rows 12.4 Usando APIs y extrayendo datos de Twitter Twitter es una red social fundada en 2006 que permite a los usuarios interactuar entre sí enviando mensajes de no más de 280 caracteres. Es ampliamente usada por servicios públicos y por políticos especialmente en las épocas de campaña electoral. Con las nuevas técnicas de análisis de datos el estudio de la interacción de los usuarios en twitter se ha vuelto muy importante por ejemplo para medir los temas sobre los que está hablando la gente (trending topics) y sobre si las opiniones sobre una persona o tema son positivas o negativas. Por ejemplo Ernesto Calvo en su libro Anatomía Política de Twitter en Argentina: Tuiteando #NISMAN en el que identifica a través del estudio de los tweets sobre las causas de la muerte del ex fiscal Alberto Nisman que reflejaban (y estaban muy correlacionados con) las divisiones políticas de la oposición del gobiernon. A partir del caso Nisman, Ernesto Calvo analiza los tweets de los usuarios argentinos y muestra que la polarización mezcla política, algoritmos y smartphones. Si has leído la sección anterior, ya conoces los paquetes R que se utilizarán en esta sección del libro. Para hacer la extracción de datos de Twitter la mejor opción en mano es Rtweet, que permite acceder gratuitamente al API de Twitter para descargar información de los usuarios, temas de tendencias y hashtags. Para extraer datos de Twitter con R se recomienda consultar Twitter as Data, que contiene algunas rutinas estandarizadas para descargar datos de esta plataforma. ##Algunos antecedentes en APIs La interfaz de programación de aplicaciones Application Program Interfaces (APIs en inglés) son un set de protocolos y funciones que gobiernan ciertas interacciones entre aplicaciones web y usuarios. las APIs son similares a los navegadores web pero cumplen diferentes propósitos: -Navegadores web reproducen contenido de los browsers -Las APIs permiten manipular y organizador datos Para que las APIs públicas sean utilizadas muchos sitios solo permiten a usuarios autorizados (por ejemplo aquellos que tienen una cuenta en la plataforma). Este es el caso para Twitter, Facebook, Instagram and Github. Si bien estas APIs son ampliamente conocidas no está demás mencionar algunas creadas por la misma comunidad de R especializada en datos políticos. Por ejemplo el paquete lobbyR creado por Daniel Alcatruz51 que permite cargar y estructurar datos desde la API lobby que se encuentra en la plataforma de Ley de Lobby (https://www.leylobby.gob.cl/) implementada para el Gobierno de Chile, que permite realizar consultas por ejemplo sobre audiencias en determinados servicios públicos y organismos del estado como el congreso y los municipios. Otro paquete que es necesario mencionar es inegiR creado por Eduardo Flores que permite interactuar con la API del INEGI (Instituto Nacional de Estadística y Geografía de México) para realizar consultas específicas. http://enelmargen.org/ds/inegiR/vignette_spa.html #Extraer los datos Lo primero es no depender de la API oficial de Twitter, y por lo tanto deberías tener tu propia cuenta de Twitter que puedes crear en (https://twitter.com/i/flow/signup). Luego procedes a cargar el paquete rtweet que te permitirá extraer datos de Twitter. Te mostraremos esta rutina y extraeremos datos de diferentes usuarios y hashtags. library(rtweet) Por ejemplo podemos obtener los IDs de las cuentas que sigue la ONU en español. Por defecto la función te muestra hasta 5000 usuarios. ## Obtener los ID que son seguidos por la cuenta de la ONU en español friends_ONU_es &lt;- get_friends(&quot;ONU_es&quot;) Para saber más información de estos usuarios utilizamos la lookup_users() info_friends_ONU_es &lt;- lookup_users(friends_ONU_es$user_id) Podemos obtener información de los seguidores también mediante la función get_followers(). Como la cuenta de twitter de ONU en español tiene más de un millón de seguidores, es más comodo hacer un anáisis de una porción de ellos. Para obtener todos los IDs de usuarios que siguen a @ONU_es, solo necesitas 2 cosas: Una conexión estable a Internet Tiempo - aproximadamente cinco días y medio followers_ONU_es &lt;- get_followers(&quot;ONU_es&quot;, n = 200, retryonratelimit = TRUE) Aquí obtenemos la información de los usuarios que siguen la cuenta @ONU_es info_followers_ONU_es&lt;-lookup_users(followers_ONU_es$user_id) 12.4.0.1 Buscando tweets especificos Ahora estamos listos para buscar tweets recientes. Busquemos por ejemplo todos los tweets que lleven el hashtag “#Piñera”, en alusión al presidente chileno. Recordemos que un hashtag es una etiqueta que corresponde a una palabra antecedida por el símbolo “#”. Esta es utilizada principalmente para encontrar tweets con contenido similar, ya que el fin es que tanto el sistema como el usuario la identifiquen de forma rápida. Utilizaremos la función rtweet::search_tweets(). Esta función requiere los siguientes argumentos q: la palabra que quieres buscar n: el número de tweets máximos que quieres extraer. Puedes pedir un máximo de 18.000 tweets cada 15 minutos. Tip: Para ver otros argumentos de la función puedes usar el documento de ayuda ?search_tweets Un aspecto importante que tenemos que aclarar con respecto a las consultas que le hacemos a la API de Twitter es que los resultados obtenidos no podrán reproducir completamente, estos cambiarán, ya que la versión gratuita de Twitter nos permite hacer consultas en un periodo terminado de tiempo en el pasado (por ejemplo una semana) por lo que los resultados de las consultas cambiarán con el tiempo. Si te interesa una cierta agenda o hashtag, un buen consejo es descargarla regularmente y guardarla en un disco duro. No podrás volver a ellas en el futuro. # Look for 500 tweets with the hashtag #piñera pinera_tweets &lt;- search_tweets(q = &quot;#piñera&quot;, n = 1000) # We see the first 3 columns head(pinera_tweets, n = 3) ## # A tibble: 3 x 90 ## user_id status_id created_at screen_name text source ## &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 185366… 12952017… 2020-08-17 03:32:14 carmelcita &quot;Si … Twitt… ## 2 147890… 12952016… 2020-08-17 03:32:02 prospeccio… &quot;Si … Twitt… ## 3 121427… 12952014… 2020-08-17 03:31:11 wenqleo &quot;Si … Twitt… ## # … with 84 more variables Para obtener información de los usuarios que están emitiendo tweets sobre #piñera en el momento que estamos escribiendo este capitulo lookup_users(pinera_tweets$user_id) ## # A tibble: 871 x 90 ## user_id status_id created_at screen_name text source ## &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 185366… 12952017… 2020-08-17 03:32:14 carmelcita &quot;Si … Twitt… ## 2 147890… 12952024… 2020-08-17 03:34:53 prospeccio… &quot;Nun… Twitt… ## 3 121427… 12952025… 2020-08-17 03:35:34 wenqleo &quot;@ma… Twitt… ## # … with 868 more rows, and 84 more variables Las consultas de tweets pueden hacerse de varias maneras dependiendo en los que estas buscando: 12.4.1 Buscar una palabra clave query_1 &lt;- search_tweets(q = &quot;piñera&quot;, n = 20) 12.4.2 Buscar una frase query_2 &lt;- search_tweets(q = &quot;piñera economia&quot;, n = 20) 12.4.2.1 Buscar múltiples palabras claves query_3 &lt;- search_tweets(q = &quot;piñera AND bachelet&quot;, n = 20) Por defecto search_tweets() retorna 100 tweets. Para retornar más el n debe ser mayor, esto se hace con n =. Ejercicio 12C. En lugar de usar AND, usa OR entre los diferentes términos de búsqueda. Ambas búsquedas devolverán tweets muy diferentes. 12.4.2.2 Busca cualquier mención de una lista de palabras query_4 &lt;- search_tweets(&quot;bolsonaro OR piñera OR macri&quot;, n = 20) Ejercicio 12D. Buscar tweets en español que no sean retweets 12.4.2.3 Especifica el lenguaje de los tweets y excluye los retweets query_5 &lt;- search_tweets(&quot;piñera&quot;, lang = &quot;es&quot;, include_rts = FALSE, n=20) 12.4.3 Descargando características relacionadas con los usuarios de Twitter 12.4.3.1 Retweets Un retweet es cuando tú o alguien más comparte un tweet para que tus seguidores puedan verlo. Es similar a la característica de “compartir” en Facebook. Hagamos la misma rutina de antes pero esta vez ignorando los retweets. Lo hacemos con el argumento include_rts definido como FALSE. Entonces podemos obtener tweets y retweets en un marco de datos separado Nota: La opción include_rts = es muy útil si estás estudiando los patrones de viralización de ciertos hashtags. # Look for 500 tweets with the hashtag #piñera, but ignore retweets pinera_tweets &lt;- search_tweets(&quot;#piñera&quot;, n = 500, include_rts = FALSE) # See the first two columns head(pinera_tweets, n = 2) ## # A tibble: 2 x 90 ## user_id status_id created_at screen_name text source ## &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 671326… 12951957… 2020-08-17 03:08:20 nnnnico &quot;Tan… Twitt… ## 2 109934… 12951949… 2020-08-17 03:05:18 nacido_aC &quot;#Pi… Twitt… ## # … with 84 more variables Ahora veamos quién está twiteando sobre el hashtag “#piñera” # Look at the column with the names - top 6 head(pinera_tweets$screen_name) ## [1] &quot;nnnnico&quot; &quot;nacido_aC&quot; &quot;nacido_aC&quot; &quot;nacido_aC&quot; &quot;nacido_aC&quot; ## [6] &quot;MolinavSeba&quot; unique(pinera_tweets$screen_name) ## [1] &quot;nnnnico&quot; &quot;nacido_aC&quot; &quot;MolinavSeba&quot; ## [4] &quot;HSBnoticias&quot; &quot;criticolunar&quot; &quot;alert_ve&quot; ## [7] &quot;CerroaIzquierda&quot; &quot;ElPobreHank&quot; &quot;1_panchovilla&quot; ## [10] &quot;Rubencorsal&quot; &quot;bullboss63&quot; &quot;Thunder_Pantera&quot; ## [13] &quot;cebef&quot; &quot;villanomacul&quot; &quot;LKatrileo&quot; ## [16] &quot;TIRSO48&quot; &quot;nacioncl&quot; &quot;minevargasg&quot; ## [19] &quot;_hexagram_&quot; &quot;CiudadannoChile&quot; ## [ reached getOption(&quot;max.print&quot;) -- omitted 338 entries ] También podemos usar la función search_users() para explorar qué usuarios están twiteando usando un hashtag particular. Esta función extrae un data.frame de los usuarios e información sobre sus cuentas. # Which users are tweeting about #piñera? users &lt;- search_users(&quot;#piñera&quot;, n = 500) # see the first 2 users (the data frame is large) head(users, n = 2) ## # A tibble: 2 x 90 ## user_id status_id created_at screen_name text source ## &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 201853… 11916828… 2019-11-05 11:45:16 Plaid_Pine… Of c… Twitt… ## 2 947001… 11362572… 2019-06-05 13:03:23 robbiepine… @PLD… Twitt… ## # … with 84 more variables Aprendamos más sobre estas personas. ¿De dónde son? Como vemos, hay 304 lugares únicos por lo que el gráfico que usamos para trazar la información no es addecuado para visualizarlo. # How many places are represented? length(unique(users$location)) ## [1] 227 ## [1] 304 users %&gt;% ggplot(aes(location)) + geom_bar() + coord_flip() + labs(x = &quot;Frecuencia&quot;, y = &quot;Ubicación&quot;, title = &quot;Cuentas de Twitter - Ubicaciones únicas&quot;) Ordenemos por frecuencia las ubicaciones más nombradas y las trazamos. Para ello utilizamos top_n() que extraerá las localizaciones con al menos 20 usuarios asociados a ella users %&gt;% count(location, sort = TRUE) %&gt;% mutate(location = reorder(location, n)) %&gt;% top_n(20) %&gt;% ggplot(aes(x = location, y = n)) + geom_col() + coord_flip() + labs(x = &quot;Frecuencia&quot;, y = &quot;Ubicación&quot;, title = &quot;¿De dónde son estas cuentas de Twitter? Ubicaciones únicas&quot;) Es mejor si quitamos los NAs para ver los lugares más claramente. En general, la geolocalización en Twitter es bastante mediocre porque es voluntaria y pocas personas la tienen activada en sus teléfonos móviles. Así que lo que vemos es gente que quiere que sepamos su ubicación, lo que significa que hay un sesgo en estos resultados. users %&gt;% count(location, sort = T) %&gt;% mutate(location = reorder(location,n)) %&gt;% na.omit() %&gt;% filter(location != &quot;&quot;) %&gt;% top_n(20) %&gt;% ggplot(aes(x = location,y = n)) + geom_col() + coord_flip() + labs(x = &quot;Ubicación&quot;, y = &quot;Frecuencia&quot;, title = &quot;Usuarios de Twitter - Ubicaciones únicas&quot;) Finalmente repetimos el ejercicio usando hashtags que se refieren a Michelle Bachelet, la ex presidenta de Chile: ## Buscar tweets que usen el hashtag #bachelet rt &lt;- search_tweets( &quot;#bachelet&quot;, n = 600, include_rts = FALSE ) ¿Cómo ha sido la tendencia temporal de este hashtag en los últimos días? ## Graficar una serie de tiempo de tweets rt %&gt;% ts_plot(&quot;3 hours&quot;) + ggplot2::theme_minimal() + ggplot2::theme(plot.title = ggplot2::element_text(face = &quot;bold&quot;)) + ggplot2::labs( x = NULL, y = NULL, title = &quot;Frecuencia de #bachelet en los tweets de los últimos 9 días&quot;, subtitle = &quot;Conteo de tweets usando intervalos de tres horas&quot;, caption = &quot;Fuente: Datos recolectados desde la API de Twitter, usando el paquete rtweet&quot; ) Esperamos que este capítulo haya sido útil. En el Capítulo 13 te mostraremos cómo explorar más a fondo los datos de Twitter una vez que los hayas descargado. E-mail: ghbarria@uc.cl↩︎ Se puede instalar desde este enlace https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb.↩︎ https://github.com/Dalcatruz/lobbyR↩︎ "],
["qta.html", "Capítulo 13 Análisis cuantitativo de textos políticos 13.1 Análisis de hashtags políticos 13.2 Wordfish 13.3 Structural Topic Modeling (STM)", " Capítulo 13 Análisis cuantitativo de textos políticos Sebastián Huneeus52 Lecturas recomendadas Salganik, M. J. (2017). Bit by Bit: Social Research in the Digital Age. Princeton University Press, Princeton, NJ. Silge, J. and Robinson, D. (2017). Text Mining with R: A Tidy Approach. O’Reilly, Sebastopol, CA. Steinert-Threlkeld, Z. C. (2018). Twitter as Data. Cambridge University Press, Cambridge. Paquetes que necesitas instalar tidyverse (Wickham 2019b), paqueteadp (Urdinez and Cruz 2020), lubridate (Spinu, Grolemund, and Wickham 2020), skimr (Waring et al. 2020), ggwordcloud (Le Pennec and Slowikowski 2019), tidytext (Robinson and Silge 2020), stopwords (Benoit, Muhr, and Watanabe 2020), quanteda (Benoit, Watanabe, Wang, Nulty, et al. 2020), quanteda.textmodels (Benoit, Watanabe, Wang, Müller, et al. 2020), qdapRegex (Rinker 2017), stm (M. Roberts, Stewart, and Tingley 2019), tidystm (Johannesson 2020), remotes (Hester et al. 2020). Este capítulo está dividido en tres secciones, las cuales emplean diferentes estrategias para analizar datos textuales de Twitter. La Subsección 13.1 cubre la exploración del análisis de texto, la Subsección 13.2.3 trabaja con Wordfish (una técnica para posicionar textos a lo largo de un eje), mientras que la Subsección 13.3 cubre el Structural Topic Modeling (STM), que nos ayuda a descubrir temas subyacentes en los datos de texto. En el estudio de políticas contenciosas, #olafeminista, #metoo, #blacklivesmatter y #niunamenos son hashtags que fueron virales y cuentan una rica historia sobre el activismo y la protesta en redes sociales. Usaremos como caso de estudio un ciclo de protesta llamado Ola Feminista, que ocurrió en Chile desde Mayo hasta Junio del 2018. El ciclo de protesta feminista denunció inequidades de género estructurales, comenzó como una mobilización de estudiantes en Santiago, y creció –gradualmente– expandiéndose a demandas más amplias, provenientes de organizaciones feministas y de trabajadores en todo Chile.53 En la primera mitad de este capítulo, podrás aprender cómo usar estadísticas descriptivas básicas para entender cómo los formuladores de políticas usan Twitter. Analizaremos cómo los diputados en Chile utilizaron hashtags relacionados al género durante la #olafeminista. Analizaremos variaciones simples de frecuencia en el uso de hashtags para abordar diferentes niveles de compromiso con el debate en línea sobre cuestiones feministas y de género. En la segunda mitad del capítulo, aprenderás cómo usar Wordfish y Structural Topic Modeling (STM), dos técnicas recientes de procesamiento de lenguaje natural (PNL por sus siglas en inglés) utilizadas en ciencia política para minería de datos sin supervisión. Mientras que Wordfish nos permitirá posicionar las coaliciones políticas a lo largo de un eje ideológico izquierda-derecha, el STM nos permitirá identificar los temas –o grupos de palabras– más regulares y ver cómo esos tópicos están correlacionados a una variable de interés. En nuestro ejemplo, exploraremos la correlación entre el género del diputado y el uso de ciertos hashtags. En general, estas técnicas son grandes herramientas para ganar conocimiento sobre cómo las coaliciones y los formuladores de políticas participan digitalmente en una conversación política. En este capítulo, utilizarás una base de datos original con variables de identificación para los diputados y diputadas, como lo son el nombre y apellido, distrito, partido político, edad, entre otras. Las variables de identificación fueron extraídas desde la página web oficial de la Cámara de Diputados. Para la extracción de datos desde Twitter usamos el paquete rtweet, que nos permite acceder libremente a la API de Twitter para descargar información de los usuarios, fechas y hashtags (ver Capítulo 12). En este tipo de análisis, la tarea más difícil es reunir y limpiar la base de datos para que se vea “tidy” (ordenada). Afortunadamente, la API de Twitter y paquetes como rtweet o twitter son muy útiles para manejar la información descargada de una manera simple y ordenada.54 13.1 Análisis de hashtags políticos ¿Qué son los hashtags (#) y cómo están relacionado con la política? Los hashtags son textos que conectan usuarios en una conversación digital. Analizarlos nos ayuda a entender cómo, cuándo y por quién estas conversaciones están teniendo lugar. Además, los hashtags pueden ayudar a la movilización política. De hecho, existe literatura que aborda la protesta social a través de la viralización de hashtags, como los trabajos recientes que estudian el hashtag-feminismo (Trott 2018) y hashtags de activismo de minorías raciales, como el #blacklivesmatter (Ince, Rojas, and Davis 2017).55 En general, usar un hashtag indica interés en un tópico, independiente de que uno esté a favor o en contra de él. Por lo tanto, este primer ejercicio no intenta medir los niveles de apoyo, sino que este análisis nos permite identificar aquellos representantes que discuten temas de género. La organización de la sección es la siguiente. La primera parte es la exploración de la base de datos y un análisis descriptivo bivariado de las frecuencias de los #. En la segunda parte, hacemos una comparación por género. Tercero, comparamos el uso de hashtags por coalición política. En la cuarta parte, veremos la variación semanal en el uso de algunos #. En la quinta parte, esta variación temporal será separada por género. 13.1.1 Exploración de datos de Twitter library(tidyverse) library(tidytext) library(paqueteadp) data(&quot;poltweets&quot;) Tras cargar la base de datos poltweets, la exploramos con skimr::skim(poltweets), como muestra la Figura 13.1. Con esta función puedes hacer una exploración rápida del tamaño de la base de datos, el número de observaciones y variables, y el tipo de variable (carácter, número entero, factor, etc.). También podemos ver el número de valores perdidos, el número de categorías o valores que la variable factor asume (n_unique), así como la dispersión estadística para las variables cuantitativas (min, max, cuantiles, media y desviación estándar). skimr::skim() es un buen primer paso que nos permite diagnosticar los datos con los que trabajaremos. En el caso de nuestra base de datos poltweets, podemos ver que hay 7 variables de tipo “caracter” y una de tipo “POSIXct”, que también ayuda a trabajar con fechas. Figura 13.1: Skim de nuestra base de datos. Mira cómo el 75,4% de las filas en la base de datos corresponden a tweets hechos por congresistas. También notamos que hay 29 congresistas mujeres y 93 congresistas hombres en el conjunto de datos. # among tweets: poltweets %&gt;% count(genero) %&gt;% mutate(freq = n / sum(n)) ## # A tibble: 2 x 3 ## genero n freq ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Femenino 4595 0.246 ## 2 Masculino 14063 0.754 # deputies&#39; characteristics: poltweets %&gt;% distinct(nombre_usuario, genero)%&gt;% count(genero) ## # A tibble: 2 x 2 ## genero n ## &lt;chr&gt; &lt;int&gt; ## 1 Femenino 29 ## 2 Masculino 93 Tras cargar la base de datos y dar un rápido vistazo al tamaño y las variables incluídas, debemos extraer los hashtags de los tweets utilizando la función unnest_tokens() de tidytext, creando una base de datos “tokenizada” con una fila por hashtag. Luego, simplemente filtramos todas las filas que empiecen con un hashtag (#), dejándonos con una base de datos de un-hashtag-por-fila. poltweets_hashtags &lt;- poltweets %&gt;% unnest_tokens(output = &quot;hashtag&quot;, input = &quot;texto&quot;, token = &quot;tweets&quot;) %&gt;% filter(str_starts(hashtag, &quot;#&quot;)) Queremos ver las diferencias en cómo los representantes, partidos y coaliciones participan en el debate político sobre género. Para hacerlo, creamos una nueva variable dummy que toma el valor “1” cada vez que la variable de cadena de caracter coincide con alguna de las expresiones regulares como “femi”, “niunamenos”, “aborto”, “mujer” y “genero”: poltweets_hashtags &lt;- poltweets_hashtags %&gt;% mutate(fem_hashtag = case_when(str_detect(hashtag, &quot;femi&quot;) ~ 1, str_detect(hashtag, &quot;niunamenos&quot;) ~ 1, str_detect(hashtag, &quot;aborto&quot;) ~ 1, str_detect(hashtag, &quot;mujer&quot;) ~ 1, str_detect(hashtag, &quot;genero&quot;)~ 1, TRUE ~ 0)) %&gt;% mutate(fem_hashtag = as.character(fem_hashtag)) Vemos que esta es una buena medida para capturar hashtags relacionados al género y el feminismo. Observa que sólo el 4.1% de las filas contienen hashtag relacionados con género bajo este criterio y que los tres hashtags más frecuentes son #aborto3causales, #interpelacionaborto3causales y #leydeidentidaddegeneroahora.56 poltweets_hashtags %&gt;% count(fem_hashtag) %&gt;% mutate(freq = n / sum(n)) ## # A tibble: 2 x 3 ## fem_hashtag n freq ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 10423 0.958 ## 2 1 455 0.0418 poltweets_hashtags %&gt;% filter(fem_hashtag == &quot;1&quot;) %&gt;% count(hashtag) %&gt;% arrange(-n) ## # A tibble: 63 x 2 ## hashtag n ## &lt;chr&gt; &lt;int&gt; ## 1 #aborto3causales 98 ## 2 #interpelacionaborto3causales 64 ## 3 #leydeidentidaddegeneroahora 61 ## # … with 60 more rows 13.1.2 Diagnóstico visual Hagamos algunos análisis bivariados agrupando por número de tweets por mes, coalición y género (Figuras 13.2, 13.3, y 13.4). Figura 13.2: Total number of tweets by month. Figura 13.3: Total number of tweets by coalicion. Figura 13.4: Total number of tweets by genero. 13.1.3 Hashtags más utilizados Para ver cuáles son los hashtags más utilizados, los ordenamos desde los más frecuentes hasta los menos frecuentes. Vemos que el hashtag más utiliazdo es #cuentapublica, vinculado al Discurso Público del Presidente Sebastián Piñera, del 21 de mayo, al Congreso. El hashtag #aborto3causales es el único relacionado a género en el ranking, con 98 menciones. Este hashtag hace referencia a la discusión sobre la ley de aborto. poltweets_hashtags %&gt;% count(hashtag, fem_hashtag) %&gt;% arrange(-n) %&gt;% slice(1:20) ## # A tibble: 20 x 3 ## hashtag fem_hashtag n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 #cuentapublica 0 381 ## 2 #valdiviacl 0 323 ## 3 #losrios 0 232 ## # … with 17 more rows 13.1.4 Wordclouds (Nubes de palabras) Una rápida e intuitiva manera de representar frecuencias de palabras son las wordclouds (nubes de palabras). Estas representaciones gráficas permiten posicionar en el centro y con letras más grandes los casos que tienen mayor frecuencia. Para eso, utilizamos el paquete ggwordcloud para los 35 hashtags más comunes. Tras crear una base de datos de conteo, emplearemos el geom_text_wordcloud() con los mapeos estéticos “label”, “tamaño” y “color”. En la inspección visual vemos los tres hashtags sobre género más utilizados: #aborto3causales, #leydeidentidaddegeneroahora e #interpelacionaborto3causales (Figure 13.5). library(ggwordcloud) data_hashtags_wordcloud &lt;- poltweets_hashtags %&gt;% count(hashtag, fem_hashtag) %&gt;% arrange(-n) %&gt;% slice(1:35) ggplot(data_hashtags_wordcloud, aes(label = hashtag, size = n, color = fem_hashtag)) + geom_text_wordcloud() + scale_size_area(max_size = 8) + # we set a maximum size for the text theme_void() Figura 13.5: Wordcloud of the most common hashtags. 13.1.5 Nubes de palabras por grupos Utilizando la función facet_wrap(), las nubes de palabras se pueden dividir por variables de interés. Clasificando por género y coalición, inmediatamente vemos cómo hashtags como #olafeminista, #agendamujer y #educacionnosexista aparecen sólo entre las cuentas de Twitter de las congresistas. Al enfrentar coaliciones, nos damos cuenta que los tweets del Frente Amplio (FA) usan una alta proporción de hashtags relacionados con género, mientras que la coalición oficialista Chile Vamos (ChV) no usa ningún hashtag (Ver Figuras 13.6 and 13.7). ggplot(poltweets_hashtags %&gt;% count(hashtag, genero, fem_hashtag) %&gt;% arrange(-n) %&gt;% group_by(genero) %&gt;% slice(1:20), aes(label = hashtag, size = n, color = fem_hashtag)) + geom_text_wordcloud() + scale_size_area(max_size = 6) + facet_wrap(~genero) Figura 13.6: Wordclouds by genero. ggplot(poltweets_hashtags %&gt;% count(hashtag, coalicion, fem_hashtag) %&gt;% arrange(-n) %&gt;% group_by(coalicion) %&gt;% slice(1:20), aes(label = hashtag, size = n, color = fem_hashtag)) + geom_text_wordcloud() + scale_size_area(max_size = 6) + facet_wrap(~coalicion, nrow = 3) Figura 13.7: Wordclouds by coalicion. 13.1.6 Gráficos de barras Ahora clasificaremos la frecuencia de los hashtags por género. Generaremos este gráfico en dos pasos. Primero, creamos una tabla con 15 de los hashtags más utilizados entre mujeres y hombres. Luego, crearemos un gráfico de barras añadiendo el argumento geom_col() a la función ggplot(). Como resultado, veremos los hashtags #aborto3causales y #leydeidentidaddegeneroahora aparecer sólo en las cuentas de las congresistas, mientras que ninguno de estos hashtags relacionados con género aparecen en cuentas masculinas (Figura 13.8). plot_15 &lt;- poltweets_hashtags %&gt;% group_by(genero) %&gt;% count(hashtag, fem_hashtag) %&gt;% arrange(-n) %&gt;% slice(1:15) ggplot(data = plot_15, mapping = aes(x = n, y = reorder_within(hashtag, n, genero), fill = fem_hashtag)) + geom_col()+ labs(x = &quot;Frequency&quot;, y = &quot;&quot;, fill = &quot;Hashtag feminista&quot;) + facet_wrap(~genero, scales = &quot;free&quot;, nrow = 2) + scale_y_reordered() Figura 13.8: Most frequent hashtags by congresswomen (April 1 - June 30). Ahora, calculamos y graficamos el estadístico tf-idf, destinado a medir cuán importante es una palabra para un documento en una colección de documentos. Este estadístico es una combinación de la frecuencia del término (tf por sus siglas en inglés) y la frecuencia de documento inversa del término (idf por sus siglas en inglés), que disminuye su valor en términos usados comunmente y lo incrementa para palabras que no se utilizan mucho a lo largo de la colección de documentos. Observamos que, cuando separamos por grupos, dos hashtags con los más altos tf_idf en el Frente Amplio están relacionados con género (#leydeidentidaddegeneroahora). hash_tf_idf &lt;- poltweets_hashtags %&gt;% # calculate tf-idf: count(coalicion, hashtag, fem_hashtag, sort = T) %&gt;% bind_tf_idf(term = hashtag, document = coalicion, n = n) %&gt;% # get 10 most distinctive hashtags per coalicion: arrange(-tf_idf) %&gt;% group_by(coalicion) %&gt;% slice(1:10) ggplot(data = hash_tf_idf, mapping = aes(x = tf_idf, y = reorder_within(hashtag, tf_idf, coalicion), fill = fem_hashtag)) + geom_col() + labs(x = &quot;tf_idf&quot;, y = &quot;&quot;, fill = &quot;Hashtag feminista&quot;) + facet_wrap(~coalicion, nrow = 3, scales = &quot;free&quot;) + scale_y_reordered() Figura 13.9: tf-idf statistic, intended to measure how important a word is to a document. 13.1.7 Variación temporal en el uso de hashtags Ciertos hashtags pueden aumentar o disminuir su uso a través del tiempo, dependiendo del contexto político. Exploraremos la frecuencia semanal de los dos hashtags más frecuentes en nuestro ejemplo. Utilizando el paquete lubridate, que trabaja con datos en formato fecha, podemos buscar por tendencias de tiempo. En nuestra base de datos tenemos una variable con una fecha: creado_en. Usando esta variable, podemos confirmar que hubo un “peak” de tweets entre el 27 de mayo y el 2 de junio (ver Figura 13.10). hashtags_weekly &lt;- poltweets_hashtags %&gt;% mutate(week = floor_date(creado_en, &quot;week&quot;, week_start = 1)) %&gt;% filter(hashtag %in% c(&quot;#aborto3causales&quot;, &quot;#leydeidentidaddegeneroahora&quot;)) %&gt;% count(hashtag, week) ggplot(data = hashtags_weekly, mapping = aes(x = week, y = n, linetype = hashtag, group = hashtag)) + geom_point() + geom_line() + labs(x = &quot;Semana&quot;, y = &quot;Uso total semanal&quot;, linetype = &quot;Hashtag&quot;) Figura 13.10: Temporal variation in the usage of hashtags. 13.1.8 Para resumir Los hashtags pueden decir mucho sobre un debate político. Podemos observar algunas diferencias evidentes entre el uso de “#” sobre género. Las congresistas usaron muchos más hashtags como #olafeminista y #agendamujer que sus contrapartes masculinas. En cuanto a las coaliciones, las de izquierda (Frente Amplio y La Fuerza de la Mayoría) los utilizaron más. Sobre la variación temporal, la mayor intensidad de menciones de temas relacionados al género ocurrieron durante la semana del 14 al 20 de mayo, la semana previa al Discurso de Cuenta Pública (21 de mayo), lo que también coincidió con manifestaciones en diversas ciudades del país. Vemos que, en términos relativos, las congresistas estuvieron cinco veces más interesadas en el movimiento feminista, ya que usaron el hashtag #agendamujer 5 veces más que sus contrapartes masculinas durante la semana del 14 al 20 de mayo. ¿Qué aprendiste en esta sección? Te mostramos cómo usar Twitter para analizar fenómenos políticos. Una vez tengas tu propia base de datos, puedes seguir nuestro análisis paso por paso. Esto será útil como punto de partida para diseños explicativos que se pregunten sobre las causas del alineamiento político en diferentes agendas. 13.2 Wordfish En esta sección del capítulo, emplearemos dos técnicas NLP comúnmente usadas en ciencia política para minería de datos sin supervisión: Wordfish y Structural Topic Models (STM). Ambos modelos de procesamiento de texto nos permiten resumir muchos documentos diferentes de manera rápida y económica y pueden complementar otras mediciones descriptivas como la frecuencia de palabras o de hashtags. Como no están supervisados, las clasificaciones se realizarán sin utilizar ningún código o diccionario previo (Welbers, Van Atteveldt, and Benoit 2017). Esto tiene la ventaja de ahorrar trabajo en la creación manual de código, así como evitar el sesgo propio de quien hace el código. Otra ventaja es que no son dependientes del lenguaje de origen, es decir, en principio, se pueden utilizar en cualquier lenguaje. Ambos métodos utilizan el enfoque de “bolsa de palabras” (“bag of words”), ya que el orden de las palabras dentro del texto no altera el análisis. Los parámetros estimados por cada algoritmo pueden ser graficados con ggplot2, lo que facilita la interpretación visual de los resultados. La organización de la sección tiene la siguiente estructura, Primero, haremos una pequeña limpieza de datos, como remover “palabras vacías” (stopwords) (con un diccionario de stopwords, en este caso, incorporado en Quanteda), caracteres y números extraños. Luego, aplicaremos el algoritmo Wordfish en los tweets de los parlamentarios chilenos durante el mismo periodo que en la sección anterior. En la segunda parte, haremos un modelamiento de tópicos con STM en el mismo corpus. 13.2.1 Inspección y limpieza de datos Volvemos a cargar la base de datos poltweets y notamos que ahora contiene una serie de variables que son necesarias para el análisis de texto. Ahora usaremos los tweets completos, no sólo los tokens. También necesitamos las variables id, necesaria para que cada tweet coincida con quien lo twitteó. library(quanteda) # dfm and corpus library(quanteda.textmodels) # wordfish library(qdapRegex) # remove non ascii characters Empecemos siempre haciendo un escaneo rápido a los datos, como hicimos en la sección previa. El análisis descriptivo nos permite resumir características básicas de la base de datos, como el tipo de las variables y el número de caracteres por observación, la cantidad de datos perdidos (NA) y el rango de unidades de texto contenidas en cada variable. Exploramos la variable de caracteres que contiene los tweets. Al usar el comando glimpse() tenemos una previsualización de cada variable, específicamente del tipo y una vista previa de las primeras observaciones. glimpse(poltweets) ## Rows: 18,658 ## Columns: 8 ## $ id &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;1… ## $ nombre_usuario &lt;chr&gt; &quot;vladomirosevic&quot;, &quot;vladomirosevic&quot;, &quot;vladomiros… ## $ nombres &lt;chr&gt; &quot;VLADO&quot;, &quot;VLADO&quot;, &quot;VLADO&quot;, &quot;VLADO&quot;, &quot;VLADO&quot;, &quot;V… ## $ apellido &lt;chr&gt; &quot;MIROSEVIC&quot;, &quot;MIROSEVIC&quot;, &quot;MIROSEVIC&quot;, &quot;MIROSEV… ## $ creado_en &lt;dttm&gt; 2018-04-11 16:19:43, 2018-04-11 16:17:56, 2018… ## $ texto &lt;chr&gt; &quot;@MarioEyza1 @bcnchile @IntendenciaXV @Insulza … ## $ coalicion &lt;chr&gt; &quot;FA&quot;, &quot;FA&quot;, &quot;FA&quot;, &quot;FA&quot;, &quot;FA&quot;, &quot;FA&quot;, &quot;FA&quot;, &quot;FA&quot;,… ## $ genero &lt;chr&gt; &quot;Masculino&quot;, &quot;Masculino&quot;, &quot;Masculino&quot;, &quot;Masculi… 13.2.2 Pre-procesamiento Antes de aplicar el algoritmo, debemos pre-procesar los textos. Esto significa utilizar expresiones regulares para lograr un texto más limpio y ordenado. Utilizaremos expresiones regulares para remover caracteres extraños, nombres de usuario, URLs, emojis, además de cambiarlo todo a minúsculas. # function to remove accents f_remove_accent &lt;- function(x){ x %&gt;% str_replace_all(&quot;á&quot;, &quot;a&quot;) %&gt;% str_replace_all(&quot;é&quot;, &quot;e&quot;) %&gt;% str_replace_all(&quot;í&quot;, &quot;i&quot;) %&gt;% str_replace_all(&quot;ó&quot;, &quot;o&quot;) %&gt;% str_replace_all(&quot;ú&quot;, &quot;u&quot;) %&gt;% str_replace_all(&quot;ñ&quot;, &quot;n&quot;) # also replace &quot;ñ&quot;, a common letter in Spanish } # now pre-process the dataset: poltweets &lt;- poltweets %&gt;% mutate(texto = texto %&gt;% # delete user names (which start with @): str_remove(&quot;\\\\@[[:alnum:]]+&quot;) %&gt;% # delete URLs: str_remove_all(&quot;http[\\\\w[:punct:]]+&quot;) %&gt;% # all text to lowercase: str_to_lower() %&gt;% # remove special characters: str_remove_all(&quot;[\\\\d\\\\.,_\\\\@]+&quot;) %&gt;% f_remove_accent() %&gt;% # remove emojis rm_non_ascii() ) Una vez el texto está limpio, queremos agrupar de acuerdo con la variable para la comparación. Como estamos interesados en obtener las estimaciones en el nivel de coalición, agrupamos los textos por coalición. Ahora, cada coalición es un documento en la base de datos. Al ordenar por coaliciones, debes colocar los niveles del factor de forma que se parezca a un eje izquierda-derecha. by_coalition &lt;- poltweets %&gt;% group_by(coalicion) %&gt;% summarize(texto = str_c(texto, collapse = &quot; &quot;)) %&gt;% ungroup() %&gt;% # reorder the variable: mutate(coalicion = fct_relevel(as.factor(coalicion), &quot;FA&quot;, &quot;LFM&quot;, &quot;ChV&quot;)) Para modelar con Quanteda, primero transformamos nuestra base de datos a formato Corpus, y luego a formato DFM (Document-feature Matrix). Esto significa transformar cada documento en filas y las “características” como columnas. Hicimos la transformación de la base de datos agrupada por coaliciones hacia formato Corpus y luego a DFM. Adicionalmente, aprovechamos el uso de un comando que ayudará eliminar números, puntuaciones, símbolos y palabras vacías (conjunciones, artículos, etc.) # Corpus format poltweets_corpus &lt;- corpus(by_coalition, text_field = &quot;texto&quot;) # DFM format poltweets_dfm &lt;- dfm(poltweets_corpus, remove_numbers = T, remove_punct = T, remove_symbols = T, remove = stopwords(&quot;spa&quot;)) Utilizando dfm_trim(), eliminamos aquellas palabras con una frecuencia igual o menor que el quinto percentil y aquellas con una frecuencia igual o mayor que el percentil 95. De esta manera, eliminamos palabras inusuales que están ubicadas en los extremos de la distribución de frecuencias, ya que pueden sesgar los resultados del algoritmo. poltweets_dfm_trimmed &lt;- dfm_trim( poltweets_dfm, min_docfreq = 0.05, max_docfreq = 0.95, docfreq_type = &quot;quantile&quot; # min 5% / max 95% ) 13.2.3 Wordfish Wordfish es un algoritmo que permite hacer una ecala unidimensional de un conjunto de textos (Slapin and Proksch 2008). Es decir, sirve para ordenar los documentos en un eje unidimensional según cuán similar son entre sí en el uso de ciertas palabras clave. Este modelamiento asume que el número de veces que una palabra es mencionada en un documento sigue una distribución de Poisson. Este modelo es extremadamente simple ya que el número de veces que una palabra aparecerá es estimado por un único parámetro λ, que es tanto la media como la varianza de la distribución de probabilidad Poisson. La distribución es la siguiente (Proksch and Slapin 2010): \\[ Wordcount_{ij} \\sim Poisson(\\lambda_{ij}) \\] where \\[ \\lambda_{ij} = exp(\\alpha_i + \\psi_j + \\beta_j * \\omega_i) \\] El conteo de una palabra \\(j\\) para el documento \\(i\\) sigue una distribución de Poisson con parámetro \\(λ\\) para la palabra \\(j\\) y el documento \\(i\\). El modelo estima el parámetro \\(\\lambda_{ij}\\), que es la función del término \\(\\alpha_i\\) (el cual es un efecto fijado para los documentos) y el término \\(\\psi_j\\), que es un efecto fijado para la palabra \\(j\\) - Al ingresar estos efectos fijados, se descuenta el hecho de que algunas palabras pueden aparecer más veces que otras. El parámetro de interés es \\(\\beta_j\\), que captura la importancia de cada palabra \\(j\\) para discriminar las posiciones de \\(i\\) en el eje latente \\(X\\). Por lo tanto, los documentos pueden ser agrupados basado en cuán similares son entre ellos, utilizando ciertas palabras. Wordfish tiene dos supuestos fundamentales, primero, que las palabras siempre tienen el mismo significado dentro del texto. Segundo, que los textos son ordenados por una dimensión latente \\(X\\), que es un eje que articula la diferenciación ideológica de los documentos (Slapin and Proksch 2008). Sin embargo, la validez de esta suposición se mantiene en la medida en que el método es robusto con otras mediciones y que el corpus de textos incluidos en el análisis son representativos de la dimensión. En ciencia política, alguno de los trabajos que han utilizado este algoritmo con Twitter es el de Andrea Ceron (2017), en el cual utiliza las estimaciones de Wordfish para predecir la heterogeneidad ideológica dentro de los partidos políticos italianos, para ver qué legislador será elegido como ministro y la probabilidad de que abandonen el partido. Aplicamos el algoritmo Wordfish al objeto de clase DFM, específicamente a Quanteda. Definimos la dirección del parámetro \\(\\theta\\) -el equivalente de \\(\\beta\\)-, en este caso, el documento 3 (FA) es el polo positivo y el documento 1 (CHV) es el polo negativo en la dimensión ideológica estimada. También, usamos el argumento sparse = T, que también nos permite trabajar con grandes cantidades de datos, ahorrando poder computacional. wf &lt;- textmodel_wordfish(poltweets_dfm_trimmed, dir = c(3, 1), sparse = T) Graficamos en la Figura 13.11: df_wf &lt;- tibble( # coalicion labels: coalicion = wf[[&quot;x&quot;]]@docvars[[&quot;coalicion&quot;]], # then we extract thetas and their SEs from the mode object: theta = wf$theta, lower = wf$theta - 1.96 * wf$se.theta, upper = wf$theta + 1.96 * wf$se.theta ) df_wf ## # A tibble: 3 x 4 ## coalicion theta lower upper ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ChV 1.07 1.07 1.08 ## 2 FA -0.905 -0.912 -0.897 ## 3 LFM -0.169 -0.180 -0.158 ggplot(data = df_wf, mapping = aes(x = theta, y = fct_reorder(coalicion, theta), xmin = lower, xmax = upper)) + geom_point() + geom_linerange() + # add vertical line at x=0: geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + scale_x_continuous(limits = c(-1.2, 1.2)) + labs(y = &quot;&quot;) Figura 13.11: Classification of coalitions by ideological positioning Vemos que las coaliciones están agrupadas a lo largo de una división izquierda-derecha. El parámetro de interés \\(\\theta\\), equivalente al parámetro beta, es el parámetro que discrimina las posiciones de los documentos a partir de la frecuencia de las palabras. Vemos que este parámetro es consistente con cómo las coaliciones están agrupadas políticamente. El más a la derecha, Chile Vamos (ChV), con un \\(\\theta\\) de 1.07, se encuentra en un extremo del eje x, por el contrario, el de más a la izquierda, Frente Amplio (FA), con un \\(\\theta\\) de -0.91, está ubicado en el extremo opuesto. Ejercicio 13A. Puedes repetir el Wordfish, pero ahora invierte la dirección del parámetro \\(\\theta\\) en el objeto wf. ¿Cómo cambia la distribución de los documentos al invertir la dirección del parámetro? Ahora repite el ejercicio agrupando por partidos políticos. 13.2.4 ¿Qué aprendimos de Wordfish? Usando los mismos tweets que en la sección anterior, vimos cómo los mensajes provenientes de los miembros del congreso eran consistentes con una división ideológica, ordenada en un eje izquierda-derecha. En particular, concluimos que las coaliciones Frente Amplio y Chile Vamos están en extremos opuestos con respecto a sus tweets durante el ciclo de protesta chileno. Sorprendentemente, el algoritmo es capaz de ubicar a las coaliciones chilenas en un eje izquierda-derecha tomando como entrada solo los tweets de los parlamentarios durante los ciclos de protesta, sin ningún tipo de etiquetado manual de los textos. Wordfish es una herramienta poderosa para ser utilizada como un método de posicionamiento espacial, que se suma al repertorio de otras mediciones de posición política, como las estimaciones de puntos ideales bayesianos (Barberá 2015), roll call y cosponsorship (Alemán et al. 2009). 13.3 Structural Topic Modeling (STM) El modelado de temas es un método computacional para identificar automáticamente agrupaciones de palabras relevantes en grandes volúmenes de textos. Una de las aplicaciones más populares en ciencia política es la Latent Dirichlet Allocation (LDA), desarrollada por Blei, Ng, and Jordan (2003) y explicada de manera didáctica por David Blei en la Machine Learning Summer School 2009 de la Universidad de Cambridge. Otro desarrollo útil es el modelado de temas estructurales (STM por sus siglas en inglés), una técnica de NLP no supervisada para bucear grandes corpus de textos. La principal innovación del STM es que incorpora metadatos en el modelado del tema, por lo que permite a los investigadores descubrir temas y estimar su relación con las covariables, mejorando la calidad de las inferencias y la interpretabilidad de los resultados. El algoritmo STM está disponible en el paquete stm creado por Molly Roberts, Brandon Stewart y Dustin Tingley. Para una revisión más detallada de este método, hay una gran cantidad de material en el sitio oficial del paquete. En esta sección, analizaremos un subconjunto de nuestros tweets para encontrar los temas más relevantes y ver cómo se correlacionan con las variables de género y coalición. Siguiendo el ejemplo de Julia Silge, primero haremos todo el preprocesamiento utilizando herramientas “tidy”, para luego cargar una base de datos corregida a stm. 13.3.1 Pre-procesamiento Solo utilizaremos tweets de mayo de 2018: library(tidyverse) library(tidytext) library(stm) library(quanteda) library(qdapRegex) poltweets_onemonth &lt;- poltweets %&gt;% filter(creado_en &gt;= &quot;2018-05-01&quot; &amp; creado_en &lt; &quot;2018-06-01&quot;) Como se mencionó anteriormente, debemos comenzar preprocesando los textos. Recuerde que en la subsección anterior eliminamos caracteres extraños del texto. A continuación, crearemos una versión “tokenizada” de poltweets_onemonth, donde cada fila es una palabra contenida en el tweet original, sumado a una columna que nos dice el número total de veces que cada palabra se dice en todo el conjunto de datos (solo guardamos las palabras que se mencionan diez o más veces). Justo después de hacer eso, eliminaremos las palabras vacías (conjunciones, artículos, etc.) usando el paquete stopwords. Tenga en cuenta que también emplearemos un diccionario “personalizado” de palabras clave, compuesto por los nombres y apellidos únicos de los diputados. # obtain unique names and surnames of deputies nombres_apellido &lt;- c(poltweets$nombres, poltweets$apellido) %&gt;% na.omit() %&gt;% unique() %&gt;% str_to_lower() %&gt;% f_remove_accent() %&gt;% str_split(&quot; &quot;) %&gt;% flatten_chr() poltweets_words &lt;- poltweets_onemonth %&gt;% unnest_tokens(word, texto, &quot;words&quot;) %&gt;% # remove stop words: filter(!word %in% stopwords::stopwords(&quot;es&quot;, &quot;stopwords-iso&quot;)) %&gt;% # remove names/surnames of deputies: filter(!word %in% nombres_apellido) %&gt;% # just keep words that are present ten or more times add_count(word) %&gt;% filter(n &gt; 10) Eso es todo en términos de pre-procesamiento! A continuación, transformaremos el conjunto de datos tokenizado en un objeto stm utilizando las funciones cast_dfm() y convert(). poltweets_stm &lt;- poltweets_words %&gt;% cast_dfm(id, word, n) %&gt;% convert(to = &quot;stm&quot;) Para estimar la relación de los temas y las covariables del documento, debemos agregar los valores de covariable en el objeto poltweets_stm$meta. El objeto metadata es un marco de datos que contiene los metadatos para cada documento en el objeto stm, que luego se puede usar como el documento “prevalence” -o metadatos-. Tenga en cuenta que para crear el objeto stm_meta, es necesario unirlo mediante la variable id, una columna que contiene un identificador único para cada tweet. metadata &lt;- tibble(id = names(poltweets_stm$documents)) %&gt;% left_join(distinct(poltweets, id, coalicion, genero), by = &quot;id&quot;) %&gt;% as.data.frame() poltweets_stm$meta &lt;- metadata Ahora tenemos todos los ingredientes necesarios para estimar nuestro structural topic model (modelo de tema estructural), almacenados en el objeto poltweets_stm: summary(poltweets_stm) ## Length Class Mode ## documents 5647 -none- list ## vocab 1378 -none- character ## meta 3 data.frame list 13.3.2 Diagnósticos Para estimar un stm, es necesario definir de antemano el número de temas (\\(K\\)). Sin embargo, no hay un número “correcto” de temas, por lo que los \\(K\\) apropiados deberían decidirse mirando los datos en sí (M. E. Roberts, Stewart, and Tingley 2019). Para hacer eso, debemos entrenar varios modelos y cálculos de diagnósticos que nos ayudarán a decidir. ¿Qué rango de \\(K\\) debemos considerar? En el manual del paquete (M. Roberts, Stewart, and Tingley 2019), los autores ofrecen los siguientes consejos: Para corpus cortos centrados en temas muy específicos (como experimentos de encuestas), 3-10 temas es un rango de inicio útil. Para corporaciones pequeñas (de unos cientos a miles), 5-50 temas es un buen lugar para comenzar. Más allá de estas pautas generales, todo dependerá de la aplicación. Aplicaciones anteriores en ciencia política que han trabajado con corpus medianos (10 mil a 100 mil documentos) han encontrado que 60-100 temas funcionan bien. Para grandes corpus\" 100 temas es un tamaño predeterminado útil. (pág. 61) Nuestro conjunto de datos tiene 5,647 documentos, por lo tanto, trataremos de 5 a 50 temas. Podemos usar la función searchK() del paquete stm para calcular los diagnósticos relevantes, que almacenaremos en el objetostm_search. Este proceso es computacionalmente costoso y puede tomar varios minutos en una computadora moderna. Si no desea esperar, puede cargar el objeto del paquete del libro (data(stm_search)) y continuar. stm_search &lt;- searchK(documents = poltweets_stm$documents, vocab = poltweets_stm$vocab, data = poltweets_stm$meta, # our covariates, mentioned above: prevalence = ~ coalicion + genero, # 5-50 topics range: K = seq(5, 50, by = 5), # use all our available cores (be careful!): cores = parallel::detectCores(), # a seed to reproduce the analysis: heldout.seed = 123) A continuación, ordenaremos nuestro objeto recién creado y graficaremos sus resultados usando ggplot2: diagnostics &lt;- stm_search$results %&gt;% # get a tidy structure to plot: mutate_all(flatten_dbl) %&gt;% pivot_longer(-K, names_to = &quot;diagnostic&quot;, values_to = &quot;value&quot;) %&gt;% # we will only use some diagnostics: filter(diagnostic %in% c(&quot;exclus&quot;, &quot;heldout&quot;, &quot;residual&quot;, &quot;semcoh&quot;)) %&gt;% # give better names to the diagnostics: mutate(diagnostic = case_when( diagnostic == &quot;exclus&quot; ~ &quot;Exclusividad&quot;, diagnostic == &quot;heldout&quot; ~ &quot;Held-out likelihood&quot;, diagnostic == &quot;residual&quot; ~ &quot;Residuos&quot;, diagnostic == &quot;semcoh&quot; ~ &quot;Coherencia semántica&quot; )) ggplot(diagnostics, aes(x = K, y = value)) + geom_point() + geom_line() + facet_wrap(~diagnostic, scales = &quot;free&quot;) Aquí presentamos cuatro de los diagnósticos obtenidos con searchK(). Tanto la probabilidad retenida como los residuos (características de los modelos, más información en M. E. Roberts, Stewart, and Tingley (2019)) parecen sugerir que aumentar los temas es el enfoque correcto. Sin embargo, en nuestra experiencia, la coherencia semántica y la exclusividad son los mejores indicadores de la adecuación de \\(K\\). La coherencia semántica es quizás el diagnóstico que se correlaciona principalmente con el juicio humano sobre la “calidad del tema” [@ robertsStmPackageStructural2019], es decir, temas que incluyen términos que tienen sentido temático. Sin embargo, Roberts et al. (2014) sostiene que la coherencia semántica se puede lograr fácilmente con pocos temas, por lo tanto, se debe considerar conjuntamente con la “exclusividad”, esto es, cuán distintivos son los términos del tema en comparación con otros temas. Siguiendo estas definiciones e inspeccionando el gráfico anterior anterior, los modelos con K = 10 y K = 25 parecen buenos contendientes. Vamos a estimarlos por separado (si no deseas esperar, puedes volver a cargar los objetos de nuestro paquete paqueteadp): stm_model_k10 &lt;- stm(documents = poltweets_stm$documents, vocab = poltweets_stm$vocab, data = poltweets_stm$meta, prevalence = ~ coalicion + genero, K = 10) stm_model_k25 &lt;- stm(documents = poltweets_stm$documents, vocab = poltweets_stm$vocab, data = poltweets_stm$meta, prevalence = ~ coalicion + genero, K = 25) ¿Cómo debemos decidir entre estas dos especificaciones? La exclusividad y la coherencia semántica presentadas en la gráfica anterior son medidas de resumen, por lo que quizás podamos obtener más valor al observar valores específicos para cada tema en los dos modelos. En la siguiente Figura 13.12 graficamos la coherencia semántica frente a la exclusividad, como lo sugiere M. E. Roberts, Stewart, and Tingley (2019). En un mundo ideal, nos gustaría que todos los temas sean lo más semánticamente coherentes y exclusivos posible. Si bien no estamos en ese mundo ideal, parece que los temas del modelo K = 10 son mejores en este sentido, ya que el modelo K = 25 tiene bastantes temas que se posicionan como valores atípicos, pues tienen baja coherencia semántica o baja exclusividad (¡o ambas!) Por lo tanto, continuaremos nuestro análisis utilizando el modelo K = 10. # obtener exclusividad y coherencia semántica de los tópicos dentro de los modelos diagnostics2 &lt;- tibble( exclusivity = c(exclusivity(stm_model_k10), exclusivity(stm_model_k25)), semantic_coherence = c( semanticCoherence(stm_model_k10, documents = poltweets_stm$documents), semanticCoherence(stm_model_k25, documents = poltweets_stm$documents) ), k = c(rep(&quot;K=10&quot;, 10), rep(&quot;K=25&quot;, 25)) ) ggplot(data = diagnostics2, mapping = aes(x = semantic_coherence, y = exclusivity, shape = k)) + geom_point(size = 2) + labs(x = &quot;Semantic coherence&quot;, y = &quot;Exclusivity&quot;, shape = &quot;&quot;) Figura 13.12: Coherencia semántica y exclusividad de los tópicos en los dos modelos. 13.3.3 Análisis Hemos elegido un modelo con 10 temas. ¿Cómo se ve? Podemos obtener los términos principales de cada tema (más sobre lo que esto significa en un segundo) y graficarlos, como se muestra en la Figura 13.13. El gráfico también ordenará los temas en función de su proporción esperada dentro de todo el conjunto de tweets. model_terms &lt;- tibble( topic = as.character(1:10), # obtain the top seven terms: terms = labelTopics(stm_model_k10)$prob %&gt;% t() %&gt;% as_tibble() %&gt;% map_chr(str_c, collapse = &quot;, &quot;), # expected proportion of each topic in the whole set of tweets: expected_proportion = colMeans(stm_model_k10$theta) ) %&gt;% arrange(-expected_proportion) ggplot(data = model_terms, mapping = aes(x = expected_proportion, y = fct_reorder(topic, expected_proportion), label = terms)) + geom_col() + geom_text(size = 3, hjust = &quot;inward&quot;) + # to use space better labs(x = &quot;Expected proportion&quot;, y = &quot;Topic&quot;) Figura 13.13: Tópicos y sus términos más relevantes. Tenga en cuenta que el Tema 2 es el que se refiere a género, donde se incluyen las palabras “mujeres”, “derechos” y “género”. Podemos usar labelTopics () para obtener más términos que caracterizan este tema, como se muestra en el resultado a continuación. Los términos con mayor probabilidad son los mismos que utilizamos en el gráfico anterior, los más comunes en el tema. Otra medida interesante es “FREX”, que selecciona términos que son distintivos de este tema, en comparación con otros (en este caso, las palabras son las mismas). Por último, “Lift” (elevación) y “Score” (puntuación) se importan de otros paquetes (M. Roberts, Stewart, and Tingley 2019), y también pueden ser útiles al describir un tema. En este caso, observe cómo los términos principales de “Lift” incluyen “olafeminista”, uno de los hashtags que discutimos anteriormente. labelTopics(stm_model_k10, topics = 2) # this is the topic we want to analyze ## Topic 2 Top Words: ## ## Highest Prob: ## mujeres, presidente, derechos, protocolo, temas, apoyo, genero ## FREX: ## mujeres, presidente, derechos, protocolo, temas, apoyo, genero ## Lift: ## equidad, olafeminista, utilidades, violencia, apoyo, dejo, discriminacion ## Score: ## mujeres, presidente, derechos, protocolo, genero, apoyo, temas ## NULL Con este tema en mente, ahora podemos analizar su relación con los metadatos, en nuestro caso, el género y la coalición de los diputados: ¿es más probable que las mujeres y los políticos de izquierda twiteen sobre la ola feminista? Como dijimos antes, la capacidad de estimar estas relaciones tema-covariable es una ventaja central de los modelos de temas estructurales. El primer paso es usar estimateEffect() para obtener los coeficientes del modelo: stm_effects &lt;- estimateEffect( # c(1:10) means that we want coefficients for all topics in our model formula = c(1:10) ~ coalicion + genero, stmobj = stm_model_k10, metadata = poltweets_stm$meta ) A continuación, usamos el paquete tidystm57 para extraer los efectos del modelo, a través de extract.estimateEffect(). Comencemos con el género. Cuando la covariable de interés tiene dos niveles, el argumento método = \"diferencia\" obtendrá el cambio en la prevalencia (“prevalence”) del tema, cambiando de un valor específico a otro. En nuestro caso, la columna “estimate” en el objeto effect_gender es el cambio en la prevalencia del tema cuando se comparan congresistas mujeres con congresistas hombres. Existe una estimación relativamente grande y positiva para el Tema 2 (como se esperaba), que es significativa en los niveles de confianza habituales. Cabe señalar que, para la prevalencia de otros temas (4, 7, 10), el género parece tener un efecto opuesto (aunque más pequeño), un hallazgo que podría ser útil en investigaciones futuras. library(tidystm) effect_gender &lt;- extract.estimateEffect(x = stm_effects, covariate = &quot;genero&quot;, method = &quot;difference&quot;, cov.value1 = &quot;Femenino&quot;, cov.value2 = &quot;Masculino&quot;, model = stm_model_k10) effect_gender %&gt;% arrange(-estimate) ## method topic covariate covariate.value estimate std.error ## 1 difference 2 genero Femenino-Masculino 0.052 0.0095 ## 2 difference 8 genero Femenino-Masculino 0.015 0.0092 ## ci.level ci.lower ci.upper ## 1 0.95 0.0339 0.071 ## 2 0.95 -0.0032 0.033 ## label ## 1 Topic 2 (Covariate Level Femenino Compared to Masculino) ## 2 Topic 8 (Covariate Level Femenino Compared to Masculino) ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 8 rows ] Es posible graficar estos resultados usando ggplot2, una gran ventaja del paquete tidystm, que se muestra en la Figura 13.14: ggplot(effect_gender, aes(x = estimate, xmin = ci.lower, xmax = ci.upper, y = fct_reorder(as.character(topic), estimate))) + # añadir línea de efecto nulo geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + geom_point() + geom_linerange() + # central la línea de efecto nulo scale_x_continuous(limits = c(-0.075, 0.075)) + labs(x = &quot;Estadístico para la diferencia Femenino - Masculino&quot;, y = &quot;Tópico&quot;) Figura 13.14: Effect of genero on topic prevalence. Podemos repetir el análisis anterior para la otra covariable en nuestro modelo, obteniendo la diferencia en la prevalencia del tema entre la coalición de más izquierda (FA o Frente Amplio) y la coalición de más derecha (ChV o Chile Vamos). Como se esperaba, el modelo estima que los políticos de izquierda tendían -probabilísticamente- a twitear más sobre la ola feminista (Tema 2). effect_coalition_diff &lt;- extract.estimateEffect(x = stm_effects, covariate = &quot;coalicion&quot;, method = &quot;difference&quot;, cov.value1 = &quot;FA&quot;, cov.value2 = &quot;ChV&quot;, model = stm_model_k10) effect_coalition_diff %&gt;% filter(topic == 2) ## method topic covariate covariate.value estimate std.error ci.level ## 1 difference 2 coalicion FA-ChV 0.041 0.0099 0.95 ## ci.lower ci.upper label ## 1 0.022 0.061 Topic 2 (Covariate Level FA Compared to ChV) Ejercicio 13B. Grafica los efectos de la covariable de coalición, mostrando la diferencia entre la coalición de derecha (ChV) y las otras (FA y LFM). Ejercicio 13C. Agregue los 7 términos principales para cada tema a la Figura 13.14. *Consejo: puede usar left_join() para fusionar las dos bases de datos de interés. 13.3.4 Observaciones finales Las comunicaciones políticas en ciclos de protesta tienen claras implicancias en el dominio digital. En esta sección hemos demostrado que el análisis de texto automatizado de tweets políticos captura variaciones en la forma en que se twittea, particularmente, en cómo las congresistas parecen estar más correlacionadas con ciertos temas, en este caso el tema 2, relacionado con género. STM es una de las técnicas de NLP más recientes utilizadas en ciencia política para minería de texto sin supervisión. Sin embargo, a pesar del claro beneficio del uso de la minería de texto, estas técnicas deben usarse con precaución ya que la interpretabilidad de los temas es sensible en muchas decisiones como la K (número de temas para un modelo dado) y la limpieza del texto. E-mail: lshuneeus@uc.cl↩︎ Para un entendimiento más profundo del alcance de este movimiento, recomendamos el libro editado por la periodista Faride Zerán, “Mayo Feminista: La rebelión contra el patriarcado” (2018).↩︎ Para aprender más sobre algunas grandes estrategias de investigación en ciencias sociales utilizando datos de Twitter, te recomendamos el capítulo “Observing Behavior” del libro “Bit by Bit” de Matthew Salganik (2017, ch. 2).↩︎ Una lectura esencial sobre cómo se puede estudiar una crisis política latinoamericana a través de Twitter es Political Anatomy of Twitter in Argentina (2015) sobre la estructura de red digital generada tras la muerte del fiscal Alberto Nisman en 2015.↩︎ El hashtag #aborto3causales está relacionado con un debate nacional de larga data: entre 1989 y 2017, Chile tenía una de las políticas de aborto más restrictivas del mundo, criminalizando su práctica sin excepción. Desde 2017, el aborto en Chile es legal en los siguientes casos: cuando la vida de la madre está en riesgo, cuando el feto no sobrevivirá el embarazo y durante las primeras 12 semanas de embarazos (14 semanas si la mujer es menor de 14 años) en caso de violación. El 12 de febrero de 2018, la Norma Técnica fue promulgada en el Diario Oficial de la Ley 21.030, que despenalizó el aborto en tres causales. La norma fue amenazada con derogación y modificación, lo que generó una gran cantidad de controversia.↩︎ tidystm necesita ser instalado desde GitHub: remotes::install_github(\"mikajoh/tidystm\").↩︎ "],
["networks.html", "Capítulo 14 Análisis de redes 14.1 Introducción 14.2 Conceptos básicos en una red 14.3 Bases de datos de la red 14.4 Presentación gráfica de una red 14.5 Medidas de centralidad 14.6 Aplicación en R", " Capítulo 14 Análisis de redes Andrés Cruz58 Lecturas sugeridas Newman, M. (2018). Networks: An Introduction (2nd ed). New York: Oxford University Press. Scott, J. (2013). *Social Network Analysis (3rd ed.). London: Sage Publications. Los paquetes que necesitas instalar tidyverse (Wickham 2019b), paqueteadp (Urdinez and Cruz 2020), tidygraph (Pedersen 2020b), ggraph (Pedersen 2020a), ggcorrplot (Kassambara 2019). 14.1 Introducción No es exagerado decir que en la política todo está conectado a todo. Por ejemplo, consideremos el caso de los legisladores. Las conexiones son obvias: partidos, coaliciones, comisiones, lazos familiares, escuelas. Para entender mejor estas relaciones enredadas, una de las herramientas de la caja de herramientas de los científicos sociales es el análisis de redes. Las redes pueden reconocer que todo está conectado con todo lo demás y dar pistas sobre cómo se construyen esas conexiones. Las redes no sólo nos permiten visualizar estas conexiones de una manera convincente, sino que también nos permiten medir cuán cercanos son los actores. En este capítulo aprenderás los fundamentos del análisis de redes con R, principalmente usando los paquetes tidygraph y ggraph, que abordan el análisis de redes desde los preceptos del tidyverse. 14.2 Conceptos básicos en una red 14.2.1 Nodos y enlaces Hay dos conceptos básicos que expresan una cierta situación en forma de red. Los nodos (a veces llamados actores) son las principales unidades de análisis: queremos entender cómo se relacionan entre sí. Los enlaces (a veces llamados conexiones) muestran cómo los nodos están conectados entre sí. Por ejemplo, entre los legisladores (los nodos), un posible enlace es “haber propuesto una ley juntos”, también llamado co-patrocinio. Una red no es más que una serie de nodos conectados por enlaces, como se puede ver en la Figura 14.1. En cuanto al ejemplo anterior, podemos imaginar que dicha red gráfica conecta a los legisladores A, B, C y D según su copatrocinio de los proyectos w, x, y o z. Así, la red muestra que el legislador B ha presentado al menos un proyecto de ley con todos los demás legisladores de la red, mientras que el legislador D sólo ha presentado un proyecto de ley con B. Los legisladores A y C tienen dos enlaces de copatrocinio cada uno, formando un trío A-B-C. Figura 14.1: Diagrama de la red de copatrocinio de cuatro legisladores 14.2.2 Matriz de adyacencia Aparte de una descripción visual, como la de la Figura 14.1, también es posible representar las redes como matrices adyacentes. La tabla 14.1 muestra la misma red que arriba, pero en formato de matriz. El “1” indica que existe un vínculo entre dos nodos (copatrocinio, en nuestro ejemplo), mientras que el 0 indica lo contrario. Observa cómo la diagonal de la matriz se rellena con ceros: es una convención útil para diferentes cálculos matemáticos. Además, este tipo de matriz para una red básica es simétrica: si sabemos que el nodo A está vinculado al nodo B, sabemos automáticamente que el nodo B está vinculado al nodo A. Tabla 14.1: Matriz de adyacencia, red básica A 0 1 1 0 B 1 0 1 1 C 1 1 0 0 D 0 1 0 0 14.2.3 Pesos y direcciones La naturaleza de nuestros datos nos permite crear una red mucho más compleja que el ejemplo anterior. Específicamente, podemos añadir pesos y direcciones a los enlaces. Para empezar con los pesos, en nuestro ejemplo actual los legisladores están conectados si alguna vez han presentado un proyecto de ley juntos. Sin embargo, a menudo es interesante no sólo saber la existencia de una conexión entre dos actores, sino también la fuerza de esa conexión: no es lo mismo que dos legisladores hayan aceptado a regañadientes presentar un proyecto de ley juntos en una sola ocasión, ya que es que han presentado varios proyectos de ley juntos y son aliados políticos. Volvemos a la matriz de adyacencia, esta vez incluyendo los pesos. En este nuevo ejemplo, de la Tabla 14.2, los legisladores A y B han introducido nueve proyectos de ley juntos, mientras que todos los demás pares de legisladores conectados han introducido dos proyectos de ley juntos. Obsérvese cómo, por convención, la diagonal de la matriz permanece llena de ceros. Tabla 14.2: Matriz de adyacencia, red ponderada. A 0 9 2 0 B 9 0 2 2 C 2 2 0 0 D 0 2 0 0 La segunda forma de añadir información adicional a los enlaces es añadiendo direcciones. En algunas legislaturas, los proyectos de ley tienen un autor principal (patrocinador), al que se unen otros legisladores (copatrocinadores). En estos casos, la red de copatrocinadores tendrá naturalmente una dirección: un legislador “patrocinará” a otro firmando su proyecto de ley, sin que esta relación sea necesariamente recíproca. Es posible incluir esta información en la red, como se muestra en Fowler (2006) para el Congreso de los Estados Unidos. Una matriz de adyacencia con direcciones y pesos se verá como la de la Tabla 14.3. Nota que la matriz es ahora asimétrica, ya que hay más información sobre la relación de copatrocinio entre los legisladores.59. Mientras que el Legislador A patrocinó siete proyectos de ley del Legislador B, el Legislador B sólo reciprocó en dos proyectos de ley. Todos los demás copatrocinios registrados previamente implican la reciprocidad de un proyecto de ley. Tabla 14.3: Matriz de adyacencia, red ponderada y dirigida. A 0 7 1 0 B 2 0 1 1 C 1 1 0 0 D 0 1 0 0 14.3 Bases de datos de la red Siguiendo el espíritu del ejemplo anterior, en este capítulo trabajaremos con datos sobre el copatrocinio de leyes en el Senado argentino. Utilizaremos datos de Alemán et al. (2009), específicamente para el año 1983, justo después del retorno de la democracia. Comencemos por cargar las bases de datos: library(tidyverse) library(paqueteadp) data(&quot;copatrocinio_arg&quot;) data(&quot;senadores_arg&quot;) Podemos comprobar que nuestras bases de datos fueron cargadas correctamente con ls(): ls() ## [1] &quot;copatrocinio_arg&quot; &quot;senadores_arg&quot; Tenemos dos bases de datos. La primera contiene información de los nodos. Cada legislador está asignado a una identificación única (a partir de 1), condición necesaria para el correcto funcionamiento del tidygraph. Además, tenemos otra información sobre los legisladores: nombre, provincia, partido y coalición política. senadores_arg ## # A tibble: 46 x 4 ## id_sen nombre_sen provincia bloque_politico ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 RAMON A ALMENDRA SANTA CRUZ JUSTICIALISTA ## 2 2 JULIO AMOEDO CATAMARCA JUSTICIALISTA ## 3 3 RAMON A ARAUJO TUCUMAN JUSTICIALISTA ## # … with 43 more rows Mientras tanto, nuestra segunda base de datos contiene información sobre los enlaces, en este caso, el copatrocinio de leyes entre los senadores. En esta base de datos, se registran los pesos y direcciones. Por lo tanto, tiene tres variables: dos variables de identificación (que vinculan las identificaciones de los senadores) y el número de leyes copatrocinadas en el par. Por ejemplo, la primera observación indica que el Legislador 1 firmó un proyecto de ley del Legislador 2. copatrocinio_arg ## # A tibble: 217 x 3 ## id_sen_desde id_sen_hacia n_copatrocinio ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 2 1 ## 2 1 9 1 ## 3 1 30 1 ## # … with 214 more rows Una vez que tengamos dos bases de datos, podemos unirlas en un solo objeto de tidygraph para empezar a trabajar con ellos: library(tidygraph) tg_copatrocinio_arg &lt;- tbl_graph(nodes = senadores_arg, edges = copatrocinio_arg, directed = T) # Nuestra red está dirigida Los objetos del tidygraph unen nodos y enlaces que serán útiles en las siguientes operaciones: tg_copatrocinio_arg ## # A tbl_graph: 46 nodes and 217 edges ## # ## # A directed simple graph with 1 component ## # ## # Node Data: 46 x 4 (active) ## id_sen nombre_sen provincia bloque_politico ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 RAMON A ALMENDRA SANTA CRUZ JUSTICIALISTA ## 2 2 JULIO AMOEDO CATAMARCA JUSTICIALISTA ## 3 3 RAMON A ARAUJO TUCUMAN JUSTICIALISTA ## 4 4 ALFREDO L BENITEZ JUJUY JUSTICIALISTA ## 5 5 ANTONIO TOMAS BERHONGARAY LA PAMPA UCR ## 6 6 DEOLINDO FELIPE BITTEL CHACO JUSTICIALISTA ## # … with 40 more rows ## # ## # Edge Data: 217 x 3 ## from to n_copatrocinio ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 2 1 ## 2 1 9 1 ## 3 1 30 1 ## # … with 214 more rows Podemos editar el contenido de cualquiera de las dos bases de datos, activando cualquiera de sus nodos o enlaces con activate(). Vamos a crear dos variables que serán de utilidad más adelante. Primero, una variable binaria para el copatrocinio usando if_else(), llamada “d_copatrocinio”. Segundo, una variable invertida para los pesos (número de proyectos de ley), que nombraremos “n_copatrocinio_inv”. tg_copatrocinio_arg &lt;- tg_copatrocinio_arg %&gt;% activate(&quot;edges&quot;) %&gt;% # o &quot;nodos&quot; si quieres editar esa base de datos mutate(d_copatrocinio = if_else(n_copatrocinio &gt;= 1, 1, 0), n_copatrocinio_inv = 1 / n_copatrocinio) Ahora los bordes se activan y se muestran primero: tg_copatrocinio_arg ## # A tbl_graph: 46 nodes and 217 edges ## # ## # A directed simple graph with 1 component ## # ## # Edge Data: 217 x 5 (active) ## from to n_copatrocinio d_copatrocinio n_copatrocinio_inv ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 1 1 1 ## 2 1 9 1 1 1 ## 3 1 30 1 1 1 ## 4 1 34 1 1 1 ## 5 1 36 3 1 0.333 ## 6 1 37 1 1 1 ## # … with 211 more rows ## # ## # Node Data: 46 x 4 ## id_sen nombre_sen provincia bloque_politico ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 RAMON A ALMENDRA SANTA CRUZ JUSTICIALISTA ## 2 2 JULIO AMOEDO CATAMARCA JUSTICIALISTA ## 3 3 RAMON A ARAUJO TUCUMAN JUSTICIALISTA ## # … with 43 more rows 14.4 Presentación gráfica de una red Teniendo nuestro objeto de red, una de las primeras acciones es graficarlo. ggraph (el paquete hermano de tidygraph) nos permitirá hacerlo usando el lenguaje de los gráficos, reutilizando conceptos y funciones que aprendiste de ggplot2 en capítulos anteriores. library(ggraph) El diseño determina dónde se posicionarán los nodos en el plano. En general, una buena representación visual será capaz de revelar la estructura de la red, optimizar el uso del espacio y mostrar adecuadamente las relaciones entre los nodos (Pfeffer 2017). Tal vez el algoritmo más común para el diseño gráfico es el de Fruchterman and Reingold (1991). Pertenece a la familia de los algoritmos de escalado de distancias, que funcionan bajo la premisa muy intuitiva de que los nodos más conectados deben estar más cerca unos de otros (Pfeffer 2017). Para usar el algoritmo FR en nuestra red, debemos empezar por generarlo con create_layout(). Nota que plantamos una semilla con set.seed(), ya que el procedimiento es iterativo y puede dar diferentes resultados en diferentes intentos. set.seed(1) layout_fr &lt;- create_layout(tg_copatrocinio_arg, &quot;fr&quot;, weights = n_copatrocinio) Habiendo creado el objeto con el diseño gráfico, procederemos a graficarlo. La función ggraph() será la base de nuestra cadena de comandos. A continuación, podemos usar geom_node_point() para mostrar los puntos del plano: ggraph(layout_fr) + geom_node_point(size = 5) + theme_void() # un lienzo vacío para nuestra red Figura 3.14: Red de copatrocinio en el Senado argentino (1983), nodos en el plano La figura anterior no es muy informativa. Podemos proceder a añadir los enlaces con geom_edge_link(). Teniendo en cuenta que nuestra red está dirigida, añadiremos las flechas con el argumento arrow = arrow(length = unit(3, 'mm')) para fines ilustrativos (aunque típicamente pueden ser un poco confusas cuando se buscan patrones generales). ggraph(layout_fr) + geom_edge_link(arrow = arrow(length = unit(3, &#39;mm&#39;)), color = &quot;lightgrey&quot;) + geom_node_point(size = 5) + theme_void() Figura 3.15: Red de copatrocinio en el Senado argentino (1983), con nodos y enlaces Ejercicio 14A. Prueba la función geom_edge_link() en lugar de geom_edge_arc(). ¿Cuál es la diferencia? ¿Qué visualización crees que es más clara? Debemos recordar que nuestra red tiene pesos. Esto se tuvo en cuenta en el diseño gráfico cuando usamos el argumento weights = n_copatrocinio para create_layout(). Podemos incorporar esta información explícitamente en nuestros enlaces gráficos, usando el ahora familiar argumento mapping = aes() en nuestra geom. En este caso, graficaremos la intensidad del color en los enlaces, usando el argumento alpha =: ggraph(layout_fr) + geom_edge_link(mapping = aes(alpha = n_copatrocinio)) + geom_node_point(size = 5) + theme_void() Figura 3.16: Red de copatrocinio en el Senado argentino (1983), vínculos con opacidad El gráfico anterior muestra un grupo de legisladores particularmente cercano en la esquina superior izquierda. Podría ser interesante añadir información a los nodos de la red. En nuestra base de datos, conocemos el bloque político de cada legislador, así que usaremos esta información para editar los mapeos estéticos de geom_node_point(): ggraph(layout_fr) + geom_edge_link(mapping = aes(alpha = n_copatrocinio)) + geom_node_point(mapping = aes(color = bloque_politico, shape = bloque_politico), size = 5) + scale_shape_manual(values = 15:18) + # Editar ligeramente las formas theme_void() Figura 3.17: Red de copatrocinio en el Senado argentino (1983), versión completa El patrón visual es particularmente claro. La red está dividida entre los dos grandes bloques políticos: el Justicialista y la Unión Cívica Radical (UCR). Los bloques provinciales más pequeños aparecen como satélites, con particulares vínculos de co-patrocinio en algunos proyectos. Una inspección gráfica de la red no es suficiente para sacar conclusiones. Ejercicio 14B. Edita la base de datos del nodo en el objeto tidygraph, con una columna que distinga los senadores metropolitanos (provincias CAP FEDERAL y BUENOS AIRES) del resto. Luego, usa esta nueva variable para colorear los nodos de la representación visual de la red. 14.5 Medidas de centralidad Cuando se trata de datos de redes en las ciencias sociales, una de las primeras preguntas que hacemos tiene que ver con la influencia relativa de cada nodo. A menudo es relevante para nuestras preguntas de investigación descubrir figuras importantes, particularmente bien conectadas, etc. Sin embargo, el desarrollo de un estimador de centralidad no es del todo sencillo. En la literatura se han sugerido diferentes alternativas, que ponen de relieve diversas dimensiones de lo que intuitivamente podemos describir como “centralidad”. Ahora revisaremos las familias más comunes de estimadores: grado, centralidad eigenvectorial, entrelazado y cercanía. Para concluir, aprenderemos a aplicarlos en R, una tarea relativamente sencilla. 14.5.1 Grado El grado es probablemente la medida más intuitiva de la centralidad: entenderemos que un nodo es central si está conectado a muchos otros nodos. De esta manera, el estimador de grados simplemente cuenta el número de conexiones de cada nodo. Dos variaciones requieren una mayor elaboración. En primer lugar, en una red con pesos, el grado también puede calcularse sumando todos los pesos de las conexiones existentes, lo que a veces se denomina la fuerza del nodo. Segundo, en redes dirigidas cada nodo tendrá dos medidas diferentes de grado: el in-degree (grado de entrada) y el out-degree (grado de salida). La primera contará las veces que los otros nodos se conectan al nodo en particular, mientras que la segunda registrará las veces que el nodo se conecta a otros. 14.5.2 Centralidad del eigenvector Uno de los problemas más obvios de grado es que es ciego a la calidad de las conexiones, tal vez siendo demasiado poco refinado en algunos contextos. La centralidad del eigenvector y sus derivados consideran la idea de que “la gente importante conoce a la gente importante” (Patty and Penn 2016, 155). En otras palabras, no sólo importa el número de vínculos entre los nodos, sino también la importancia de los que componen esos vínculos. Calcular la centralidad eigenvectorial de un nodo es un poco más complicado que obtener el grado, y su explicación algebraica está fuera del alcance de este capítulo (ver Newman 2018, 159–63). Intuitivamente, se debe tener en cuenta que es una medida algo tautológica, ya que “en lugar de conceder un punto por cada vecino de la red que tiene un nodo [como lo hace el grado], la centralidad eigenvectorial concede un número de puntos proporcional a las puntuaciones de centralidad de los vecinos”. (Newman 2018, 159, énfasis original). El estimador crudo de centralidad de los vectores propios no funciona muy bien con las redes dirigidas, junto con algunos otros problemas (Newman 2018, 161–63). Uno de los estimadores alternativos más populares que refinan la centralidad de los vectores propios es el PageRank, el algoritmo utilizado por el motor de búsqueda de Google. En el contexto de las búsquedas en la web, “considera (1) el número de enlaces entrantes (es decir, los sitios que enlazan con tu sitio), (2) la calidad de los enlazantes (es decir, el PageRank de los sitios que enlazan con tu sitio), y (3) la propensión a enlazar de los enlazantes (es decir, el número de sitios a los que los enlazantes se vinculan)” (Hansen et al. 2019, 41). El PageRank puede ser usado en redes ponderadas y dirigidas, como nuestra red de ejemplo de copatrocinio en el Senado argentino. 14.5.3 Centralidad de intermediación (betweenness) En otra línea, la centralidad de intermediación busca medir una dimensión diferente de la centralidad: cuán importante es el nodo para las conexiones entre el resto de los nodos. Intuitivamente, podríamos considerar que un nodo es más importante si los demás en la red “lo necesitan” para conectarse entre sí (Patty and Penn 2016). En general, interpretamos que los nodos con altos valores de centralidad de entrelazamiento funcionan como puentes de conexión dentro de la red. Para calcular esta medida de centralidad en su versión más básica contaremos los pares de nodos de la red cuyo camino más corto entre ellos (caminos geodésicos) pasa por el nodo en cuestión: \\[\\begin{equation} betweenness_i = \\sum\\limits_{st} \\end{equation}\\] donde \\(st\\) son los pares de nodos en la red; y es igual a 1 si el nodo \\(i\\) está en el camino geodésico entre \\(st\\) y 0 si no lo está (o no existe tal camino). En una red dirigida, sólo se considerarán los enlaces que conducen al nodo específico cuando se calculen sus rutas geodésicas. Si queremos incluir pesos, éstos se interpretarán como distancias que separan los nodos: pesos más altos significarán una conexión más débil. Sin embargo, en múltiples aplicaciones, incluido nuestro ejemplo de copatrocinio, los pesos están destinados a indicar la fuerza de la relación entre dos nodos. En esos casos, podemos invertir los pesos al calcular las distancias geodésicas (¿recuerdas la variable “n_copatrocinio_inv” que creamos anteriormente?), como sugiere Newman (2001). 14.5.4 Cercanía Por último, la cercanía trata de medir la facilidad de acceso del nodo en cuestión a los demás nodos de la red. En pocas palabras, la idea es que los nodos importantes tendrán conexiones lo suficientemente buenas para acceder fácilmente a cualquier parte de la red. Más formalmente, la proximidad de un nodo es el promedio de las distancias geodésicas (las longitudes de los trayectos geodésicos) que mantiene con todos los demás nodos de la red: \\[closeness_i = \\frac{1}{n} \\sum{d_{ij}}\\] donde \\(i\\) es el nodo en cuestión, \\(n\\) es su número de enlaces, \\(j\\) es un nodo al que se une, y \\(d\\) es la distancia geodésica. Como también se basa en las distancias geodésicas, las mismas consideraciones para las redes ponderadas y dirigidas que revisamos para la interrelación se aplican a la cercanía. 14.6 Aplicación en R Las medidas de centralidad anteriores tratan de captar las dimensiones de distancia del mismo fenómeno: la importancia de cada nodo de la red. El código para calcularlas usando el tidygraph es relativamente simple. Después de activar el marco de datos de los “nodos”, usaremos mutate() para añadir las estadísticas de centralidad con las múltiples funciones específicas que comienzan con centrality_. Observa que en algunos casos usaremos argumentos para especificar el modo o identificar la variable con pesos en el marco de datos “bordes”. Aquí está la secuencia de tuberías(pipes)60: tg_copatrocinio_arg_centr &lt;- tg_copatrocinio_arg %&gt;% activate(&quot;nodes&quot;) %&gt;% # Las próximas operaciones editarán los &quot;nodos&quot; del tibble mutate( # grado c_in_degree = centrality_degree(mode = &quot;in&quot;), c_out_degree = centrality_degree(mode = &quot;out&quot;), # strength (weighted degree) c_in_strength = centrality_degree(mode = &quot;in&quot;, weights = n_copatrocinio), c_out_strength = centrality_degree(mode = &quot;out&quot;, weights = n_copatrocinio), # weighted pagerank (eigenvector centrality alternative) c_pagerank = centrality_pagerank(weights = n_copatrocinio), # betweenness, con o sin pesos invertidos c_betweenness = centrality_betweenness(), c_betweenness_w = centrality_betweenness(weights = n_copatrocinio_inv), # cercanía, con o sin pesos invertidos c_closeness = centrality_closeness(), c_closeness_w = centrality_closeness(weights = n_copatrocinio_inv) ) ## Warning in closeness(graph = graph, vids = V(graph), mode = mode, weights ## = weights, : At centrality.c:2784 :closeness centrality is not well- ## defined for disconnected graphs ## Warning in closeness(graph = graph, vids = V(graph), mode = mode, weights ## = weights, : At centrality.c:2617 :closeness centrality is not well- ## defined for disconnected graphs Podemos ver que nuestras nueve medidas de centralidad han sido añadidas al marco de datos de los nodos: tg_copatrocinio_arg_centr ## # A tbl_graph: 46 nodes and 217 edges ## # ## # A directed simple graph with 1 component ## # ## # Node Data: 46 x 13 (active) ## id_sen nombre_sen provincia bloque_politico c_in_degree c_out_degree ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 RAMON A A… SANTA CR… JUSTICIALISTA 8 6 ## 2 2 JULIO AMO… CATAMARCA JUSTICIALISTA 25 4 ## 3 3 RAMON A A… TUCUMAN JUSTICIALISTA 1 4 ## 4 4 ALFREDO L… JUJUY JUSTICIALISTA 1 4 ## 5 5 ANTONIO T… LA PAMPA UCR 14 6 ## 6 6 DEOLINDO … CHACO JUSTICIALISTA 9 7 ## # … with 40 more rows, and 7 more variables ## # ## # Edge Data: 217 x 5 ## from to n_copatrocinio d_copatrocinio n_copatrocinio_inv ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 1 1 1 ## 2 1 9 1 1 1 ## 3 1 30 1 1 1 ## # … with 214 more rows Comencemos construyendo un simple diagrama de correlación con las medidas de centralidad: library(ggcorrplot) corr_centralidad &lt;- tg_copatrocinio_arg_centr %&gt;% select(starts_with(&quot;c_&quot;)) %&gt;% as.data.frame() %&gt;% # calcular la correlación y redondear a un decimal cor(use = &quot;pairwise&quot;) %&gt;% round(1) ggcorrplot(corr_centralidad, type = &quot;lower&quot;, lab = T, show.legend = F) Figura 3.19: Gráfico de correlación entre distintas medidas de centralidad Observa que las correlaciones más fuertes en la trama existen entre las medidas de grado, intensidad y PageRank, lo que generalmente se espera. Notablemente, in-degree no exhibe esa fuerte correlación con out-degree (lo mismo ocurre con in-strength y out-strength), lo que sugiere que el grupo de legisladores bien patrocinados no es el mismo que el grupo de legisladores que generalmente actúan como patrocinadores. Podemos obtener fácilmente los legisladores con el mayor in-degree y out-degree de nuestro objeto tg_copatrocinio_arg_centrality: tg_copatrocinio_arg_centr %&gt;% arrange(-c_in_degree) %&gt;% select(id_sen, nombre_sen, c_in_degree) ## # A tbl_graph: 46 nodes and 217 edges ## # ## # A directed simple graph with 1 component ## # ## # Node Data: 46 x 3 (active) ## id_sen nombre_sen c_in_degree ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 JULIO AMOEDO 25 ## 2 43 HECTOR J VELAZQUEZ 22 ## 3 34 OLIJELA DEL VALLE RIVAS 20 ## 4 9 ORALDO N BRITOS 19 ## 5 31 ANTONIO O NAPOLI 17 ## 6 5 ANTONIO TOMAS BERHONGARAY 14 ## # … with 40 more rows ## # ## # Edge Data: 217 x 5 ## from to n_copatrocinio d_copatrocinio n_copatrocinio_inv ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 1 1 1 1 ## 2 9 4 1 1 1 ## 3 9 8 1 1 1 ## # … with 214 more rows tg_copatrocinio_arg_centr %&gt;% arrange(-c_out_degree) %&gt;% select(id_sen, nombre_sen, c_out_degree) ## # A tbl_graph: 46 nodes and 217 edges ## # ## # A directed simple graph with 1 component ## # ## # Node Data: 46 x 3 (active) ## id_sen nombre_sen c_out_degree ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 12 PEDRO A CONCHEZ 8 ## 2 31 ANTONIO O NAPOLI 8 ## 3 6 DEOLINDO FELIPE BITTEL 7 ## 4 10 JORGE A CASTRO 7 ## 5 23 MARGARITA MALHARRO DE TORRES 7 ## 6 28 FAUSTINO M MAZZUCCO 7 ## # … with 40 more rows ## # ## # Edge Data: 217 x 5 ## from to n_copatrocinio d_copatrocinio n_copatrocinio_inv ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8 28 1 1 1 ## 2 8 34 1 1 1 ## 3 8 24 1 1 1 ## # … with 214 more rows La escala de esas dos medidas es notable. El senador de mayor rango, Julio Amoedo, recibió el patrocinio de 25 legisladores diferentes (¡más de la mitad del Senado!). En cambio, el senador de mayor grado de salida, Pedro Conchez, patrocinó los proyectos de ley de sólo 8 legisladores distintos. Esto podría indicar que, si bien el papel de un “patrocinador principal” está bien definido, el de un “apoyador” no lo está. Otra forma común de utilizar los puntajes de centralidad implica analizarlos de acuerdo a algunas de las otras variables relevantes en nuestros datos. Esto se facilita con la estructura de objetos del tidygraph, que integra en un marco de datos toda la información de los nodos (incluyendo las variables descriptivas y los puntajes de centralidad). Por ejemplo, podemos trazar la relación entre PageRank/in-degree/in-strengh y los dos principales bloques políticos con un simple gráfico de cajas: ggplot(data = tg_copatrocinio_arg_centr %&gt;% as_tibble() %&gt;% filter(bloque_politico %in% c(&quot;UCR&quot;, &quot;JUSTICIALISTA&quot;)), aes(x = bloque_politico, y = c_pagerank)) + geom_boxplot() Figura 3.22: PageRank por bloque político Si bien el rango de las medidas del PageRank es comparable, la forma de los gráficos de cajas sugiere que los dos bloques políticos se comportan de manera diferente cuando se trata de copatrocinio. Mientras que el bloque justicialista tiene unos pocos legisladores notables, que presumiblemente actúan como patrocinadores principales bien respaldados, el resto de sus senadores no son tan influyentes en ese sentido. Por otra parte, aunque la UCR también tiene un par de legisladores notables, la distribución parece ser más equilibrada. Ejercicio 14C. Crea un gráfico para comparar algún puntaje de centralidad por si el legislador es del área metropolitana (como en el ejercicio anterior). A lo largo de este capítulo, cubrimos las nociones básicas del análisis de redes en las ciencias sociales. Te recomendamos leer la bibliografía recomendada al principio del capítulo para poder expandir tu análisis aún más. Correo electrónico: arcruz@uc.cl↩︎ Esta tercera red, como se describe en nuestro ejemplo, no registra cuando dos legisladores apoyan conjuntamente un proyecto de ley de un legislador diferente. Sólo mide el apoyo directo entre pares de legisladores↩︎ R devuelve una advertencia para las funciones de cercanía, porque el algoritmo tiene problemas cuando se trata de redes desconectadas. Para obtener la cercanía de un nodo en particular, es necesario conocer su distancia geodésica a cada uno de los otros nodos de la red. Lo que hay que hacer cuando algunas rutas geodésicas no existen no es sencillo, lo que hace que el cálculo y la interpretación sean complicados (ver Opsahl, Agneessens, and Skvoretz 2010)↩︎ "],
["indexes.html", "Capítulo 15 Análisis de componentes principales 15.1 Introducción 15.2 Cómo funciona el Análisis de Componentes Principales 15.3 Nociones básicas en R 15.4 Dimensionalidad del concepto 15.5 Variación del concepto", " Capítulo 15 Análisis de componentes principales Caterina Labrín61 y Francisco Urdinez62 Lecturas sugeridas Abeyasekera, S. (2005). Multivariate methods for index construction. In Household Sample Surveys in Developing and Transition Countries, pages 367–387. United Nations, New York, NY. Bryant, F. B. (2000). Assessing the Validity of Measurement. In Grimm, L. G. and Yarnold, P. R., editors, Reading and Understanding MORE Multivariate Statistics, pages 99–146. American Psychological Association, Washington, DC. Collier, D., Laporte, J., and Seawright, J. (2008). Typologies: Forming concepts and creating categorical variables. In Box-Steffensmeier, J. M., Brady, H. E., and Collier, D., editors, The Oxford Handbook of Political Methodology, pages 152–173. Oxford University Press, Oxford. Goertz, G. (2006). Social Science Concepts: A User’s Guide. Princeton University Press, Princeton, NJ. Capítulo 4 - “Increasing Concept-Measure Consistency.” Jackman, S. (2008). Measurement. In Box-Steffensmeier, J. M., Brady, H. E., and Collier, D., editors, The Oxford Handbook of Political Methodology, pages 119–151. Oxford University Press, Oxford. Los paquetes que necesitas instalar tidyverse (Wickham 2019b), paqueteadp (Urdinez and Cruz 2020), FactoMineR (Husson et al. 2020), factoextra (Kassambara and Mundt 2020), GGally (Schloerke et al. 2020), ggcorrplot (Kassambara 2019). 15.1 Introducción A estas alturas del libro has aprendido muchas herramientas para trabajar bien con los datos y conceptos de las ciencias sociales. En este capítulo verás cómo crear de forma sencilla variables que midan conceptos complejos que muchas veces en las ciencias sociales, y especialmente en las ciencias políticas, queremos utilizar para probar hipótesis o simplemente visualizar cómo se comportan. La relación entre guerra y democracia o entre democracia y desarrollo económico son cosas que se suelen estudiar en las ciencias políticas. Sin embargo, estos conceptos a veces no están claramente definidos de antemano o si lo están, no sabemos cuál de las definiciones puede servirnos. Conceptos como terrorismo o populismo son utilizados a diario por políticos y periodistas, muchas veces de forma incorrecta, demostrando que el problema no sólo se da a nivel académico sino en toda la sociedad, generando niveles de desinformación bastante graves. Esta fragilidad de nuestros conceptos se hace aún más grave si tenemos que operar con datos, ya que muchas veces queremos medir conceptos complejos con indicadores que nos muestren las variaciones o relaciones con otros conceptos de una manera más cuantitativa o visual. Para ilustrar la situación, tomemos un caso hipotético: pretendamos que estamos trabajando en un proyecto que entre otras cosas pide comparar cómo se comportan los países democráticos de América Latina y por lo tanto debe ser capaz de visualizarlo claramente en una tabla o gráfico. Normalmente, en este caso, buscaríamos algún indicador de la democracia que V-dem o Freedom House nos da para todos los países de América Latina. Asumiríamos por la práctica que estos indicadores tienen una validez evidente y que fueron construidos adecuadamente, basadas en dimensiones claras. Nadie cuestionaría nuestra elección. Ahora, imaginemos que queremos investigar un concepto diferente y más específico. Para esta tarea no tenemos un indicador similar a los de la democracia y debemos empezar de cero: ¿qué estamos midiendo? ¿cómo lo medimos? ¿Qué datos utilizamos? Ante este problema, en este capítulo queremos ilustrar la utilidad del Análisis de Componentes Principales (ACP) ya que nos permite generar las diferentes dimensiones que conforman un concepto abstracto a través de la reducción de diferentes indicadores previamente elegidos. Esto es principalmente para analizar cómo se compondría un concepto de este estilo y para entender lo que estamos explorando. Además, presentaremos algunas formas de operativizar los conceptos a través de índices que nos pueden mostrar la variabilidad de la medición entre las diferentes unidades de análisis, para poder compararlos o incluirlos dentro de un análisis más complejo. 15.1.1 Conceptos, medidas y validez Cuando construimos una medición empírica a partir de un concepto abstracto (como son democracia, populismo, desarrollo), además de tener en cuenta su complejidad, queremos que sea válida (ver Grimm and Yarnold 2000 para un excelente resumen sobre los diferentes tipos de validez). La validez del contenido es la que nos permite asegurar que la medición corresponde a lo que está definido conceptualmente para esa medición. Esto debe garantizarse cada vez que queramos generar un concepto a través de una revisión teórica que determine adónde irá nuestro concepto, cuáles serán las partes que lo compondrán y principalmente qué variables utilizaremos para medirlas. Una adecuada revisión de la literatura sobre el concepto concreto en el que estamos trabajando, que pueda situarse en un marco teórico más amplio, siempre será más que deseable cuando se trabaje con conceptos abstractos y complejos. La validez de la construcción se refiere a la propiedad de un concepto que mide lo que dice medir. Esta validez siempre debe ser cuestionada, precisamente por lo que dijimos anteriormente sobre la complejidad de los conceptos con los que trabajamos. Por ejemplo, es muy frecuente con las medidas que existen de la democracia. Incluso cuando hemos avanzado enormemente como disciplina y tenemos mediciones comparables entre países y a lo largo del tiempo, como en el caso Polity IV y V-Dem, existen diferentes opiniones sobre cómo debe ser operativizado este concepto y siempre hay espacio para mejoras. Hay dos maneras de ganar validez de construcción para nuestra variable de interés. La primera alternativa para obtener la validez de constructo se llama validez discriminatoria, es decir, nuestro constructo no debe estar altamente correlacionado con variables que nuestro indicador no pretende medir. Este concepto puede ejemplificarse muy bien con la forma en que V-Dem ha elegido para crear su índice de democracia (al que llaman poliarquía): como su nombre indica, miden variedades de democracia de diferentes dimensiones, a saber, las dimensiones liberal, participativa, deliberativa, electoral e igualitaria. Cabe esperar que las dimensiones no estén muy correlacionadas entre sí para tener la seguridad de que captan realmente las diferentes caras de este concepto polifacético. La segunda alternativa es la de la validez convergente, es decir, la correlación de mi medida de interés con las medidas de los demás. Por ejemplo, la democracia medida por V-Dem tiene una correlación de 0,8 con la forma en que es medida por Polity IV. Si creo una tercera medida, cuya correlación es 0,70 con Polity y 0,9 con V-Dem, puedo estar seguro de que las tres variables son similares a la variable latente. La técnica de Análisis de Componentes Principales (PCA, por sus siglas en inglés) nos ayuda a definir qué combinación de datos representa mejor el concepto que queremos medir. 15.2 Cómo funciona el Análisis de Componentes Principales La ACP (o PCA) es una técnica muy útil cuando queremos trabajar con diferentes variables con un solo concepto. Al elegir previamente las variables con las que queremos trabajar, la técnica PCA nos da diferentes componentes, que son la reducción de la dimensionalidad de una base de datos multivariante que condensan las dimensiones en una sola variable. Consiste literalmente en trabajar con la correlación entre las variables, y dar cuenta de lo que miden en común y lo que cada una mide individualmente. El PCA aglutina las correlaciones entre las variables y las condenas en un solo componente. Cuanto más parecidas sean las variables (altas correlaciones y en la misma dirección) menos componentes se necesitarán para captar toda la complejidad de los datos. La PCA nos dará un número de “componentes” igual al número de variables. Sin embargo, no necesitaremos todos ellos para condensar las variables y sólo conservaremos unos pocos. Ayuda pensar en PCA como un batido, las variables que ejecutamos son como la fruta que ponemos en la licuadora, mientras que los componentes son los sabores que obtenemos de la mezcla. Por ejemplo, si hacemos funcionar PCA con cuatro variables (X, Y, W, Z) obtenemos cuatro componentes (siempre obtenemos el mismo número de componentes que de variables). Suponga que obtenemos que el componente 1 está alta y positivamente correlacionado con las variables X y W, el componente 2 está alta y negativamente correlacionado con Y y W, el componente 3 está alta y positivamente correlacionado con la variable Z y débilmente y negativamente correlacionado con X, y el componente 4 está débilmente y positivamente correlacionado con Y y W. Del análisis sabemos que si mantenemos los componentes 1 y 2 nuestro índice será lo suficientemente bueno. ¿Cómo decidimos cuál y cuantos mantenemos? Los componentes vendrán con un Eigenvalor, que es un puntaje que mide cuánta varianza de las variables originales explica cada componente. El Eigenvalor ordena los componentes de mayor a menor varianza explicada. Como regla general, mantenemos los componentes cuyos Eigenvalor sean mayores que 1. La creación de diferentes componentes debido al PCA le será útil para crear índices. Ahora daremos un ejemplo en R. 15.3 Nociones básicas en R En América Latina, dado el pasado de gobiernos dictatoriales y el reciente debilitamiento democrático de muchos países, puede ser importante para una posible investigación considerar la opinión pública sobre las instituciones de los países. Una buena pregunta que se puede hacer es ¿cómo se puede saber cuál es la opinión de los ciudadanos latinoamericanos sobre las instituciones democráticas de sus países? El Proyecto de Opinión Pública Latinoamericana (LAPOP por sus siglas en inglés) - coordinado desde la Universidad de Vanderbilt - se especializa en la realización de estudios de evaluación de impacto y en la producción de informes sobre las actitudes, evaluaciones y experiencias de los individuos en los países de América Latina. Este proyecto proporciona a los investigadores diferentes preguntas que, en conjunto, podrían ayudarnos a aproximar el grado de confianza que existe en la región respecto de las instituciones democráticas del país correspondiente a cada individuo. Ejercicio 15A. Asumamos que se te pide que midas el antiamericanismo en América Latina. ¿Cómo medirías este concepto? Elije 5 o más preguntas de la encuesta LAPOP (puedes consultar el libro de códigos aquí) que usarías para medir el antiamericanismo. Como primer paso, es importante seleccionar las variables que se utilizarán para llevar a cabo el LAPOP, dejando de lado todo lo que no queremos utilizar en el índice final. Estos datos contienen doce preguntas de la encuesta LAPOP, realizada a una muestra de alrededor de 7000 personas en 10 países de América Latina. Se puede acceder al cuestionario completo en este link. Este primer paso es esencial. Es el paso que implica más trabajo teórico para el investigador, ya que a partir de aquí se definen las variables que se integrarán en el análisis (y que se dejarán fuera). En nuestro análisis, las variables que dejamos fuera son las siguientes: Nombre Descripción Fuente justifica_golpe **Variable dicotómica que mide el nivel de justificación de los golpes de estado militares en el país del encuestado contra un gran número de delitos. Basada en la pregunta “jc10” de la encuesta del LAPOP. justifica_cierre_cong Variable dicotómica que mide el nivel de justificación del cierre del congreso en situaciones difíciles por el presidente. Basada en la pregunta “jc15a” de la encuesta del LAPOP. conf_cortes Mide en una escala del 1 al 7 el nivel de confianza en los tribunales en el país del encuestado. Encuesta LAPOP. conf_instit Mide en una escala del 1 al 7 el nivel de respeto a las instituciones políticas en el país del encuestado. Encuesta LAPOP. conf_congreso Mide en una escala de 1 a 7 el nivel de confianza en el Congreso Nacional (poder legislativo) del país del encuestado. Encuesta LAPOP. conf_presidente Mide en una escala de 1 a 7 el nivel de confianza en el presidente (poder ejecutivo) del país del encuestado Basado en la pregunta “b21a” de la escuesta LAPOP. conf_partidos Mide en una escala de 1 a 7 el nivel de confianza en los partidos políticos del país del encuestado Basado en la pregunta “b21a” de la escuesta LAPOP. conf_medios Mide en una escala del 1 al 7 el nivel de confianza en los medios de comunicación en el país del encuestado Basado en la pregunta “b37” de la encuesta LAPOP. conf_elecciones Mide en una escala del 1 al 7 el nivel de confianza en las elecciones del país del encuestado. satisfecho_dem Variable dicotómica que mide el nivel de satisfacción con la democracia de los encuestados. Basada en la pregunta “pn4” de la encuesta LAPOP. dervote Mide en una escala del 1 al 7 la satisfacción con la idea de que aquellos que están en contra del gobierno en el poder pueden votar en las elecciones del país del encuestado Basada en la pregunta “d1” de la encuesta LAPOP. manifestaciones Mide en una escala del 1 al 7 la satisfacción con la idea de que aquellos que están en contra del gobierno en el poder pueden llevar a cabo manifestaciones pacíficas para expresar su punto de vista Basada en la pregunta “d2” de la encuesta LAPOP. Para poder seleccionar las variables y usar los pipes (%&gt;% ) cargamos ‘tidyverse’. Luego, cargamos la base de datos del paquete del libro, paqueteadp. library(tidyverse) library(paqueteadp) data(&quot;lapop&quot;) Estas variables pueden ser exploradas gráficamente. Por ejemplo, podemos ver -por país- la confianza que tienen sus ciudadanos en las elecciones: lapop &lt;- lapop %&gt;% group_by(pais_nombre) %&gt;% mutate(conf_elecciones_prom = mean(conf_elecciones)) %&gt;% ungroup() ggplot(lapop, aes(x = conf_elecciones)) + geom_histogram() + labs(title = &quot; Confianza en las elecciones &quot;, x = &quot; En azul, el promedio nacional &quot;, y = &quot;conteo&quot;)+ facet_wrap(~pais_nombre) + geom_vline(aes(xintercept = conf_elecciones_prom), color = &quot;blue&quot;, linetype = &quot;dashed&quot;, size = 1) Figura 7.1: Histogramas de confianza en las elecciones por país. En azul, los promedios nacionales Seleccionemos nuestras variables de interés, esto significa que dejamos fuera cualquier variable que no sea representativa de la democracia, por ejemplo la variable ‘pais’ que indica el país de cada individuo no debe ser incorporada. Para ello, generamos una nueva base de datos que contiene sólo las variables que queremos utilizar, en este caso, para saber cuál es la opinión que los latinoamericanos tienen de sus instituciones democráticas. lapop_num &lt;- lapop %&gt;% select(justifica_golpe, justifica_cierre_cong, conf_cortes, conf_instit, conf_congreso, conf_presidente, conf_partidos, conf_medios, conf_elecciones, satisfecho_dem, voto_opositor, manifestaciones) %&gt;% mutate_all(as.numeric) lapop ## # A tibble: 7,655 x 15 ## id pais_nombre justifica_golpe justifica_cierr… conf_cortes ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Argentina 0 1 6 ## 2 2 Argentina 1 0 4 ## 3 3 Argentina 0 0 4 ## # … with 7,652 more rows, and 10 more variables Habiendo ya seleccionado las variables, el primer paso es estandarizarlas. La técnica PCA acepta variables de diferentes tipos, pero es importante omitir o imputar a los promedios los valores perdidos. En este caso, omitiremos los posibles NA que tenga la base de datos. lapop_num &lt;- lapop_num %&gt;% scale() %&gt;% na.omit() %&gt;% as_tibble() El siguiente paso es observar la correlación entre variables. Esto es útil para saber cómo se relacionan las variables elegidas, y también para ver si hay correlaciones extremadamente altas entre dos o más variables. Si se da el caso de que tenemos dos o más variables con alta correlación entre ellas, éstas tendrán una enorme influencia en la orientación de los componentes. La mejor alternativa para observar la correlación entre las variables es la que ofrece el paquete GGally. Cuanto más intenso es el color, más fuerte es la correlación. En el azul tendremos correlaciones negativas, y en el rojo correlaciones positivas. library(ggcorrplot) corr_lapop &lt;- lapop_num %&gt;% # calculate correlation matrix and round to 1 decimal place: cor(use = &quot;pairwise&quot;) %&gt;% round(1) ggcorrplot(corr_lapop, type = &quot;lower&quot;, lab = T, show.legend = F) Figura 3.5: Matriz de correlación de las variables que usaremos en nuestro análisis Lo que vemos es que las variables de confianza están positivamente correlacionadas. Es decir, los que confían en el presidente tienden a confiar en los medios de comunicación, las elecciones, por ejemplo. Una vez que vemos las correlaciones, estamos listos para hacer el PCA. Para ello, utilizaremos dos paquetes diferentes que realizan el PCA, esto para proporcionar opciones de cómo realizar la técnica en R. El PCA nos permitirá definir a través de los componentes cuáles serán las dimensiones del concepto que guardamos, para luego avanzar a realizar un índice simple que denote la variación del concepto a través de diferentes unidades de análisis. 15.4 Dimensionalidad del concepto Primero, con el subconjunto de datos, generamos un PCA con el paquete de stats, que es uno de los paquetes básicos de R (ya viene instalado). Luego con el comando summary() podemos ver lo que el análisis nos da. pca &lt;- princomp(lapop_num) summary(pca, loadings = T, cutoff = 0.3) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 ## Standard deviation 1.86 1.22 1.11 0.976 0.893 0.882 0.827 ## Comp.8 Comp.9 Comp.10 Comp.11 Comp.12 ## Standard deviation 0.800 0.750 0.725 0.686 0.657 ## [ reached getOption(&quot;max.print&quot;) -- omitted 2 rows ] ## ## Loadings: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 ## justifica_golpe 0.693 0.501 ## Comp.8 Comp.9 Comp.10 Comp.11 Comp.12 ## justifica_golpe 0.304 ## [ reached getOption(&quot;max.print&quot;) -- omitted 11 rows ] Como puedes ver, hay tres componentes que nos dan un Eigenvalor (denotado por la desviación estándar) mayor que 1. Este límite será nuestro criterio para seleccionar los componentes que mantendremos. También nos muestra cómo se construye cada componente con las variables que seleccionamos. Otra forma de ver los Eigenvalores de cada componente es con el comando get_eigenvalue() del paquete factoextra. Como obtuvimos con summary(), get_eigenvalue nos da además del Eigenvalor, la varianza combinada que cada componente explica. library(factoextra) eig_val &lt;- get_eigenvalue(pca) eig_val ## eigenvalue variance.percent cumulative.variance.percent ## Dim.1 3.44 28.7 29 ## Dim.2 1.48 12.3 41 ## Dim.3 1.23 10.3 51 ## Dim.4 0.95 7.9 59 ## Dim.5 0.80 6.6 66 ## Dim.6 0.78 6.5 72 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 6 rows ] Como puede verse, el componente 1 representa alrededor del 28,7% de la varianza total de las variables elegidas. La varianza de los otros dos componentes importantes es del 12,3% y del 10,3% respectivamente. Otra forma de entregar esta información es a través de gráficos. El siguiente comando nos da en su forma más simple, el porcentaje de varianza explicada por cada uno de los componentes que el PCA nos da. fviz_eig(pca, addlabels = T, ylim = c(0, 50)) Figura 3.8: Porcentaje de la varianza explicada de cada componente Un pequeño cambio en el comando, también nos da un gráfico con los valores propios de cada uno de los componentes: fviz_eig(pca, choice = c(&quot;eigenvalue&quot;), addlabels = T, ylim = c(0, 3)) Figura 3.9: Eigenvalor de cada componente Para saber cómo está compuesto cada uno de estos componentes, podemos generar un Biplot. Este tipo de gráfico nos aparecerá como vectores en dos dimensiones (que serán los dos primeros componentes del análisis). fviz_pca_biplot(pca, repel = F, col.var = &quot;black&quot;, col.ind = &quot;gray&quot;) Figura 3.10: Biplot con las dimensiones 1 y 2 Este gráfico bidimensional muestra claramente que tenemos tres grupos de variables. Estos conjuntos de variables son los que se reducen precisamente a tres componentes, que pueden representar tres dimensiones de opinión sobre las instituciones políticas que queremos medir en América Latina. También podemos saber, cómo se compondrán las dimensiones del concepto que queremos medir con el comando fviz_contrib, cambiando los componentes en cada eje: fviz_contrib(pca, choice = &quot;var&quot;, axes = 1, top = 10) fviz_contrib(pca, choice = &quot;var&quot;, axes = 2, top = 10) fviz_contrib(pca, choice = &quot;var&quot;, axes = 3, top = 10) Figura 3.11: Las variables que contribuyen a cada dimensión en el PCA Por ejemplo, el primer componente es el más diverso, pero se alimenta en gran medida de variables de confianza. Si recuerdas la matriz de correlación que hicimos con GGally, todas estas variables tenían altas correlaciones entre sí. El segundo componente se alimenta de la fuerte correlación entre manifestaciones y voto_opositor. La línea punteada roja expresa el valor que supondría un escenario en el que todas las variables contribuyen por igual, es decir, 1/12 (8,33%), y sirve sólo como referencia visual. La composición de cada componente puede ayudarnos a nombrar cada dimensión del concepto que estamos creando. Esto nos dará más apoyo a la hora de justificar la forma en que se mide la opinión pública orientada hacia las instituciones políticas. La primera dimensión, la que tiene mayor grado de variación, representa la confianza en instituciones como los poderes del Estado, los partidos políticos y las elecciones. Es la más diversa, pero se alimenta en gran medida de variables de confianza. Si recuerdas la matriz de correlación que hicimos con GGally, todas estas variables tenían altas correlaciones entre sí. Esto significa que cuando queremos medir la confianza en la política de los individuos, lo que nuestros datos representarán principalmente es esa confianza en la política más formal, en el concepto más clásico de la política. La segunda de las dimensiones es la que denota el nivel de tolerancia política de la disidencia, que podría orientar una opinión hacia una forma de controlar los derechos de voto y de manifestación pública pacífica. Esto puede denotar lo propensos que están los ciudadanos a suspender ciertos derechos democráticos de aquellos que piensan de manera diferente. La tercera dimensión, que representa el 10% de la varianza, es la compuesta por las variables que representan la propensión a justificar los golpes de Estado o los cierres del Congreso. Nuestro concepto tendrá entonces componentes que miden cómo se forman las opiniones individuales de las rupturas democráticas, midiendo cuán frágiles son las instituciones formales dentro de la opinión pública en América Latina. Los resultados cualitativos de cada dimensión pueden ser representados en una tabla como esta: Dimensión Etiqueta Variables Varianza explicada 1 Trust in institutions conf_cortes, conf_instit, conf_congreso, conf_presidente, conf_partidos, conf_elecciones 28.7% 2 Democratic rights voto_opositor, manifestaciones 12.3% 3 Anti-Democratic values justifica_golpe, justifica_cierre_cong 10.3% A medida que se crean estas nuevas dimensiones, pueden considerarse como nuevas variables a integrar en nuestra base de datos original, como se muestra a continuación: lapop &lt;- bind_cols(lapop, as_tibble(pca$scores)) Éstas pueden tratarse como nuevas variables que pueden integrarse por separado en el análisis o, como veremos más adelante, integrarse en un índice de confianza en las instituciones políticas, que evalúa las tres dimensiones en su conjunto. Ejercicio 15B. Estandariza y omite las NAs de las preguntas que elegiste en la primera parte para el sentimiento antiamericano. Realiza un Análisis de Componentes Principales y responde las siguientes preguntas: ¿Cuáles son los componentes más importantes? ¿Qué variables los componen? ¿Qué dimensiones están involucradas en el concepto de antiamericanismo que estás realizando? 15.5 Variación del concepto Otra aplicación de esta herramienta en R permitirá generar un índice que caracterice a cada uno de los individuos diferenciando quiénes tienen más probabilidades de confiar en las instituciones y quiénes no. A partir de este índice individual, podremos entonces hacer una amplia variedad de estudios, desde comparaciones entre países, hasta comparaciones utilizando la edad, el género, los ingresos u otra variable de interés. Para esta parte, generaremos de nuevo un PCA, pero esta vez lo haremos con el comando PCA del paquete FactoMineR. Esto nos dará el mismo resultado que el paquete anterior, pero queremos mostrar que esta herramienta puede ser utilizada de varias maneras en R. library(FactoMineR) pca_1 &lt;- PCA(lapop_num, graph = F) Como vimos antes, debemos retener los componentes que contienen un Eigenvalor mayor que 1, que podemos ver de nuevo en el gráfico: fviz_eig(pca_1, choice = &quot;eigenvalue&quot;, addlabels = T, ylim = c(0, 3)) Figura 3.15: Eigenvalores de los componentes. Como regla general, retenemos los componentes con valores mayores que 1 Como pudimos ver de antemano, los componentes mayores de 1 son los tres primeros. Dejaremos en este caso el componente 4 para añadir más variables a nuestro índice. Hay un enorme grado de discrecionalidad en todo este proceso. Para condensar los componentes elegidos en una sola variable, es necesario recordar cuánta varianza acumulada representan del total: eig &lt;- get_eig(pca_1) eig ## eigenvalue variance.percent cumulative.variance.percent ## Dim.1 3.44 28.7 29 ## Dim.2 1.48 12.3 41 ## Dim.3 1.23 10.3 51 ## Dim.4 0.95 7.9 59 ## Dim.5 0.80 6.6 66 ## Dim.6 0.78 6.5 72 ## [ reached getOption(&quot;max.print&quot;) -- omitted 6 rows ] Habíamos visto que los cuatro componentes representaban casi el 60% de la variación total: el primer componente 28,7%, el segundo 12,3%, el tercero 10,3% y el cuarto 7,9%. El siguiente paso es sumar estos cuatro componentes, pero ponderando cada uno de ellos por el porcentaje de la varianza que representan. Lo hacemos de la siguiente manera: data_pca &lt;- pca_1$ind$coord%&gt;% as_tibble() %&gt;% mutate(pca_01 = (Dim.1 * 28.7 + Dim.2 * 12.3 + Dim.3 * 10.3 + Dim.4 * 7.9) / 60) lapop &lt;- bind_cols(lapop, data_pca %&gt;% select(pca_01)) Por lo tanto, hemos creado una única variable, que llamamospca_01. ¡Estamos muy cerca de tener esta variable como nuestro indicador de calificación de la democracia! Sucede que la variable pca_01 está en una escala poco amistosa. Idealmente queremos que nuestro indicador vaya de 0 a 1, de 0 a 10, o de 0 a 100 para que sea más fácil de interpretar. Haremos esto para que sea de 0 a 100, si quieres que sea de 0 a 10 o de 0 a 1 tienes que reemplazar 100 en la fórmula de abajo con el número que te interesa. lapop &lt;- lapop %&gt;% mutate(democracy_index = GGally::rescale01(pca_01) * 100)%&gt;% select(democracy_index, everything()) Ya con nuestro nuevo índice reescalado, podemos ver cómo se ve su densidad: index_density &lt;- ggplot(data = lapop, mapping = aes(x = democracy_index)) + labs(x=&quot; Índice de confianza en la democracia&quot;, y = &quot;densidad&quot;) + geom_density() index_density Figura 15.1: Gráfica de densidad de nuestro índice Ahora que tenemos el índice listo, podemos hacer todo tipo de análisis. Por ejemplo, podemos hacer comparaciones por país. Si tuviéramos variables individuales, podríamos proceder a modelos de regresión con controles para el género, la ideología, los ingresos, el nivel de educación. Para hacer esto, puedes usar lo que aprendiste en los capítulos 7 y 8. Descriptivamente, podemos hacer una comparación de cómo nuestro índice de confianza en las instituciones de los países de América Latina: lapop &lt;- lapop %&gt;% group_by(pais_nombre) %&gt;% mutate(democracy_avg = mean(democracy_index)) %&gt;% ungroup() ggplot(lapop, aes(x = democracy_index)) + geom_density() + labs(title = &quot; Confianza en la democracia en América Latina (N = 7000)&quot;, x = &quot; En azul el promedio de cada país&quot;, y = &quot;densidad&quot;) + facet_wrap(~pais_nombre) + geom_vline(aes(xintercept = democracy_avg), color = &quot;blue&quot;, linetype = &quot;dashed&quot;, size = 1) Figura 3.19: Gráficos de densidad de nuestro índice de confianza en la democracia por país. En azul, la media nacional Ejercicio 15C. Utilizando el índice de confianza en la democracia en América Latina que acabamos de crear, analiza con modelos de regresión lineal qué variables tienen un alto poder explicativo sobre esta variable, son las variables de ideología, ingresos o edad importantes? Ejercicio 15D. Con el conjunto de variables que elijas, crea un índice de antiamericanismo siguiendo las indicaciones del capítulo. E-mail: cilabrin@uc.cl↩︎ E-mail: furdinez@uc.cl↩︎ "],
["maps.html", "Capítulo 16 Mapas y datos espaciales 16.1 Introducción 16.2 Datos espaciales en R 16.3 Gestión de Datos Espaciales 16.4 Mapeo 16.5 Inferencia a partir de datos espaciales", " Capítulo 16 Mapas y datos espaciales Andrea Escobar63 y Gabriel Ortiz64 Lecturas sugeridas Brunsdon, C. and Comber, L. (2015). An Introduction to R for Spatial Analysis and Mapping. SAGE, Thousand Oaks, CA. Lansley, G. and Cheshire, J. (2016). An Introduction to Spatial Data Analysis and Visualisation in R. Consumer Data Research Centre. Lovelace, R., Nowosad, J., and Münchow, J. (2019). Geocomputation with R. CRC Press, Boca Raton, FL. Pebesma, E. (2018). Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal, 10(1):439–446. Los paquetes que necesitas instalar tidyverse (Wickham 2019b), paqueteadp (Urdinez and Cruz 2020), sf (Pebesma 2020), ggrepel (Slowikowski 2020), gridExtra (Auguie 2017), rnaturalearthhires (South 2020), spdep (Bivand 2019). 16.1 Introducción En este capítulo, aprenderemos a trabajar con datos espaciales usando R y tidyverse. Nos centraremos en la riqueza que tienen los datos espaciales, tanto como herramienta para el análisis de datos exploratorios sobre diversos fenómenos, como para visualizar y comunicar nuestros hallazgos de una forma atractiva y efectiva. Como tal, este capítulo tiene los siguientes objetivos: Describir el formato de los datos espaciales. Explicar por qué estos datos son interesantes para ser utilizados en el análisis de las ciencias políticas. Su manejo y manipulación en R, centrándose en su visualización mediante la creación de mapas estáticos y dinámicos con ggplot2. No entraremos en la elaboración de modelos estadísticos inferenciales a partir de datos espaciales (Modelos con lag espacial), ni tampoco en la generación de datos geográficos, ya que trabajaremos sobre conjuntos de datos geográficos ya creados. El conjunto de datos que utilizaremos en este capítulo será un shapefile (formato que explicaremos más adelante) de los estados de Brasil, obtenido del Spatial Data Repository of New York University, que se fusionará con los datos de Freire (2018). Por un lado, el depósito de datos espaciales de la Universidad de Nueva York proporciona muchos datos espaciales de diferentes países, así como de divisiones supranacionales y subnacionales. En este caso, utilizaremos una base de datos de las fronteras estatales de Brasil en el año 1991, proporcionadas por el Instituto Brasileño de Geografía y Estadística. Por otro lado, el trabajo de Freire (2018) explica la causa de la disminución de la tasa de homicidios en el estado de Sao Paulo como resultado de la implementación de políticas estatales para la reducción del crimen. Este trabajo es útil ya que recoge datos a nivel estatal de diferentes indicadores socioeconómicos como el PIB, el Índice de Gini, el promedio de años de escolaridad, entre otros, en un período de veinte años (1990-2010). Estos datos se unirán a nuestro shapefile para trazar estas variables en mapas. También presentamos una serie de ejercicios que te ayudarán a ganar experiencia utilizando un shapefile de América del Sur fusionado con datos del PIB per cápita y la calidad de la democracia electoral de cada país tomados de la base de datos de V-Dem. 16.1.1 Datos espaciales: ¿qué son? Los datos espaciales, o más bien los datos geoespaciales, se refieren a los datos obtenidos de lugares geográficos, es decir, de zonas de la superficie de la Tierra. El uso de este tipo de datos reconoce “la importancia de conceptos espaciales como distancia, ubicación, proximidad, vecindad y región en la sociedad humana”, permitiéndonos enfrentar los fenómenos desde una perspectiva multivariante y multidimensional, proporcionándonos información adicional para nuestras observaciones (CSISS 2004). En ciencia política, la perspectiva espacial para el análisis de datos ha avanzado gracias a trabajos como el de Gary King (1997) sobre cómo hacer inferencias ecológicas, y el de Gimpel y Schuknecht (2003) sobre la accesibilidad a los lugares de votación. Este tipo de datos permite responder a preguntas como la influencia de la proximidad de los países vecinos en la difusión de ciertas políticas, pero también resulta ser una herramienta clave en la exploración de datos para observar la existencia de patrones territoriales en otros estudios. En los últimos años, los datos espaciales se han difundido ampliamente, sobre todo gracias a las iniciativas de los gobiernos que proporcionan datos para diversos procesos sociales y económicos del país. Las nuevas plataformas de acceso abierto, como Google Maps o Open Street Map, y las redes sociales como Twitter y Facebook también contribuyeron al interés por los datos espaciales. Mientras tanto, para los investigadores y estudiantes, las bases datos como la ya mencionada del Depósito de Datos Espaciales de la Universidad de Nueva York proporcionan archivos shapefile de fácil acceso y manipulación para generar visualizaciones y unir datos georreferenciados de otras bases de datos. 16.1.2 Estructura de datos espaciales Los archivos que contienen datos espaciales se conocen comúnmente como shapefiles, que generalmente se colocan en una carpeta que contiene al menos tres archivos con las extensiones .shp, .dbf y .shx. Figura 16.1: Ejemplo de una carpeta que contiene un shapefile en Windows. Ten en cuenta que el archivo .shp contiene la gran mayoría de la información. Además, esta carpeta contiene un archivo .prj adicional En R, los shapefiles se suelen representar mediante vectores, que consisten en la descripción de la geometría o forma (shape) de los objetos. Generalmente contienen variables adicionales, llamadas atributos, sobre los mismos datos. Usando como ejemplo los datos que serán analizados a lo largo de este capítulo, nuestra base de datos describe las fronteras de los estados de Brasil (geometría), y también contiene información de la tasa de homicidios más otros datos socioeconómicos (atributos). Los datos espaciales son diversos en sus características y generalmente se dividen en tipos de vectores que en cada caso consisten en un conjunto de pares de coordenadas (x,y): Puntos: Una sola ubicación descrita por un punto como la dirección de una casa geocodificada. Líneas: Un segmento compuesto por una serie de puntos conectados entre sí que no forman un objeto cerrado. Polígonos: Una superficie plana compuesta por una serie de líneas conectadas entre sí que forman un objeto cerrado. Multi-polígonos: Multi-superficies compuestas de polígonos que no necesariamente se intersectan. Figura 16.2: Tipos de formas en los datos espaciales. (1) Punto, (2) Línea, (3) Polígono Tip. Como hemos dicho, el Repositorio de Datos Espaciales de la Universidad de Nueva York puede ser una buena alternativa si quieres empezar a trabajar con datos espaciales de tu país, estado o municipio. Puede buscar fácilmente archivos de forma entrando en la página web del Repositorio y seleccionando “Polígono” en la categoría “Tipo de datos”. Después de eso, tienes que introducir el nombre del área de interés en el buscador, seleccionar un resultado y presionar “Descargar Shapefile” en la esquina superior derecha de la página. Otra ventaja de estos datos es que los archivos suelen ser livianos, pero de suficiente calidad, lo que resulta conveniente en el caso de los equipos con limitaciones para procesar archivos de datos de gran tamaño. 16.2 Datos espaciales en R En los últimos años, R ha mejorado considerablemente sus herramientas para manipular y analizar datos geográficos de manera que sean compatibles con la sintaxis y las funcionalidades de otros paquetes del programa. En el pasado, estas tareas eran demasiado desafiantes. En este capítulo, nos centraremos en el paquete sf, que fue creado en 2016 sobre las funcionalidades base de tres paquetes anteriores: sp, rgeo y rgdal, y que implementa el modelo estándar de código abierto características simples65. sf permite la representación de objetos del mundo real en el ordenador, que poseen atributos tanto espaciales como no espaciales, basados en la geometría 2D con interpolación lineal entre vértices (Pebesma 2018). 16.2.1 Características simples en R La principal ventaja que proporciona el paquete sf es que nos permite trabajar con datos espaciales dentro del tidyverse, es decir, podemos manejar los datos espaciales como si fueran cualquier otro tipo de base de datos. Podemos hacerlo mediante funciones de R contenidas en el metapaquete tidyverse, como ggplot2 y dplyr, en línea con lo que hemos aprendido a lo largo del libro^[Algunas opciones alternativas para construir mapas en R son los paquetes tmap y cartogram, que se explican en profundidad en el Capítulo 8 de “Geocomputación con R” de66. Es importante tener la última versión de tidyverse para ejecutar las utilidades de ggplot2 que se usarán en este capítulo. Puedes actualizarlo a la última versión con el comando install.packages(\"tidyverse\"): library(tidyverse) Ahora tenemos que instalar el paquete sf. Para que esto funcione en Mac y Linux necesitas las últimas versiones de FDAL, GEOS, Proj.4 y UNDUNITS ya instaladas67. Una vez hecho esto podemos instalar el paquete sf: devtools::install_github(&quot;robinlovelace/geocompr&quot;) install.packages(&quot;sf&quot;) library(sf) 16.2.2 Estructura Para ver la estructura de los objetos de tipo sf cargamos nuestra base de datos en el formato shapefile, usando la función read_sf() (nota que estamos cargando el archivo shp_brasil.shp contenido en la carpeta shp_brasil): shp_brasil &lt;- read_sf(&quot;datos_espaciales/shp_brasil.shp&quot;) En la siguiente imagen, detallamos las partes de nuestro shapefile con sus nombres: Figura 16.3: Estructura de un objeto sf Como se puede observar en la figura anterior, las características simples se guardan por sf en un formato data.frame y con un formato adicional de clase sf. Entonces, como se ha dicho antes, podemos dirigir este conjunto de datos como cualquier otro dentro del tidyverse, usando sus funciones, pipas (pipes o %&gt;%), etc. Aquí, cada fila consiste en una simple característica, y cada columna consiste en un atributo. La diferencia con una base de datos normal es que aquí encontramos una nueva columna de geometrías de rasgos simples (single-featured geometry column - sfc), lo que resulta en un marco de datos con una columna extra para información espacial. Podemos verificar esto comprobando el tipo de archivo que hemos cargado con la función class(). Observe que shp.brasil es, al mismo tiempo, un archivo de tipo sf y un data.frame: class(shp_brasil) ## [1] &quot;sf&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Ahora, si exploramos nuestros datos con mayor detalle encontraremos que este estándar puede ser implementado en diferentes tipos de geometría en nuestra base de datos: Un vector numérico para un solo punto (POINT) Una matriz numérica (cada fila es un punto) para una serie de puntos (MULTIPOINT o LINE) Una lista de matrices para un grupo de conjuntos de puntos (MULTILINESTRING, POLYGON) Una lista de matrices (MULTIPOLYGON) que se convierte en la más utilizada cuando los datos geográficos se representan como forma y ubicación de los países u otras unidades administrativas de éstos. Una lista de cualquiera de los elementos anteriormente mencionados (GEOMETRYCOLLECTION) Usando la función st_geometry() podemos ver la geometría incluida en nuestro shapefile: st_geometry(shp_brasil) ## Geometry set for 28 features ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -2200000 ymin: -3700000 xmax: 2400000 ymax: 590000 ## projected CRS: Polyconic ## First 5 geometries: Una vez cargado nuestro shapefile, podemos empezar a generar mapas. Ten en cuenta que la forma de hacerlo se basa en ggplot2; sólo tenemos que seleccionar nuestra base de datos y añadir el geom geom_sf()68. ggplot(data = shp_brasil) + geom_sf() Figura 3.8: Mapa de los estados de Brasil Ejercicio 16A. 1. Descarga el shapefile de Sudamérica de ArcGIS y cárgalo en R usando read_sf(). Selecciona sólo las variables “CNTRY_NAME”, “ISO_3DIGIT” y “geometry”. 2. Filtra la base de datos por la variable ‘CNTRY_NAME’ para eliminar las observaciones de “South Georgia &amp; the South Sandwich Is.” y “Falkland Is.”. 3. Trazar el shapefile usando ggplot() y geom_sf(). 16.3 Gestión de Datos Espaciales Ya aprendimos a cargar archivos de forma con read_sf(). Ahora que hemos cargado el archivo, procederemos a aprender qué tipo de modificaciones podemos hacer para generar nuevos datos a partir de la información que ya tenemos. También mostraremos cómo unir datos de otras bases de datos a nuestro shapefile. 16.3.1 Modificaciones Hay dos maneras de hacer modificaciones en nuestra base de datos georeferenciada. La primera consiste en aplicar las técnicas que ya hemos aprendido usando el tidyverse, mientras que la segunda consiste en usar las funciones incorporadas en el paquete sf. En este paquete, todas las funciones empiezan con st_, para que puedan ser fácilmente comprendidas en la herramienta de completado de RStudio. Estas funciones se utilizan principalmente para transformar y realizar operaciones geográficas. En estas secciones combinaremos ambas técnicas para generar nuevas variables y datos asociados a nuestra base de datos. 16.3.1.1 Filtrar y seleccionar por unidades geográficas Una de las primeras funciones que podemos utilizar las funciones de dplyr para seleccionar datos dentro de nuestra base. Así, por ejemplo, podemos ocupar filter() para seleccionar ciertos estados de Brasil, como por ejemplo el estado de São Paulo: shp_brasil %&gt;% filter(estado == &quot;São Paulo&quot;) ## Simple feature collection with 1 feature and 1 field ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 92000 ymin: -2800000 xmax: 1e+06 ymax: -2200000 ## projected CRS: Polyconic ## # A tibble: 1 x 2 ## estado geometry ## * &lt;chr&gt; &lt;MULTIPOLYGON [m]&gt; ## 1 São Paulo (((719692 -2716734, 719500 -2717678, 719210 -2716889, 719362… Un desafío común que a menudo enfrentamos cuando trabajamos con shapefiles de países enteros es la existencia de regiones o zonas insulares que forman parte del territorio administrativo pero que están aisladas geográficamente, por ejemplo, la Isla de Pascua en Chile o las Islas Galápagos en Ecuador. Por diversas razones, el acceso a los datos de esas regiones es limitado y a menudo se dejan fuera del análisis. Por lo tanto, estamos interesados en sacar estas regiones de nuestro shapefile. En Brasil, el Distrito Estatal de Fernando de Noronha, un archipiélago de 21 islas situado en el Océano Atlántico es uno de este tipo de casos (es posible identificarlo como un pequeño punto en la parte superior derecha del mapa anterior). Podemos eliminar fácilmente este tipo de datos de nuestro shapefile, de nuevo, con la función filter(). shp_brasil &lt;- shp_brasil %&gt;% filter(estado != &quot;State District of Fernando de Noronha (PE)&quot;) Esto se reflejará en los siguientes mapas: ggplot(data = shp_brasil) + geom_sf() Figura 3.11: Mapa de los estados brasileños excepto el Distrito Estatal de Fernando de Noronha 16.3.1.2 Generar nuevas unidades con st_union() Otra opción interesante es generar nuevas variables que agrupen múltiples unidades geográficas. Al hacer esto, estaremos generando efectivamente nuevas unidades geográficas más allá de la información inicial contenida en nuestro shapefile. Por ejemplo, en 1969 el Instituto Brasileño de Geografía y Estadística dividió el país en 5 regiones que agrupan los 27 estados del país. Dado que se trata de una división con fines académicos, y no reconocida en términos político-administrativos, no es posible encontrar archivos de tipo shapefile que muestren estas regiones. Sin embargo, usando mutate() y case_when(), podemos generar fácilmente esta categoría nosotros mismos: shp_brasil &lt;- shp_brasil %&gt;% mutate(region = case_when( estado %in% c(&quot;Goiás&quot;, &quot;Mato Grosso&quot;, &quot;Mato Grosso do Sul&quot;, &quot;Distrito Federal&quot;) ~ &quot;Centro-Oeste&quot;, estado %in% c(&quot;Acre&quot;, &quot;Amapá&quot;, &quot;Amazonas&quot;, &quot;Pará&quot;, &quot;Rondônia&quot;, &quot;Roraima&quot;, &quot;Tocantins&quot;) ~ &quot;Norte&quot;, estado %in% c(&quot;Alagoas&quot;, &quot;Bahia&quot;, &quot;Ceará&quot;, &quot;Maranhão&quot;, &quot;Paraíba&quot;, &quot;Pernambuco&quot;, &quot;Piauí&quot;, &quot;Rio Grande do Norte&quot;, &quot;Sergipe&quot;) ~ &quot;Noroeste&quot;, estado %in% c(&quot;Espírito Santo&quot;, &quot;Minas Gerais&quot;, &quot;Rio de Janeiro&quot;, &quot;São Paulo&quot;) ~ &quot;Sudeste&quot;, estado %in% c(&quot;Paraná&quot;, &quot;Rio Grande do Sul&quot;, &quot;Santa Catarina&quot;) ~ &quot;Sur&quot;) ) Una vez generada, podemos incorporar esta variable a nuestro mapa: ggplot(data = shp_brasil)+ geom_sf(aes(fill = region))+ labs(fill = &quot;Región&quot;) Figura 3.13: Mapa de los estados brasileños agrupados por región Mejor aún, podemos usar esta categoría para generar nueva geometría con group_by(), summarize() y st_union(): shp_brasil_regions &lt;- shp_brasil %&gt;% group_by(region) %&gt;% summarize(geometry = st_union(geometry)) %&gt;% ungroup() Podemos graficar este resultado con ggplot(): ggplot(shp_brasil_regions) + geom_sf(aes(fill = region))+ labs(fill = &quot;Región&quot;) Figura 3.15: Mapa de las regiones del Brasil Observa que esto no genera objetos completamente “planos”, es decir, todavía podemos observar algunas líneas dentro de ellos, probablemente porque los polígonos de nuestro shapefile no se superponen perfectamente. Esta es una dificultad común cuando se realizan este tipo de operaciones, e incluso puede ocurrir cuando se trabaja con shapefiles sofisticados. A pesar de las dificultades, estas operaciones son beneficiosas cuando, por ejemplo, se elaboran distritos electorales compuestos por varias provincias o municipios. 16.3.1.3 Crear nuevos shapefiles con st_write() Podemos guardar este nuevo shapefile con las funciones st_write(), en las que sólo tenemos que seleccionar el objeto que queremos guardar y la ruta donde queremos que se guarde. En este caso, guardaremos el archivo shp_brasil_regions.shp en una carpeta con el mismo nombre. Esto generará automáticamente no sólo el .shp, sino también todos los demás archivos que constituyen el shapefile: dir.create(&quot;shp_brasil_regions/&quot;) st_write(shp_brasil_regions, &quot;shp_brasil_regions/shp_brasil_regions.shp&quot;) Advertencia: El comando st_write() no puede sobreescribir los archivos existentes y al intentarlo se informará automáticamente de un error. Si quieres modificar un shapefile ya generado, tienes que borrarlo manualmente de su carpeta antes de generar los nuevos archivos. 16.3.2 Añade datos de otras bases de datos con left_join() En primer lugar, aprenderemos a añadir datos de otras bases de datos a nuestro shapefile, ya que normalmente querremos conocer los atributos de nuestras ubicaciones geográficas para generar un análisis de inferencia exploratorio o estadístico. Para ejemplificarlo, cargaremos la base de datos de Freire (previamente eliminamos algunos datos para simplificar las base de datos): library(paqueteadp) data(&quot;estados_brasil&quot;) Nota que nuestra base de datos contiene la variable estado. Esta misma variable se encuentra en nuestro shapefile: head(estados_brasil$estado) ## [1] &quot;Acre&quot; &quot;Acre&quot; &quot;Acre&quot; &quot;Acre&quot; &quot;Acre&quot; &quot;Acre&quot; head(shp_brasil$estado) ## [1] &quot;Rondônia&quot; &quot;Acre&quot; &quot;Amazonas&quot; &quot;Roraima&quot; &quot;Pará&quot; &quot;Amapá&quot; Nos hemos asegurado previamente de que las observaciones estén codificadas de la misma manera tanto en la base de datos como en el shapefile.69 Haciendo esto, podemos unir ambas bases de datos con el comando left_join(): shp_brasil_data &lt;- shp_brasil %&gt;% left_join(estados_brasil) Usando head() podemos ver el resultado de esta operación: head(shp_brasil_data) ## Simple feature collection with 6 features and 7 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -1400000 ymin: -1500000 xmax: -630000 ymax: -890000 ## projected CRS: Polyconic ## # A tibble: 6 x 8 ## estado geometry region estado_cod anio ## &lt;chr&gt; &lt;MULTIPOLYGON [m]&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rondô… (((-977421 -892385, -975… Norte 11 1990 ## 2 Rondô… (((-977421 -892385, -975… Norte 11 1991 ## 3 Rondô… (((-977421 -892385, -975… Norte 11 1992 ## # … with 3 more rows, and 3 more variables Una vez que la información de Freire se incorpore a nuestro shapefile, estos datos serán útiles cuando empecemos a mapear las variables. Ejercicio 16B. 1. Selecciona los países del Cono Sur (Chile, Argentina y Uruguay) y grafícalos. 2. Genera un nuevo shapefile con las subregiones de América del Sur. Sugerimos lo siguiente: + Mar Caribe, correspondiente a Colombia, Venezuela, Surinam, Guayana y Guayana Francesa. + Región Andina, correspondiente a Bolivia, Ecuador, Perú y Colombia. + Región Oriental, correspondiente a Brasil y a Paraguay. + Cono Sur, correspondiente a Chile, Argentina y Uruguay. 3. Descarga la base de datos ampliada (‘Country-Year: V-Dem Full+Others’) de V-Dem y selecciona sólo las siguientes variables: ‘country_name’, ‘country_text_id’, ‘year’, ‘v2x_polyarchy’,’ e_migdppc’. Fíltralas para considerar sólo el período entre 1996 y 2016 (los últimos 20 años, para los que hay datos disponibles para todas las variables). 4. Usando left_join(), añade el shapefile original a la base de datos cargada del ejercicio anterior. Consejo: utiliza los argumentos by.x=\"ISO_3DIGIT\" y by.y=\"country_text_id\"\"). Revisa la base de datos. Notarás que falta un país. ¿Cuál es? 16.4 Mapeo Los mapas han sido históricamente la principal técnica para almacenar y comunicar datos espaciales, y los objetos y sus atributos pueden ser fácilmente moldeados de manera que el ojo humano pueda reconocer rápidamente patrones y anomalías en un mapa bien diseñado. (ver Spatial Analysis Online). En esta sección, aprenderás a hacer diferentes tipos de mapas (tanto estáticos como animados) a partir de datos geográficos y sus atributos, utilizando el formato ggplot2 como base. 16.4.1 Generando centroides La primera opción es generar nuevas variables asociadas a nuestras unidades usando el comando mutate. Una acción común es generar lo que se llama “centroides”, es decir, puntos situados en el centro de nuestras unidades. Para generar los centroides, necesitamos crear las siguientes variables asociadas a nuestros geoms: centroid, coords, coords_x y coords_y. Podemos hacer esto con los comandos map() y map_dbl(), del paquete purr, y los comandos st_centroid() y st_coordinates(), del paquete sf. shp_brasil &lt;- shp_brasil %&gt;% mutate(centroid = map(geometry, st_centroid), coords = map(centroid, st_coordinates), coords_x = map_dbl(coords, 1), coords_y = map_dbl(coords, 2)) head(shp_brasil) ## Simple feature collection with 6 features and 6 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -2200000 ymin: -1500000 xmax: 880000 ymax: 590000 ## projected CRS: Polyconic ## # A tibble: 6 x 7 ## estado geometry region centroid coords coords_x ## &lt;chr&gt; &lt;MULTIPOLYGON [m]&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; ## 1 Rondô… (((-977421 -892385, -975… Norte &lt;XY [2]&gt; &lt;dbl[… -9.68e5 ## 2 Acre (((-2179954 -836549, -20… Norte &lt;XY [2]&gt; &lt;dbl[… -1.81e6 ## 3 Amazo… (((-1482456 230568, -147… Norte &lt;XY [2]&gt; &lt;dbl[… -1.19e6 ## # … with 3 more rows, and 1 more variable Una vez generadas, podemos graficar estas variables con ggplot() y el paquete ggrepel para generar el texto: library(ggrepel) ggplot(data = shp_brasil) + geom_sf()+ geom_text_repel(mapping = aes(coords_x, coords_y, label = estado), size = 4, min.segment.length = 0)+ labs(x = &quot;&quot;, y = &quot;&quot;) Figura 3.22: Mapa de Brasil con los nombres de sus estados 16.4.2 Mapeando variables Podemos generar visualizaciones más complejas de elementos que ya hemos visto en secciones anteriores. Por ejemplo, podemos mapear una variable continua de acuerdo a un color, usando el mapeo estético fill = de ggplot2: ggplot(data = shp_brasil_data) + geom_sf(aes(fill = gini)) + labs(fill = &quot;Índice Gini&quot;) Figura 3.23: Mapa de Brasil sin coordenadas y líneas cartesianas 16.4.3 Mapeando puntos Si, aparte de los polígonos, tenemos datos que denotan las ubicaciones de los eventos que han ocurrido dentro del espacio que cubre todo nuestro shapefile, también es posible graficar toda esa información en un grupo. En este caso, aprenderemos dos maneras de realizar esta tarea; una usando geom_sf, y la otra, usando un geom que ya hemos visto anteriormente en el libro: geom_point(). Para este ejemplo, utilizaremos un ejemplo adaptado de Freire et al. (2019) que corresponde a un estudio que georeferencia las víctimas mortales del periodo dictatorial de Augusto Pinochet entre 1973 y 1990. Este paquete, bajo el nombre de pinochet, está subido al depósito de CRAN. Para empezar, instalemos este paquete junto con el paquete rnaturalearthhires, que nos proporciona la forma de todos los países del mundo desde la página web de Natural Earth Data’s: install.packages(&quot;pinochet&quot;) install.packages(&quot;rnaturalearthhires&quot;, repos=&quot;http://packages.ropensci.org&quot;, type = &quot;source&quot;) Tip: El paquete rnaturalearthhires puede ser de gran utilidad si se posee una base de datos con variables que puedan ser comparadas entre países, usando las técnicas que ya se han aprendido. A continuación, filtraremos la base de datos para seleccionar Chile y sus países vecinos: chile &lt;- rnaturalearthhires::countries10 %&gt;% st_as_sf() %&gt;% filter(SOVEREIGNT %in% c(&quot;Chile&quot;, &quot;Argentina&quot;, &quot;Peru&quot;, &quot;Bolivia&quot;)) A continuación, cargamos la base de datos pinochet y seleccionamos las variables que se utilizarán, que corresponden al tipo de lugar donde la víctima fue atacada, así como la latitud y la longitud del lugar. También contiene una descripción de los lugares, que se utilizará más adelante en un ejercicio. pinochet &lt;- pinochet::pinochet %&gt;% select(last_name, first_name, place_1, longitude_1, latitude_1, location_1) head(pinochet) ## last_name first_name place_1 longitude_1 latitude_1 ## 1 Corredera Reyes Mercedes del Pilar In Public -71 -34 ## 2 Torres Torres Benito Heriberto Home -71 -33 ## 3 Lira Morales Juan Manuel In Public -71 -33 ## location_1 ## 1 Calle Gran Avenida ## 2 Santiago ## 3 La Legua shantytown ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 3 rows ] Luego, usamos la función st_as_sf() para transformar las variables de longitud y latitud en geometrías del tipo sf. El argumento crs representa el tipo de Sistema de Referencia de Coordenadas (CRS, por sus siglas en inglés) según su código llamado EPSG. En este caso, utilizaremos el código “4326”, que corresponde al estándar WGS84, el más común. Por otro lado, usamos el comando remove = F para que la función no elimine las variables originales de latitud y longitud de la base de datos, de tal forma que se pueda usar para generar puntos con geom_points. También eliminaremos previamente los valores NA. Tip: Cuando repliques este ejercicio, ten en cuenta que en el argumento coords siempre tienes que poner primero la longitud y la latitud. De lo contrario, tus variables pueden quedar con sus coordenadas invertidas. pinochet_sf &lt;- pinochet %&gt;% filter(place_1 != &quot;NA&quot; &amp; !is.na(longitude_1) &amp; !is.na(latitude_1)) %&gt;% st_as_sf(coords = c(&quot;longitude_1&quot;, &quot;latitude_1&quot;), crs = 4326, remove = F) head(pinochet_sf) ## Simple feature collection with 6 features and 6 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -71 ymin: -34 xmax: -71 ymax: -33 ## geographic CRS: WGS 84 ## last_name first_name place_1 longitude_1 latitude_1 ## 1 Corredera Reyes Mercedes del Pilar In Public -71 -34 ## 2 Torres Torres Benito Heriberto Home -71 -33 ## location_1 geometry ## 1 Calle Gran Avenida POINT (-71 -34) ## 2 Santiago POINT (-71 -33) ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 4 rows ] class(pinochet_sf) ## [1] &quot;sf&quot; &quot;data.frame&quot; Observa que con esta acción hemos transformado la base de datos en un shapefile. A continuación, generamos el mapa con geom_sf, usándolo en dos instancias: primero, para mapear los polígonos de los países, y segundo, para graficar los puntos espaciales. Usamos el atributo size para el tamaño del punto. Considera que usamos el argumento coords_sf para delimitar el área que se mostrará en el mapa. Haciendo esto, también podemos identificar las víctimas atacadas fuera del territorio nacional de Chile, sin perder el foco en este país. ggplot() + geom_sf(data = chile) + geom_sf(data = pinochet_sf, size = 1) + coord_sf(xlim = c(-75.6, -67), ylim = c(-55, -19)) + scale_x_continuous(breaks = c(-76, -67)) # también limpiar la escala x Figura 3.27: Mapa de la ubicación de las víctimas de la dictadura de Pinochet en Chile y sus países vecinos Podemos obtener un resultado similar con geom_point(). En este caso, añadimos el argumento color = para mostrar el tipo de lugar donde la víctima fue atacada. ggplot() + geom_sf(data = chile) + geom_sf(data = pinochet_sf, mapping = aes(color = place_1), size = 1) + coord_sf(xlim = c(-75.6, -67), ylim = c(-55, -19)) + scale_x_continuous(breaks = c(-76, -67)) + labs(color = &quot;&quot;) # también limpiar la escala x Figura 3.28: Mapa de la ubicación de las víctimas de la dictadura de Pinochet en Chile y sus países vecinos, codificado por color según el tipo de lugar Ejercicio 16C. Generar centroides para los países (Tip: usar CNTRY_NAME). Generar un mapa usando el argumento fill de ggplot() para usar un color diferente para cada país. Grafique un mapa combinando los atributos de los dos ejercicios anteriores. Crear un mapa con el PIB per cápita (e_migdppc) de cada país en el año 2016. ¿Cuáles son los países que no tienen datos para el 2016? Crear una tabla con el valor de Democracia (v2x_polyarchy) de cada país en los años 2013, 2014, 2015 y 2016. 16.5 Inferencia a partir de datos espaciales Más allá del análisis exploratorio y descriptivo que se puede hacer con los datos espaciales, éstos son también de gran utilidad para hacer inferencias sobre la relación de varios fenómenos. La inferencia basada en los datos espaciales parte del reconocimiento de que las observaciones espaciales no pueden asumirse como mutuamente independientes, ya que las observaciones que están cerca unas de otras tienden a ser similares. Por lo tanto, debemos prestar mucha atención a las diferentes pautas de asociación que existen en los fenómenos que estamos estudiando. Estas pautas espaciales (autocorrelación espacial), miden la influencia de la distancia sobre una determinada variable, y pueden utilizarse como información relevante de los tipos de influencia que aún no se han observado o considerado (Bivand, Pebesma, and Gómez-Rubio 2013, 11). 16.5.1 Indicador local de asociación espacial (LISA, por sus siglas en inglés) En esta sección encontrarás los mecanismos básicos para entrar en la correlación espacial, basados en el Indicador Local de Asociación Espacial (LISA), introducido por Luc Anselin (1995). Éstos permiten indicar la similitud (correlación) existente entre observaciones cercanas entre sí, es decir, si están agrupadas en cluster espacialmente. Para ello, Anselin indica que: El LISA para cada observación da un indicador del grado de agrupación espacial significativa de valores similares alrededor de esa observación. La suma de los LISA de las observaciones es proporcional a un indicador de correlación espacial global. Mediante la prueba de la significancia estadística de estos indicadores podemos identificar los lugares en los que hay un grado significativo de clusterización (Brunsdon and Comber 2015, 249). Se dice que las variables tienen una correlación espacial positiva cuando los valores similares tienden a estar más cerca unos de otros que los valores diferentes (Lansley and Cheshire 2016, 77) 16.5.2 Matriz de pesos espaciales El primer paso para hacer este tipo de análisis es determinar el conjunto de vecindarios para cada observación, es decir, identificar los polígonos que comparten fronteras entre ellos. Luego, necesitamos asignar un peso para cada relación de vecindad, lo que permite definir la fuerza de esta relación basada en la proximidad. En las matrices de pesos, los vecinos se definen de forma binaria [0,1] en cada fila, indicando si existe o no una relación. Para hacer este tipo de análisis, primero tenemos que cargar el paquete spdep, y guardar las coordenadas de las unidades en nuestro mapa: library(spdep) coords &lt;- coordinates(as((shp_brasil), &#39;Spatial&#39;)) En esta sección, también trabajaremos con el shapefile al que hemos añadido datos de la base de datos de Freire (2018), shp_brasil_data, pero sólo utilizaremos los datos del último año disponible, 2009: shp_brasil_data &lt;- shp_brasil_data %&gt;% filter(anio == 2009) Hay tres criterios diferentes para calcular los barrios: 16.5.2.1 El criterio Rook (de la torre) El criterio Rook considera como vecinos a cualquier par de células que compartan un borde. Piensa en una torre en un tablero de ajedrez. Figura 16.4: El criterio Rook de vecindad Para generar este criterio, usamos la función poly2nb() del paquete spdep. Tenemos que asegurarnos de que el argumento queen = está configurado como F. rook_brasil &lt;- poly2nb(as(shp_brasil_data, &#39;Spatial&#39;), queen = FALSE) Podemos graficar esto sobre nuestro mapa con ggplot(). Primero, necesitamos pasar nuestras coordenadas a un marco de datos con esta función de Maxwell B. Joseph. nb_to_df &lt;- function(nb, coords){ x &lt;- coords[, 1] y &lt;- coords[, 2] n &lt;- length(nb) cardnb &lt;- card(nb) i &lt;- rep(1:n, cardnb) j &lt;- unlist(nb) return(data.frame(x = x[i], xend = x[j], y = y[i], yend = y[j])) } Generamos el dataframe: rook_brasil_df &lt;- nb_to_df(rook_brasil, coords) Ahora, podemos generar el gráfico con geom_point() y geom_segment(): ggplot(shp_brasil_data) + geom_sf()+ geom_point(data = rook_brasil_df, mapping = aes(x = x, y = y))+ geom_segment(data = rook_brasil_df, mapping = aes(x = x, xend = xend, y = y, yend = yend))+ labs(x = &quot;&quot;,y = &quot;&quot;) Figura 5.13: El criterio Rook de vecindad en Brasil 16.5.2.2 Criterio Queen (de la Reina) El criterio de la reina considera como vecinos a cualquier par de celdas que compartan un borde o un vértice. Figura 16.5: El criterio Queen de vecindad Para generar este criterio, también usamos la función poly2nb(), y después el dataframe: queen_brasil &lt;- poly2nb(as(shp_brasil_data, &#39;Spatial&#39;), queen = T) queen_brasil_df &lt;- nb_to_df(queen_brasil, coords) Ahora podemos generar el gráfico, lo cual hacemos directamente sobre nuestro mapa de Brasil: ggplot(shp_brasil_data) + geom_sf()+ geom_point(data = queen_brasil_df, mapping = aes(x = x, y = y))+ geom_segment(data = queen_brasil_df, mapping = aes(x = x, xend = xend, y = y, yend = yend))+ labs(x = &quot;&quot;, y = &quot;&quot;) Figura 16.6: El criterio Rook de vecindad en Brasil 16.5.2.3 Criterio k-cercanos (K-nearest) En el criterio K-más cercano se generan barrios basados en la distancia entre vecinos, donde ‘k’ se refiere al número de vecinos de un lugar en particular, calculado como la distancia entre los puntos centrales de los polígonos. Se suele aplicar cuando las áreas tienen diferentes tamaños, de modo que cada ubicación contiene el mismo número de vecinos, independientemente del tamaño de las áreas vecinas (Fuente: https://geodacenter.github.io/glossary.html). En este caso, usaremos los seis vecinos más cercanos. Podemos hacerlo con el comando knn2nb. También generamos inmediatamente el dataframe: kn_brasil &lt;- knn2nb(knearneigh(coords, k = 6)) kn_brasil_df &lt;- nb_to_df(kn_brasil,coords) Entonces, genera el mapa con ggplot(): ggplot(shp_brasil_data) + geom_sf()+ geom_point(data = kn_brasil_df, mapping = aes(x = x, y = y))+ geom_segment(data = kn_brasil_df, mapping = aes(x = x, xend = xend, y = y, yend = yend))+ labs(x = &quot;&quot;, y = &quot;&quot;) Figura 16.7: Mapa de los vecinos, criterio de cercanía Obsérvese que, mientras que Rook y Queen generan, para nuestro caso, resultados similares en términos de vecindad en nuestro mapa, el modelo de k-cercanos añade muchas más relaciones. 16.5.3 I de Moran La I de Moran es la estadística más utilizada para identificar la correlación espacial: \\[ I = \\frac{n}{\\sum_{i=1}^{n}(yi - \\bar{y})^2} \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}wij(yi - \\bar{y})(yj - \\bar{y})}{\\sum_{i=1}^{n}\\sum_{j=1}^{n}wij}\\] Esta fórmula, aunque parece compleja, no es más que una versión ampliada de la fórmula para calcular el coeficiente de correlación, a la que se añade una matriz de peso espacial70. A través de ella, podemos probar y visualizar la presencia de la autocorrelación espacial. Primero, haremos una prueba global que crea una única medida de correlación espacial. Este test de Moran crea una correlación entre -1 y 1, en la que: 1 determina una perfecta correlación espacial positiva (indica que nuestros datos están agrupados en clusters). 0 significa que nuestros datos están distribuidos aleatoriamente. -1 representa una correlación espacial negativa (valores disímiles que están cerca unos de otros). Figura 16.8: Ilustración de la autocorrelación espacial Utilizando una simulación de Monte Carlo, en la que los valores se asignan aleatoriamente a los polígonos para calcular el I de Moran, la simulación se repite muchas veces para establecer una distribución de los valores esperados. Después de esto, el valor del I de Moran observado se compara con la distribución simulada para ver cuán probable es que los valores observados puedan considerarse aleatorios, lo que permite determinar si existe una autocorrelación espacial significativa (RSpatial). Para ejecutar el test del I de Moran utilizamos el comando moran.test. La variable para la que diagnosticaremos la correlación espacial es el índice de Gini, y utilizaremos un criterio Queen. Antes, necesitamos generar un objeto tipo listw (matriz de peso), basado en el criterio Queen. queen_brasil_lw &lt;- nb2listw(queen_brasil) Con este objeto podemos ejecutar el test I de Moran, donde seleccionamos la matriz de peso que acabamos de crear: moran.test(shp_brasil_data$gini, listw = queen_brasil_lw) ## ## Moran I test under randomisation ## ## data: shp_brasil_data$gini ## weights: queen_brasil_lw ## ## Moran I statistic standard deviate = 2, p-value = 0.01 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.284 -0.038 0.020 El resultado del test I de Moran muestra que hay una ligera relación de correlación positiva, frente a la expectativa de una ligera relación negativa. Sin embargo, el test resulta no ser estadísticamente significativo, ya que tiene un valor p inferior a un umbral de significativo de 0,05. Por lo tanto, podemos indicar que la tasa de homicidios no presenta altos grados de autocorrelación espacial al analizar estos datos a nivel estatal en Brasil. Lo más probable es que muestre una correlación espacial si utilizamos una unidad de análisis más pequeña (microrregiones dentro de los estados, por ejemplo). También creamos un gráfico de dispersión para visualizar el signo y la fuerza de la correlación espacial. Para generar un gráfico de dispersión utilizamos el comando moran.plot(): moran.plot(shp_brasil_data$ratio_homicidios, listw = queen_brasil_lw, xlab = &quot;Tasa de homicidios&quot;, ylab = &quot;Tasa de homicidios (con corr. espacial)&quot;) Figura 16.9: Correlación espacial del índice de Gini en Brasil a nivel estatal En este mapa, la línea sólida del gráfico indica el valor del I de Moran, es decir, la medida global de autocorrelación espacial en nuestros datos. Como vimos en la prueba anterior, es ligeramente positivo. El eje horizontal del gráfico muestra los datos de la tasa de homicidios a nivel estatal en Brasil, y el eje vertical muestra los mismos datos, pero con retraso espacial. Los cuatro cuadrantes del gráfico describen el valor de la observación en relación con sus vecinos: alto-alto, bajo-bajo (autocorrelación espacial positiva), bajo-alto o alto-bajo (autocorrelación espacial negativa), y el gráfico también señala los valores considerados atípicos en esta relación, como el Distrito Federal o Acre.71 Si bien el test I de Moran nos permite identificar la existencia de clusters a nivel global, no permite identificar si existen clusters locales significativos en la variable que estamos analizando (Lansley and Cheshire 2016, 78). Por eso queremos hacer un test I de Moran a nivel local, donde se calculan los indicadores locales de asociación espacial para cada unidad de nuestros datos, y se prueba la relación para probar su significancia estadística. Esto nos da datos sobre el patrón geográfico de la relación de los datos espaciales, y si hay desviaciones locales de los patrones globales de autocorrelación espacial. Después de crear un nuevo listw de objeto de vecinos, este tipo con el argumento style = \"B\" para indicar una clasificación binaria, utilizamos localmoran() para ejecutar el test I de Moran. Añadimos sus resultados (estimación y valor p) a nuestro objeto sf existente con el conocido mutate(): queen_brasil_b_lw &lt;- nb2listw(queen_brasil, style = &quot;B&quot;) shp_brasil_data &lt;- shp_brasil_data %&gt;% mutate(lmoran = localmoran(x = gini, listw = queen_brasil_b_lw)[, 1], lmoran_pval = localmoran(x = gini, listw = queen_brasil_b_lw)[, 5] ) A continuación, mapeamos los resultados con ggplot2: ggplot(shp_brasil_data) + geom_sf(aes(fill = lmoran))+ labs(fill = &quot; Estadística local de Moran&quot;) + scale_fill_viridis_c() Figura 11.6: El test de Moran a nivel local para diferentes valores de Gini Este mapa nos permite observar la variación de la autocorrelación en todo el espacio, pero no permite identificar si los patrones de autocorrelación son cúmulos con valores altos o bajos. Esto permitiría analizar el tipo de autocorrelación espacial que existe y su significado. Para ello necesitamos crear un mapa de clusters LISA, el cual creará una etiqueta basada en los tipos de relación que comparte con sus vecinos (alto-alto, bajo-alto, insignificante, etc.) en concordancia con los valores de la variable que estamos analizando (Gini). Para ello, necesitamos hacer una serie de transformaciones que detallamos a continuación: shp_brasil_data &lt;- shp_brasil_data %&gt;% mutate( # Estandarizar el Gini y el Moran local a sus valores medios: st_gini = gini - mean(gini), st_lmoran = lmoran - mean(lmoran), # Crear la nueva variable categórica: cuadrante = case_when( lmoran_pval &gt; 0.05 ~ &quot;Insignificante&quot;, st_gini &gt; 0 &amp; st_lmoran &gt; 0 ~ &quot;Alto-Alto&quot;, st_gini &lt; 0 &amp; st_lmoran &lt; 0 ~ &quot;Bajo-Bajo&quot;, st_gini &lt; 0 &amp; st_lmoran &gt; 0 ~ &quot;Bajo-Alto&quot;, st_gini &gt; 0 &amp; st_lmoran &lt; 0 ~ &quot;Alto-Bajo&quot; ) ) Ahora podemos generar el gráfico con ggplot(). Nota que usamos scale_fill_manual() para cambiar los colores para que reflejen la intensidad de la autocorrelación: ggplot(shp_brasil_data, aes(fill = cuadrante)) + geom_sf()+ labs(fill = &quot;Cuadrante&quot;)+ scale_fill_manual(values = c(&quot;red&quot;, &quot;lightblue&quot;, &quot;white&quot;)) Figura 16.10: Patrones geográficos de agrupación para diferentes valores de Gini Este mapa nos proporciona mayor información sobre los patrones geográficos de autocorrelación espacial. Este mapa nos muestra la existencia de clusters; por lo tanto, muestra regiones agrupadas en lugar de ubicaciones individuales. Es importante señalar que estos mapas no son significativos, pero nos permiten encontrar ubicaciones o relaciones que pueden ser interesantes para un análisis más profundo. En este caso, podemos interpretar que en la mayor parte de Brasil no existe una correlación espacial para los valores de Gini a nivel estatal, es decir, su valor no está influenciado por la proximidad de otros valores similares. Sin embargo, en algunos estados situados en el noreste de Brasil, podemos observar la existencia de un cluster donde se concentran altos valores de Gini, que tienen vecinos que también tienen altos valores en la misma variable (puntos calientes). Esto indica que estas regiones contribuyen significativamente a una autocorrelación espacial positiva global. Por otro lado, en los estados que están coloreados en azul claro encontramos clusters que concentran valores bajos de Gini donde sus vecinos tienen valores altos de las mismas variables y, por lo tanto, contribuyen significativamente a una autocorrelación espacial global negativa (ya que esto ocurre cuando los valores disímiles están próximos entre sí). Ejercicio 16D. Genera y objeta con las coordenadas del shapefile usando la función coordinates(). Filtra la base de datos para usar sólo los datos del 2018. Genera una matriz de peso siguiendo el criterio Queen usando poly2nb. Genera el marco de datos usando nb_to_df() y grafícalo usando geom_point() y geom_segment(). Realiza el test I de Moran con el comando moran.test() usando la base de datos y la variable de Democracia Electoral. Grafícalo usando moran.plot(). Realiza el test I de Moran local con el comando localmoran (usa los parámetros del ejercicio anterior), átalo al conjunto de datos con cbind() y grafica el resultado con ggplot(). Comentarios finales: Referencias para un mejor análisis de datos espaciales Como dijimos al principio, el objetivo de este capítulo era hacer una introducción al análisis de datos espaciales en R, centrándose en la creación de mapas como un valioso instrumento para el análisis descriptivo de los datos políticos. Sin embargo, el análisis de los datos espaciales no comienza ni termina con lo que se explicó en este capítulo. Si estás interesado en aprender más sobre ciertos temas como el aprendizaje estadístico y el modelado de bases de datos geográficos, la interacción con otros softwares de análisis de datos geográficos como QGIS, y la aplicación de éstos en áreas como el transporte o la ecología, le recomendamos que eches un vistazo a Geocomputación en R (Lovelace, Nowosad, and Münchow 2019), un libro donde encontrarás una guía de introducción a estos temas. También te recomendamos mirar en el envoltorio de R highcharter para la librería de JavaScript Highcharts si quieres encontrar nuevas formas de visualizar los datos espaciales y crear mapas más complejos. E-mail: abescobar@uc.cl↩︎ E-mail: goortiz@uc.cl↩︎ Las características simples se refieren a un estándar de formato (ISO 19125-1:2004) que describe cómo los objetos del mundo real pueden ser representados en las computadoras. Ver r-spatial.github.io/sf/articles/sf1.html.↩︎ Lovelace, Nowosad, and Münchow (2019)↩︎ Para más información, consulta https://github.com/r-spatial/sf#installling↩︎ Puede que veas las coordenadas en los ejes. Esto puede ser útil para análisis exploratorio de datos, pero deberías eliminarlos si quieres compartir tu mapa, lo que se puede realizar añadiendo ... + theme_void()↩︎ Si en tu trabajo encuentras que esto no es así, deberás usar la herramienta de unión fuzzy que mostramos en el capítulo de manejo avanzado de datos.↩︎ REspatial↩︎ Fuente: https://geodacenter.github.io/glossary.html↩︎ "],
["referencias.html", "Capítulo 17 Referencias", " Capítulo 17 Referencias "]
]
