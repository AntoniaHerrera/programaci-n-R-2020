--- 
title: "analizaR datos políticos"
author: "Francisco Urdinez y Andrés Cruz"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "Proyecto de libro"
favicon: "favicon.ico"
---
```{r include=FALSE, cache=FALSE}
# Run before each chapter:

rm(list = ls())
set.seed(789)

options(digits = 2)
options(max.print = 8)

knitr::opts_chunk$set(
  collapse = TRUE,
  fig.align = 'center',
  fig.show = "hold",
  fig.width = 5,
  cache = TRUE
)

```

# Inicio {-}

```{r echo=FALSE, out.width='60%'}
knitr::include_graphics("00-images/tapa.png")
```

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
# Run before each chapter:

rm(list = ls())
set.seed(789)

options(digits = 2)
options(max.print = 8)

knitr::opts_chunk$set(
  collapse = TRUE,
  fig.align = 'center',
  fig.show = "hold",
  fig.width = 5,
  cache = TRUE
)

```
# Prefacio

Este libro nació haciendo análisis de datos políticos. Es decir, es hijo de la praxis. Por ello su naturaleza es aplicada, y tiene su foco puesto en ser una caja de herramientas para el lector. AnalizaR datos políticos está pensado para ser un manual de referencia que podrá ser consultado tanto por un estudiante universitario viviendo en Bogotá, como por un consultor político viviendo en México DF o un o funcionário público en Brasilia, todos con la necesidad de transformar sus bases de datos en conclusiones sustantivas y fácilmente interpretables.



Trabajando juntos en la cátedra de Análisis Cuantitativo de Datos II del Instituto de Ciencia Política de la Universidad Católica de Chile encontramos que ni aquí, ni en otras universidades de la región, había material didáctico y aplicado hecho en casa para enseñar a nuestros alumnos de ciencia política cómo extraer conclusiones a partir de datos duros. Todo el material utilizado en nuestra cátedra era publicado en inglés, por politólogos anglosajones trabajando en universidades anglosajonas. Por ello analizaR datos políticos tiene como público imaginario al politólogo latinoamericano, ya sea alumno de pregrado o posgrado, o ya en el mercado. Hemos querido que nuestro libro esté disponible en español y portugués, y esto lo hace extensible a otras universidades de realidades similares fuera de América Latina, como en los países lusófonos de África y en la región ibérica.



Las universidades latinoamericanas han hecho grandes esfuerzos en que sus alumnos de politología se alfabeticen en herramientas estadísticas y de análisis de datos, algo que hasta hace diez años era algo poco frecuente. Hoy las cinco mejores universidades de la región, según el ranking de Times Higher Eductaion, tienen cursos de análisis cuantitativo de datos en sus programas de ciencia política. Algunos departamentos, como el Departamento de Ciencia Política de la Universidad de São Paulo, que co-organiza la escuela de verano de IPSA en métodos, o el Instituto de Ciencia Política de la Universidad Católica de Chile, que organiza su escuela de verano en métodos mixtos, han hecho esfuerzos por exponer a sus alumnos a profesores norteamericanos y europeos, quienes cuentan con muchas décadas de tradición cuantitativa en sus programas. Entendemos que, hoy por hoy, ningún politólogo puede salir al mercado laboral sin saber utilizar con holgura software de análisis cuantitativo, y es a esa demanda a la que apuntamos aquí.



Ahora mismo, R es probablemente la mejor opción que el mercado provee para análisis estadístico de datos. Esto puede ser sorpresivo para un lector recién salido de una máquina del tiempo: hace diez años, o tal vez menos, R era simplemente mirado como la alternativa gratis a los programas comerciales de verdad, que sí podían realizar análisis cuantitativo serio. Sin embargo, esto ha cambiado drásticamente en los últimos años. La Figura \@ref(fig:pref-gtrends) muestra las tendencias de búsqueda en Google en América Latina para los programas más comúnmente utilizados en ciencia. R ha pasado a ocupar un lugar en el mercado que hace 15 años le correspondía a SPSS, y los programas de nicho -como Stata y Minitab- son cada vez menos buscados. La tendencia sugiere que R será cada vez más popular en la ciencia latinoamericana, siguiendo una tendencia global. 



```{r pref-gtrends, echo=FALSE, fig.cap="Elaborada por los autores usando el paquete ggplot2 de R, y datos extraídos de Google Trends. Los datos corresponden a promedios anuales para países latinoamericanos en el sector 'ciencia'", out.width='100%'}
knitr::include_graphics("00-images/pref-gtrends.png")
```


El modelo de software libre en el que se basa R —con licencias de derechos de autor permisivas, que ponen prácticamente todas las herramientas en forma gratuita a disposición del público, tanto para su uso como para su reformulación— finalmente rindió frutos. Una activa comunidad de desarrolladores se ha anclado en R, añadiéndole nuevas funcionalidades que lo han dotado de elegancia, simplicidad y flexibilidad. R ya no solo brilla en la generación de modelos estadísticos, sino que hoy es hogar de un vasto universo de herramientas que permite al usuario importar, ordenar, transformar, visualizar, modelar y comunicar los datos que le interesen, sin tener que cambiar de herramienta. 
Es esta la novedad tecnológica que queremos acercar al lector interesado en el análisis político, con la esperanza de que contribuya a optimizar el proceso entre la pregunta que le quita el sueño (y/o le promete el pan) y su solución. Esto sin desconocer, claro está, que el beneficio inicial de R, el que estaba incluso cuando nadie quisiera usarlo si no era a regañadientes, permanece. Sabemos que el lector —y tal vez su casa de estudios— agradecerá la amable coincidencia de que el mejor software disponible en términos de calidad es también el mejor para su bolsillo.



## Agradecimientos
Por ahora nadie.

<!--chapter:end:01-prefacio.Rmd-->

```{r include=FALSE, cache=FALSE}
# Run before each chapter:

rm(list = ls())
set.seed(789)

options(digits = 2)
options(max.print = 8)

knitr::opts_chunk$set(
  collapse = TRUE,
  fig.align = 'center',
  fig.show = "hold",
  fig.width = 5,
  cache = TRUE
)

```
# Introducción

El análisis cuantitativo de datos es una de las tantas herramientas que los investigadores tenemos para abordar las preguntas que nos interesan, ya sea en el mundo profesional o en la academia (o “por amor al arte” en muy encendidas noches de viernes, por qué no). Es por esto que *AnalizaR datos políticos* tiene un fuerte énfasis en ejemplos politológicos aplicados. Utilizar ejemplos de texto trillados e idealizados sobre autitos o islas imaginarias sería una falta de respeto para el lector, a quien sabemos ávido por ocupar las herramientas de este libro en las preguntas de investigación política que le parecen importantes. Por el contrario, queremos mostrar el potencial de dichas herramientas metiendo las manos en la masa, con datos de verdad, investigaciones que colegas ya han realizado y dificultades particulares de llevar el análisis de datos a preguntas políticas.



## Organización del libro

El libro está organizado en tres secciones temáticas. Dentro de las secciones, cada capítulo se esfuerza por resolver problemas puntuales, balanceando teoría y práctica en R.



<descripción de los capítulos de la sección I>
La sección I está dedicada al manejo de datos. Lo ideal es que el lector consiga algo más que mirar una base de datos con cara de no entender nada. Introduciremos R desde su instalación y aprenderemos a sacarle el jugo para obtener datos, conocerlos en profundidad, transformarlos de acuerdo a las preguntas que nos interesan y representarlos gráficamente en formas tanto funcionales como atractivas.



<descripción de los capítulos de la sección II>
En la sección II está el corazón del libro. Veremos cómo responder a preguntas políticas desde una perspectiva estadística —siempre podemos contestar desde la perspectiva de lo que nos dijo nuestra abuelita, aunque esto suela ser menos serio—. En general, la sección trata modelos estadísticos, que intentan explicar y predecir la variación de ciertas variables (dependientes) de acuerdo a cómo varían otras variables (independientes). Exploraremos distintos tipos de modelos de acuerdo a las distintas formas de variables dependientes que se encuentran comúnmente en la arena de lo político. Revisaremos cómo interpretar resultados y presentarlos en forma clara y atractiva, cómo elegir entre modelos competidores y cómo comprobar simplemente algunos de los supuestos estadísticos necesarios para que los modelos funcionen. Debemos notar que este no es un libro de econometría, claro está, por lo que para cada modelo haremos referencia a trabajos más avanzados en términos teóricos, con el fin de que el lector pueda profundizar por su cuenta si cree que debe utilizar algún modelo en específico para responder a sus preguntas de interés.



<descripción de los capítulos de la sección III>
Por último, en la sección III dejaremos el mundo ideal y nos adentraremos en la resolución de problemas. Ya sea porque un colega nos prestó su base de datos y se vé más bien como una obra de arte surrealista, o simplemente porque la dificultad de los problemas a los que nos enfrentamos deja corto lo que aprendimos al principio del libro, aquí presentaremos un popurrí de herramientas para que el lector integre en su flujo de trabajo cotidiano. Estas han sido seleccionadas desde nuestra experiencia y son cuáles creemos las más requeridas en la práctica del análisis de datos políticos.



## Prerrequisitos

Este libro está pensado para alumnos que más que brillantes son motivados: el análisis cuantitativo de datos exige sobre todo tenacidad y curiosidad. Es altamente deseable que el lector tenga nociones básicas de matemática, probabilidad y/o estadística universitaria antes de leer este libro, aun cuando nos esforzamos por mantenerlo lo más simple que pudimos en dichas materias. En términos de hardware, prácticamente cualquier computador moderno con acceso a internet será suficiente, pues las herramientas que utilizaremos son más bien livianas. Todo el software que utilizaremos es gratuito.

<!--chapter:end:02-intro.Rmd-->

```{r include=FALSE, cache=FALSE}
# Run before each chapter:

rm(list = ls())
set.seed(789)

options(digits = 2)
options(max.print = 8)

knitr::opts_chunk$set(
  collapse = TRUE,
  fig.align = 'center',
  fig.show = "hold",
  fig.width = 5,
  cache = TRUE
)

```
# (PART) Manejo de datos {-}

# R básico {#rbas}

## Instalación

### R

R (R Core Team, 2017) es un lenguaje de programación especialmente desarrollado para realizar análisis estadístico. Una de sus principales características, como se ha dejado a entrever en el prefacio, es que es de *código libre*: aparte de ser gratis, esto significa que las licencias que protegen legalmente a R son muy permisivas. Al amparo de esas licencias, miles de desarrolladores alrededor del mundo han añadido su granito de arena a la usabilidad y atractivo de R. ¡En *analizaR datos políticos* le sacaremos el jugo a esa diversidad!

Instalar R es fácil, independiente de si el usuario utiliza Windows, Mac o Linux. Basta con ingresar a https://cran.r-project.org/ y seguir las instrucciones de descarga a instalación.


### RStudio

Como dijimos, R es un lenguaje de programación. En términos informales, es una forma ordenada de pedirle al computador que realice ciertas operaciones. Esto significa que es posible usar R exclusivamente desde una consola o terminal -las pantallas negras de los hackers de las películas. Aunque esto tiene algunos atractivos -entre ellos, parecer hacker-, en general queremos interfaces más amigables. Ahí es cuando entra al ruedo RStudio, el programa de facto para utilizar R. Una vez esté instalado, todos nuestros análisis ocurrirán dentro de RStudio, que, para más remate, es también de código libre.
Para instalar RStudio, es necesario ya haber instalado R. Como para este, la descarga e instalación es accesible en Windows, Mac y Linux. El link es https://www.rstudio.com/products/rstudio/download/#download

Instale ambos R y RStudio, que nosotros lo esperamos aquí. 

Será bueno que nos acompañe a lo largo del capítulo con el programa abierto en su computador, nada mejor que aprender juntos. 

## Partes de RStudio

Si el lector consiguió descargar e instalar R y RStudio, bastará con ingresar a RStudio para comenzar a trabajar. Se pillará con una pantalla como esta (*pulir! mejor fuente, fondo blanco*):

*(F:creo que antes de largar habría que explicarle al lector como cambiar el idioma y ver ese tema del UTF8)*

```{r rbas-rstudio, echo=FALSE}
knitr::include_graphics("00-images/rbas-rstudio.png")
```


La pantalla de RStudio se divide en cuatro paneles. A continuación, vamos a explicar sus funciones. La idea en esta sección es familiarizar al lector con lo básico de R en el camino.

### Consola

El panel inferior izquierdo de RStudio. Es nuestro espacio de comunicación directa con el computador, en el que le solicitamos, hablando R, realizar tareas específicas. Llamaremos **comandos** a estas solicitudes. Probemos correr un comando que realiza una operación aritmética básica:

```{r}
2 + 2
```

Un truco importante de la consola es que con los botones de arriba y abajo es posible navegar en el historial de comandos recientes. Recomendamos al lector probar de realizar otros comandos con operaciones aritméticas y volver atrás con los botones de arriba y abajo.

*cambiar imagen de resumen*

```{r rbas-basiccalc, echo=FALSE}
knitr::include_graphics("00-images/rbas-basiccalc.png")
```



Por ejemplo:



```{r}
sqrt(100) - 2^3 * 3
```


### Script

El panel superior izquierdo de RStudio puede describirse como una suerte de "bitácora de comandos". Aunque la consola puede ser útil para unos pocos comandos, análisis complejos requerirán que llevemos un registro de nuestros comandos.

Para abrir un script nuevo, basta con presionar `Ctrl + Shift + N` o ir a File > New File > R Script (utilizar atajos de teclado suele ser una buena idea, y no solo por el factor hacker *A:footnote al anexo tips?* *F: Yo creo que puede ir a la Parte III*). La pantalla en blanco de un nuevo script es similar a un bloc de notas sin usar, con la particularidad de que cada línea debe pensarse como un comando. El lector debe notar que escribir un comando en el script y presionar `Enter` no consigue nada más que un salto de párrafo. Para correr el comando de una línea basta con presionar `Ctrl + Enter` (en el caso de Mac, `Cmd + Enter`) mientras se tiene el teclado en ella. ¡Es posible seleccionar múltiples líneas/comandos a la vez y correrlas de una pasada con `Ctrl + Enter`!

Es fundamental el dejar comentarios explicativos en nuestros scripts. Esto no es solo relevante en el trabajo en grupo (el código ajeno puede ser inentendible sin una guía clara), sino que también denota atención por nuestros yo del futuro. En varias ocasiones nos ha tocado revisar código que escribimos hace un par de meses, no entender nada, y maldecir a nuestros yo del pasado por su poca consideración. A la hora de interpretar comandos, R reconoce que todo lo que siga a un numeral (# o *hashtag*, en estos días) es un comentario. Así, hay dos formas de dejar comentarios, como "comandos estériles" o como apéndices de comandos funcionales:

```{r}
# Este es un comando estéril. R sabe que es solo un comentario, por lo que no retorna nada.
```

```{r}
2 + 2 # Este es un comando-apéndice. ¡R corre el comando hasta el gato y luego sabe que es un comentario!
```

Para guardar un script, basta con presionar `Ctrl + S` o clickear File > Save.

### Objetos

El panel superior derecho de RStudio. Aunque tiene tres pestañas, la gran estrella es "Environment", que sirve como registro para los objetos que vayamos creando a medida que trabajamos. Una de las características centrales de R es que permite almacenar objetos, para luego correr comandos en ellos. La forma tipo para crear un objeto es `nombre_del_objeto <- contenido`. Por ejemplo:

```{r}
objeto_1 <- 2 + 2
```

El lector notará que en la pestaña "Environment" aparece un nuevo objeto, objeto_1. Este contiene *el resultado* de 2 + 2. Es posible preguntarle a R qué contiene un objeto simplemente corriendo su nombre como si fuera un comando:

```{r}
objeto_1
```

Los objetos pueden insertarse en otros comandos, haciendo referencia a sus contenidos. Por ejemplo:

```{r}
objeto_1 + 10
```

También es posible reasignar a los objetos. ¡Si nos aburrimos de objeto_1 como un 4, podemos asignarle cualquier valor que queramos! Valores de caracter o no númericos se pueden asignar entre comillas:

```{r}
objeto_1 <- "democracia"
```

```{r}
objeto_1 
```

Borrar objetos es también muy simple. ¡Aunque suene como perder nuestro duro trabajo, tener un "Environment" limpio y fácil de leer a menudo lo vale!

```{r}
rm(objeto_1)
```


#### Vectores

Hasta ahora hemos conocido los objetos más simples de R, que contienen un solo valor. Objetos un poco más complejos son los vectores, "lineas" de valores. Crear un vector es simple, basta con insertar sus componentes dentro de `c()`, separados por comas:

```{r}
vector_1 <- c(15, 10, 20)
```

```{r}
vector_1
```

#### Funciones

Sin notarlo, hemos ya utilizado a través `sqrt()`, `log()` y `c()`una de las cualidades más importantes de R, las funciones. En términos muy básicos, una función es un procedimiento como el siguiente:

```{r rbas-funs, echo=FALSE}
knitr::include_graphics("00-images/rbas-funs.png")
```

`sqrt()` toma un valor numérico como input y devuelve su raíz cuadrada como output. `log()` toma el mismo input, pero devuelve su logaritmo común (o en base a 10).  `c()` toma distintos valores únicos como input y devuelve un vector que los concatena.

Es a propósito de los vectores que las funciones de R comienzan a brillar y a alejarse de las cualidades básicas de una calculadora (que, a grandes rasgos, es lo que hemos visto ahora de R, nada muy impresionante). Veamos algunas funciones que extraen información útil sobre nuestro vector. ¿Qué hace cada una?

```{r}
mean(vector_1) # media
median(vector_1) # mediana
sd(vector_1) # desviación estándar
sum(vector_1) # suma
min(vector_1) # valor mínimo
max(vector_1) # valor máximo
length(vector_1) # longitud (cantidad de valores)
sort(vector_1) # ...
```

El lector podría haber deducido que `sort()`, la última función del lote anterior, ordena al vector de menor a mayor. ¿Qué pasa si quisiéramos ordenarlo de mayor a menor? Esto nos permite introducir a los *argumentos*, partes de las funciones que nos permiten modificar su comportamiento. A continuación agregaremos el argumento `decreasing = TRUE` al comando anterior, consiguiendo nuestro objetivo:

```{r}
sort(vector_1, decreasing = TRUE)
```


### Archivos / gráficos / paquetes / ayuda

En el panel inferior derecho de RStudio estas cuatro pestañas son las que se roban la película.

#### Archivos

Esta pestaña es una ventana a nuestros archivos. Funcionando como un pequeño gestor, nos permite moverlos, renombrarlos, copiarlos, etcétera. A propósito de archivos, una de las grandes novedades recientes de R son los *RStudio Projects*, o proyectos de RStudio. Los desarrolladores de RStudio se dieron cuenta de que sus usuarios tenían scripts y otros archivos de R (de los que aprenderemos luego, como bases de datos) desperdigados a lo largo y ancho de sus discos duros, sin orden alguno. Por eso implementaron la filosofía de "un proyecto, una carpeta". Es tan simple como suena: la idea es que cada proyecto en el que trabajemos sea autosuficiente, que incluya todo lo que necesitemos en una sola carpeta. Se pueden manejar los proyectos desde la esquina superior derecha de R. El lector debe ser cuidadoso y notar que crear o abrir un proyecto reiniciará su sesión de R, borrando todo el trabajo que no guarde.

*(F: creo que podemos expandir un poquito mas en la utilidad de los proyectos con un ejemplo, a mi me ha resultado muy grato el tema de los proyectos!)*

*(screenshot de RStudio Projects)*

#### Gráficos

Aquí aparecen los gráficos que realizamos con R. ¡En el capítulo *X* aprenderemos a crearlos!

#### Paquetes

Una de las cualidades de R a la que más hincapié hemos dado es su versatilidad. Su código libre hace que muchos desarrolladores se sientan atraídos a aportar a la comunidad de R con nuevas funcionalidades. En general, realizan esto a través de paquetes, que los usuarios pueden instalar como apéndices adicionales a R. Los paquetes contienen nuevas funciones, bases de datos, etcétera. La pestaña de RStudio aquí reseñada nos permite acceder a nuestros paquetes instalados.

Instalar un paquete es bastante simple, a través de la función `install.packages()`. A continuación vamos a instalar el paquete "tidyverse", central en nuestros próximos análisis. El tidyverse es una recopilación que incluye algunos de los mejores paquetes modernos para análisis de datos en R.

```{r, eval = F}
install.packages("tidyverse")
```

Cada vez que el usuario abre una nueva sesión de R, este carga "como de fábrica". No solo sin objetos, sino que solo con los paquetes básicos que permiten a R funcionar. Tenemos que cargar los paquetes extra que queramos usar, entonces. Es más o menos como cuando compramos un *smartphone* y descargamos las aplicaciones que más usaremos, para que se ajuste a nuestras necesidades cotidianas. La forma más común de hacer esto es a través de la función `library()`, como se ve a continuación:

```{r}
library("tidyverse")
```

El lector puede ahorrarse el trabajo de instalar los paquetes utilizados en *AnalizaR Datos Políticos* en el futuro y hacerlo todo ahora, corriendo el siguiente súper comando. Ojo: ¡esto no lo salvará de cargar los paquetes en cada nueva sesión de R que inicie! Ojo 2: puede que esto le tome un tiempo considerable.

*paquetes a instalar*
*F: excelente que vamos a ofrecer esto, puede haber un paquete llamado ADP que tenga todo?*

#### Ayuda

Buscar ayuda es central a la hora de programar (y no solo de programar...). Esta pestaña de RStudio abre los archivos de ayuda que necesitemos, permitiéndonos buscar en ellos. Las funciones tienen archivos de ayuda para sí solas. Por ejemplo, podemos acceder al archivo de ayuda de la función `sqrt()` a través del comando `help(sqrt)`. Los paquetes en su conjunto también tienen archivos de ayuda, más comprensivos. Por ejemplo, para ver los archivos de ayuda del tidyverse solo debemos recurrir al argumento "package": `help(package = tidyverse)`. El lector debe notar que los archivos de ayuda de paquetes y funciones de paquetes solo están disponibles si el paquete ha sido cargado.

## Ejercicios

* ¿Qué significa "correr" un comando desde un script? ¿Cómo se hace?
* ¿Cuál es la media de los dígitos del hit de Rafaella Carrà, 0 3 0 3 4 5 6? ¿Y la mediana? Por último, órdenelos de mayor a menor.
* Busque ayuda para el paquete "googledrive". Recomendado: maravillarse con la variedad de los paquetes de R.

<!--chapter:end:03-rbasico.Rmd-->

```{r include=FALSE, cache=FALSE}
# Run before each chapter:

rm(list = ls())
set.seed(789)

options(digits = 2)
options(max.print = 8)

knitr::opts_chunk$set(
  collapse = TRUE,
  fig.align = 'center',
  fig.show = "hold",
  fig.width = 5,
  cache = TRUE
)

```
# Manejo

**F: EN GENERAL YO METERIA MAS SUB-SUBTITULOS, CASI UNO POR FUNCION. POR EJ, UNO PARA ARRANGE (4.3.1), OTRO PARA SELECT (4.4.1)... O AL LADO DEL TITULO PONDRIA LA FUNCION: 4.4. SELECCIONAR COLUMNAS CON **'SELECT'*****

Cuando hablamos de análisis de datos, casi siempre nos referimos a análisis de *bases de datos*.  Aunque hay varios formatos de bases de datos disponibles, en ciencias sociales generalmente usamos y creamos *bases de datos tabulares*, que son las que este libro tratará. Muy probablemente el lector estará familiarizado con la estructura básica de este tipo de bases, gracias a las planillas de Microsoft Excel, Google Spreadsheets y/o LibreOffice Calc. La primera fila suele ser un **header** o encabezado, que indica qué datos registran las celdas de esa columna.
En general, queremos que nuestras bases de datos tabulares tengan una estructura *tidy*, como la siguiente:



![Tidy Data](http://garrettgman.github.io/images/tidy-4.png)
(traducir!)



La idea de una base *tidy* es simple: cada columna es una variable, cada fila una observación (de acuerdo a la unidad de análisis) y, por lo tanto, cada celda es una observación. *(explicar que este nombre lo tomamos del dios Wickham y que es una idea tan simple que mucha gente no la va a entender. Quien solo ha trabajado con Stata o Excel nunca vio datos no-tidy)*



## Nuestra base de datos

Para este capítulo usaremos una sección de la base de datos de *Quality of Government*(QoG, 2017)[https://qog.pol.gu.se], un proyecto que registra diversos datos de países. Sus primeras observaciones son las siguientes:

```{r include=FALSE}
library(tidyverse); qog_mod <- read_csv("00-datos/04_qog_mod.csv")
```


```{r echo=FALSE}
knitr::kable(qog_mod %>% slice(1:6))
```

Las variables son las siguientes:

variable           | descripción
------------------ | -------------------------------------------------------------------------------------------
cname              | Nombre del país
wdi_gdppppcon2011  | GDP PPP, en dólares del 2011, según los datos de WDI (p. 635 del codebook)
wdi_pop            | Población, según los datos de WDI (p. 665)
ti_cpi             | Índice de Percepción de la Corrupción de TI. Va de 0 a 100, con 0 lo más corrupto (p. 560)
lp_muslim80        | Porcentaje de población de religión musulmana, para 1980, según LP (p. 447)
fh_ipolity2        | Nivel de democracia según FH. Va de 0 a 10, con 0 como menos democrático (p. 291)
region             | Región del país, según WDI (añadida a la base)



Para comenzar a trabajar carguemos el paquete `tidyverse`, uno de los centrales del libro, que nos dará funciones útiles para trabajar con nuestra base datos.

```{r}
library("tidyverse")
```



Ahora carguemos la base de datos a nuestro ambiente de trabajo en R. Vamos a llamarla "qog_mod" (QoG modificada). El archivo está en formato .csv, por lo que utilizaremos la función del tidyverse `read_csv()`

```{r}
qog_mod <- read_csv("00-datos/04_qog_mod.csv")
```



(hay que decidir cómo se va a hacer esto: desde carpeta local, url, paquete, etc.)
*(F:opino que creemos un url del libro y usemos los html, en el paquete ADP solo pondria funciones)*


## Describir la base

(aquí podría ir `describe_all()` u otra función de descripción de la base, habría que decidir si esto tiene sentido en términos pedagógicos; otra opción es hacer otro capítulo con `group_by()`, `tabyl()`, `crosstab()`, etc; me inclino por esta última opción)

Para aproximarnos a nuestra base recién cargada tenemos varias opciones. Podemos, como antes, simplemente usar su nombre como un comando para un resumen rápido:

```{r}
qog_mod
```

También podemos utilizar la función `glimpse()` para tener un resumen desde otra perspectiva:

```{r}
glimpse(qog_mod)
```

Una alternativa que nos permite ver la base completa es la función `View()`, análoga a clickear nuestro objeto en la pestaña "Environment" de Rstudio:

```{r, eval=FALSE}
View(qog_mod)
```



## Ordenar la base con `arrange()`

Una de las operaciones más comunes con bases de datos es ordenarlas de acuerdo a alguna de las variables. Esto nos puede dar insights (¿traducción?) inmediatos sobre nuestras observaciones. Por ejemplo, ordenemos la base de acuerdo a población:

```{r}
arrange(qog_mod, wdi_pop)
```

El lector debe notar cómo el primer argumento, "qog_mod", toma la base de datos y los siguientes enuncian **cómo** ordenarla, en este caso, por "wdi_pop", la variable de población.

Debe notar también cómo el comando anterior no crea ningún objeto, solo muestra los resultados en la consola. Para crear uno tenemos que seguir la fórmula típica de asignación:

```{r}
qog_mod_ordenada <- arrange(qog_mod, wdi_pop)
```

Podemos realizar ambas operaciones, mostrar los resultados y crear el objeto, rodeando este último comando con paréntesis:

```{r}
( qog_mod_ordenada <- arrange(qog_mod, wdi_pop) )
```

La operación para ordenar realizada antes iba de menor a mayor, en términos de población. Si queremos el orden inverso (decreciente), basta con añadir un signo menos (-) antes de la variable:

```{r}
arrange(qog_mod, -wdi_pop)
```

¡Con eso tenemos los países con mayor población en el mundo! ¿Qué pasa si queremos los países con mayor población **dentro de cada región**? Tendríamos que realizar un ordenamiento en dos pasos: primero por región y luego por población. Con `arrange()` esto es simple:

```{r}
arrange(qog_mod, region, -wdi_pop)
```

A propósito del resultado anterior, el lector puede deducir que cuando `arrange()` ordena variables categóricas (en vez de numéricas) lo hace alfabéticamente. Añadir un signo menos (-) antes de la variable hará que el orden sea al revés en términos del alfabeto:

```{r}
arrange(qog_mod, desc(region), -wdi_pop) # no sé por qué - no funciona, ARREGLAR
```



## Seleccionar columnas de la base con `select()`

A veces queremos trabajar solo con algunas variables de una base de datos. Para esto existe la función `select()`. Pensemos que queremos solo el nombre de cada país (cname) y su porcentaje de población musulmana para 1980:

```{r}
select(qog_mod, cname, lp_muslim80)
```

Al igual que para `arrange()`, aquí el primer argumento designa la base a modificar y los demás cómo se debería hacer eso -en este caso, qué variables deben ser seleccionadas.



Añadir un signo menos (-) aquí indica qué variables *no* seleccionar. Por ejemplo, quitemos el porcentaje de población musulmana para 1980 de la base:

```{r}
select(qog_mod, -lp_muslim80)
```

Aparte de seleccionar variables específicas, `select()` es capaz de entender referencias a intervalos de variables. Por ejemplo, podemos querer las cuatro primeras variables:

```{r}
select(qog_mod, cname:ti_cpi)
select(qog_mod, 1:4) # lo mismo, aunque no recomendado
```

Otra herramienta para complejizar nuestra selección se encuentra en las funciones de ayuda. Entre ellas, `starts_with` es de particular utilidad, permitiendo seleccionar variables que empiecen con cierto patrón. Por ejemplo, podríamos querer, a partir del nombre del país, todas las variables que provengan de los World Development Indicators (WDI) del Banco Mundial:

```{r}
select(qog_mod, cname, starts_with("wdi_"))
```

Otra función de ayuda útil es `everything()`, que se lee como "todas las demás variables". Es especialmente útil para cambiar el orden de las variables en una bases de datos. Por ejemplo, pasemos región al segundo lugar entre las variables:

```{r}
select(qog_mod, cname, region, everything())
```



## Renombrar columnas de la base con `rename()`

La notación para el GDP es un poco confusa. ¿Y si queremos cambiar el nombre de la variable? Aprovechemos también de cambiar el nombre de la variable de identificación por país.

```{r}
rename(qog_mod, wdi_gdp = wdi_gdppppcon2011, country_name = cname)
```



## Filtrar observaciones de la base con `filter()`

Es muy común el querer filtrar nuestras observaciones de acuerdo a algún tipo de criterio lógico. Para esto R cuenta con operadores lógicos. Los más comunes son los siguientes:

| operador | descripción
|:--------:|------------
| ==       | es igual a
| !=       | es distinto a
| >        | es mayor a
| <        | es menor a
| >=       | es mayor o igual a
| <=       | es menor o igual a
| &        | y (intersección)
| |        | o (unión)

Por ejemplo, podríamos querer solo los países (observaciones) sudamericanos. Hacer esto con `filter()` es simple, con la ayuda de operadores lógicos:

```{r}
filter(qog_mod, region == "South America")
```

¿Qué pasa si queremos solo las filas de países sudamericanos con más de 10 millones de habitantes (nos quedamos con 8 de 12)?



¿Cuáles son los filtros que aplican los siguientes comandos?

```{r}
filter(qog_mod, fh_ipolity2 > 9)

filter(qog_mod, wdi_pop > 10e7)

filter(qog_mod, cname != "Albania")

filter(qog_mod, lp_muslim80 >= 95)

filter(qog_mod, region == "South America" & wdi_pop > 10e6)

filter(qog_mod, region == "South America" | region == "South-Eastern Asia")
```



## Crear nuevas variables en la base con `mutate()`

Muchas veces queremos crear nuevas variables, a partir de las que ya tenemos. Por ejemplo, podríamos querer el GDP per capita, en vez del absoluto. Tenemos los ingredientes para calcularlo: el GDP absoluto y la población. Creemos una nueva variable, entonces:

```{r}
mutate(qog_mod, gdp_ppp_per_capita = wdi_gdppppcon2011/wdi_pop)
```

Otra nueva variable que podría interesarnos es el número de musulmanes por país. Con la proporción de musulmanes y la población total del país podemos hacer una buena estimación:

```{r}
mutate(qog_mod, n_muslim = wdi_pop * lp_muslim80)
```

¡Es posible crear más de una variable con el mismo comando! Creemos las dos de antes, a la vez:

```{r}
mutate(qog_mod, 
       gdp_ppp_per_capita = wdi_gdppppcon2011/wdi_pop,
       n_muslim           = wdi_pop * lp_muslim80)
```



## Concatenar comandos: las pipes (`%>%`)

A menudo no queremos hacer una sola de las operaciones con bases de datos reseñadas antes, sino que una seguidilla de estas. Si quisiéramos crear una nueva base a través de, por ejemplo, (1) seleccionar las variables de país, población y GDP, (2) crear la variable de GDP per capita, y (3) ordenar los países de mayor a menor según GDP per capita, nuestro procedimiento en R sería algo como esto:

```{r}
qog_mod_seguidilla_1 <- select(qog_mod, cname, wdi_pop, wdi_gdppppcon2011)
qog_mod_seguidilla_2 <- mutate(qog_mod_seguidilla_1, 
                               gdp_ppp_per_capita = wdi_gdppppcon2011/wdi_pop)
qog_mod_seguidilla_3 <- arrange(qog_mod_seguidilla_2, -gdp_ppp_per_capita)
```

```{r}
qog_mod_seguidilla_3
```

El lector notará que esto es bastante complicado y nos deja con dos **objetos intermedios** que no nos interesan, "qog_mod_seguidilla_1" y "qog_mod_seguidilla_2".



La solución del paquete tidyverse que estamos utilizando son **las pipes**. El lector notará que en las tres funciones de nuestra seguidilla anterior (select, mutate y arrange) el primer argumento es la base de datos a tratar. En vez de crear objetos intermedios podemos "chutear" la base de datos a través de nuestros comandos con pipes, omitiendo los primeros argumentos:

```{r}
qog_mod_seguidilla <- qog_mod %>%
  select(cname, wdi_pop, wdi_gdppppcon2011) %>%
  mutate(gdp_ppp_per_capita = wdi_gdppppcon2011/wdi_pop) %>%
  arrange(-gdp_ppp_per_capita)
```

```{r}
qog_mod_seguidilla
```

Las pipes pueden leerse como "pero luego". Nuestra seguidilla anterior, entonces, se leería de la siguiente forma:

> qog_mod_seguilla es igual a qog_mod; pero luego seleccionamos las variables cname, wdi_pop, wdi_gdppppcon2011; pero luego creamos la variable gdp_ppp_per_capita; pero luego ordenamos la base en forma decreciente según gdp_ppp_per_capita.



## Ejercicios

* Cree una base nueva, llamada qog_mod_2, con una nueva variable llamada "porc_muslim", que sea el porcentaje de población musulmana del país.
* Cree una base nueva, llamada qog_mod_3, que incluya solo países latinoamericanos, China y Sudáfrica.
* Cree una base nueva, llamada qog_mod_4, que incluya solo países con población mayor a la media de población entre todos los países. Debe contener solo las variables de nombre del país, región y población (en ese orden). Ordene la base según población, de mayor a menor. ¡Use pipes!

<!--chapter:end:04-manejo.Rmd-->

```{r include=FALSE, cache=FALSE}
# Run before each chapter:

rm(list = ls())
set.seed(789)

options(digits = 2)
options(max.print = 8)

knitr::opts_chunk$set(
  collapse = TRUE,
  fig.align = 'center',
  fig.show = "hold",
  fig.width = 5,
  cache = TRUE
)

```
# (PART) Modelos {-}

# Modelos binarios {#bin}

```{block, type="books"}
**Lectura de referencia:** 

- Box-Steffensmeier, J. M., Brady, H. E., & Collier, D. (Eds.). (2008). *The Oxford Handbook of Political Methodology* (Vol. 10). Oxford Handbooks of Political Science. Oxford: Oxford University Press. Cap. 22 – Discrete Choice Methods.
- Agresti, A. (2007). *An Introduction to Categorical Data Analysis*, 2nd Ed. Hoboken: Wiley. Cap. 3, 4 y 5 – Generalized Linear Models; Logistic Regression; Building and Applying Logistic Regression Models.
- Greenhill, B., Ward, M. D., & Sacks, A. (2011). The separation plot: A new visual method for evaluating the fit of binary models. *American Journal of Political Science*, 55(4), 991-1002.

```

En el capítulo anterior vimos cómo hacer regresiones linarias en R de una manera simple y cubriendo los paquetes más útiles a nuestro alcance. En este capítulo veremos cómo hacer los mismos para variables dependientes dicotómicas. Al igual que en los capítulos previos, no cubriremos aspectos sustanciales a la teoría por tras de cada modelo, ni desglosaremos en detalle las fórmulas. Para ello dejamos tres referencias que van a ayudarte a acompañar lo que describimos si nunca has leído al respecto. 

##Conceptos principales
Los modelos para variables dependientes dicotómicas (aquellas que asumen una de dos posibilidades, comúnmente 0 y 1) son utilizados para estimar la probabilidad de ocurrencia de un evento. Es importante remarcar que en inglés existen dos conceptos diferentes que en español y portugués se traducen como una única palabra: *probability* y *likelihood* se traducen como probabilidad en diccionarios comúnes (haz la búsqueda así nos crees), si bien la distinción entre ambos es vital para comprender como funcionan los modelos dicotómicos estimados por Máxima Verosimilitud (*Maximum Likelihood* en inglés). Aunque no vamos a ahondar en su distinción, es importante comprender que una probabilidad se estima a partir de una "población" de la cual conocemos sus "parámetros", mientras que la verosimilitud recorre el camino inverso, es decir, estima los valores de los parámetros para los cuales el resultado observado mejor se ajusta a ellos (ver Figura \@ref(fig:bin-realmuestra)). 

```{r bin-realmuestra, echo=FALSE, fig.cap="El camino de doble vía de probabilidad y verosimilitud"}
knitr::include_graphics("00-images/bin-realmuestra.png")
```

Cuando tenemos una variable dependiente dicotómica que queremos modelar, asumimos que la misma tiene una distribución de Bernoulli con una probabilidad  que desconocemos. Así, estimamos por medio de Máxima Verosimilitud nuestra probabilidad, hasta ahora desconocida,  dada una determinada combinación linear  de variables independientes de nuestra elección (ver Figura \@ref(fig:bin-bernou)). Un muy buen ejercicio para comprender como se estima un parámetro cuya distribución es binomial por medio de Máxima Verosimilitud es ofrecida por [RPubs](https://rpubs.com/felixmay/MLE).

```{r bin-bernou, echo=FALSE, fig.cap="Bernoulli"}
knitr::include_graphics("00-images/bin-bernou.png")
```

La Ciencia Política ha hecho extensivo el uso de modelos Logit, por sobre los modelos Probit, en buena medida debido a que los primeros permiten el cálculo de razones de oportunidades (*odds ratios*). Casi todos los manuales econométricos discuten las diferencias y similitudes entre ambos, las cuales son muchas a los fines prácticos de estimar un modelo. Por ello, siendo que son métodos que derivan en resultados muy similares, sólo utilizaremos Logit en este capítulo. Ambos métodos utilizan funciones de enlace (*link functions*) diferentes y Logit lleva su nombre debido a que su función está dada por el logaritmo natural de las razones de oportunidad ("log odds"->logit!).

$$ ln(odds) = ln(\frac {p}{1 - p})
  (\#eq:bin-logodds) $$

Despejando los términos podemos calcular, de tal forma que obtenemos.

$$ logit^{-1}(\alpha) =  \frac {1}{1+e^{-\alpha}} = \frac {e^\alpha}{1+e^\alpha}
  (\#eq:bin-invdespejada) $$

Donde es la combinación linear de las variables independientes y sus coeficientes. La inversa del Logit nos dará la probabilidad de la variable dependiente ser igual a "1" dada una cierta combinación de valores para nuestras variables independientes. Asi,

```{r bin-invlogit, echo=FALSE, fig.cap="Inversa del logit"}
knitr::include_graphics("00-images/bin-invlogit.png")
```

Si profundizas en manuales de econometría, notarás que la función es indefinida en 0 y en 1, es decir, la probabilidad se aproxima infinitamente al límite sin nunca tocarlo.

**[falta un buen parrafo para cerrar esta seccion]**

##Aplicación en R
Los modelos probabilísticos han ganado enorme preeminencia en la Ciencia Política en los últimos años y es probable que estés buscando una guía aplicada para saber qué hacer y qué no hacer cuando tiene una variable dependiente dicotómica. Para ello vamos a ilustrar un paso a paso en R utilizando como ejemplo la base de datos del libro [“Democracies and Dictatorships in Latin America: Emergence, Survival, and Fall” de Scott Mainwaring y Aníbal Perez-Liñan (2013)](https://kellogg.nd.edu/democracies-and-dictatorships-latin-america-emergence-survival-and-fall). 
A lo largo del libro los autores analizan cuales variables ayudan a explicar por qué ocurrieron quiebres democráticos en América Latina durante todo el siglo XX y comienzos del XXI . En el capítulo 4, los autores se preguntan qué factores explican la supervivencia de los regímenes políticos. Si bien prueban varios modelos, algunos logísticos y otros de supervivencia (que están desarrollados en el Capítulo 10), a los fines prácticos haremos un ejemplo muy sencillo para que nos acompañes desde tu computador en el paso a paso. 
Suponiendo que la variable dependiente asume el valor “1” si el país sufre un quiebre de su régimen político democrático y “0” si no, ¿qué efecto tiene sobre la probabilidad de un quiebre de este tipo ocurrir que un país latinoamericano de mayores poderes constitucionales al poder ejecutivo? Como argumentan los autores, se puede medir estos poderes por medio del índice creado por [Shugart y Carey](http://www.cambridge.org/gb/academic/subjects/politics-international-relations/comparative-politics/presidents-and-assemblies-constitutional-design-and-electoral-dynamics?format=PB&isbn=9780521429900) de poder presidencial (1992) que los autores incluyen en su base de datos. Así, 

```{r include=FALSE}
library(tidyverse)
library(haven)
```


```{r}
datos_mp <- read_stata("00-datos/Cap 7_base_mainwaring_perez.dta")
```

```{r}
#
```

```{r}
modelo_1 <- glm(breakdown ~ shugart, 
                data   = datos_mp,
                family = binomial("logit"))
```

```{r}
summary(modelo_1)
```

El coeficiente para la variable *shugart* está negativamente asociado a la probabilidad de ocurrencia de un quiebre de régimen, y es estadísticamente significativo (p=0.0026). Ahora bien, a diferencia de los modelos por MCO del capítulo anterior, donde podíamos interpretar directamente el efecto de la variable independiente sobre la dependiente a partir de los coeficientes de la regresión, para el caso de regresiones logísticas esto no es tan sencillo. Si partimos de que la función de enlace de logit es el logaritmo de las razones de oportunidades, tenemos que 

$$ln(\frac {p}{1 - p}) = \beta_{0} + \beta_{1}x_{1}
  (\#eq:bin-funenlace)$$


Despejando $ln$, tenemos que 

$$(\frac {p}{1 - p}) = e^{\beta_{0}+\beta_{1}x_{1}}
  (\#eq:bin-lndespejado) $$


Y despejando los términos nuevamente tenemos que 

$$\hat{p} = \frac {e^{\beta_{0}+\beta_{1}x_{1}}}{1 + e^{\beta_{0}+\beta_{1}x_{1}}}
  (\#eq:bin-términosdespejados)$$

Lo que queremos, entonces, es transformar los coeficientes tal y como los reporta `R` en una probabilidad asociada a que la variable dependiente asuma el valor “1”. Sabemos que la variable independiente (*shugart*) es un índice que a mayor valor, mayor concentración de poder del ejecutivo *vis a vis* el legislativo, por lo tanto el coeficiente de la regresión nos indica que a menor concentración de poder del ejecutivo, mayor la probabilidad de un quiebre de régimen. 
La muestra del libro cubre 20 países latinoamericanos entre 1900 y 2010, y el índice oscila de un mínimo de 5 (Haití, para varios años) a un máximo de 25 (Brasil en 1945) (ver Figura \@ref(fig:bin-shugarthist) ¿Cómo puedo saber en qué magnitud se afecta la probabilidad de un quiebre democrático si el nivel de concentración de poder del ejecutivo pasa de un puntaje de 5 (mínimo) a uno de 25 (máximo) cuando no controlamos por nada más en la regresión?

```{r, bin-shugarthist, fig.cap="Histograma Shugart"}
attach(datos_mp)
qplot(shugart, geom="histogram") 
```

Para ello podemos reemplazar los valores de nuestra última fórmula, en la que hemos aislado en el lado izquierdo de la fórmula a $\hat {p}$. Primero debemos calcular cuál es la probabilidad de sufrir un quiebre de régimen en un nivel de Shugart de 5 y en un nivel 25, respectivamente, para luego calcular la diferencia. Así tenemos que

$$\hat{p} = \frac {e^{(0+(-0.019*5))}}{1 + e^{(0+(-0.019*5))}}
  (\#eq:bin-formreemp)$$

Notarás que el valor correspondiente al intercepto es igual a 0 pues ese coeficiente no ha resultado estadísticamente significativo. Sabemos que para un índice de Shugart y Carey de 5, luego de hacer el cálculo en la fórmula arriba, la probabilidad es igual a 0.47 o 47%. Si repetimos el proceso para un valor de Shugart de 25 la probabilidad cae a 38%. Con las probabilidades podemos calcular oportunidades, que son simplemente $\frac {p}{1-p}$. De esta manera, la oportunidad (*odd* en inglés) para un valor 5 del índice de Shugart y Carey es de 0.90 mientras que para un índice de Shugart y Carey de 25 es de 0.62. La utilidad de las oportunidades es que permite calcular razones de probabilidades (*odds ratios*). 
¿Cuál es la gracia de calcular una razón de probabilidades? Veamos. Si calculo la probabilidad de un cambio en el índice de Shugart y Carey de 23 a 24, la magnitud será diferente a si calculamos un cambio en la probabilidad si el índice pasa de 12 a 13, por ejemplo. Es decir, los efectos de la variable independiente sobre la probabilidad de la variable dependiente ocurrir no son lineares (recuerde la función en "S" de la Figura \@ref(fig:bin-invlogit)). Por el contrario, las razones de probabilidades tienen la propiedad de poder reflejar cambios independientemente de la curvatura de la función, es decir, son cambios “constantes”. Así, podemos expresar el efecto de la variable sin tener que especificar un valor determinado para ella. Siendo que los modelos Probit y Logit generan resultados muy similares, y debido a que los modelos Logit permiten el cálculo de razones de probabilidad, la que la literatura de Ciencia Política se ha inclinado hacia esta opción.
Veamos cómo sería el cálculo de razones de probabilidad siguiendo el ejemplo que acabamos de crear con la base de datos de Mainwaring y Perez-Liñan. Dijimos que la oportunidad está dada por $\frac {p}{1 - p}$. Una razón de oportunidades se expresaría, entonces, como $\frac {\frac {p_1}{1-p_1}}{\frac {p_2}{1-p_2}}$. 
Supongamos que Chile en el año 1992 tenía un índice de Shugart de 15, y que en el año 1993 ese índice subió a 16 (éstos no son valores reales). 
 
$$ Pr(quiebre democr\'atico){_{Chile,1992}} = \frac {e^{(0+(-0.019*15))}}{1 + e^{((0+(-0.019*15))}} = 0.42$$
$$ Pr(quiebre democr\'atico){_{Chile,1993}} = \frac {e^{(0+(-0.019*16))}}{1 + e^{(0+(-0.019*16))}} = 0.43$$

La probabilidad difiere poco y cae en un 2.4% lo que parece ser un efecto pequeño. La razón de oportunidades se calcula como el cociente de ambas oportunidades, así: 

$$\frac {0.42}{0.43}=0.97$$


De esta manera, toda razón de oportunidades mayor a 1 expresa un cambio positivo, mientras que todo valor menor a 1 (entre 0 y 1) representa un cambio negativo en las probabilidades estimadas. Si hiciéramos el mismo ejercicio para otros valores del índice de Shugart y Carey, por ejemplo, un cambio de 3 a 4 o de 23 a 24, el cociente de las oportunidades daría 0.97. 

**SERIA BUENO CREAR UN BLOCK CON UN LAPIZ EN VEZ DE UN LIBRO, PARA USAR EN COSAS ASÍ, COLOR GRIS**
```{block, type="books"}
Ahora, haz el cálculo para el valor real de Chile en 1992 y 1993 como práctica, te esperamos.
```

`R` ofrece paquetes para que este análisis sea fácil de hacerse. Podemos visualizar fácilmente los cocientes de oportunidades utilizando el paquete `sjPlot`. Podemos calcular probabilidades predichas, y además podemos hacer tests para saber la capacidad explicativa de nuestros modelos. Utilizando la misma base de datos haremos un ejemplo de una rutina típica, que puedes recrear en casa utilizando tus propios datos. Los pasos a seguir son (a) estimar los modelos, (b) crear tablas formateadas para pegar en nuestros procesadores de texto, (c) crear figuras para visualizar la magnitud de los coeficientes por medio de cociente de oportunidades, (d) visualizar probabilidades predichas para variables de interés, (e) calcular capacidad explicativa de los modelos (porcentaje correctamente predicho, AIC, BIC, curvas ROC, *Brier scores* o *separation plots*, que explicaremos a continuación).
Cuando uno trabaja con una variable dependiente binaria, y lo que quiere es rodar algunos modelos logísticos para incorporar a su trabajo, primero es recomendable utilizar Pacman. `pacman` es un paquete de R que hace mucho más fácil trabajar con otros paquetes, pues permite cargar todos al mismo tiempo. Comencemos por cargarlo:
```{r}
if (!require("pacman"))
    install.packages("pacman"); library(pacman)  
```
Para que sea simple utilizar la función `pacman`, recomendamos añadir `library(pacman)` a [su archivo de .Rprofile](http://www.statmethods.net/interface/customizing.html), para que se cargue automáticamente cada vez que abra R Studio. De esta manera no habrá que ejecutarlo cada vez que abra R Studio. La principal gracia de `pacman` es su función `p_load`, que nos permite cargar varios paquetes en un solo comando y, si nos los tenemos instalados, lo hace por nosotros (en el siguiente paso la utilizaremos). Si no tienes instalados los siguientes paquetes, `p_load` los instalará por ti. Si nos has acompañado desde los capítulos anteriores, este paso te resultará familiar.
```{r}
p_load(haven,    # parte del tidyverse, para cargar bases de datos en formatos foráneos
       verification,
       janitor,  # nos da la función tabyl(), para hacer tablas tidy
       sjPlot,
       stargazer, # nos ayuda a hacer tablas de modelos de regresión
       tidyverse,
       pscl,
       separationplot
       )
```

##Estimar los modelos
¿Recuerde el `ADP`? Una de las funciones que hemos facilitado es la de creación de tablas editables para artículos académicos utilizando la función `stargazer`. Si utilizas nuestro paquete te ahorrarás muchos pasos que son engorrosos.
```{r}
library(stargazer)
source("00-funs/ADP.R")
```
Por medio de `stargazer` podemos exportar nuestras tablas formateadas en html para poder incorporarlas en nuestros artículos directamente. Para ejemplificar este paso lo que haremos es agregar al modelo 1 dos modelos más: El modelo 2 tendrá como variables independientes al índice de Shugart y Carey más la variable *age* que mide en años la edad del régimen político. 
 qplot(age, geom="histogram") 
```{r}
modelo_2 <- glm(breakdown ~ shugart+age, 
                data   = datos_mp,
                family = binomial("logit"))
```

```{r}
summary(modelo_2)
```
El modelo 3 agrega a las dos variables del modelo 2 una tercer variable llamada *fh* que corresponde al [Freedom House  score](https://freedomhouse.org/report/methodology-freedom-world-2017) de democracia.

```{r fhhist, fig.cap="Histograma FH"}
qplot(fh, geom="histogram") 
```

```{r}
modelo_3 <- glm(breakdown ~ shugart+age+fh, 
                data   = datos_mp,
                family = binomial("logit"))
```

```{r}
summary(modelo_3)
```
Una vez creados los tres modelos de interés, los agrupamos en una lista por medio de la función `list`.
```{r}
mp_modelos <- list(modelo_1, 
                   modelo_2,
                   modelo_3)
```
Para exportar la tabla a html demos definir la opción **type** y un nombre para el archivo html en la opción **out**. Así el comando sería

stargazer_easy_binary(mp_modelos,
                      type   = "html", # OJO
                      out    = "output/tabla_mp_modelos.htm", # OJO
                      report = "vct*",
                      title  = "Modelos 1-3 en base a Mainwaring y Perez Liñan (2013)",
                      align  = TRUE, 
                      dep.var.labels = c("Quiebre de régimen"),
                      covariate.labels =c ("Indice de Shugart & Carey (1992)","Edad del régimen",
                      "Freedom House"),
                      no.space = TRUE)

A simple vista observamos que *shugart* deja de ser estadísticamente significativa cuando controlamos por *fh* y, además,  ésta pasa a ser la única variable estadísticamente significativa en el tercer modelo. Vemos como el número de observaciones cae significativamente al incluir la variable *fh* lo que hace difícil comparar los modelos. Entonces al obtener una tabla como la que acabamos de crear tenemos dos desafíos: comparar los modelos para saber cuál tiene mejor ajuste, y saber si la magnitud de los efectos es substantiva desde un punto de vista científico (por ejemplo, si la variable *fh* resulta estadísticamente significativa pero la probabilidad de un quiebre de régimen cae en 0.03% si un país pasa del peor score de *fh* al mejor, entonces diríamos que, a pesar de estadísticamente significativa, nuestra variable carece de significancia substantiva).
Jane Miller hace mucho énfasis en su [libro](http://www.press.uchicago.edu/ucp/books/book/chicago/C/bo15506942.html) respecto a la diferencia entre significancia estadística y significancia substantiva: no por ser una variable significativa estadísticamente la magnitud del efecto será el esperado. 
Para explorar las magnitudes de los coeficientes vamos a concentrarnos en el tercer modelo. Una tabla individual, podrán anticipar, se haría así:

stargazer_easy_binary(modelo_3,
                      type   = "text",
                      report = "vct*",
                      title  = "Modelo 3 en base Mainwaring y Perez Liñan (2013)",
                      )

Comencemos reemplazando los coeficientes en la tabla por cocientes de oportunidades. Noten cómo el procedimiento es muy similar al de reemplazar errores estándar:

stargazer_easy_binary(modelo_3,
                      type   = "text",
                      report = "vct*",
                      title  = "Modelo 3 en base Mainwaring y Perez Liñan (2013), odds ratios",
                      coef   = list(exp(modelo_3$coefficients)))

##Visualización de resultados
Podemos representar visualmente la última tabla con la función `sjp.glm()` del paquete `sjPlot`:

```{r bin-m3or, message=FALSE, fig.cap="Odds ratios del Modelo 3, en base a Mainwaring y Pérez Liñán (2013)"}
sjp.glm(modelo_3, 
        show.ci     = T)
```

En muchas ocasiones es preferible utilizar este tipo de figuras a tablas. Un precursor en la disciplina en el uso de figuras fue [Edward Tufte](http://pages.mtu.edu/~hcking/Tufte_hKing.pdf). La ciencia política, sin embargo, no prestó demasiada atención a la presentación de resultados por medio de figuras hasta hace unas dos décadas, pero hoy la [tendencia](https://www.princeton.edu/~jkastell/Tables2Graphs/graphs.pdf) en la disciplina es a prescindir de tablas cuando estas no sean esenciales. Y hoy en día con software como R es muy simple de hacer. 
Con el argumento `type = "slope"` en `sjp.glm()` podemos apreciar cómo es la relación entre cada variable independiente y la variable dependiente, cuando las demás variables independientes están en "0". Como `age`y `shugart`resultaron no significativas, los efectos se mantienen constantes. Distinto es el caso de `fh`que muestra como entre 4 y 6 están las variaciones más fuertes en cambios de probabilidades predichas.
```{r bin-m3prpr-cero, fig.cap="Probabilidades predichas (con el resto de las variables en "0") del Modelo 3, en base a Mainwaring y Pérez Liñán (2013)"}
sjp.glm(modelo_3,
        type        = "slope",
        show.ci     = TRUE)
```
Con el argumento `type = "pred"` en `sjp.glm()` podemos apreciar cómo es la relación entre cada variable independiente y la variable dependiente, cuando las demás variables independientes están en sus medias (en este caso `fh`con `age`y `shugart`en valores medios).

```{r bin-m3prpr-medias, fig.cap="Probabilidades predichas (con el resto de las variables en sus medias) del Modelo 3, en base a Mainwaring y Pérez Liñán (2013)"}
sjp.glm(modelo_3, 
        type        = "pred",
        show.ci     = TRUE,
        vars        = "fh",
        title       = "Modelo 3 en base Mainwaring y Perez Liñan (2013), pr. predichas con otras variables en sus medias")
```
Y, finalmente, con el argumento `type = "eff"` en `sjp.glm()` podemos calcular efectos marginales de cada variable independiente en relación a la variable dependiente, dejando las demás variables independientes están en sus medias. El efecto marginal es el incremento previsto de la variable dependiente asociada al aumento de una unidad en una de las variables independientes, manteniendo las otras constantes. En la regresión lineal, es solo el parámetro beta. En la regresión logística, depende del valor de la variable independiente.
```{r bin-m3efsmarg, fig.cap="Efectos marginales del Modelo 3, en base a Mainwaring y Pérez Liñán (2013)"}
sjp.glm(modelo_3, 
        type        = "eff",
        show.ci     = TRUE,
        title       = "Modelo 3 en base Mainwaring y Perez Liñan (2013), efectos marginales")
```

##Ajuste de los modelos
Una vez que uno ha analizado la significancia substantiva de los modelos por medio de figuras analizando las probabilidades predichas y los efectos marginales, podemos explorar el ajuste de los modelos.  Así como en MCO podemos usar el $R^2$ y el *Mean Root Square Error* ,existe una serie de estadísticas diseñadas para saber cuál de los modelos logísticos tiene mejor fit. 

stargazer_easy_binary(mp_modelos,
                      type   = "text",
                      report = "vct*",
                      title  = "Modelos 1-3 en base Mainwaring y Perez Liñan (2013)"
                     )

El *wrapper* que hemos creado ya nos provee de varios indicadores de ajuste, que también podemos calcular por separado:


### $Pseudo-R^2$
Para entender como e interpreta el Pseudo-$R^2$ (normalmente se usa el de McFadden) es importante compreender como se diferencia de um $R^2$ por MCO (puedes usar este link para $R^2$ en el cap de [OLS](http://setosa.io/ev/ordinary-least-squares-regression/)). La fórmula, en este caso es 
$Pseudo-R^2= 1-\frac {ln \hat{L}(Modelo completo)}{ln \hat{L}(Modelo sólo con intercepto)}$
Donde $\hat{L}$ es la verosimilitud estimada por el modelo. Básicamente, lo que la fórmula está haciendo es comparar el modelo con todas nuestras covariables al modelo que apenas tiene el intercepto, para ver cuanto mejora la capacidad explicativa del mismo. Como $L$ está entre 0 y 1, su log es menor o igual a 0. Así, cuanto menor la razón, mayor la diferencia entre el modelo elegido y el modelo con apenas el intercepto. 
```{r}
pR2(modelo_1)[["McFadden"]]
pR2(modelo_2)[["McFadden"]]
pR2(modelo_3)[["McFadden"]]

```
También se podría implementar un $Pseudo-R^2$ ajustado, es decir, una versión que penalice por cantidad de covaraibles. Siendo que $c$ es cantidad de covariables, tenemos que 
$Pseudo-R^2= 1-\frac {ln \hat{L}(Modelo completo)-c}{ln \hat{L}(Modelo sólo con intercepto)}$

### AIC

El Akaike Information Criterion (AIC) también usa información de $ln(\hat {L})$ como el $Pseudo-R^2$. El AIC lo que hace es medir la “distancia” que existe entre los verdaderos parámetros y los estimadores del modelo, por medio de la distancia de Kullback-Leibler. Por ello, cuanto menor esta distancia, mejor el modelo. Es muy útil a la hora de comparar diferentes modelos. Se calcula como 
$AIC = 2p-2ln(\hat {L})$
Donde $p$ es la cantidad de regresores incluyendo al intercepto, y $\hat{L}$ es la verosimilitud estimada por el modelo.

```{r}
AIC(modelo_1)
AIC(modelo_2)
AIC(modelo_3)
```

### BIC
BIC (Bayesian information criterion) al igual que AIC es un criterio de comparación de modelos según su ajuste. A los fines prácticos, y para no entrar en las diferencias entre AIC y BIC, es importante saber que BIC penaliza de manera más rigurosa que AIC la complejidad del modelo, siendo que su fórmula es
$BIC=ln(n)p-2ln(\hat {L})$
donde agrega a la formula $n$ que es el número de observaciones en la muestra.
```{r}
BIC(modelo_1)
BIC(modelo_2)
BIC(modelo_3)
```

### Brier Score 
Ésta es otra medida de ajuste. Cuanto más próximo el score de Brier a 0, mejor el ajuste del modelo. En general uno no utiliza todas (AIC, BIC, Brier, etc) sino que elige dos o tres que sean de su agrado. El Brier se utiliza poco en ciencia política, pero es bastante común en epidemiología. Creemos que situaciones en que se quiere “castigar” mucho las predicciones erróneas, ésta es una alternativa ideal ya que su fórmula viene dada por
$B=frac\{1}{N} \sum(\hat{p} - x)^2$
Donde $N$ es el número de observaciones, $\hat{p}$ es la probabilidad predicha para cada observación, y $x$ es el valor real de la observación en nuestra base de datos. El score es el promedio para todas las observaciones de la muestra. ¿Cuál de los tres modelos tiene menor score?

brier_score(modelo_1)
brier_score(modelo_2)
brier_score(modelo_3)


### Porcentaje de predicciones correctas

Para entender el porcentaje de predicciones correctas en un modelo es importante tener en claro que un modelo produce cuatro combinaciones posibles: 

```{r bin-porcpred, echo=FALSE}
knitr::include_graphics("00-images/bin-porcpred.png")
```
Toda observación será clasificada como "correcta" si corresponde a la casilla superior izquierda (verdadero positivo) o a la inferior derecha (verdadero negativo). El porcentaje de observaciones que pertenecen a estas dos casillas determina el porcentaje de predicciones correctas en el modelo. Como criterio estándar, si la probabilidad estimada para una observación es mayor o igual a 50% se estima que es una probabilidad positiva, y si es menor a 50% será una probabilidad negativa.

corr_pred_binary(modelo_1, type = "prop")
corr_pred_binary(modelo_2, type = "prop")
corr_pred_binary(modelo_3, type = "prop")


### ROC plot
Las curvas de ROC tienen la ventaja de no definir un límite arbitrario a partir del cual se decide si la observación ha sido correcta o incorrectamente clasificada. Su desventaja es que es una figura extra que deberemos incluir en nuestro artículo (¿quizás pensemos en un apéndice?). Para interpretar estas figuras, lo que nos interesa es el área debajo de la curva. A mayor el área bajo la curva, mejor el ajuste del modelo. Si quieren leer más al respecto, el área conforma un score que se denomina AUC score (que viene de "Area Under the Curve"). Vamos a construirlo con la función `roc.plot()` del paquete `verification`

```{r bin-rocplot, fig.cap="Roc Plot de nuestros modelos."}
roc.plot(x    = modelo_1$y, # tienen el mismo x!
         pred = cbind(predict.glm(modelo_1, type="response"),
                      predict.glm(modelo_2, type="response"),
                      predict.glm(modelo_3, type="response")),
         threshold = seq(0,1, 0.1),legend = T, show.thres = F,
         xlab="Ratio de falsas alarmas", 
         ylab="Ratio de aciertos",
         leg.text = c("Modelo 1","Modelo 2", "Modelo 3"), 
         main="ROC plot - Modelos 1-3 en base a Mainwaring y Perez Liñan (2013)")
```

En el eje vertical tenemos la *sensibilidad* del modelo mientras que en el eje horizontal tenemos (1-*especificidad*) del modelo. La sensibilidad es la razón entre los verdaderos positivos (o sea, aquellas observaciones predichas como "1", que realmente eran "1" en la base de datos), y la suma de los verdaderos postivos más los falsos negativos (aquellos preichos como "0" que en verdad eran "1"). La especificidad es la razón entre los verdaderos negativos (aquellas observaciones predichas como "0" que eran "0" en la base de datos) y la suma de los falsos positivos (aquellas observaciones predichas como "1" que en verdad eran "0") sumado a los verdaderos negativos. 


### Separation plots

Nótese cómo ocupamos el argumento `type = "bands"`, en tanto nuestro n es muy alto.

```{r bin-sepplots, fig.cap="Separation plots de nuestros modelos."}
separationplot(pred    = predict.glm(modelo_1, type = "response"),
               actual  = as.vector(modelo_1$y),
               type    = "bands",
               newplot = F, 
               heading = "Separation plot - Modelo 1 en base a Mainwaring y Perez Liñan (2013)")

separationplot(pred    = predict.glm(modelo_3, type = "response"),
               actual  = as.vector(modelo_3$y),
               type    = "bands",
               newplot = F, 
               heading = "Separation plot - Modelo 3 en base a a Mainwaring y Perez Liñan (2013)")
```


<!--chapter:end:09-bin.Rmd-->

```{r include=FALSE, cache=FALSE}
# Run before each chapter:

rm(list = ls())
set.seed(789)

options(digits = 2)
options(max.print = 8)

knitr::opts_chunk$set(
  collapse = TRUE,
  fig.align = 'center',
  fig.show = "hold",
  fig.width = 5,
  cache = TRUE
)

```
# Modelos de supervivencia {#surv}

Hay una serie de preguntas recurrentes al análisis de datos políticos que aún no hemos cubierto. Muchas veces nos interesa saber por qué ciertos eventos duran lo que duran, o porqué algunas observaciones duran más que otras. ¿Por qué la paz es tan duradera entre algunos países mientras que otros guerrean con frecuencia? ¿Cuál es la probabilidad de que Turquía ingrese a la ingresar a la Unión Europea en 2018? ¿Por qué algunos legisladores permanecen en sus cargos por varios periodos consecutivos mientras que otros no logran reelegirse tan solo una vez? ¿Cuánto demora un sindicato en entrar en huelga durante una crisis económica? 



Todas estas preguntas tienen en común que la duración y el momento de ocurrencia de un evento son parte de la respuesta que buscamos. Necesitamos un modelo que nos permita llegar a esta respuesta. Janet Box-Steffensmeier, la principal referencia en Ciencia Política de este método, se refiere a ellos a “modelos de eventos históricos” aunque buena parte de la literatura los llama modelos de supervivencia o modelos de duración. Si bien en la Ciencia Política no son modelos tan utilizados como uno creería (en el fondo, casi todas las preguntas que nos hacemos pueden ser reformuladas en una pregunta sobre la duración del evento), las ciencias médicas han explorado estos métodos en profundidad, y muchas las referencias que uno encuentra en R sobre paquetes accesorios a estos modelos son de departamentos bioestadísticos y médicos. De allí que “modelos de supervivencia” sea el nombre más frecuentemente utilizado para estos modelos, ya que en medicina comenzó a utilizárselos para modelar qué variables afectaban la sobrevida de sus pacientes enfermos.  



Podemos tener dos tipos de bases de datos para estos problemas. Por un lado podemos tener una base en formato de panel en el que para un momento dado  nuestra variable dependiente codifica si el evento ha ocurrido (=1) o no (=0). Así, por ejemplo, podemos tener una muestra de veinte países para cincuenta años (1965-2015) en los que nuestra variable de interés es si el país ha implementado una reforma constitucional. La variable independiente asumirá el valor 1 para el año 1994 en Argentina, pero será 0 para el resto de los años en este país. Por otro lado, podemos tener una base de datos transversal en la que cada observación aparece codificada apenas una vez. En este caso necesitamos, además de la variable que nos dirá si en el periodo de interés el evento ocurrió o no para cada observación (por ejemplo, Argentina debería ser codificada como “1”), una variable extra que codifique el tiempo de “supervivencia” de cada observación, es decir, cuánto tiempo pasó hasta que finalmente el evento sucedió. Para el caso de Argentina, esta variable codificará 29 (años), que es lo que demoró en implementarse una reforma constitucional desde 1965. La elección del año de partida, como podrá sospechar, es decisión del investigador, pero tiene un efecto enorme sobre nuestros resultados. 



Supongamos que nos hacemos la pregunta que se hizo David Altman: “¿Por qué algunos países demoran menos que otros en implementar instancias de democracia directa?”. Para ello tenemos una base de datos en formato de panel que parte del año 1900 y que llega a 2016 para 202 países (algunas observaciones, como la Unión Soviética se transforman en otras observaciones a partir de un determinado año en que dejan de existir). Al observar sus datos uno nota algo que probablemente también te suceda en tu base de datos. Para el año 2016 apenas un pequeño porcentaje de países había implementado este tipo de mecanismos (27% para ser más precisos) pero la base está censurada ya que a partir de ese año no sabemos que ha ocurrido con los países que aún no han implementado mecanismos de democracia directa. No todas las observaciones han “muerto” aún, ¿cómo saber cuándo lo harán? Ésta es una pregunta válida, que podremos responder con este tipo de modelos, ya que podemos calcular el tiempo que demorará cada uno de los países censurados en nuestra muestra (con la información que le damos al modelo, que siempre es incompleta). 



En nuestra base de datos tendremos, al menos, tres tipos de observaciones (ver figura x): (a) aquellas que, para el momento en que tenemos datos ya estaban en la muestra, aunque no siempre sabremos hace cuanto que “existen” (en la base de datos de Altman, por ejemplo, México ya existía como entidad política en 1900, cuando su base de datos parte. Sabemos que la Primera República Federal existió como entidad política desde octubre de 1824, por lo que México sería codificado como existente a partir de esa fecha). Lo que sí sabemos es que en 2012, por primera vez, México implementó una iniciativa de democracia directa, lo que define como positiva la ocurrencia del evento que nos interesa medir; (b) Algunas observaciones estarán desde el comienzo de la muestra, y existirán hasta el último momento sin haber registrado el evento de interés. Tal es el caso, en la muestra de Altman, de Argentina que ya en 1900 está registrado en la base, y  hasta el último año de la muestra no había registrado instancias de democracia directa, lo que la transforma en una observación censurada; (c) Algunas observaciones pueden entrar “tarde” en la muestra. Por ejemplo, Eslovenia entra a la muestra de Altman en 1991, que es cuando se independiza de Yugoslavia. 



Figura x. (*) hay que hacer un equivalente propio a esta figura.. en el que el país 1 entre en t2 a la muestra y el país 4 en t5

(insertar fórmulas a continuación)

Los modelos de supervivencia se interpretan a partir de la probabilidad de que en un momento dado el evento de interés ocurra dado que no ha ocurrido aun. Esta probabilidad recibe el nombre de tasa de riesgo. Partimos sabiendo que tenemos una variable, que llamaremos $T$, y que representa un valor aleatorio positivo y que tiene una distribución de probabilidades (correspondiente a la probabilidad del evento ocurrir en cada uno de los momentos posibles), que llamaremos $f(t)$, y que se puede expresar de manera acumulada, como una densidad acumulada $F(t)$. Como dijimos que $T$ es una variable aleatoria, podemos calcular su distribución que viene dada por la fórmula, en la que vemos que $F(t)$ viene dada por la probabilidad de que el tiempo de supervivencia $T$ sea menor o igual a un tiempo específico $t$.

$F(t)=\int\limits_0^t f(u)d(u)=Pr(T)\leq t)$

La función de supervivencia $\hat S(t)$, que es un concepto clave en estos modelos, está relacionada a $F(t)$, ya que 

$\hat S(t)= 1-F(t)=Pr(T\geq t)$


Es decir, la función de supervivencia es la probabilidad inversa de $F(t)$, pues dice respecto a la probabilidad de que el tiempo de supervivencia $T$ sea mayor o igual un tiempo $t$ de interés. Para el ejemplo concreto de Altman, uno podría preguntarse cuál es la probabilidad de un país no implementar un mecanismo de democracia directa (lo que sería equivalente a “sobrevivir” a dicha implementación) siendo que ya ha sobrevivido a los mismos por 30 años. A medida que más y más países en la muestra van implementando iniciativas de democracia directa, la probabilidad de supervivencia va disminuyendo. 



Los coeficientes de los modelos de supervivencia se suelen interpretar como tasas de riesgo (o “hazard rates” en inglés), que es el cociente de la probabilidad de que el evento suceda y la función de supervivencia

$h(t)=\frac{f(t)}{S(t)}$

Así, la tasa de riesgo indica la tasa a la que las observaciones “mueren” en nuestra muestra en el momento $t$, considerando que la observación ha sobrevivido hasta el momento $t$. Veremos más adelante como en el ejemplo de Altman podemos interpretar los coeficientes de nuestras regresiones como tasas de riesgo. En definitiva, la tasa de riesgo $h(t)$ es el riesgo de que el evento ocurra en un intervalo de tiempo determinado, que viene dado por 

$f(t)=\lim_{\bigtriangleup x \to 0} \frac {P(t+\bigtriangleup t > T \geq t)}{\bigtriangleup t}$

## El modelo Cox de riesgos proporcionales:

Hay dos tipos de modelos de supervivencia, los llamados modelos paramétricos y los llamados semi-parametricos. Los primeros son aquellos que hacen supuestos sobre las características de la población a la que la muestra pertenece. En este caso, los supuestos son sobre el “baseline hazard”, es decir, sobre el riesgo de que el evento ocurra cuando todas nuestras variables independientes son iguales a cero. El tipo de modelo de surpervivencia más común para esta categoría es el modelo de Weibull. Por otro lado, los modelos semi-parametricos no hacen ningún tipo de asunciones sobre la función de base, ya que ésta es estimada a partir de los datos. El ejemplo más famoso de ésta especificación es la del modelo de Cox. 



El Oxford Handbook sobre metodología política dedica un capítulo entero a discutir modelos de supervivencia, y en él se toma una posición fuerte en favor de los modelos semi-parametricos. Por un lado, como no se hacen presupustos sobre la función del riesgo de base, su estimación es mucho más precisa. En una estimación paramétrica, elegir un “baseline hazard” equivocado siginificará que todo nuestro trabajo analítico estará sesgado. La decisión de la forma que adopta la curva de base en un modelo de Weibull debería estar orientado por razones teóricas de cuál es el efecto de nuestra variable independiente sobre la probabilidad de supervivencia de la observación (ver figura x). Sin embargo, no siempre hay tales presupuestos. Elegir una especificación por Cox nos ahorra de tomar una decisión tan costosa.


Figura x. diferentes riesgos de base en el modelo de Weibull (dibujar manualmente)


Una segunda ventaja de los modelos semi-parametricos sobre los paramétricos tiene que ver con el presupuesto de riesgos proporcionales. Ambos, modelos paramétricos y semi-parametricos asumen que los riesgos entre dos individuos cualquiera de la muestra se mantienen constantes a lo largo de todo su periodo de supervivencia. Es decir, se asume que la curva de riesgo de cada individuo sigue la misma curva en el tiempo.  Ésta es una asunción cara para trabajos en ciencia política, en los que las observaciones cambian en el tiempo y se diferencian unas de otras. Piénsense en el trabajo de Altman, por ejemplo. Uno puede teorizar que la probabilidad de una iniciativa de democracia directa suceder en el tiempo  estará afectada por el nivel de solidez de sus instituciones democráticas, que podemos medir con algún tipo de variable estándar como los 21 puntos de Polity IV o la más reciente medición de V-Dem. Podemos, entonces, esperar que, a mayor solidez institucional, mayor probabilidad de implementar mecanismos de democracia directa. Sin embargo los valores de estas variables no solo difieren ente países, sino que a lo largo del tiempo estas variables cambian mucho para un mismo país. Piénsese en Colombia, por ejemplo, que en la variable de V-Dem “v2x_polyarchy” sufrió avances y retrocesos entre 1900 y 2016. Cada vez que el valor de esta variable cambia, necesariamente cambia la tasa de riesgo de democracia directa para Colombia, rompiendo el presupuesto de proporcionalidad de los riesgos (ver figura x).



Figura x. evolución en el tiempo de la variable de V-Dem “v2x_polyarchy” para Colombia (hacer en R)


 
La ventaja del modelo de Cox sobre sus contrapartes paramétricas es que existen tests para saber si alguna variable de nuestro modelo rompe el presupuesto de proporcionalidad de los riesgos, y de esa forma podremos corregirlo generando interacciones entre estas variables y variables temporales. De esta forma, permitimos que en nuestro modelo haya dos tipos de coeficientes: coeficientes constantes en el tiempo, y coeficientes cambiantes en el tiempo. Por ejemplo, podemos imaginar que ante un aumento brusco en la calidad de las instituciones democráticas de un país la tasa de riesgo de implementar democracia directa se dispare, pero que dicho efecto de desvanezca en el lapso de cuatro o cinco años. 
La recomendación dada por el Oxford Handbook para una buena implementación de modelos de supervivencia es la siguiente. Primero, dada las ventajas de los modelos semi-paramétricos sobre los paramétricos, se recomienda el uso de Cox sobre Weibull u otro modelo paramétrico. Una vez que hemos definido nuestra variable dependiente (el evento), el tiempo de “nacimiento” y de “muerte” de cada observación, podemos especificar nuestro modelo. Los coeficientes, se recomienda, deben ser interpretados en tasas de riesgo (hazard rates), lo que exige exponenciar los coeficientes brutos.  Una vez que tenemos el modelo que creemos correcto, en función de nuestras intuiciones teóricas, es necesario testear que ninguno de los coeficientes viole el presupuesto de proporcionalidad de los riesgos. Para ello ejecutamos un test de Grambsch y Therneau, o mediante el análisis de los residuos de Schoenfeld. Una vez identificados los coeficientes problemáticos, permitimos que estos interactúen con el logaritmo natural de la variable que mide la duración del evento. De esta forma, permitimos que haya coeficientes cuyo efecto se desvanece o se potencia con el tiempo. Una vez corregidos los coeficientes problemáticos, podemos si, proceder a interpretar nuestro modelo y la función de supervivencia del modelo. 

Código en R

<!--chapter:end:12-surv.Rmd-->

```{r include=FALSE, cache=FALSE}
# Run before each chapter:

rm(list = ls())
set.seed(789)

options(digits = 2)
options(max.print = 8)

knitr::opts_chunk$set(
  collapse = TRUE,
  fig.align = 'center',
  fig.show = "hold",
  fig.width = 5,
  cache = TRUE
)

```
# Créditos {-}

| Recurso | Crédito |
| :-----: | :------ |
| <img src="00-images/icon_book.png" width="60"></img> |  <a href="https://thenounproject.com/search/?q=book&i=1335286#">Book by UNiCORN from the Noun Project</a>

<!--chapter:end:99-créditos.Rmd-->

